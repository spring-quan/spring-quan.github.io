<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml">
<meta name="keywords" content="ACL2019,dialog system">
<meta property="og:type" content="article">
<meta property="og:title" content="ACL2019-对话系统">
<meta property="og:url" content="http://yoursite.com/2019/07/07/ACL2019-对话系统/index.html">
<meta property="og:site_name" content="spring&#39;s Blog">
<meta property="og:description" content="记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/07/07/ACL2019-对话系统/gallery/42.png">
<meta property="og:updated_time" content="2019-07-20T08:03:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ACL2019-对话系统">
<meta name="twitter:description" content="记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml">
<meta name="twitter:image" content="http://yoursite.com/2019/07/07/ACL2019-对话系统/gallery/42.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>ACL2019-对话系统</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/07/10/NAACL2019-对话系统/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/04/24/自然语言处理-会议列表/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/07/07/ACL2019-对话系统/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&text=ACL2019-对话系统"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&is_video=false&description=ACL2019-对话系统"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=ACL2019-对话系统&body=Check out this article: http://yoursite.com/2019/07/07/ACL2019-对话系统/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&name=ACL2019-对话系统&description=&lt;p&gt;记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;ACL2019的会议列表链接：&lt;a href=&#34;http://www.acl2019.org/EN/program.xhtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.acl2019.org/EN/program.xhtml&lt;/a&gt;&lt;/p&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》"><span class="toc-number">1.</span> <span class="toc-text">《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#《Dialogue-Natural-Language-Inference》"><span class="toc-number">2.</span> <span class="toc-text">《Dialogue Natural Language Inference》</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》"><span class="toc-number">3.</span> <span class="toc-text">《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        ACL2019-对话系统
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">spring's Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-07-07T11:19:27.000Z" itemprop="datePublished">2019-07-07</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/论文/">论文</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/ACL2019/">ACL2019</a>, <a class="tag-link" href="/tags/dialog-system/">dialog system</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。<br>ACL2019的会议列表链接：<a href="http://www.acl2019.org/EN/program.xhtml" target="_blank" rel="noopener">http://www.acl2019.org/EN/program.xhtml</a></p>
<a id="more"></a>
<h3 id="《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》"><a href="#《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》" class="headerlink" title="《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》"></a>《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.01788" target="_blank" rel="noopener">https://arxiv.org/abs/1906.01788</a><br>【源码】：无</p>
<p>中科院自动化所发表的短论文。在多轮对话中，对话历史（context information）对回复（response）的生成有重要作用。任务型对话中的管道模型分为4个模块：NLU、对话状态追踪、对话策略学习 及NLG。对话状态追踪又包含任务：domain classification、intent detection和slot filling。domain classification和intent detection任务当做分类任务来处理，常采用SVM或深度神经网络的方法；slot filling任务被当做序列标注任务来处理，常采用BiLSTM+CRF模型。NLU能否充分利用context information，对这三个下游任务有很大影响。<br>为了更好的利用context information，本文提出了对话逻辑推断任务（DLI,dialog logic inference），任务定义为：将打乱顺序的多轮对话重新排序；输入之前的对话，从剩余的utterance candidates中选中下一句对话。NLU任务采用了所谓的memory network，其实就是采用多个encoder对context information进行编码，再用attention机制或别的方法得到context information总的向量化表示。本文联合训练DLI任务和NLU任务，通过两个任务共享encoder和memory retrieve模块，来让NLU任务更好地利用context information。其实是得到context information更合理的向量化表示，来作为下游domain classification、intent detection和slot filling任务的输入。</p>
<p>论文提出的将打乱顺序的对话重新排序的DLI任务，可以进一步深入，将句子切分为几段打乱顺序再重新排序；可以应用到闲聊式对话系统中。</p>
<h3 id="《Dialogue-Natural-Language-Inference》"><a href="#《Dialogue-Natural-Language-Inference》" class="headerlink" title="《Dialogue Natural Language Inference》"></a>《Dialogue Natural Language Inference》</h3><p>【链接】：<a href="https://arxiv.org/abs/1811.00671" target="_blank" rel="noopener">https://arxiv.org/abs/1811.00671</a><br>【代码】：无<br>【数据集】：<a href="https://wellecks.github.io/dialogue_nli/" target="_blank" rel="noopener">https://wellecks.github.io/dialogue_nli/</a></p>
<p>加利福尼亚大学、Facebook AI Lab发表的论文。核心是提出用NLI(natural language inference)任务来提高persona-based dialog system的一致性。这里就要先搞清楚NLI任务和一致性问题两个概念。</p>
<ul>
<li><p>先从问题出发，所谓对话的一致性问题。可以分为两类：</p>
<ul>
<li><p>logical contradiction，逻辑矛盾。比如同一个人的两句话:”我有一只狗”，”我没养过狗”。就是逻辑矛盾的。</p>
</li>
<li><p>比较模糊的非逻辑矛盾。同一个人不可能说出的两句话：“我从来不运动”，“我去篮球了”。就是这种非逻辑矛盾。真香警告。</p>
<p>至于persona一致性问题，就是回复的utterance不能与说话人的persona矛盾，也不能与之前的回复有矛盾。</p>
</li>
</ul>
</li>
<li><p>具体介绍NLI任务。这其实是一个分类问题。论文公开了一个自己标注的NLI数据集。</p>
<ul>
<li>训练阶段：训练集形式是 {$（s_1,s_2）$,label }，对应labels $\in$（一致、无关、矛盾）。</li>
<li>在test阶段，给定一个句子对（句子1，句子2）来判断对应的label。</li>
</ul>
</li>
</ul>
<p>论文的最终目的是通过NLI任务训练的模型来提高persona dialog system的一致性。这是如何来实现的呢？对于一个dialog system，给定对话历史$（u_1,u_2,…,u_t）$ 及说话人的persona文本描述$（p_1,p_2,…,p_n）$,从response candidates$（y_1,y_2,…,y_m）$中选择一个$u_{t+1}$（如何生成多个responses不是这篇论文要解决的）。<br>用NLI任务的模型来预测$(y_i,u_j),(y_i,p_k)其中：i\in [1,m],j\in [1,t],k \in [1,n]$对应的label，如果句子之间是矛盾的，则添加惩罚项。从而得到一致性最好的utterance作为response。</p>
<h3 id="《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》"><a href="#《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》" class="headerlink" title="《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》"></a>《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》</h3><p>【链接】：<a href="https://arxiv.org/abs/1907.05339" target="_blank" rel="noopener">https://arxiv.org/abs/1907.05339</a><br>【数据集】：<a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" target="_blank" rel="noopener">English Ubuntu dialogue corpus</a><br>【代码】：<a href="https://github.com/zhanghainan/ReCoSa" target="_blank" rel="noopener">https://github.com/zhanghainan/ReCoSa</a></p>
<p>中科院发表的论文。<br>在多轮对话中，生成response时，对话历史中最相关的部分起着重要的作用。论文要解决的问题：如何更准确地找到并利用relevant context来生成response。<br>多轮对话中广泛使用的HRED模型,[<a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="noopener">(Serban et al.,2016;</a>,<a href="https://arxiv.org/abs/1507.02221" target="_blank" rel="noopener">Sordoni et al., 2015</a>]无差别地利用context information，忽略了relevant context。虽然有利用relevant context的相关工作，但这些工作都有各自的问题。[<a href="https://www.aclweb.org/anthology/P17-2036" target="_blank" rel="noopener">Tian et al., 2017</a>]提出计算context 与post之间的cosine similarity来衡量context relevance，其假设是context与response之间的relevance等价于post与response之间的relevance，这个假设是站不住脚的。[<a href>Xing et al., 2018</a>]向HRED模型引入了attention机制，但attention机制定位relevant context时会产生偏差，因为基于RNN的attention机制倾向于最靠近的context（close context）。论文提出了自己的解决办法，用self-attention机制来衡量context于response之间的relevance。self-attention机制的优点是可以有效捕捉到长距离的依赖关系。</p>
<p>模型分为三个部分：<br>context包含N轮对话： ${s_1,s_2,…,s_N}$其中，$s_i = {x_1,x_2,…,x_M}$，M为句子长度。<br>response为$Y = {y_1,y_2,…,y_M}$</p>
<ol>
<li><strong>context representation encoder</strong>：<br> 将context encode为vector。<ol>
<li>word-level encoder：<br> 用LSTM对sentence编码，将LSTM最后一个时间步的hidden state作为sentence representation: $h^{s_i}$；<br> 由于self-attention机制不能区分word位置信息，还需要添加position embedding: $p^{s_i}$,<br> 把两个向量做concatenate操作，得到总得sentence representation:$(h^{s_i},p^{s_i})$。<br> 对于context中的N个句子有${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$</li>
<li>context self-attention:<br> 采用multi-head self-attention机制，将${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$经过不同的线性变换作为query、keys、values matrix,由N个sentence representation得到总的context representation $O_s$。</li>
</ol>
</li>
<li><strong>response representation encoder</strong><br> 同样用multi-head self-attention机制,将response的word embedding及position embedding ${(w_1,p_1),…,(w_{t-1},p_{t-1})}$经过不同的线性变换作为query、keys、values matrix，得到response representation $O_r$。<ol>
<li>在train阶段<br> 采用mask操作，在时间步t对于word $y_t$，mask掉${y_t,y_{t+1},…,y_M}$，只保留${y_1,y_2,…,y_{t-1}}$来计算response representation。</li>
<li>在infer阶段<br> 在生成response的时间步t，将生成的response ${g_1,…,g_{t-1}}$，来作为response representation。</li>
</ol>
</li>
<li><strong>context-response attention decoder</strong><br> 采用multi-head self-attention机制，将context attention representation $O_s$作为keys、values matrix，将response hidden representation $O_r$作为query matrix。得到输出$O_d$. <img src="./gallery/42.png" alt="模型框架图"></li>
</ol>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》"><span class="toc-number">1.</span> <span class="toc-text">《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#《Dialogue-Natural-Language-Inference》"><span class="toc-number">2.</span> <span class="toc-text">《Dialogue Natural Language Inference》</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》"><span class="toc-number">3.</span> <span class="toc-text">《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/07/07/ACL2019-对话系统/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&text=ACL2019-对话系统"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&is_video=false&description=ACL2019-对话系统"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=ACL2019-对话系统&body=Check out this article: http://yoursite.com/2019/07/07/ACL2019-对话系统/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&title=ACL2019-对话系统"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/07/07/ACL2019-对话系统/&name=ACL2019-对话系统&description=&lt;p&gt;记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;ACL2019的会议列表链接：&lt;a href=&#34;http://www.acl2019.org/EN/program.xhtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.acl2019.org/EN/program.xhtml&lt;/a&gt;&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 spring
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


</body>
</html>
