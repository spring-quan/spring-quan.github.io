<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>pytorch实现Word embedding |</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="spring"><meta name="designer" content="minfive"><meta name="keywords" content="null"><meta name="description" content="游龙当归海，海不迎我自来也。"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2019/03/20/pytorch实现Word-embedding/index.html"><link rel="icon" type="image/png" href="undefined" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="spring's Blog"><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(undefined)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="spring's Blog" alt="spring's Blog"><img src="undefined" alt="spring's Blog"></a><nav class="page__nav"><ul class="nav__list clearfix"></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="undefined" alt="pytorch实现Word embedding"></div><header class="post__info"><h1 class="post__title">pytorch实现Word embedding</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/">undefined</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-03-20</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/pytorch/">Pytorch</a></li><li class="mark__item"><a href="/tags/word-embedding/">Word Embedding</a></li></ul></div></div></header><div class="post__content"><p>word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p><a id="more"></a><h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。<br>这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。<br>举个例子，我们想要构建一个语言模型。有以下三个句子</p><ul><li>数学家待在实验室里。</li><li>物理学家待在实验室里。</li><li>数学家解决了一个难题。</li></ul><p>我们又看到一个新的句子：</p><ul><li>物理学家解决了一个难题。</li></ul><p>我们希望语言模型可以学习到以下特点：</p><ul><li><code>数学家</code>和<code>物理学家</code>在一个句子中同样的位置出现。这两个词之间有某种语义上的联系</li><li><code>数学家</code>曾经出现在我们看到的这个新句子中<code>物理学家</code>出现的位置。</li></ul><p>这就是<strong>语义相似性</strong>想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为<strong>distributional hypothesis</strong><br>值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。</p><h2 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h2><p>我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码<code>物理学家</code>和<code>数学家</code>。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$<br>我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \frac{q_物\cdot q_数}{|q_物| \cdot |q_数|}=cos(\phi) 其中\phi是两个向量之间的夹角。$$<br>但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。<br>神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。<br>总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p><h2 id="PyTorch实现word-embedding"><a href="#PyTorch实现word-embedding" class="headerlink" title="PyTorch实现word embedding"></a>PyTorch实现word embedding</h2><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># 词汇表字典</span><br><span class="line">word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;</span><br><span class="line">vocab_size = len(word_to_ix) </span><br><span class="line">embedding_dim = 15</span><br><span class="line">word_embeddings = nn.Embedding(vocab_size,embedding_dim)</span><br></pre></td></tr></table></figure><p><code>nn.Embedding()</code>随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。<br>接下来我们查询”dog”这个词的向量表示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。</span><br><span class="line">dog_idx = Variable(dog_idx)</span><br><span class="line">dog_embed = word_embeddings(dog_idx) #注意不是索引</span><br><span class="line">print(dog_embed)</span><br></pre></td></tr></table></figure><p>上述代码中，要访问<code>dog</code>的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。<br>接下来，我们查询一句话的向量表示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sent = &apos;The dog ate the apple&apos;.split()</span><br><span class="line">sent_idxs = [word_to_ix[w] for w in sent]</span><br><span class="line">sent_idxs = torch.LongTensor(sent_idxs)</span><br><span class="line">sent_idxs = Variable(sent_idxs)</span><br><span class="line">sent_embeds = embeds(sent_idxs) </span><br><span class="line">print(sent_embeds)</span><br></pre></td></tr></table></figure><h2 id="pytorch加载预训练词向量"><a href="#pytorch加载预训练词向量" class="headerlink" title="pytorch加载预训练词向量"></a>pytorch加载预训练词向量</h2><p>之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。<br>方式一：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">pretrain_embedding  = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径</span><br><span class="line">word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中</span><br></pre></td></tr></table></figure><p>方式二：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding))</span><br></pre></td></tr></table></figure><h3 id="涉及函数详解"><a href="#涉及函数详解" class="headerlink" title="涉及函数详解"></a>涉及函数详解</h3><h4 id="numpy-与from-numpy"><a href="#numpy-与from-numpy" class="headerlink" title="numpy()与from_numpy()"></a>numpy()与from_numpy()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(ndarray) $\to$ tensor</span><br></pre></td></tr></table></figure><ul><li>作用：numpy桥，将<code>numpy.ndarray</code>转换为pytorch的<code>tensor</code>.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor.numpy()</span><br></pre></td></tr></table></figure><ul><li>作用：numpy桥，将pytorch的<code>tensor</code>转换为<code>numpy.ndarray</code>.二者共享同一内存空间，修改一个另一个也会被修改。</li></ul><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">c = b.numpy()</span><br></pre></td></tr></table></figure><h4 id="tensor-copy-src"><a href="#tensor-copy-src" class="headerlink" title="tensor.copy_(src)"></a>tensor.copy_(src)</h4><ul><li>作用：将<code>src</code>中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。</li></ul><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(1,5)</span><br><span class="line">b = torch.randn(1,5)</span><br><span class="line">b.copy_(a)</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html?highlight=embed" target="_blank" rel="noopener">Word Embeddings: Encoding Lexical Semantics</a></li><li><a href="https://ptorch.com/news/11.html" target="_blank" rel="noopener">PyTorch快速入门教程七（RNN做自然语言处理）</a></li></ul><div class="post-announce">Thank you for reading, this article belongs to <a href="http://yoursite.com">spring's Blog</a> copyright, if reproduced, please indicate the source：spring's Blog（<a href="http://yoursite.com/2019/03/20/pytorch实现Word-embedding/">http://yoursite.com/2019/03/20/pytorch实现Word-embedding/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2019/03/14/去新藏线上骑车/" title="去新藏线上骑车"><i class="iconfont icon-prev"></i>去新藏线上骑车</a></div><div class="post__prev post__prev--right"><a href="/2019/03/20/pytorch实现基于LSTM的循环神经网络/" title="pytorch实现基于LSTM的循环神经网络">pytorch实现基于LSTM的循环神经网络<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">Introduction</h3><p class="block__text">游龙当归海，海不迎我自来也。</p></div><div class="sidebar__block"><h3 class="block__title">Categories</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/论文/">论文</a><span class="block-list-count">2</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/生活记录/">生活记录</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/机器学习/">机器学习</a><span class="block-list-count">2</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/技术资料/">技术资料</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/年度总结/">年度总结</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/学习笔记/">学习笔记</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/web搜索/">web搜索</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/pytorch/">pytorch</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/python/">python</a><span class="block-list-count">2</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">Latest Post</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2019/07/12/pip安装python模块报错/" title="pip安装python模块报错"><div class="item__cover"><img src="undefined" alt="pip安装python模块报错"></div><div class="item__info"><h3 class="item__title">pip安装python模块报错</h3><span class="item__text">2019-07-12</span></div></a></li><li class="latest-post-item"><a href="/2019/07/10/NAACL2019-对话系统/" title="NAACL2019-对话系统"><div class="item__cover"><img src="undefined" alt="NAACL2019-对话系统"></div><div class="item__info"><h3 class="item__title">NAACL2019-对话系统</h3><span class="item__text">2019-07-10</span></div></a></li><li class="latest-post-item"><a href="/2019/07/07/ACL2019-对话系统/" title="ACL2019-对话系统"><div class="item__cover"><img src="undefined" alt="ACL2019-对话系统"></div><div class="item__info"><h3 class="item__title">ACL2019-对话系统</h3><span class="item__text">2019-07-07</span></div></a></li><li class="latest-post-item"><a href="/2019/04/24/自然语言处理-会议列表/" title="自然语言处理---会议列表"><div class="item__cover"><img src="undefined" alt="自然语言处理---会议列表"></div><div class="item__info"><h3 class="item__title">自然语言处理---会议列表</h3><span class="item__text">2019-04-24</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">Tags</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/ACL2019/">ACL2019</a></li><li class="tag-item"><a class="tag-link" href="/tags/Elasticsearch/">Elasticsearch</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Linux/">Linux</a></li><li class="tag-item"><a class="tag-link" href="/tags/Numpy/">Numpy</a></li><li class="tag-item"><a class="tag-link" href="/tags/csv/">csv</a></li><li class="tag-item"><a class="tag-link" href="/tags/cuda/">cuda</a></li><li class="tag-item"><a class="tag-link" href="/tags/dialog-system/">dialog system</a></li><li class="tag-item"><a class="tag-link" href="/tags/linux/">linux</a></li><li class="tag-item"><a class="tag-link" href="/tags/nvidia-smi/">nvidia-smi</a></li><li class="tag-item"><a class="tag-link" href="/tags/panda/">panda</a></li><li class="tag-item"><a class="tag-link" href="/tags/python/">python</a></li><li class="tag-item"><a class="tag-link" href="/tags/pytorch/">pytorch</a></li><li class="tag-item"><a class="tag-link" href="/tags/sublime/">sublime</a></li><li class="tag-item"><a class="tag-link" href="/tags/ubuntu/">ubuntu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word-embedding/">word embedding</a></li><li class="tag-item"><a class="tag-link" href="/tags/交叉熵/">交叉熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/会议列表/">会议列表</a></li><li class="tag-item"><a class="tag-link" href="/tags/前向后向算法/">前向后向算法</a></li><li class="tag-item"><a class="tag-link" href="/tags/年度总结/">年度总结</a></li><li class="tag-item"><a class="tag-link" href="/tags/循环神经网络/">循环神经网络</a></li><li class="tag-item"><a class="tag-link" href="/tags/搭建博客/">搭建博客</a></li><li class="tag-item"><a class="tag-link" href="/tags/新藏线/">新藏线</a></li><li class="tag-item"><a class="tag-link" href="/tags/服务器/">服务器</a></li><li class="tag-item"><a class="tag-link" href="/tags/条件随机场/">条件随机场</a></li><li class="tag-item"><a class="tag-link" href="/tags/生活记录/">生活记录</a></li><li class="tag-item"><a class="tag-link" href="/tags/相对熵/">相对熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/维特比算法/">维特比算法</a></li><li class="tag-item"><a class="tag-link" href="/tags/骑车/">骑车</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">About</h3><div class="item__content"><p class="item__text"></p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span></span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span></span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>