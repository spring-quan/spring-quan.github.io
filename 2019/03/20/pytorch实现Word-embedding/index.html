<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    <script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>
    <link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">
  
  

  <!-- PACE Progress Bar START -->

  
  <title>pytorch实现word embedding | spring&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="pytorchword embedding">
  
  
  
  
  <meta name="description" content="word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。">
<meta name="keywords" content="pytorch,word embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch实现Word embedding">
<meta property="og:url" content="http://yoursite.com/2019/03/20/pytorch实现Word-embedding/index.html">
<meta property="og:site_name" content="spring&#39;s Blog">
<meta property="og:description" content="word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-07-07T07:11:01.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pytorch实现Word embedding">
<meta name="twitter:description" content="word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。">
  
    <link rel="alternate" href="/atom.xml" title="spring&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/hiero.css">
  <link rel="stylesheet" href="/css/glyphs.css">
  
    <link rel="stylesheet" href="/css/vdonate.css">
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">
  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
</head>
</html>
<script>
var themeMenus = {};

  themeMenus["/"] = "首页"; 

  themeMenus["/archives"] = "归档"; 

  themeMenus["/categories"] = "分类"; 

  themeMenus["/tags"] = "标签"; 

  themeMenus["/about"] = "关于"; 

</script>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" rel="home" >
                <img style="margin-bottom: 10px;"  width="94px" height="120px" alt="Hike News" src=" /css/images/avatar.jpg">
              </a>
            
          </h1>

          
            <div class="site-description">游龙当归海，海不迎我自来也。</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">关于</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/pose.jpg,https://source.unsplash.com/collection/954550/1920x1080".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-pytorch实现Word-embedding" style="width: 66%; float:left;" class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" class="article-title" itemprop="name">
      pytorch实现Word embedding
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/03/20/pytorch实现Word-embedding/" class="article-date">
	  <time datetime="2019-03-20T12:16:21.000Z" itemprop="datePublished">三月 20, 2019</time>
	</a>

      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p>
<a id="more"></a> 
<h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。<br>这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。<br>举个例子，我们想要构建一个语言模型。有以下三个句子</p>
<ul>
<li>数学家待在实验室里。</li>
<li>物理学家待在实验室里。</li>
<li>数学家解决了一个难题。</li>
</ul>
<p>我们又看到一个新的句子：</p>
<ul>
<li>物理学家解决了一个难题。</li>
</ul>
<p>我们希望语言模型可以学习到以下特点：</p>
<ul>
<li><code>数学家</code>和<code>物理学家</code>在一个句子中同样的位置出现。这两个词之间有某种语义上的联系</li>
<li><code>数学家</code>曾经出现在我们看到的这个新句子中<code>物理学家</code>出现的位置。</li>
</ul>
<p>这就是<strong>语义相似性</strong>想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为<strong>distributional hypothesis</strong><br>值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。</p>
<h2 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h2><p>我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码<code>物理学家</code>和<code>数学家</code>。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$<br>我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \frac{q_物\cdot q_数}{|q_物| \cdot |q_数|}=cos(\phi)    其中\phi是两个向量之间的夹角。$$<br>但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。<br>神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。<br>总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p>
<h2 id="PyTorch实现word-embedding"><a href="#PyTorch实现word-embedding" class="headerlink" title="PyTorch实现word embedding"></a>PyTorch实现word embedding</h2><p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># 词汇表字典</span><br><span class="line">word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;</span><br><span class="line">vocab_size = len(word_to_ix) </span><br><span class="line">embedding_dim = 15</span><br><span class="line">word_embeddings = nn.Embedding(vocab_size,embedding_dim)</span><br></pre></td></tr></table></figure>

<p><code>nn.Embedding()</code>随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。<br>接下来我们查询”dog”这个词的向量表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。</span><br><span class="line">dog_idx = Variable(dog_idx)</span><br><span class="line">dog_embed = word_embeddings(dog_idx) #注意不是索引</span><br><span class="line">print(dog_embed)</span><br></pre></td></tr></table></figure>

<p>上述代码中，要访问<code>dog</code>的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。<br>接下来，我们查询一句话的向量表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sent = &apos;The dog ate the apple&apos;.split()</span><br><span class="line">sent_idxs = [word_to_ix[w] for w in sent]</span><br><span class="line">sent_idxs = torch.LongTensor(sent_idxs)</span><br><span class="line">sent_idxs = Variable(sent_idxs)</span><br><span class="line">sent_embeds = embeds(sent_idxs) </span><br><span class="line">print(sent_embeds)</span><br></pre></td></tr></table></figure>

<h2 id="pytorch加载预训练词向量"><a href="#pytorch加载预训练词向量" class="headerlink" title="pytorch加载预训练词向量"></a>pytorch加载预训练词向量</h2><p>之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。<br>方式一：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">pretrain_embedding  = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径</span><br><span class="line">word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中</span><br></pre></td></tr></table></figure>

<p>方式二：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding))</span><br></pre></td></tr></table></figure>

<h3 id="涉及函数详解"><a href="#涉及函数详解" class="headerlink" title="涉及函数详解"></a>涉及函数详解</h3><h4 id="numpy-与from-numpy"><a href="#numpy-与from-numpy" class="headerlink" title="numpy()与from_numpy()"></a>numpy()与from_numpy()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(ndarray) $\to$ tensor</span><br></pre></td></tr></table></figure>

<ul>
<li>作用：numpy桥，将<code>numpy.ndarray</code>转换为pytorch的<code>tensor</code>.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor.numpy()</span><br></pre></td></tr></table></figure>

<ul>
<li>作用：numpy桥，将pytorch的<code>tensor</code>转换为<code>numpy.ndarray</code>.二者共享同一内存空间，修改一个另一个也会被修改。</li>
</ul>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">c = b.numpy()</span><br></pre></td></tr></table></figure>

<h4 id="tensor-copy-src"><a href="#tensor-copy-src" class="headerlink" title="tensor.copy_(src)"></a>tensor.copy_(src)</h4><ul>
<li>作用：将<code>src</code>中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。</li>
</ul>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(1,5)</span><br><span class="line">b = torch.randn(1,5)</span><br><span class="line">b.copy_(a)</span><br></pre></td></tr></table></figure>

<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html?highlight=embed" target="_blank" rel="noopener">Word Embeddings: Encoding Lexical Semantics</a></li>
<li><a href="https://ptorch.com/news/11.html" target="_blank" rel="noopener">PyTorch快速入门教程七（RNN做自然语言处理）</a></li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/pytorch/">pytorch</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/">pytorch</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word-embedding/">word embedding</a></li></ul>

      
        <div id="donation_div"></div>

<script src="/js/vdonate.js"></script>
<script>
var a = new Donate({
  title: '如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作!', // 可选参数，打赏标题
  btnText: '打赏支持', // 可选参数，打赏按钮文字
  el: document.getElementById('donation_div'),
  wechatImage: 'https://i.ibb.co/Tm0Tnd2/14.jpg',
  alipayImage: 'https://i.ibb.co/Tm0Tnd2/14.jpg'
});
</script>
      
            
      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>


      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/03/20/pytorch实现基于LSTM的循环神经网络/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          pytorch实现基于LSTM的循环神经网络
        
      </div>
    </a>
  
  
    <a href="/2019/03/14/去新藏线上骑车/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">去新藏线上骑车</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article" style="overflow-y: scroll; max-width: 28%;">
    <strong class="toc-title">文章目录</strong>
    
      <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#one-hot编码"><span class="nav-number">1.</span> <span class="nav-text">one-hot编码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word-embedding"><span class="nav-number">2.</span> <span class="nav-text">word embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch实现word-embedding"><span class="nav-number">3.</span> <span class="nav-text">PyTorch实现word embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch加载预训练词向量"><span class="nav-number">4.</span> <span class="nav-text">pytorch加载预训练词向量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#涉及函数详解"><span class="nav-number">4.1.</span> <span class="nav-text">涉及函数详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#numpy-与from-numpy"><span class="nav-number">4.1.1.</span> <span class="nav-text">numpy()与from_numpy()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-copy-src"><span class="nav-number">4.1.2.</span> <span class="nav-text">tensor.copy_(src)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考链接"><span class="nav-number">5.</span> <span class="nav-text">参考链接</span></a></li></ol>
    
    </div>
  </aside>
</section>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 spring&#39;s Blog All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>
    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>


<script src="/js/scripts.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="https://dnqof95d40fo6.cloudfront.net/atw7f8.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
