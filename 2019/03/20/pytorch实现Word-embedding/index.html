<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。">
<meta name="keywords" content="pytorch,word embedding">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch实现Word embedding">
<meta property="og:url" content="http://yoursite.com/2019/03/20/pytorch实现Word-embedding/index.html">
<meta property="og:site_name" content="spring&#39;s Blog">
<meta property="og:description" content="word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-07-07T07:11:01.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pytorch实现Word embedding">
<meta name="twitter:description" content="word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>pytorch实现Word embedding</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/03/20/pytorch实现基于LSTM的循环神经网络/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/03/14/去新藏线上骑车/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&text=pytorch实现Word embedding"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&is_video=false&description=pytorch实现Word embedding"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=pytorch实现Word embedding&body=Check out this article: http://yoursite.com/2019/03/20/pytorch实现Word-embedding/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&name=pytorch实现Word embedding&description=&lt;p&gt;word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。&lt;/p&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#one-hot编码"><span class="toc-number">1.</span> <span class="toc-text">one-hot编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#word-embedding"><span class="toc-number">2.</span> <span class="toc-text">word embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch实现word-embedding"><span class="toc-number">3.</span> <span class="toc-text">PyTorch实现word embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch加载预训练词向量"><span class="toc-number">4.</span> <span class="toc-text">pytorch加载预训练词向量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#涉及函数详解"><span class="toc-number">4.1.</span> <span class="toc-text">涉及函数详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#numpy-与from-numpy"><span class="toc-number">4.1.1.</span> <span class="toc-text">numpy()与from_numpy()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensor-copy-src"><span class="toc-number">4.1.2.</span> <span class="toc-text">tensor.copy_(src)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考链接"><span class="toc-number">5.</span> <span class="toc-text">参考链接</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        pytorch实现Word embedding
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">spring's Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-03-20T12:16:21.000Z" itemprop="datePublished">2019-03-20</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/pytorch/">pytorch</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/pytorch/">pytorch</a>, <a class="tag-link" href="/tags/word-embedding/">word embedding</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p>
<a id="more"></a> 
<h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。<br>这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。<br>举个例子，我们想要构建一个语言模型。有以下三个句子</p>
<ul>
<li>数学家待在实验室里。</li>
<li>物理学家待在实验室里。</li>
<li>数学家解决了一个难题。</li>
</ul>
<p>我们又看到一个新的句子：</p>
<ul>
<li>物理学家解决了一个难题。</li>
</ul>
<p>我们希望语言模型可以学习到以下特点：</p>
<ul>
<li><code>数学家</code>和<code>物理学家</code>在一个句子中同样的位置出现。这两个词之间有某种语义上的联系</li>
<li><code>数学家</code>曾经出现在我们看到的这个新句子中<code>物理学家</code>出现的位置。</li>
</ul>
<p>这就是<strong>语义相似性</strong>想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为<strong>distributional hypothesis</strong><br>值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。</p>
<h2 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h2><p>我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码<code>物理学家</code>和<code>数学家</code>。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$<br>我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \frac{q_物\cdot q_数}{|q_物| \cdot |q_数|}=cos(\phi)    其中\phi是两个向量之间的夹角。$$<br>但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。<br>神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。<br>总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p>
<h2 id="PyTorch实现word-embedding"><a href="#PyTorch实现word-embedding" class="headerlink" title="PyTorch实现word embedding"></a>PyTorch实现word embedding</h2><p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># 词汇表字典</span><br><span class="line">word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;</span><br><span class="line">vocab_size = len(word_to_ix) </span><br><span class="line">embedding_dim = 15</span><br><span class="line">word_embeddings = nn.Embedding(vocab_size,embedding_dim)</span><br></pre></td></tr></table></figure>

<p><code>nn.Embedding()</code>随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。<br>接下来我们查询”dog”这个词的向量表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。</span><br><span class="line">dog_idx = Variable(dog_idx)</span><br><span class="line">dog_embed = word_embeddings(dog_idx) #注意不是索引</span><br><span class="line">print(dog_embed)</span><br></pre></td></tr></table></figure>

<p>上述代码中，要访问<code>dog</code>的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。<br>接下来，我们查询一句话的向量表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sent = &apos;The dog ate the apple&apos;.split()</span><br><span class="line">sent_idxs = [word_to_ix[w] for w in sent]</span><br><span class="line">sent_idxs = torch.LongTensor(sent_idxs)</span><br><span class="line">sent_idxs = Variable(sent_idxs)</span><br><span class="line">sent_embeds = embeds(sent_idxs) </span><br><span class="line">print(sent_embeds)</span><br></pre></td></tr></table></figure>

<h2 id="pytorch加载预训练词向量"><a href="#pytorch加载预训练词向量" class="headerlink" title="pytorch加载预训练词向量"></a>pytorch加载预训练词向量</h2><p>之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。<br>方式一：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">pretrain_embedding  = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径</span><br><span class="line">word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中</span><br></pre></td></tr></table></figure>

<p>方式二：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding))</span><br></pre></td></tr></table></figure>

<h3 id="涉及函数详解"><a href="#涉及函数详解" class="headerlink" title="涉及函数详解"></a>涉及函数详解</h3><h4 id="numpy-与from-numpy"><a href="#numpy-与from-numpy" class="headerlink" title="numpy()与from_numpy()"></a>numpy()与from_numpy()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(ndarray) $\to$ tensor</span><br></pre></td></tr></table></figure>

<ul>
<li>作用：numpy桥，将<code>numpy.ndarray</code>转换为pytorch的<code>tensor</code>.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor.numpy()</span><br></pre></td></tr></table></figure>

<ul>
<li>作用：numpy桥，将pytorch的<code>tensor</code>转换为<code>numpy.ndarray</code>.二者共享同一内存空间，修改一个另一个也会被修改。</li>
</ul>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">c = b.numpy()</span><br></pre></td></tr></table></figure>

<h4 id="tensor-copy-src"><a href="#tensor-copy-src" class="headerlink" title="tensor.copy_(src)"></a>tensor.copy_(src)</h4><ul>
<li>作用：将<code>src</code>中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。</li>
</ul>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(1,5)</span><br><span class="line">b = torch.randn(1,5)</span><br><span class="line">b.copy_(a)</span><br></pre></td></tr></table></figure>

<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html?highlight=embed" target="_blank" rel="noopener">Word Embeddings: Encoding Lexical Semantics</a></li>
<li><a href="https://ptorch.com/news/11.html" target="_blank" rel="noopener">PyTorch快速入门教程七（RNN做自然语言处理）</a></li>
</ul>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#one-hot编码"><span class="toc-number">1.</span> <span class="toc-text">one-hot编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#word-embedding"><span class="toc-number">2.</span> <span class="toc-text">word embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch实现word-embedding"><span class="toc-number">3.</span> <span class="toc-text">PyTorch实现word embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch加载预训练词向量"><span class="toc-number">4.</span> <span class="toc-text">pytorch加载预训练词向量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#涉及函数详解"><span class="toc-number">4.1.</span> <span class="toc-text">涉及函数详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#numpy-与from-numpy"><span class="toc-number">4.1.1.</span> <span class="toc-text">numpy()与from_numpy()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensor-copy-src"><span class="toc-number">4.1.2.</span> <span class="toc-text">tensor.copy_(src)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考链接"><span class="toc-number">5.</span> <span class="toc-text">参考链接</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&text=pytorch实现Word embedding"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&is_video=false&description=pytorch实现Word embedding"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=pytorch实现Word embedding&body=Check out this article: http://yoursite.com/2019/03/20/pytorch实现Word-embedding/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&title=pytorch实现Word embedding"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/03/20/pytorch实现Word-embedding/&name=pytorch实现Word embedding&description=&lt;p&gt;word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。&lt;/p&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 spring
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/projects_url">项目</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


</body>
</html>
