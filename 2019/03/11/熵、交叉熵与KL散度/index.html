<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><title>熵、交叉熵与KL散度 |</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="spring"><meta name="designer" content="minfive"><meta name="keywords" content="null"><meta name="description" content="游龙当归海，海不迎我自来也。"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/index.html"><link rel="icon" type="image/png" href="undefined" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="spring's Blog"><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(undefined)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="spring's Blog" alt="spring's Blog"><img src="undefined" alt="spring's Blog"></a><nav class="page__nav"><ul class="nav__list clearfix"></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="undefined" alt="熵、交叉熵与KL散度"></div><header class="post__info"><h1 class="post__title">熵、交叉熵与KL散度</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/">undefined</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2019-03-11</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/交叉熵/">交叉熵</a></li><li class="mark__item"><a href="/tags/相对熵/">相对熵</a></li></ul></div></div></header><div class="post__content"><p>介绍交叉熵和KL散度。</p><a id="more"></a><h2 id="从信息量到信源熵"><a href="#从信息量到信源熵" class="headerlink" title="从信息量到信源熵"></a>从信息量到信源熵</h2><ol><li>信息量是通信专业的名词。一个变量的主要特征就是不确定性，也就是发生的概率。信息量用来衡量不确定性的大小。一个事情发生的概率越小，使人越感到意外，则这件事的信息量越大；反之，概率越大，越不意外，信息量越小。举个例子，有一架波音747飞机失事，发生的概率很小，让人很意外，带给人的信息量很大。<br>信息量函数应满足两个特性：1）随着概率的增大而减小，即是概率的减函数；2）信息量函数满足可加性，即两个统计独立的消息提供的信息量等于他们分别提供的信息量之和。同时满足递减性和可加性的函数是对数函数，即<br>$$ I[p(x_i)] = log \frac{1}{p(x_i)} = -log p(x_i)$$</li><li>信源熵定义为信源输出的平均信息量，即信息量的数学期望。$$ H(X) = E(I[p(x_i)]) = E(-log p(x_i)) = - \sum_{i=1}^{n}p(x_i)log p(x_i)$$信源实际上是一个概率分布，信源熵可以解释为表示这个概率分布至少需要的信息量。</li></ol><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>对于一个随机事件，真实概率分布是$p(x_i)$ 是未知的，从数据中得到概率分布为$q(x_i)$。我们用概率分布$q(x_i)$来近似和逼近真实的概率分布$p(x_i)$ 。交叉熵定义为：$$H(p,q) = \sum_{i=1}^{n}p(x_i) I[q(x_i)] =- \sum_{i=1}^{n}p(x_i)log(x_i) $$交叉熵$H(p,q)$是用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量。上面我们说过，信源熵$H(X)$是表示真实概率分布$p(x_i)$需要的最小信息量。可以得到结论：$$H(p,q) \ge H(p)$$由吉布斯不等式可以证明，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。这个不等式的意义是：用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量一定大于等于概率分布$p(x_i)$本身的信源熵。交叉熵比信源熵多出来的这部分，就是冗余信息量，我们称为KL散度（相对熵）。<br>$$KL(p||q)= H(p,q) - H(p) \ge 0$$容易看出交叉熵并不是一个对称量，即$ H(p,q) \not=H(q,p)$。同样的,KL散度也不是一个对称量，即$KL(p||q) \not =KL(q||p) $<br>给定概率分布$p(x_i)$,信源熵$H(p)$就是固定不变的。在机器学习中，交叉熵常用作分类问题的损失函数。交叉熵刻画了预测概率分布$q(x_i)$与真实概率分布$p(x_i)$之间的距离。通过减小交叉熵$H(p,q)$,我们可以使得预测概率分布$q(x_i)$不断逼近真实概率分布$p(x_i)$</p><h2 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h2><p>真实的概率分布为$p(x_i)$，我们用预测概率分布$q(x_i)$对它进行建模和近似。我们需要的平均附加量，也就是冗余量是：<br>$$KL(p,q) = H(p,q) - H(q) = -\sum_{i=1}^{n}p(x_i)logq(x_i) - \biggl(-\sum_{i=1}^{n}p(x_i)logp(x_i)\biggr) = -\sum_{i=1}^{n}p(x_i)log{\frac{q(x_i)}{p(x_i)}}$$KL散度有以下几个特性：</p><ul><li>KL散度不是一个对称量，即$KL(p||q) \not =KL(q||p) $</li><li>$KL(p||q)\ge 0$，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。</li><li>KL散度可以看做两个分布之间不相似程度的度量。KL散度越小，两个分布的不相似程度越小，分布$q(x_i)$越适合来近似$p(x_i)$。</li></ul><h2 id="tensorflow用交叉熵做损失函数"><a href="#tensorflow用交叉熵做损失函数" class="headerlink" title="tensorflow用交叉熵做损失函数"></a>tensorflow用交叉熵做损失函数</h2><p>在机器学习中交叉熵常常用作分类问题的损失函数。这里有个问题，交叉熵用于概率分布，但神经网络的输出并不一定是一个概率分布。<br>概率分布应满足2个条件:<br>1) $0 \le p(X =x) \le 1$<br>2) $\sum_{x}{} p(X=x) = 1$<br>如何把神经网络的输出变成概率分布呢？这里就要用到softmax回归。假设输出层的输出为$y_0,y_1,y_2 \dots y_n$,则softmax函数的形式为：$$softmax(y_i) = \frac{exp(y_i)}{\sum_{j}exp(y_j)}$$由于交叉熵一般会与softmax回归一起使用，TensorFlow对这两个功能进行了统一，可以直接用函数<a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">tf.nn.softmax_cross_entropy_with_logits</a>来计算softmax后的交叉熵函数。对于只有一个正确答案的分类问题，可以用函数<a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">tf.nn.sparse_nn.softmax_cross_entropy_with_logits</a>来加速计算过程。</p><h2 id="pytorch中交叉熵损失函数的实现"><a href="#pytorch中交叉熵损失函数的实现" class="headerlink" title="pytorch中交叉熵损失函数的实现"></a>pytorch中交叉熵损失函数的实现</h2><p>在多分类问题中，实际概率分布是 $y = [y_0,y_1,…,y_{C-1}]$,其中C为类别数;y是样本标签的one-hot表示，当样本属于第$i$类时$y_i=1$,否则$y_i=0$。预测概率分布为$p = [p_0,p_1,p_2,…,p_{C-1}]$。$c$是样本标签。此时，交叉熵损失函数为$$loss = -\sum_{i=0}^{C-1}y_i log(p_i) = - y_c \cdot log(p_c) = - log(p_c)$$<br>接下来介绍pytorch中具体实现这个数学式子的函数。</p><h3 id="torch-nn-functional-log-softmax-与class-torch-nn-NLLLoss"><a href="#torch-nn-functional-log-softmax-与class-torch-nn-NLLLoss" class="headerlink" title="torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()"></a>torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.log_softmax()</span><br></pre></td></tr></table></figure><ul><li>作用：先做softmax运算，再做log运算。在数学上等价于$log(softmax(x))$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class torch.nn.NLLLoss(weight = None)</span><br></pre></td></tr></table></figure><ul><li>作用：这是neg log likelihood loss（NLLLoss），即负对数似然函数。</li><li>参数：<ul><li>weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。</li></ul></li><li>输入：<ul><li>input(N,C): C是类别个数。为<code>log_probabilities</code>形式，即概率分布再取log。可以在最后一层加<code>log_softmax</code>,这就要用到函数<code>torch.nn.functional.log_softmax()</code></li><li>targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为<code>2</code>,而不是<code>[0,0,1,0,0]</code></li></ul></li></ul><p>loss可以表示为：$$loss(x,class) = -x[class]$$如果指定了weight，可以表示为：$$loss(x,class) = - weight[class]*x[class]$$<br>举个例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">log_m = torch.nn.functional.log_softmax()</span><br><span class="line">loss_function = torch.nn.NLLLoss()</span><br><span class="line">inputs = torch.randn(3,5) #batch_size * num_classes = 3 * 5</span><br><span class="line">target = torch.LongTensor([1,0,4])</span><br><span class="line">loss = loss_function(log_m(inputs),target)  # inputs要先做log_softmax，再送入loss_function</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><h3 id="class-torch-nn-CrossEntropyLoss-weight-None"><a href="#class-torch-nn-CrossEntropyLoss-weight-None" class="headerlink" title="class torch.nn.CrossEntropyLoss(weight = None)"></a>class torch.nn.CrossEntropyLoss(weight = None)</h3><ul><li>作用：将函数<code>log_softmax</code>和<code>NLLLoss</code>集成到一起。在多分类问题中非常有用。</li><li>参数：<ul><li>weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。</li></ul></li><li>输入：<ul><li>input(N,C): C是类别个数。每个类别的分数，不用过softmax层。</li><li>targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为<code>2</code>,而不是<code>[0,0,1,0,0]</code>。</li></ul></li></ul><p>loss可以表示为：$$loss(x,class) = - \text{log}\frac{e^{x[class]}}{ \sum_{j=0}^{C-1}e^{x[j]}} = -x[class] + \text{log}(\sum_{j=0}^{C-1}e^{x[j]}) $$当指定了weight时，loss计算公式为： $$ loss(x, class) = weights[class] \cdot (-x[class] + \text{log}(\sum_{j=0}^{C-1}e^{x[j]})) $$<br>参见：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/56638625" target="_blank" rel="noopener">PyTorch学习笔记——多分类交叉熵损失函数</a></li><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#loss-functions" target="_blank" rel="noopener">pytorch官方手册</a><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2></li><li><a href="https://wizyoung.github.io/%E4%BF%A1%E6%81%AF%E7%86%B5%EF%BC%8C%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%8C%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E7%90%86%E8%A7%A3/#more" target="_blank" rel="noopener">信息熵，相对熵，交叉熵的理解</a></li><li><a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">Tensorflow基础知识—损失函数详解</a></li></ul><div class="post-announce">Thank you for reading, this article belongs to <a href="http://yoursite.com">spring's Blog</a> copyright, if reproduced, please indicate the source：spring's Blog（<a href="http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/">http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2019/03/10/python的一些函数/" title="python的一些函数"><i class="iconfont icon-prev"></i>python的一些函数</a></div><div class="post__prev post__prev--right"><a href="/2019/03/11/sublime插件/" title="sublime插件">sublime插件<i class="iconfont icon-next"></i></a></div></div></div></article></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">Introduction</h3><p class="block__text">游龙当归海，海不迎我自来也。</p></div><div class="sidebar__block"><h3 class="block__title">Categories</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/论文/">论文</a><span class="block-list-count">2</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/生活记录/">生活记录</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/机器学习/">机器学习</a><span class="block-list-count">2</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/技术资料/">技术资料</a><span class="block-list-count">4</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/年度总结/">年度总结</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/学习笔记/">学习笔记</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/web搜索/">web搜索</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/pytorch/">pytorch</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/python/">python</a><span class="block-list-count">2</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">Latest Post</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2019/07/12/pip安装python模块报错/" title="pip安装python模块报错"><div class="item__cover"><img src="undefined" alt="pip安装python模块报错"></div><div class="item__info"><h3 class="item__title">pip安装python模块报错</h3><span class="item__text">2019-07-12</span></div></a></li><li class="latest-post-item"><a href="/2019/07/10/NAACL2019-对话系统/" title="NAACL2019-对话系统"><div class="item__cover"><img src="undefined" alt="NAACL2019-对话系统"></div><div class="item__info"><h3 class="item__title">NAACL2019-对话系统</h3><span class="item__text">2019-07-10</span></div></a></li><li class="latest-post-item"><a href="/2019/07/07/ACL2019-对话系统/" title="ACL2019-对话系统"><div class="item__cover"><img src="undefined" alt="ACL2019-对话系统"></div><div class="item__info"><h3 class="item__title">ACL2019-对话系统</h3><span class="item__text">2019-07-07</span></div></a></li><li class="latest-post-item"><a href="/2019/04/24/自然语言处理-会议列表/" title="自然语言处理---会议列表"><div class="item__cover"><img src="undefined" alt="自然语言处理---会议列表"></div><div class="item__info"><h3 class="item__title">自然语言处理---会议列表</h3><span class="item__text">2019-04-24</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">Tags</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/ACL2019/">ACL2019</a></li><li class="tag-item"><a class="tag-link" href="/tags/Elasticsearch/">Elasticsearch</a></li><li class="tag-item"><a class="tag-link" href="/tags/GPU/">GPU</a></li><li class="tag-item"><a class="tag-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-item"><a class="tag-link" href="/tags/LSTM/">LSTM</a></li><li class="tag-item"><a class="tag-link" href="/tags/Linux/">Linux</a></li><li class="tag-item"><a class="tag-link" href="/tags/Numpy/">Numpy</a></li><li class="tag-item"><a class="tag-link" href="/tags/csv/">csv</a></li><li class="tag-item"><a class="tag-link" href="/tags/cuda/">cuda</a></li><li class="tag-item"><a class="tag-link" href="/tags/dialog-system/">dialog system</a></li><li class="tag-item"><a class="tag-link" href="/tags/linux/">linux</a></li><li class="tag-item"><a class="tag-link" href="/tags/nvidia-smi/">nvidia-smi</a></li><li class="tag-item"><a class="tag-link" href="/tags/panda/">panda</a></li><li class="tag-item"><a class="tag-link" href="/tags/python/">python</a></li><li class="tag-item"><a class="tag-link" href="/tags/pytorch/">pytorch</a></li><li class="tag-item"><a class="tag-link" href="/tags/sublime/">sublime</a></li><li class="tag-item"><a class="tag-link" href="/tags/ubuntu/">ubuntu</a></li><li class="tag-item"><a class="tag-link" href="/tags/word-embedding/">word embedding</a></li><li class="tag-item"><a class="tag-link" href="/tags/交叉熵/">交叉熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/会议列表/">会议列表</a></li><li class="tag-item"><a class="tag-link" href="/tags/前向后向算法/">前向后向算法</a></li><li class="tag-item"><a class="tag-link" href="/tags/年度总结/">年度总结</a></li><li class="tag-item"><a class="tag-link" href="/tags/循环神经网络/">循环神经网络</a></li><li class="tag-item"><a class="tag-link" href="/tags/搭建博客/">搭建博客</a></li><li class="tag-item"><a class="tag-link" href="/tags/新藏线/">新藏线</a></li><li class="tag-item"><a class="tag-link" href="/tags/服务器/">服务器</a></li><li class="tag-item"><a class="tag-link" href="/tags/条件随机场/">条件随机场</a></li><li class="tag-item"><a class="tag-link" href="/tags/生活记录/">生活记录</a></li><li class="tag-item"><a class="tag-link" href="/tags/相对熵/">相对熵</a></li><li class="tag-item"><a class="tag-link" href="/tags/维特比算法/">维特比算法</a></li><li class="tag-item"><a class="tag-link" href="/tags/骑车/">骑车</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">About</h3><div class="item__content"><p class="item__text"></p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span></span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span></span></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© <a href="https://github.com/Mrminfive/hexo-theme-skapp" target="_blank">Skapp</a> 2017 powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, made by <a href="https://github.com/Mrminfive" target="_blank">minfive</a>.</p><ul class="footer__social-network clearfix"></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>