{"meta":{"title":"spring's Blog","subtitle":"游龙当归海，海不迎我自来也。","description":"游龙当归海，海不迎我自来也。","author":"spring","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-07-21T14:19:35.000Z","updated":"2019-07-21T14:19:56.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"收藏、资源、好文","date":"2019-07-28T03:02:05.000Z","updated":"2019-08-01T09:09:34.000Z","comments":true,"path":"collection/index.html","permalink":"http://yoursite.com/collection/index.html","excerpt":"","text":"工具@card{ 网址 简介 推荐指数 Font Awesome图标中文网 提供可缩放矢量图标 ⭐⭐⭐⭐ 正则表达式 正则表达式 ⭐⭐⭐⭐ Emoji Homepage markdown表情 ⭐⭐⭐⭐⭐ 常用数学符号的 LaTeX 表示方法 常用数学符号的 LaTeX 表示方法 ⭐⭐⭐⭐⭐ }"},{"title":"友情链接","date":"2019-07-28T03:02:05.000Z","updated":"2019-07-30T07:55:09.000Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":"持续更新中…欢迎留言，互换友链~ 大牛的博客@card{ 名称 博主 语言 备注 colah's blog Christopher Olah 英文 Google Brain研究员，OpenAI成员 Jason Weston Jason Weston 英文 Facebook研究科学家，纽约大学教授。研究领域是：memory network，dialog system，QA system Richard Socher Richard Socher 英文 教授斯坦福cs224d课程 Lil'Log Lilian 英文 OpenAI成员 facebook research facebook research 英文 facebook research Harvard NLP Harvard NLP 英文 Harvard NLP主页 The BAIR Blog 伯克利学院 英文 伯克利人工智能研究院 Free MindFree Mind's old blog 张驰原 中文 本硕浙大计算机，现Google研究科学家 李纪为的主页 李纪为 中文 北大本科，斯坦福博士，香农科技创始人 黄民烈教授的主页 黄民烈教授 中文 清华大学计算机学院教授 }"},{"title":"tags","date":"2019-07-21T14:19:23.000Z","updated":"2019-07-21T14:20:13.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"论文笔记《Bridging the Gap between Training and Inference for Neural Machine Translation》","slug":"论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》","date":"2019-08-02T06:46:00.000Z","updated":"2019-08-05T05:06:42.000Z","comments":true,"path":"2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/","link":"","permalink":"http://yoursite.com/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/","excerpt":"【来源】：ACL2019【链接】：https://arxiv.org/abs/1906.02448【代码、数据集】： 无","text":"【来源】：ACL2019【链接】：https://arxiv.org/abs/1906.02448【代码、数据集】： 无 这篇论文由中科院发表，获得了ACL2019的 “best long paper”。 要解决的问题在Neural Machine Translation(NMT)任务中，模型通常采用encoder-decoder框架，基于RNN 或 CNN 或attention。假设输入为$X = \\lbrace{x_1,x_2,…,x_m}\\rbrace$，真实输出为$Y = \\lbrace{y_1^*,y_2^*,…,y_n^*}\\rbrace$，预测输出为$Y’ = \\lbrace {y_1’,y_2’,…,y_m’}\\rbrace$。 第一个问题是：decoder会一个词一个词地生成整个回复。在train阶段，在时间步t生成$y_t’$时，decoder会根据之前真实的词$\\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\\rbrace$来预测$y_t’$。在infer阶段，由于不可能知道真实输出，在时间步生成$y_t’$时，decoder会根据之前预测的词$\\lbrace{y_1’,y_2’,…,y_{t-1}’}\\rbrace$来预测$y_t’$。可以看到train阶段与infer阶段所依据的词是不同的，train阶段和infer阶段预测的词$y_t’$来自两个不同的概率分布，分别是数据分布(data distribution)和模型的分布(model distribution)，这种差别称为“爆炸偏差(exposure bias)”。随着预测序列的长度增加，错误会逐渐累积。为了解决第一个问题，消除train阶段和infer阶段的这种差别，一个可能的解决方法是：在train阶段，decoder同时根据真实的词$\\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\\rbrace$和预测的词$\\lbrace{y_1’,y_2’,…,y_{t-1}’}\\rbrace$来生成$y_t’$。 第二个问题是: NMT模型通常最优化$Y与Y’$之间的交叉熵目标函数来更新模型参数，但交叉熵函数会严格匹配预测输出$Y’$与真实的输出$Y$。但在NMT任务中，一句话可以有多个不同但合理的翻译。一旦预测输出$Y’$的某个词与$Y$不同，尽管它是合理的，也会被交叉熵函数纠正。这种情况称为“过度纠正的现象”。 为了消除train阶段与infer阶段的差别，论文提出了一种在train阶段做改进的解决方案。首先，从预测的词中选择oracle word $y_{j-1}^{oracle}$，设真实输出中上一个词为$y_{j-1}^{*}$。接着从$\\lbrace{y_{j-1}^{oracle},y_{j-1}^{*}}\\rbrace$中抽样一个词，抽中$y_{j-1}^{*}$的概率为$p$，抽中$y_{j-1}^{oracle}$的概率为$1-p$。最后，decoder根据抽样的这个词来预测$y_j$。 在train阶段刚开始时，抽中真实的词$y_{j-1}^{*}$的概率比较大，随着模型逐渐收敛，抽中预测的词$y_{j-1}^{oracle}$的概率变大，让模型有能力处理”过度纠正的问题”。 Fig.1 论文提出的方法的结构图 RNN-based NMT ModelNMT任务常采用encoder-decoder框架，可以基于RNN或CNN或纯attention。论文提出的消除train阶段和infer阶段差别的方法，可以用于任何NMT模型。论文以基于RNN的NMT模型为例，来介绍这种方法。这一节先介绍RNN-based NMT模型。下一节介绍NMT模型如何结合这种方法。 encoder记输入为$X = \\lbrace{x_1,x_2,…,x_m}\\rbrace$，真实输出为$Y = \\lbrace{y_1,y_2,…,y_n}\\rbrace$。encoder采用bi-GRU分别获取正向和反向的隐藏状态$\\overrightarrow{h_i},\\overleftarrow{h_i}$。$x_i$的embedding向量为$e_{x_i}$。$$\\overrightarrow{h_i} = GRU(e_{x_i},h_{i-1})$$ $$\\overleftarrow{h_i} = GRU(e_{x_i},h_{i+1})$$ 将$\\overrightarrow{h_i},\\overleftarrow{h_i}$连接起来，作为$x_i$对应的隐藏状态：$$h_i = [\\overrightarrow{h_i},\\overleftarrow{h_i}] \\tag{1}$$ attentionattention机制用来联系encoder和decoder，更好地捕捉source sequence的信息。也就是在时间步t,通过encoder所有的隐藏状态$\\lbrace h_1,h_2,…,h_m \\rbrace$来计算context vector $c_t$。记decoder上一时间步的隐藏状态为$s_{t-1}$。 $c_t$是encoder所有隐藏状态$\\lbrace h_1,h_2,…,h_m \\rbrace$的加权和：$$c_t = \\sum_{i=1}^{m}\\alpha_{ti}h_i \\tag{2}$$ 其中$\\alpha_{ti}$是attention权重，计算方式为:$$\\beta_{ti} = v_a^\\top tanh(W_as_{t-1} + U_ah_i) \\tag{3}$$ $$\\alpha_{ti} = softmax(\\beta_{ti}) = \\frac{exp(\\beta_{ti})}{\\sum_jexp(\\beta_{tj})}$$ decoderdecoder采用单向GRU的变体，隐藏状态更新公式为:$$s_t = GRU(s_{t-1},e_{y_{t-1}^*},c_t) \\tag{4}$$ 最后根据e_{y_{t-1}^*}，decoder的隐藏状态$s_t$，对应的context vector $c_t$来预测$y_t$。 $$o_t = W_og(e_{y_{t-1}^*},s_t,c_t) \\tag{5}$$ 在词汇表上的概率分布为：$$P_t(y_t = w) = softmax(o_t) \\tag{6}$$ 方法为了消除或减轻train阶段和infer阶段的差别，论文提出 从真实的词$y_{t-1}*$和$y_{t-1}^{oracle}$预测的词中抽样，decoder根据抽样的词来预测下一个词$y_t$。使用论文提出的方法，在时间步t预测$y_t$分为三步： 先从预测的词中选择$y_{t-1}^{oracle}$。 论文提出了两种方法来选择oracle word，分别是词级别的方法和句子级别的方法。 从$\\lbrace{y_{t-1}^{oracle},y_{t-1}*}\\rbrace$中抽样得到$y_{t-1}$，抽中$y_{t-1}*$的概率为$p$，抽中$y_{t-1}^{oracle}$的概率为$1-p$。 用抽样的词$y_{t-1}$来替换公式$(4)(5)$中的$y_{t-1}^*$来预测下一个词。 oracle word的选择传统的方法中，decoder会根据上一个时间步真实的$y_{t-1}^*$来预测$y_t$。为了消除train阶段的infer阶段的差别，可以从预测的词中选择oracle word $y_{t-1}^{oracle}$来代替$y_{t-1}^*$。一种方法是每个时间步采用词级别的greedy search来生成oracle word，称为word-level oracle(WO)，另一种方法是采用beam-search，扩大搜索空间，用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO). word-level oracle选择$y_{t-1}^{oracle}$最简单直观的方法是，在时间步t-1，选择公式$P_{t-1}$中概率最高的词作为$y_{t-1}^{oracle}$，如Fig.2所示。 为了获得更健壮的$y_{t-1}^{oracle}$，更好地选择是使用gumbel max技术来冲离散分布中进行抽样，如Fig.3所示。具体地讲，将gumbel noise $\\eta$作为正则化项加到公式(5)中的$o_{t-1}$，再进行softmax操作得到$y_{t-1}$的概率分布。$$\\eta = -log(-log(u)) $$ $$\\tilde{o_{t-1}} = \\frac{o_{t-1} + \\eta}{\\tau} \\tag{7}$$ $$\\tilde{P_{t-1}} = softmax(\\tilde{o_{t-1}}) \\tag{8}$$ 其中变量$u \\sim U(0,1)$服从均匀分布。$\\tau$为温度系数，当$\\tau \\to 0$时，公式(8)的softmax()逐渐相当于argmax()函数；当$\\tau \\to \\infty$时，softmax()函数逐渐相当于均匀分布。则$y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{WO} =argmax(\\tilde{P_{t-1}}) \\tag{9}$$需要注意的是gumbel noise $\\eta$只用来选择oracle word，而不会影响train阶段的目标函数。 Fig.2. word level oracle without gumbel noise Fig.3. word level oracle with gumbel noise sentence-level oracle为了选择sentence-level oracle word，首先要进行beam-search解码，设beam size为k，得到k个candidate句子。在beam-search解码的过程中，生成每个词时也应用gumbel max技术。接着，得到k个candidate句子后，用句子级衡量指标BLEU来给这k个句子打分，得分最高的句子为oracle sentence $Y^S = \\lbrace{y_1^S,y_2^S,..,y_{|y^S|}^S}\\rbrace$。则时间步t解码对应的oracle word $y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{SO} = y_{t-1}^{S} \\tag{10}$$ 当模型从真实输出$Y$和sentence oracle $Y^S$抽样，这有一个前提是，这两个序列的长度需要是一致的。但beam-search decode不能保证解码序列的长度。为了保证这两个序列长度一致，论文提出了force decoding的解决方法。 force decoding设真实输出$Y = \\lbrace{y_1,y_2,…,y_n}\\rbrace$的序列长度为n。force decoding需要解码得到长度同样为n的序列，以特殊字符”EOS”结束。设beam search decode时，时间步t对应的概率分布为$P_t$。 当$t&lt; n$时，对于概率分布$P_t$，即使字符”EOS”是概率最高的词，那么生成概率次高的词。 当$t = n+1$时，对于概率分布$P_{n+1}$，即使字符”EOS”不是概率最高的词，也要生成”EOS”。 果真是强制生成长度为n的序列。这样beam-search decode得到的序列与真实输出序列的长度就是一致的，都为n。 递减抽样根据公式(9)或(10)得到$y_{t-1}^{oracle}$后，下一步是从$\\lbrace{y_{t-1}^{oracel},y_{t-1}^*}\\rbrace$中抽样，抽中$y_{t-1}^*$的概率是p，抽中$y_{t-1}^{oracle}$的概率是1-p。在训练的初始阶段，如果过多地选择$y_{t-1}^{oracle}$，会导致模型收敛速度慢；在训练的后期阶段，如果过多地选择$y_{t-1}^*$，会导致模型在train阶段没有学习到如何处理infer阶段的差别。因此，好的选择是：在训练的初始阶段，更大概率地选择$y_{t-1}^*$来加快模型收敛，当模型逐渐收敛后，以更大概率选择$y_{t-1}^{oracle}$，来让模型学习到如何处理infer阶段的差别。从数学表示上，概率$p$先大后逐渐衰减，$p$随着训练轮数$e$的增大而逐渐变小。$$p = \\frac{\\mu}{\\mu + exp(\\frac{e}{\\mu})} \\tag{11}$$其中，$\\mu$是超参数。$p$是轮数$e$的单调递减函数。$e$从0开始，此时，$p=1$。 训练将采样得到的$y_{t-1}$代替公式(4)-(6)中的$y_{t-1}^*$来预测$y_t$在词汇表上的概率分布。采用最大似然估计，相当于最小化以下目标函数：$$L(\\theta) = -\\sum_{n=1}^{N}\\sum_{j=1}^{|y_n|}logP_j^n[y_j^n]$$","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"ACL2019","slug":"ACL2019","permalink":"http://yoursite.com/tags/ACL2019/"},{"name":"Neural Machine Translation","slug":"Neural-Machine-Translation","permalink":"http://yoursite.com/tags/Neural-Machine-Translation/"}]},{"title":"论文笔记《Multi-Level Memory for Task Oriented Dialogs》","slug":"论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》","date":"2019-08-01T06:14:37.000Z","updated":"2019-08-01T08:32:54.000Z","comments":true,"path":"2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/","link":"","permalink":"http://yoursite.com/2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/","excerpt":"【来源】：NAACL2019【链接】：https://arxiv.org/pdf/1810.10647.pdf【代码、数据集】：https://github.com/DineshRaghu/multi-level-memory-network","text":"【来源】：NAACL2019【链接】：https://arxiv.org/pdf/1810.10647.pdf【代码、数据集】：https://github.com/DineshRaghu/multi-level-memory-network 已有工作中，端到端的任务型对话系统采用memory network来结合外部的知识库(knowledgt base) 和 对话历史(context)。为了使用从跑一趟 network，通常将二者放在同一个memory中。这样带来的问题是：memory变得太大，模型在读取memory时需要区分外部知识库和对话历史，并且在memory上的推理变得很难。为了解决这个问题，论文将外部知识库和对话历史区分开，另外，将外部知识库保存为分层的memory。 模型结构模型主要包括三个部分。 分级encoder： 分别编码对话历史中的句子。 milti-level memory 保存了目前为止所有的query以及对应的知识库查询结果，是以分级的方式保存在memory中的。 copy机制增强的decoder： 从词汇表中生成词，或者从知识库multi-level memory中复制词，或者从对话历史(context)中复制词。 Fig.1. 模型的整体框架图来源:Revanth Reddy2019 分级encoder在第t轮，对话历史共有2t-1个句子$\\lbrace{c_1,c_2,…,c_{2t-1}}\\rbrace$，其中用户对话为t轮，回复对话为t-1轮。 每个句子$c_i$都是词序列$\\lbrace{w_{i1},w_{i2},…,w_{im}}\\rbrace$。每个句子$c_i$先经过embedding layer得到词向量表示，再经过单层bi-GRU得到句子的向量表示$\\varphi(c_i)$。$h_{ij}^e$表示词$w_{ij}$对应的隐藏状态。再将$\\varphi{c_i}$经过另一个单词GRU来得到context的向量表示$c$。 multi-level memorymemory的关键是分级的分为三级：query $\\to$ result $\\to$ result key和result value。见Fig.2。记本轮对话之前所有的知识库query为$q_1,…,q_k$。每个query $q_i$是一个(key,value)对，$q_i = \\lbrace{k_a^{q_i}:v_a^{q_i},0&lt; a&lt; n_{q_i}}\\rbrace $。其中key和value分别对应query的槽(slots)和槽值，$n_{q_i}$是query $q_i$的槽值个数。第j轮对话，用query $q_i$查询知识库的返回结果为result $r_{ij}$。$r_{ij}$也是一个key-value对，$r_{ij} = \\lbrace{k_a^{r_{ij}}:v_a^{r_{ij}},0&lt; a &lt; n_{r_{ij}}}\\rbrace$。其中$n_{r_{ij}}$是key-value对的个数。 Fig.2. multi memory来源:Revanth Reddy2019 第一级memory是query的向量表示。 $q_i$的向量表示为$q_i^v$，$q_i^v$为所有values $v_a^{q_i}$的词袋(bag of words)向量表示。 第二级memory是result的向量表示。同样地，$r_{ij}$的向量表示为$r_{ij}^v$，$r_{ij}^v$为所有values $v_a^{r_{ij}}$的词袋(BOW)向量表示。 第三级memory是result的key-value对，$(k_a^{r_{ij}}:v_a^{r_{ij}})$，其中value $v_a^{r_{ij}}$可能会被复制到回复中。 copy机制增强的decoderdecoder一个词一个词地生成回复。在时间步t生成词$y_t$时，可能从词汇表中生成，也能从两个分开的memory上复制。用门$g_1$来选择是从词汇表上生成，还是从memory中复制。如果是后者，用另一个门$g_2$来选择是从context中复制，还是从知识库复制。 从词汇表生成词 时间步t，decoder的隐藏状态$h_t$为$$h_t = GRU(y_{t-1},s_{t-1})$$用$h_t$计算在encoder的所有隐藏状态上的attention权重，采用”concat attention”机制：$$a_{ij} = softmax(w_1^\\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))) = \\frac{w_1^\\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}{\\sum_{ij}w_1^\\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}$$则context vector为$$d_t = \\sum_{ij}a_{ij}h^e_{ij}$$ $h_t$和$d_t$连接后经过线性层和softmax层得到在词汇表上的概率分布：$$P_g(y_t) = softmax(W_1[h_t,d_t] + b_1)$$ 从context memory中复制词 直接将计算context vector时的attention权重，作为在context所有词$w_{ij}$上的概率分布：$$P_{con}(y_t = w) = \\sum_{ij:w_{ij}=w}a_{ij}$$ 从KB memory中复制实体 时间步t的隐藏状态$h_t$和context vector $d_t$用来计算在所有query上的attention权重。第一级在所有query $q_1,q_2,…,q_k$的attention权重为$$\\alpha_i = softmax(w_2^\\top tanh(W_4[h_t,d_t,q_i^v])) = \\frac{w_2^\\top tanh(W_4[h_t,d_t,q_i^v])}{\\sum_{i}w_2^\\top tanh(W_4[h_t,d_t,q_i^v])}$$ 第二级$\\beta_i$在$q_i$对应的$r_i$上的attention权重为$$\\beta_{ij} = softmax(w_3^\\top tanh(W_5[h_t,d_t,r_{ij}^v])) = \\frac{w_3^\\top tanh(W_5[h_t,d_t,r_{ij}^v])}{\\sum_{j}w_3^\\top tanh(W_5[h_t,d_t,r_{ij}^v])}$$ 第一级attention和第二级attention的乘积是在所有result上的attention权重分布。则memory总的向量表示为$$m_t = \\sum_{i}\\sum_j\\alpha_i\\beta_{ij}r_{ij}^v$$ 第三级memory为result的key-value对$(k_a^{r_{ij}}:v_a^{r_{ij}})$，类似于(Eric and Manning, 2017)，用key $k_a^{r_{ij}}$来计算attention权重，将对应的value $v_a^{r_{ij}}$复制到回复中。在$r_{ij}$所有keys上的attention权重为$$\\gamma_{ijl} = softmax(w_4^\\top tanh(W_6[h_t,d_t,m_t,k_l^{r_{ij}}]))$$则在所有values $v_a^{r_{ij}}$的概率分布为:$$P_{kb}(y_t = w) = \\sum_{ijl:v_l^{r_{ij}}=w}\\alpha_i\\beta_{ij}\\gamma_{ijl}$$ decoding 我们用门机制$g_2$来来结合$P_{con}(y_t)$和$P_{kb}(y_t)$，得到memory上的copy概率分布$P_c(y_t)$。$$g_2 = sigmoid(W_7[h_t,d_t,m_t]+b_2)$$ $$P_c(y_t) = g_2P_{kb}(y_t) + (1-g_2)P_{con}(y_t)$$ 用门机制$g_1$来结合$P_{c}(y_t)$和$P_{g}(y_t)$来得到总的概率分布$P(y_t)$：$$g_1 = sigmoid(W_8[h_t,d_t,m_t]+b_3)$$ $$P(y_t) = g_1P_g(y_t) + (1-g_1)P_c(y_t)$$","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"},{"name":"NAACL2019","slug":"NAACL2019","permalink":"http://yoursite.com/tags/NAACL2019/"},{"name":"Memory Network","slug":"Memory-Network","permalink":"http://yoursite.com/tags/Memory-Network/"}]},{"title":"Neural Turing Machines与Memory Network","slug":"Neural-Turing-Machines与Memory-Network","date":"2019-07-26T03:03:15.000Z","updated":"2019-08-01T02:30:21.000Z","comments":true,"path":"2019/07/26/Neural-Turing-Machines与Memory-Network/","link":"","permalink":"http://yoursite.com/2019/07/26/Neural-Turing-Machines与Memory-Network/","excerpt":"介绍Memory Networks。","text":"介绍Memory Networks。 Neural Turing Machines-神经图灵机Google DeepMind团队在Alex Graves2014提出Neural Turing Machines，第一次提出用external memory来提高神经网络的记忆能力。这之后又出现了多篇关于Memory Networks的论文。我们先看看Turing Machines的概念。 Turing Machines-图灵机计算机先驱turing在1936年提出了Turing Machines这样一个计算模型。它由三个基本的组件： tape: 一个无限长的纸带作为memory，包含无数个symbols，每个symbol的值为0、1或”$\\space$”。 head: 读写头，对tape上的symbols进行读操作和写操作。 controller： 根据当前状态来控制head的操作。 理论上Turing Machines可以模拟任何一个计算算法，不管这个算法多么复杂。但现实中，计算机不可能有无限大的memory space，因此Turing Machines只是数学意义上的计算模型。 Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/) Neural Turing MachinesNeural Turing Machines(NTM,Alex Graves2014)用external memory来提高神经网络的记忆能力。LSTM(Long and short memory)通过门机制有效缓解了RNN的’梯度消失和梯度爆炸问题’，可以通过internal memory实现长期记忆。当LSTM的internal memory的记忆能力有限，需要用external memory来提高神经网络的记忆能力。 Neural Turing Machines包含两个基本组件：a neural network controller和memory bank。memory是一个 $N\\cdot M$阶的矩阵，包含N个向量，每个向量的维度是M。我们把每个memory vector称为memory location。controller控制heads对memory进行读写操作。 如何对memory matrix进行读写操作呢？关键问题是如何让读写操作是可微的，这样才能用梯度下降法来更新模型参数。具体来说，问题是让模型关于memory location是可微的，但memory locations是离散的。Neural Turing Machines用了一个很聪明的方法来解决这个问题：不是对单独某个memory location进行读写操作，而是对所有的memory locations进行不同程度的读写操作，这个程度是通过attention的权重分布来控制的。 Fig. 2. Neural Turing Machine Architecture 读操作记时间步t memory matrix为$N\\cdot M$阶矩阵$M_t$，$w_t$是在N个memory向量上的权重分布，是一个N维向量。则时间步t的read vector $r_t$为$$r_t = \\sum_{i=1}^{N}w_t(i)\\cdot M_t(i)$$ $$where: \\sum_{i=1}^{N}w_t(i) = 1; 0 \\le w_t(i) \\le 1,\\forall i $$其中，$w_t(i)$是$w_t$的第i个元素，$M_t(i)$是$M_t$的第i个行向量。 写操作受LSTM门机制的启发，将写操作分成两步：先erase，再add。先根据erase vector $e_t$擦去旧的内容，再根据add vector $a_t$添加新的内容。 先erase： 在时间步t，attention权重分布为$w_t$，erase vector $e_t$是一个M维向量，每个元素取值[0,1]，上一个时间步的memory vector为$M_{t-1}$。则erase操作为$$\\tilde{M_{t}}(i) = M_{t-1}(i)[\\vec{1}-w_t(i)e_t]$$ $\\vec{1}$是一个M维的全1向量。对memory vector的erase操作是逐点进行的。当$e_t$的元素和memory location对应权重$w_t(i)$的元素值都是1时，memory vector $M_t(i)$的元素值才会置为0。如果$e_t$或$w_t(i)$的元素值为0时，memory vector $M_t(i)$的元素值保持不变。 再add: 每个write head会产生一个M维的add vector a_t，则：$$M_t(i) = \\tilde{M_{t}}(i) + w_t(i)a_t$$至此，就完成了写操作。 寻址机制进行读写操作前，要搞清楚对哪个memory location进行读写呢？这就是寻址。为了让模型关于memory locatios可微，Neural Turing Machines不是对某个单独的memory location进行读写操作，而是对所有memory locations进行不同程度的读写操作，这个程度就是由权重分布$w_t$来控制的。模型结合并同时使用了content-based和location-based两种寻址方式来计算这个权重分布$w_t$。具体地，权重计算分为以下几步： content-based addressing 时间步t，每个head产出一个M维的key vector $k_t$，通过$k_t$与memory vectors $M_t(i)$之间的相似性来计算content-based attention权重分布$w_{t}^{c}$。相似性是通过余弦相似度来衡量的。$$w_{t}^{c} = softmax(\\beta_tK(k_t,M_t(i))) = \\frac{\\beta_tK(k_t,M_t(i))}{\\sum_{j}K(k_t,M_t(j))}$$ $$K(u,v) = \\frac{u\\cdot v}{|u|\\cdot |v|}$$ $\\beta_t$可以放大或缩小权重的精度。 内插法 每个head产生一个interpolation gate $g_t$，取值[0,1]。content-based attention权重分布为$w_t^{c}$，上一个时间步的attention权重分布为$w_{t-1}$。则门控制的权重分布$w_t^g$为：$$w_t^g = g_tw_t^c + (1-g_t)w_{t-1}$$当$g_t$为0时，采用上一个时间步的权重分布$w_{t-1}$，当$g_t$为1时，采用content-based attention权重分布$w_t^c$。 循环卷积 对经过插值后的权重分布$w_t^g$进行循环卷积，主要功能是对权重进行旋转位移。比如当权重分布关注某个memory location时，经过循环卷积就会扩展到附近的memory locations，也会对附近的memory locations进行少量的读写操作。每个head产生的转移权重为$s_t$,循环卷积的操作为:$$\\tilde{w_t(i)} = \\sum_{j=0}^{N-1}w_t^g(i)s_t(i-j)$$ 关于$s_t$的详细介绍可以见attention?attenion!; 循环卷积的详细介绍可以见Neural Turing Machines-NTM系列（一）简述 锐化 循环卷积往往会造成权重泄漏和分散，为了解决这个问题，需要最后进行锐化操作。$$w_t(i) = \\frac{\\tilde{w_t(i)^{\\gamma_t}}}{\\sum_j\\tilde{w_t(j)^{\\gamma_t}}}$$其中$\\gamma_t &gt;1$。至此，就得到了时间步t的权重分布$w_t$。可以根据这个权重分布$w_t$对memory matrix进行读写操作。 总结以下这4步操作。第一步content-based addressing根据输入得到关于memory locations的相似度；后三步实现了location-based addressing。第二步插值操作引入了上一个时间步的权重分布，对content-based 权重进行修正；第三步循环卷积将每个位置的权重向两边分散；第四步锐化操作将权重突出化，大的更大，小的更小。 Fig.3. 寻址机制的4步操作 Fig.4. 寻址机制的4步操作来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401) 参考链接 Attention and Augmented Recurrent Neural Networks 用动图直观地表现Neural Turing Machines的计算过程。推荐！👍 记忆网络之Neural Turing Machines，中文 attention?attenion! Memory Network在Neural Turing Machines提出仅仅五天后，Facebook研究员Jason Weston发表了MEMORY NETWORKS。在QA系统的领域，应用memory network。虽然RNN或LSTM可以通过hidden state和weights来进行短期记忆，但它们的记忆能力是有限的。要实现长期记忆，需要memory network。 Memory Network的一般框架memory network包括一个记忆单元memory，和四个基本组件： I(input feature map): 将input x进行向量化表示，编码为feature representation I(x)。 G(generalization): 对memory进行写操作。根据input 来更新memory $m_i$。$m_i = G(m_i,I(x),m)$ O(output feature map): 对memory进行读操作。根据input和memory生成output feature。$o = O(I(x),m)$ R(response): 根据output feature o来生成response。 Fig.5. memory network的框架图 memory network框架的实现–MemNNs在I模块将input $x_i$编码为$I(x_i)$后，G模块之间将$I(x_i)$保存到下一个空的memory slot中，而不更新旧的memory slots。真正实现inference的核心模块是O和R。 O模块在给定x的条件下，依次找到与x最相关的k个memory slots。论文中采用k = 2。先找到第一个最相关的memory slot：$$m_{o1} = \\mathop{argmax}\\limits_{i = 1,…,N} s_{o1}(x,m_i)$$其中$s_o()$是一个匹配函数，计算x与$m_i$之间的相关程度。接着，根据x和第一个memory找到下一个memory：$$m_{o2} = \\mathop{argmax}\\limits_{i = 1,…,N} s_{o2}([x,m_{o1}],m_i)$$将output feature o = $[x,m_{o1},m_{o2}]$作为R模块的输入。 R模块将词汇表中所有词与output feature进行匹配，选择匹配度最高的词作为response。这样生成的response只有一个词。$$r = \\mathop{argmax}\\limits_{w \\in W}s_R([x,m_{o1},m_{o2}],w)$$其中$s_R()$是一个匹配函数。 匹配函数$s_O$和$s_R$都采用以下函数：$$s(x,y) = \\Phi_x(x)^\\top U^\\top U \\Phi_y(y)$$其中$\\Phi_x(x),\\Phi_y(y)$分别将x/y编码为向量。目标函数在训练阶段采用最大边缘目标函数，设对于question x，真实的label为r，对应的memory为$m_{o1},m_{o2}$。则最大边缘目标函数为：$$\\sum_{m_i\\ne m_{o1}}max(0,\\gamma - s_{O1}(x,m_{o1}) + s_{O1}(x,m_i)) + $$ $$\\sum_{m_j\\ne m_{o2}}max(0,\\gamma - s_{O2}([x,m_{o1}],m_{o2}) + s_{O2}([x,m_{o1}],m_j)) + $$ $$\\sum_{r’ \\ne r}max(0,\\gamma - s_{R}([x,m_{o1},m_{o2}],r) + s_{R}([x,m_{o1},m_{o2}],r’))$$ 由于argmax()函数的存在，这个模型是不可微的。而且中间过程找到相关memory需要监督，这个模型不是端到端的。总的来说，这个memory network是一种普适性的架构，是很初级很简单的，很多部分还不完善，不足以应用具体的任务上。不过，通过多跳方式找到相关memory的思路是很值得学习的。 End-to-End Memory NetworkJason Weston作为三作的Sainbayar Sukhbaatar2015对Memory network工作的改进，主要改进是实现了端到端，减少了监督。End-to-End Memory Network采用soft attention而不是hard attention来read memory，因此是端到端的。另外不需要对相关memory进行监督。提高memory network的可用性。假设多个句子input $x_1,…,x_n$作为memory，对于query q，输出对应的answer a。给定query q，经过多跳找到相关的memory，并生成对应的answer a。 single layer给定input $x_1,x_2,…,x_n$，采用两个不同的embedding matrix A和C分别编码为向量$\\lbrace{m_1,…,m_n}\\rbrace$，$\\lbrace{c_1,…,c_n}\\rbrace$,分别对应attention机制的keys和values。将query q经过embedding matrix B编码为向量表示u。 采用dot-product attention计算权重：$$p_i = softmax(u^\\top m_i) = \\frac{exp(u^\\top m_i)}{\\sum_{j}exp(u^\\top m_j)}$$则memory representation为：$$o = \\sum_{i}p_i m_i$$根据u和o来进行预测：$$\\hat{a} = softmax(W (o + u))$$通过最小化a与$\\hat{a}$之间的交叉熵来训练模型参数A,B,C,W。这个single layer end-to-end Memory network是简单而直观的。核心是用soft attention来read memory，找到相关的memory，并进行inference。 Fig.6.左:single layer;右:multi layers来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895) multi layers将K层single layer进行stack得到K层memory network，进行K跳memory查询操作。具体地stack方式为： 将第k层的输入$u^k$和memory representation $o^k$相加作为第k+1层的输入:$$u^{k+1} = u^k + o^k$$ 每一层都有单独的embedding matrix $A^k$和$C^k$ 最后一层的预测输出为：$$\\hat{a} = softmax(W u^{K+1}) = softmax(W(u^K + o^K))$$ 为了减少参数量，有两种方法： adjacent: 让相邻层的embedding matrix A=C，共享参数。即：$C^k = A^{k+1}$，对第一层有$A^1 = B$，最后一层有：$C^K = W$。这样就减少了一半的参数量。 RNN-like: 跟RNN一样，采用完全参数共享的方法，$A^1 = A^2 = … = A^K$;$C^1 = C^2 = … = C^K$。参数数量大大减少导致模型效果变差，在层与层之间添加一个线性映射：$u^{k+1} = Hu^k + o^k$ key-value Memory NetworksJason Weston作为作者之一的Alexander Miller2016在End-to-End Memory networks的基础上继续推进，可以更好的通过memory来编码和利用先验知识，并且具体地应用到了QA系统中。 作为memory的先验知识可以是结构化的三元组知识库，也可以是非结构化的文本。 三元组知识库。三元组的形式是”实体-关系-实体”，或”主语-谓语-宾语”。三元组知识库的优点是结构化的，便于机器处理。但缺点是与一句完整的话比较，三元组缺少了一些信息。由于三元组知识库是人工构建的，难免会有覆盖不到的知识，对于某个问题可能知识库中根本就没有对应的知识。另外，三元组中的实体可以有多种不同的表达，比如知识库中有三元组”中国-首都-北京”。当问题是“中华人民共和国的首都是？”时，可能就不能很好地回答。 像“维基百科”这样的非结构化文本。优点时覆盖面广，几乎包含所有问题的知识。缺点是非结构化的，有歧义，需要经过复杂的推理才能找到答案。 作为先验知识的memory是(key,value)形式的。 key memory用于寻址(addressing/lookup)阶段，通过计算query与key memory的相关程度来计算attention权重，因此在设计key memory时，key memory的特征应该更好地匹配query。 value memory用于read阶段，将value memory的加权和作为memory总的向量表示，因此在涉及value memory时，value memory的特征应该更好地匹配response。 比较一下end-to-end memory network与key-value memory network的区别： 前者是将相同的输入经过两个不同的embedding matrix编码分为作为key memory和value memory。而后者可以将不同的知识(key,value)分别编码为key memory和value memory，可以更灵活地利用先验知识。 后者的每个hop之间添加了用$R_j$来进行线性映射。 模型结构在问答系统中，记memory slots为$(k_1,v_1),…,(k_M,v_M)$，问题query为x，真实回复为a，预测回复为$\\hat{a}$。$\\Phi_{X},\\Phi_{Y},\\Phi_{K},\\Phi_{V}$分别是x,a,key,value的embedding matrix，将文本编码为向量表示。 则单次memory的寻址和读取可以分为三步： key hashing: 当知识库很大时，这一步是非常必要的。根据query从知识库中检索筛选出相关的facts $ (k_{h_1},v_{h_1}),(k_{h_2},v_{h_2}),…,(k_{h_N},v_{h_N})$，筛选条件可以是key中至少包含query中一个相同的词（去除停用词）。这一步可以在数据预处理时进行，直接将query和相关的facts作为模型的输入。 key addressing(寻址阶段) 计算query与memory的相关程度来分配在memory上的概率分布：$$p_{h_i} = softmax(A\\Phi_{X}(x) \\cdot A\\Phi_{K}(k_{h_i}))$$其中$\\Phi$将文本编码为D维向量，A是一个$d\\times D$的可训练矩阵。 value reading： 将value的加权求和作为memory总的向量表示。$$o = \\sum_{i}p_{h_i}A\\Phi_{V}(v_{h_i})$$ memory的读取过程是由controller神经网络通过query $q = A\\Phi_{X}(x)$来控制的。模型会利用query $q$与上一跳(hop)的$o$来更新query，进而迭代地寻址和读取memory，这个迭代的过程称为多跳(hops)。用多跳方式来迭代地寻址和读取memory，可以这样来理解：浅层神经网络可以学习到低级的特征，随着神经网络层数增多就可以学习到更高级的特征。类比CNN处理人脸图片时，第一层可以学习到一些边缘特征，第二层可以学习到眼睛、鼻子、嘴巴这样的特征，最后一层得到整个人脸的特征。同样地，用多跳方式来寻址和读取memory，可以得到更相关更突出的memory，同时可以起到推理的作用。 query的更新公式为:$$q_2 = R_1(q + o)$$其中R是一个$d\\times d$的可训练矩阵。每一跳使用不同的矩阵$R_j$。则第j跳更新query后，寻址阶段的计算公式为$$p_{h_i} = softmax(q_{j+1}^\\top \\cdot A\\Phi_{K}(k_{h_i}))$$在经过H跳之后，用controller神经网络的最终状态进行预测:$$\\hat{a} = argmax_{i=1,…,C}softmax(q_{H+1}B\\Phi_{y}(y_i))$$其中B是一个$d\\times D$的可训练矩阵，形状跟A一样。$y_i$可以是知识库中的实体，或者候选句子。 模型的目标函数为预测回复$\\hat{a}$与真实回复$a$之间的交叉熵，用梯度下降的方法来更新模型参数：$A,B,R_1,…,R_H$ Fig.7. 问答系统key-value memory networks的模型框架来源:Alexander Miller2016 key-value的选择与编码方式论文根据不同形式的先验知识，提出了key-value不同的编码方式： 知识库三元组。三元组形式为”subject-relation-object”，将”subject-relation”作为寻址的key，将”object”作为记忆的value。 sentence level。直接将句子的词袋向量表示作为key和value，key和value是一样的。每个memory slot存一个句子。 window level。以大小为W的窗口对文档进行分割（只保留中心词为实体的窗口），将单个窗口内的词作为寻址的key，将窗口的中心词作为value。 参考链接 记忆网络-Memory Network Memory network (MemNN) &amp; End to end memory network (MemN2N), Dynamic memory network Memory Networks for Language Understanding, ICML Tutorial 2016","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"Neural Turing Machines","slug":"Neural-Turing-Machines","permalink":"http://yoursite.com/tags/Neural-Turing-Machines/"},{"name":"Memory Network","slug":"Memory-Network","permalink":"http://yoursite.com/tags/Memory-Network/"}]},{"title":"attention? attention!","slug":"attention-attention","date":"2019-07-23T08:20:18.000Z","updated":"2019-07-30T03:41:02.000Z","comments":true,"path":"2019/07/23/attention-attention/","link":"","permalink":"http://yoursite.com/2019/07/23/attention-attention/","excerpt":"读了博主Weng, Lilian的文章attention? attention!，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。","text":"读了博主Weng, Lilian的文章attention? attention!，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。 why we need attention?从seq2seq模型谈起seq2seq模型与14年提出(Sutskever, et al. 2014)，实现输入序列(source sequence)到输出序列(target sequence)的映射，这两个序列的长度都是可变的。序列到序列映射的任务包括机器翻译、问答系统、对话系统、摘要生成等。 用数学语言来定义序列到序列的任务，给定输入序列(source sequence) $X = \\lbrace{ x_1,x_2,…,x_n \\rbrace}$，需要生成输出序列(target sequence) $Y = \\lbrace{ y_1,y_2,…,y_m \\rbrace}$，其中source sequence长度为$n$,target sequence长度为$m$。 seq2seq模型基于encoder-decoder框架，包括2个部分： encoder将source sequence编码（映射）为一个固定维度的向量表示(context vector,或称为sentence embedding)，我们希望这个向量表示可以很好的表示source sequence的意思。 encoder可以采用卷积神经网络CNN，也可以采用循环神经网络RNN，但用的更多的效果也更好的还是RNN。通常使用LSTM 或 GRU。 encoder RNN的隐藏状态更新公式为：$$\\begin{gather}h_t = f(h_{t-1},x_t)\\end{gather}$$其中$h_t$为RNN在时间步t的隐藏状态，f为LSTM 或GRU. 对于长度为n的source sequence，一个词接一个词地输入RNN后，可以得到n个隐藏状态$(h_1,h_2,…,h_n)$，通常将最后一个时间步最后一个词对应的隐藏状态$h_t$作为source sequence的向量表示，也就是context vector，记为$c$。 decoder根据source sequence的向量表示context vector，来一个词一个词的生成target sequence。 decoder采用单向RNN，decoder RNN隐藏状态的更新公式为:$$\\begin{gather}s_t = f(s_{t-1},y_{t-1},c)\\end{gather}$$其中$s_t$为decoder在时间步t的隐藏状态，$y_{t-1}$为target sequence中的上一个词，在train阶段，$y_{n-1}$为真实target sequence中的上一个词，在infer阶段，$y_{t-1}$为预测输出的上一个词；c为context vector。 时间步t，隐藏状态$s_t$再经过线性层和softmax得到在词表上的概率分布，将概率最大的词作为prediction word $y_t$。迭代循环直到输出整个target sequence。 Fig.1. seq2seq模型的框架图 我们可以看到当生成不同的$y_t$时，所依据的context vector都是固定不变的。固定的context vector有一个缺点是：当encoder编码完整个source sequence时，会偏向于最近的词，而遗忘了距离更远的最开始的一些词。(Bahdanau et al., 2015)提出了attention机制来解决这个问题。 attention机制:born for Translationattention机制最先在机器翻译(neural machine translation,NMT)任务上提出。从解决长期依赖问题的角度，attention可以实现长距离的记忆；从注意力的角度，attention机制可以实现对齐(alignment)，用更多的注意力关注到相关的部分，而忽略或低注意力关注到不相关的部分。 上文中提到，在生成不同的$y_t$时，直接将encoder最后一个时间步的隐藏状态$h_n$作为固定context vector。不同于这种方法，attention机制将所有encoder隐藏状态$\\lbrace{ h_1,h_2,…,h_n }\\rbrace$的加权和作为context vector，这样在每个时间步t生成$y_t$时，所依据的context vector都是专门针对于$y_t$的。一方面，context vector可以获取到所有隐藏状态，也就是整个source sequence的信息，这样就可以实现长距离的记忆。另一方面，source sequence 与target sequence之间的语义对齐(aligenment)是也是通过context vector实现的。在计算时间步t生成$y_t$对应的context vector $c_t$的计算需要三个部分的信息： 所有的encoder隐藏状态： $\\lbrace{ h_1,h_2,…,h_n }\\rbrace$ 上个时间步t-1的decoder 隐藏状态： $s_{t-1}$ source与target之间的alignment. Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf) attention的数学定义在计算时间步t生成$y_t$对应的context vector $c_t$时，encoder的所有隐藏状态为 $\\lbrace{ h_1,h_2,…,h_n }\\rbrace$，时间步t-1的decoder隐藏状态为 $s_{t-1}$，decoder RNN的隐藏状态更新公式变为：$$\\begin{gather}s_t = f(s_{t-1},y_{t-1},c_t)\\end{gather}$$ context vector $c_t$为encoder hidden state的加权和：$$c_t = \\sum_{i=1}^{n}\\alpha_{t,i}h_i$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i}) = \\frac{exp(\\beta_{t,i})}{\\sum_{j = 1}^{n}exp(\\beta_{t,j})}$$ $$\\beta_{t,i} = score(s_{t-1},h_i)$$其中权重$\\alpha_{t,i}$是时间步t生成$y_t$与隐藏状态$h_i$之间的score，从某种意义上说，$h_i$可以看作是$x_i$的表示，也可以看作是$\\lbrace{x_1,x_2,…,x_{i}}\\rbrace$的表示。因此，$\\alpha_{t,i}$可以看作是$y_t$与$x_i$之间联系（相关性）的score。所有权重$\\lbrace{\\alpha_{t,1},\\alpha_{t,2},…,\\alpha_{t,n}}\\rbrace$衡量了生成$y_t$时应该如何关注到所有的encoder hidden state。 score()为打分函数，有多种计算方法，下文会详细介绍。在Bahdanau et al., 2015.中，score()采用前馈神经网络，采用非线性激活函数$tanh()$,score()的数学形式为：$$score(s_{t},h_i) = v_a^\\top tanh(W_a[s_t;h_i])$$其中$v_a,W_a$是可训练参数。attention权重可视化矩阵很直观地表明了source words与target words之间的关联关系: Fig.3.来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf) 各种attention机制汇总下表总结了使用比较广泛的attention机制，及其对应的alignment score function。 名字 alignment score funtion 来源 content-based attention $score(s_t,h_i) = cosine(s_t,h_i)$ Graves2014 concat/additive $score(s_{t},h_i) = v_a^\\top tanh(W_a[s_t;h_i])$ Bahdanau2015 location-based $\\alpha_{t,i} = softmax(W_as_t)$将alignment简化为只依赖于target position Luong2015 general $score(s_{t},h_i) = s_t^\\top W_ah_i$ Luong2015 dot-product $score(s_{t},h_i) = s_t^\\top h_i$note:当general attention的$W_a$为单位矩阵时，就退出为dot-product attention Luong2015 scaled dot-product(*) $score(s_{t},h_i) = \\frac{s_t^\\top h_i}{\\sqrt{n}}$note:跟dot-product attention很像，n是encoder hidden state $h_i$的维度 Vaswani2017 (*)scaled dot-product attention机制添加了比例因子$/frac{1}{/sqrt{n}}$，动机是：对于softmax()函数，当输入很大时，对应的梯度很小（梯度逐渐消失），难以进行高效的优化和学习。因此，添加比例因子可以减小$score(s_t,h_i)$。 下表列出了更广范畴上的attention机制。 名字 定义 来源 self attention(&) 将input sequence的不同部分联系起来，只用到input sequence本身，而不用target sequence。可以使用上表中的所有score function，只要将target sequence替换为input sequence即可。 Cheng2016 global/soft attention context vector是整个input sequence的加权和，注意到整个input sequence Xu2015 local/hard attention context vector是局部input sequence的加权和，注意到局部input sequence Xu2015，Luong2015 (&)self-attention在一些论文中也被称为intra-attention. self-attentionself-attention,最先在Cheng2016提出称为”intra-attention”，后来在大作attention is all you need中发挥了更大的影响力。self-attention将同一个sequence的不同位置的tokens联系起来，建模tokens之间的关系，计算这个sequence的向量表示。[Cheng2016]提出self-attention的动机是什么呢？ 我们先看以下LSTM的局限。LSTM在编码sequence的向量表示时，隐藏状态更新公式为：$$h_t = f(h_{t-1},x_t)$$从这个更新公式可以看到：在给定$h_t$的条件下，$h_{t+1}$与之前的状态$\\lbrace{h_1,h_2,…,h_{t-1}}\\rbrace$及之前的tokens $\\lbrace{x_1,x_2,…,x_t}\\rbrace$是条件独立的。LSTM的潜在假设是当前状态$h_t$包含了之前所有tokens的信息，这相当于假设LSTM有无限大的memory，这个假设实际上是不成立的。实际上LSTM会偏向于更近的tokens，而逐渐遗忘距离更远的tokens。另一方面，LSTM在编码token的隐藏状态时，没有建模tokens之间的关系。而这恰恰就是self-attention要解决的问题，也就是self-attention的核心思想：在计算sequence的向量表示时，引入tokens之间的关系。 接下来看self-attention的数学表示。对于sequence $\\lbrace{x_1,x_2,…,x_n}\\rbrace$，每个token $x_t$分别对应一个hidden vector 和memory vector。当前的memory tape $C_{t-1} = \\lbrace{c_1,c_2,…,c_{t-1}}\\rbrace$，hidden state tape为$H_{t-1} = \\lbrace{h_1,h_2,…,h_{t-1}}\\rbrace$。self-attention计算$x_t$与$\\lbrace{x_1,x_2,…,x_{t-1}}\\rbrace$之间的关系：$$\\beta_{t,i} = score(x_t,h_i) = v^\\top tanh(W_hh_i,W_xx_t,W_{\\tilde{h}}\\tilde{h_{t-1}})$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i}) ;i\\in[1,t-1]$$ attention权重$\\alpha_{t,i}$是t时间步x_t在之前的tokens $\\lbrace{x_1,x_2,…,x_{t-1}}\\rbrace$对应的hidden vector上的概率分布。 Fig.4.红色表示当前token，蓝色的深浅表示相关程度。来源:[Cheng2016](https://arxiv.org/pdf/1601.06733.pdf) 比较一下self-attention机制与传统attention机制的区别： 传统的attention机制是将target sequence与source sequence联系起来，attention权重$\\lbrace{\\alpha_{t,1},\\alpha_{t,2},…,\\alpha_{t,n}}\\rbrace$是在encoder hidden states $\\lbrace{h_1,h_2,…,h_n}\\rbrace$上的概率分布。而self-attention是将同个sequence不同位置的tokens联系起来，attention权重$\\alpha_{t,i}$是t时间步$x_t$在之前的tokens $\\lbrace{x_1,x_2,…,x_{t-1}}\\rbrace$对应的hidden vector上的概率分布。 传统的attention机制常与RNN联合使用，在transformer中self-attention可以与RNN解耦开（也就是分开使用），单独用self-attention也可以编码sequence的表示向量。 soft vs hard attentionShow, Attend and Tell,Kelvin Xu2015将attention机制用到了”给图片生成描述”的任务，第一次明确区分了hard attention与soft attention，区分的依据是attention是关注到整张图片，还是图片的局部。 soft attention：attention关注到整张图片，或者是整个序列。alignment 权重$\\alpha_{t,i}$是在整个序列上的概率分布。就像普通的attention一样。 好处：模型是可微的。 坏处：计算量比较大。 hard attention：attention关注到图片的局部，或者是序列的一部分。 好处：减少了计算量。 坏处：模型不可微，需要用更复杂的技术，比如强化学习或者方差缩减来训练模型。Luong2015 global vs local attentionLuong2015在NMT任务上提出了global 和local attention的概念。区分的依据是attention是关注到整个序列，还是关注到序列的一部分。 global attention。 类似于soft attention，关注到整个序列。这里比较下Luong2015的global attention与Bahdanau2015中attention的区别。 Bahdanau2015中attention的计算路径是：$s_{t-1} \\to \\alpha_{t} \\to c_t \\to s_t$$$\\beta_{t,i} = score(s_{t-1},h_i)$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$c_t = \\sum_{i = 1}^{n}\\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = g(y_{t-1},c_t,s_t)$$ Luong2015的global attention的计算路径是：$s_t \\to \\alpha_{t} \\to c_t \\to \\tilde{s_t}$$$\\beta_{t,i} = score(s_{t},h_i)$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$c_t = \\sum_{i = 1}^{n}\\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$\\tilde{s_t} = tanh(W_c[c_t,s_t])$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = softmax(W_s\\tilde{s_t})$$ local attention。是soft 与hard attention的结合，关注到序列的一部分。对hard attention进行改进，使得模型可微，训练和计算变得更容易。改进的方法如下： 对于时间步t的target token $y_t$先用模型预测，生成一个对齐的位置$p_t$， 再根据固定窗口大小内$[p_t - D,p_t + D]$的encoder hidden state来计算context vector。D是窗口大小，是按经验定义好的。 Fig.5. global and local attenion.来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf) pointer network对于输出序列的类别数依赖于输入序列的长度的问题，seq2seq模型或神经图灵机不能解决。因为这类问题中，输出的类别数是可变的，而seq2seq模型的decoder只能在固定数目的类别上生成一个概率分布。Vinyals2017提出了pointer network（Pr_Net）来解决输出词表可变的问题。pointer network实际上是以attention为基础的。 我们比较下attention机制与pointer network的区别。记输入序列$X = \\lbrace{x_1,…,x_n}\\rbrace$,输出序列$Y = {y_1,…,y_m}$，$y_j$是X的位置索引，$y_i \\in [1,n] $。encoder的所有hidden state为$\\lbrace{h_1,h_2,…,h_n}\\rbrace$，decoder在时间步t的隐藏状态为$s_t$，则： attention机制用alignment权重来计算context vector： $$\\beta_{t,i} = score(s_t,h_i) = v^\\top tanh(W_ss_t,W_hh_i); i \\in [1,n]$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$c_t = \\sum_{i=1}^{n}\\alpha_{t,i}h_i$$ pointer network则用alignment权重在作为在输入序列上的概率分布，将输入序列中的token直接复制到输出序列中： $$\\beta_{t,i} = score(s_t,h_i) = v^\\top tanh(W_ss_t,W_hh_i); i \\in [1,n]$$ $$p(y_i|y_{&lt; i},X) = softmax(\\beta_{t,i})$$ Fig.6.Pointer Network model来源:[Vinyals2017](https://arxiv.org/abs/1506.03134) pointer network解决OOV问题什么是OOV（out of vocabulary）问题？在序列（source sequence）到序列（target sequence）的映射问题（对话系统，问答系统）中，会根据训练集语料来构建词表，根据完成$word \\to index \\to embedding$的向量化表示。而在测试集的source sequence中难免会出现一些词表中没有的词，通常会将这些out of vocabulary的词映射到一个特定的字符”UNK”，而decoder在生成response时也可能生成”UNK”这个特殊字符。这就是OOV问题。 pointer network是解决OOV问题的有效方法。当source sequence中出现不在词表中的词时，pointer network可以直接将这个生词从输入序列复制到输出序列中。Abigail See2017就用了pointer network来解决OOV问题。 记时间步t decoder的隐藏状态为$s_t$,对应的context vector为$c_t$,alignment权重为$\\alpha_{t,i},i \\in [1,n]$ 在词汇表上的概率分布为:$p_{vocab} = softmax(W[s_t,c_t] + b)$ 在输入序列的概率分布为:$p_{ptr} = \\alpha_{t,i} = softmax(\\beta_{t_i})$ 选择开关为: $p_{gen} = sigmoid(W_ss_t + W_cc_t + W_xx_t + b)$为逻辑回归，取值为[0,1] 最终在extend vocabulary上的概率分布为:$p(w) = p_{gen}p_{vocab} + (1-p_{gen})p_{ptr}$.当$p_{gen}$为1时，从词汇表中生成word；当$p_{gen}$为0时，将输入序列的词复制到输出序列中。 Fig.7.Pointer-generator model.来源：[Abigail See2017](https://arxiv.org/abs/1704.04368) 类似的论文还有：CopyNet,Jiatao Gu2016 transformerattention is all you need!(Vaswani, et al., 2017)提出了transformer。transformer也是基于encoder-decoder框架的，也可以看作是一个seq2seq模型。但不同于encoder和decoder都采用RNN的seq2seq模型，transformer完全依赖self-attention机制来计算input和output的向量表示，而不使用RNN或CNN。一般来说，attention机制是与RNN联合使用的，transformer把attention和RNN解耦开了，只使用attention机制。 为什么transformer用self-attention来编码input和output，而不使用RNN呢？一方面，由RNN的更新公式$s_t = f(s_{t-1},x_t)$可以看到，RNN处理序列时是串行计算的，尤其是处理长序列时更费时间。不利于并行化，计算效率低。而transformer采用attention来编码计算向量，可以进行并行化计算，提高计算效率。另一方面，RNN在编码长序列时，随着距离的增大，往往会偏向于最近的部分，而学习不到长期依赖。但self-attention机制不受距离的限制，可以有效地学习到长期依赖。 scaled dot-product attention与key,value,querytransformer的主要组件是multi-heads self-attenion mechanism，这个组件用到了scaled dot-product attention机制。一般地，attention机制将query和(key,value)映射为output，其中query,key,value,output都是vector。output是所有values的加权和，权重是通过计算query与对应key之间的关联度得到。 具体来说，将什么作为key,value,query？分两种情况，从框图可以直观的看到： 在encoder-decoder框架中，联系encoder与decoder的attention机制通常将encoder的所有hidden states乘以两个不同的矩阵$W^Q,W^K$分别作为keys和values。将decoder上一个时间步的hidden state作为query。 在encoder模块，self-attention机制将input词级别的向量表示乘以三个不同的矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算input总的句子级别的向量表示。同样地，在decoder模块中self-attention机制将output词级别的向量表示分别乘以三个不同矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算output总的句子级别的向量表示。 有了具体的key,value,query后，scaled dot-product attention机制怎么来计算output呢？记query,key,value的矩阵形式分别为Q,K,V。query和key维度为$d_k$,value维度为$d_v$。则output的计算方式为：$$Attention(Q,K,V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$$ 多种attention机制中，transformer为什么选择采用scale dot-product attention机制呢？最常用的两种attention机制是dot-product和additive attention机制。dot-product attention用点乘来做打分函数，additive attenion将有一层隐藏层的前馈网络作为打分函数。理论上来说，这两种attention的计算复杂度是一样的；但实际上，dot-product attention计算更快，占用内存更小。因为dot-product attention机制可以采用高度优化的矩阵乘法代码。当维度$d_k$较小时，这两种attention机制的效果是差不多的。当维度$d_k$更大时，additive attention的效果要好于dot-product attention。这可能是因为当维度$d_k$变大时，点乘的值变得过大，而softmax()函数在值过大的范围梯度是很小的，类似于梯度消失问题。因此，添加比例因子$\\frac{1}{\\sqrt{d_k}}$来减小点乘的值。 multi-head attention并不是只用一次attention机制，将维度为$d_{model}$的key,value,query映射为output。而是将query,key,value映射到维度为$d_k,d_k,d_v$不同的子向量空间，并行的计算$h$次，分别得到output做concat操作，得到总的output。$h$为head的个数，也就是并行attention layer的层数。有关系$d_{model} = d_v \\cdot h$，论文中采用$d_{model} =512,h=8,d_k = d_v = \\frac{d_{model}}{h} = 64$ $$MultiHead(Q,K,V) = concat(head_1,head_2,…,head_h)W^O$$ $$where \\quad head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$ 其中$W_i^Q \\in R^{d_{model} \\cdot d_k},W_i^K \\in R^{d_{model} \\cdot d_k},W_i^V \\in R^{d_{model} \\cdot d_v},W^O \\in R^{hd_{v} \\cdot d_{model}}$ Fig.7.multi-head scaled dot-product attention来源:Vaswani, et al., 2017 encoderencoder将input text编码为基于attention的包含位置信息的向量表示。 由6个完全相同的层堆叠起来。 每一层包含两个子层。第一子层是multi-head attention层，第二层是一个简单的全连接层。 两个子层之间采用残差连接，并进行归一化。这样所有子层的输出都有相同的维度$d_{model} = 512$ Fig.9. transformer encoder来源:Vaswani, et al., 2017 decoder从encoder output得到总的context vector，并据此生成response。 与encoder相同，由6个完全相同的层堆叠起来。 每一层除了encoder中的两个子层外，还插入了一个multi-head layer来在所有encoder output上进行attention操作。 两个子层之间采用残差连接，并进行归一化。 第一个multi-head attention sub-layer进行mask操作，mask掉output当前时间步后所有的tokens。防止attention机制看到未来的信息。 Fig.10. transformer decoder来源:Vaswani, et al., 2017 transformer的总体结构 input和output都先经过一个embedding layer得到各自的向量表示，维度为$d_{model} = 512$ 由于self-attention不能像RNN一样自动地编码位置信息，因此需要额外地将位置信息添加到输入。 在最后decoder的输出外接一个线性层和softmax层。 Fig.11. transformer的整体框架来源:Vaswani, et al., 2017 参考链接 Attention? Attention! by Weng, Lilian","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"rnn","slug":"rnn","permalink":"http://yoursite.com/tags/rnn/"},{"name":"attention","slug":"attention","permalink":"http://yoursite.com/tags/attention/"}]},{"title":"pip安装python模块报错","slug":"pip安装python模块报错","date":"2019-07-12T01:51:58.000Z","updated":"2019-07-12T02:11:58.000Z","comments":true,"path":"2019/07/12/pip安装python模块报错/","link":"","permalink":"http://yoursite.com/2019/07/12/pip安装python模块报错/","excerpt":"在使用pip install命令安装python模块时，报错： 1Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.","text":"在使用pip install命令安装python模块时，报错： 1Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. 错误分析报错信息告诉我们：“不能卸载‘pyyaml’模块，因为这个模块是distutils方式安装的，不能确定哪些文件属于这个模块，因此不能完整地卸载这个模块。” distutils是python最初的模块安装和分发系统，distutils不会保留哪些文件属于哪个安装包的信息，甚至不会保留安装包之间的依赖关系。直接使用distutils的方式已经被淘汰，取而代之的是setuptools. 所谓模块的分发，就是开发者打包并发布自己的模块，供其他人使用。 这样我们就知道了，因为pyyaml模块时通过distutils方式安装的，因此不能明确文件与包之间的隶属关系，不能正确卸载。 解决办法使用下面的命令忽略已安装的模块，强制安装和更新 1pip3 install &lt;package-name&gt; --ignore-installed &lt;pyyaml&gt; --upgrade 参考链接 强制安装和更新 如何在Windows操作系统中升级/卸载distutils软件包（PyYAML）？ python官方手册-安装python模块 setuptools与distutils","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"NAACL2019-对话系统","slug":"NAACL2019-对话系统","date":"2019-07-10T12:55:17.000Z","updated":"2019-07-21T15:03:16.000Z","comments":true,"path":"2019/07/10/NAACL2019-对话系统/","link":"","permalink":"http://yoursite.com/2019/07/10/NAACL2019-对话系统/","excerpt":"记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。NAACL2019的会议列表链接：https://naacl2019.org/program/accepted/","text":"记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。NAACL2019的会议列表链接：https://naacl2019.org/program/accepted/ 《Evaluating Coherence in Dialogue Systems using Entailment》【链接】https://arxiv.org/abs/1904.03371【代码】https://github.com/nouhadziri/DialogEntailment 加拿大阿尔伯塔大学发表的论文。论文提出了一种评估对话系统生成回复好坏的指标。这篇论文的想法来源于：发表在ACL2019上的论文《Dialogue Natural Language Inference》提出利用NLI(natural language inference)任务来提高对话系统生成回复的一致性。本文的作者则想到用NLI任务来评估对话系统生成回复的好坏。具体地，论文用了BERT[Devlin et al., 2018]和The Enhanced Sequential Inference Model(ESIM)[Chen et al., 2016] 这两种方法来训练NLI模型。另外论文还公开了一个用于NLI任务的数据集。","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"},{"name":"NAACL2019","slug":"NAACL2019","permalink":"http://yoursite.com/tags/NAACL2019/"}]},{"title":"ACL2019-对话系统","slug":"ACL2019-对话系统","date":"2019-07-07T11:19:27.000Z","updated":"2019-07-31T03:40:18.000Z","comments":true,"path":"2019/07/07/ACL2019-对话系统/","link":"","permalink":"http://yoursite.com/2019/07/07/ACL2019-对话系统/","excerpt":"记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml","text":"记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml 《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》【链接】：https://arxiv.org/abs/1906.01788【源码】：无 中科院自动化所发表的短论文。在多轮对话中，对话历史（context information）对回复（response）的生成有重要作用。任务型对话中的管道模型分为4个模块：NLU、对话状态追踪、对话策略学习 及NLG。对话状态追踪又包含任务：domain classification、intent detection和slot filling。domain classification和intent detection任务当做分类任务来处理，常采用SVM或深度神经网络的方法；slot filling任务被当做序列标注任务来处理，常采用BiLSTM+CRF模型。NLU能否充分利用context information，对这三个下游任务有很大影响。为了更好的利用context information，本文提出了对话逻辑推断任务（DLI,dialog logic inference），任务定义为：将打乱顺序的多轮对话重新排序；输入之前的对话，从剩余的utterance candidates中选中下一句对话。NLU任务采用了所谓的memory network，其实就是采用多个encoder对context information进行编码，再用attention机制或别的方法得到context information总的向量化表示。本文联合训练DLI任务和NLU任务，通过两个任务共享encoder和memory retrieve模块，来让NLU任务更好地利用context information。其实是得到context information更合理的向量化表示，来作为下游domain classification、intent detection和slot filling任务的输入。 论文提出的将打乱顺序的对话重新排序的DLI任务，可以进一步深入，将句子切分为几段打乱顺序再重新排序；可以应用到闲聊式对话系统中。 《Dialogue Natural Language Inference》【链接】：https://arxiv.org/abs/1811.00671【代码】：无【数据集】：https://wellecks.github.io/dialogue_nli/ 加利福尼亚大学、Facebook AI Lab发表的论文。核心是提出用NLI(natural language inference)任务来提高persona-based dialog system的一致性。这里就要先搞清楚NLI任务和一致性问题两个概念。 先从问题出发，所谓对话的一致性问题。可以分为两类： logical contradiction，逻辑矛盾。比如同一个人的两句话:”我有一只狗”，”我没养过狗”。就是逻辑矛盾的。 比较模糊的非逻辑矛盾。同一个人不可能说出的两句话：“我从来不运动”，“我去篮球了”。就是这种非逻辑矛盾。真香警告。 至于persona一致性问题，就是回复的utterance不能与说话人的persona矛盾，也不能与之前的回复有矛盾。 具体介绍NLI任务。这其实是一个分类问题。论文公开了一个自己标注的NLI数据集。 训练阶段：训练集形式是 {$（s_1,s_2）$,label }，对应labels $\\in$（一致、无关、矛盾）。 在test阶段，给定一个句子对（句子1，句子2）来判断对应的label。 论文的最终目的是通过NLI任务训练的模型来提高persona dialog system的一致性。这是如何来实现的呢？对于一个dialog system，给定对话历史$（u_1,u_2,…,u_t）$ 及说话人的persona文本描述$（p_1,p_2,…,p_n）$,从response candidates$（y_1,y_2,…,y_m）$中选择一个$u_{t+1}$（如何生成多个responses不是这篇论文要解决的）。用NLI任务的模型来预测$(y_i,u_j),(y_i,p_k)其中：i\\in [1,m],j\\in [1,t],k \\in [1,n]$对应的label，如果句子之间是矛盾的，则添加惩罚项。从而得到一致性最好的utterance作为response。 《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》【链接】：https://arxiv.org/abs/1907.05339【数据集】：English Ubuntu dialogue corpus【代码】：https://github.com/zhanghainan/ReCoSa 中科院发表的论文。在多轮对话中，生成response时，对话历史中最相关的部分起着重要的作用。论文要解决的问题：如何更准确地找到并利用relevant context来生成response。多轮对话中广泛使用的HRED模型,[(Serban et al.,2016;,Sordoni et al., 2015]无差别地利用context information，忽略了relevant context。虽然有利用relevant context的相关工作，但这些工作都有各自的问题。[Tian et al., 2017]提出计算context 与post之间的cosine similarity来衡量context relevance，其假设是context与response之间的relevance等价于post与response之间的relevance，这个假设是站不住脚的。[Xing et al., 2018]向HRED模型引入了attention机制，但attention机制定位relevant context时会产生偏差，因为基于RNN的attention机制倾向于最靠近的context（close context）。论文提出了自己的解决办法，用self-attention机制来衡量context于response之间的relevance。self-attention机制的优点是可以有效捕捉到长距离的依赖关系。 模型分为三个部分：context包含N轮对话： ${s_1,s_2,…,s_N}$其中，$s_i = {x_1,x_2,…,x_M}$，M为句子长度。response为$Y = {y_1,y_2,…,y_M}$ context representation encoder： 将context encode为vector。 word-level encoder： 用LSTM对sentence编码，将LSTM最后一个时间步的hidden state作为sentence representation: $h^{s_i}$； 由于self-attention机制不能区分word位置信息，还需要添加position embedding: $p^{s_i}$, 把两个向量做concatenate操作，得到总得sentence representation:$(h^{s_i},p^{s_i})$。 对于context中的N个句子有${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$ context self-attention: 采用multi-head self-attention机制，将${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$经过不同的线性变换作为query、keys、values matrix,由N个sentence representation得到总的context representation $O_s$。 response representation encoder 同样用multi-head self-attention机制,将response的word embedding及position embedding ${(w_1,p_1),…,(w_{t-1},p_{t-1})}$经过不同的线性变换作为query、keys、values matrix，得到response representation $O_r$。 在train阶段 采用mask操作，在时间步t对于word $y_t$，mask掉${y_t,y_{t+1},…,y_M}$，只保留${y_1,y_2,…,y_{t-1}}$来计算response representation。 在infer阶段 在生成response的时间步t，将生成的response ${g_1,…,g_{t-1}}$，来作为response representation。 context-response attention decoder 采用multi-head self-attention机制，将context attention representation $O_s$作为keys、values matrix，将response hidden representation $O_r$作为query matrix。得到输出$O_d$. 模型框架图 《Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good》【链接】：https://arxiv.org/abs/1906.06725【代码、数据集】：https://gitlab.com/ucdavisnlp/persuasionforgood/tree/master 浙江大学、加利福尼亚大学发表。获得ACL2019 best paper提名。论文的主要贡献是公开了一个包含说话人个人信息的劝说数据集，在子集上标注了十种不同的劝说策略。并训练了用于分类不同劝说策略的分类器。 《Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study》【链接】：https://arxiv.org/abs/1906.01603【代码】：https://github.com/chinnadhurai/ParlAI/ 论文获得ACL2019 best short paper提名。论文的研究点是：生成式对话系统是否有效利用或正确理解了对话历史？论文通过向对话历史中引入不同类型的扰动，来研究生成式对话系统生成回复的困惑度变化。这个方法的一个前提是如果生成式对话系统对某种信息的扰动不敏感，那么它没有有效利用这种信息。 论文在比较了三种模型。 基于LSTM的seq2seq模型。 基于LSTM的seq2seq模型 + attention机制。 基于transformer的seq2seq模型。 论文在四个多轮对话数据集上进行实验。 bAbI dialog。(Bordes and Weston, 2016) Persona Chat。(Zhang et al., 2018) Dailydialog。 (Li et al., 2017) MutualFriends。(He et al., 2017) 论文向对话历史引入了不同的扰动。 句子级别的扰动： 随机打乱对话历史中句子的顺序。 倒序对话历史中句子的顺序。 随机去掉对话历史中特定的句子。 对话历史中有n个句子，只保留最近的k个句子$(k \\le n)$。 词级别的扰动： 随机打乱一个句子中词的顺序。 倒序一个句子中词的顺序。 随机去掉对话历史中30%的词。 去掉所有的名词。 去掉所有的动词。","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"ACL2019","slug":"ACL2019","permalink":"http://yoursite.com/tags/ACL2019/"},{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"}]},{"title":"自然语言处理---会议列表","slug":"自然语言处理-会议列表","date":"2019-04-24T06:55:43.000Z","updated":"2019-07-07T09:00:06.000Z","comments":true,"path":"2019/04/24/自然语言处理-会议列表/","link":"","permalink":"http://yoursite.com/2019/04/24/自然语言处理-会议列表/","excerpt":"记录自然语言处理方向的国际会议列表。","text":"记录自然语言处理方向的国际会议列表。 A类会议AAAI，Association for the Advancement of Artificial Intelligence 其他ICLR，The International Conference on Learning Representations，国际学习表征会议2013年才刚刚成立了第一届。这个一年一度的会议已经被学术研究者们广泛认可，被认为「深度学习的顶级会议」。这个会议的来头不小，由位列深度学习三大巨头之二的 Yoshua Bengio 和 Yann LeCun 牵头创办。Yoshua Bengio 是蒙特利尔大学教授，深度学习三巨头之一，他领导蒙特利尔大学的人工智能实验室（MILA）进行 AI 技术的学术研究。MILA 是世界上最大的人工智能研究中心之一，与谷歌也有着密切的合作。而 Yann LeCun 就自不用提，同为深度学习三巨头之一的他现任 Facebook 人工智能研究院（FAIR）院长、纽约大学教授。作为卷积神经网络之父，他为深度学习的发展和创新作出了重要贡献。 参考链接 中国计算机学会推荐国际学术会议和期刊目录","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"会议列表","slug":"会议列表","permalink":"http://yoursite.com/tags/会议列表/"}]},{"title":"python读写csv文件","slug":"python读写csv文件","date":"2019-04-19T10:32:22.000Z","updated":"2019-07-21T15:22:55.000Z","comments":true,"path":"2019/04/19/python读写csv文件/","link":"","permalink":"http://yoursite.com/2019/04/19/python读写csv文件/","excerpt":"介绍csv文件的读写。","text":"介绍csv文件的读写。 csv模块csv.writer(csvfile)12345678910import csv row = [&apos;Symbol&apos;,&apos;Price&apos;,&apos;Date&apos;,&apos;Time&apos;,&apos;Change&apos;,&apos;Volume&apos;]rows = [(&apos;AA&apos;, 39.48, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.18, 181800), (&apos;AIG&apos;, 71.38, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.15, 195500), (&apos;AXP&apos;, 62.58, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.46, 935000), ]with open(&apos;name.csv&apos;,&apos;w&apos;) as csvfile: writer = csv.writer(csvfile,delimiter = &apos;\\t&apos;,lineterminator = &apos;\\n&apos;) #delimiter和lineterminator分别是分隔符，行结束符 writer.writerow(row) #写入单行 writer.writerows(rows) #写入多行 csv.reader(csvfile)该函数接收一个可迭代对象，返回对象reader是一个生成器，不能直接用下标访问。可以用for循环和next()函数访问。 12345with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile: reader = csv.reader(csvfile,delimiter = &apos;\\t&apos;) #迭代器 rows = [row for row in reader] #用for循环访问：for row in rows: print(row) 输出结果为： 如果要读取csv文件的某列，可以看下面的例子 1234with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile: reader = csv.reader(csvfile,delimiter = &apos;\\t&apos;) #迭代器 column = [row[2] for row in reader] #用for循环访问：print(column) 输出结果为： csv.DictReader(csvfile)与csv.reader()函数相同，接收一个可迭代对象，返回一个生成器。不同之处是，返回的每个单元格放在字典的值中，字典的键就是这个单元格的列头。 12345with open(&apos;./name.csv&apos;,&apos;r&apos;) as f: reader = csv.DictReader(f,delimiter = &apos;\\t&apos;) rows = [row for row in reader]for row in rows: print(rows) 输出结果为： 如果要读取csv文件的某列，可以看下面的例子: 1234with open(&apos;./name.csv&apos;,&apos;r&apos;) as f: reader = csv.DictReader(f,delimiter = &apos;\\t&apos;) column = [row[&apos;Time&apos;] for row in reader]print(column) 输出结果为： csv.DictWriter(csvfile)pandas读写csv也可以直接用pandas的函数read_csv()来读取csv文件的列。 1234import pandas as pdf = pd.read_csv(&apos;name.csv&apos;,delimiter = &apos;\\t&apos;)time = f.Timeprint(time) 输出结果为： 参考链接 python官方文档-csv模块 读写csv数据 使用python获取csv文本的某行或某列数据","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"csv","slug":"csv","permalink":"http://yoursite.com/tags/csv/"},{"name":"panda","slug":"panda","permalink":"http://yoursite.com/tags/panda/"}]},{"title":"torch.cuda.is_available()返回False,但nvidia-smi正常","slug":"torch-cuda-is-available-返回False-但nvidia-smi正常","date":"2019-04-16T09:04:44.000Z","updated":"2019-07-22T01:27:54.000Z","comments":true,"path":"2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/","link":"","permalink":"http://yoursite.com/2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/","excerpt":"torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。","text":"torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。 问题描述在pytorch用GPU来加速计算时发现。torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。 123&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()False 此时，nvidia-smi可以正常运行。 可能原因在nvidia-smi的运行结果中可以看到，driver version是390.xx。可能是driver version版本太低，造成了这个问题，实际上也是如此。driver version的常见版本是384.xx,390.xx,396.xx。接下来，把driver version升级到396.xx看能不能解决问题。 升级nvidia driver version 卸载旧版本的NVIDIA driver 1sudo apt-get remove --purge nvidia-\\* 添加NVIDIA的ppa源. 1sudo add-apt-repository ppa:graphics-drivers/ppa 安装新版本的NVIDIA driver 1sudo apt-get update &amp;&amp; sudo apt-get install nvidia-driver-396 此时，运行nvidia-smi，会报以下错误。 1Failed to initialize NVML: Driver/library version mismatch 这是更新NVIDIA driver版本后的常见问题。这个问题出现的原因是kernel mod的NVIDIA driver版本没有更新。执行以下命令可以查看nvidia kernel mod的version。 1cat /proc/driver/nvidia/version 可以看到已经加载的nvidia kernel mod的版本是还是旧版本390.xx。一般情况下，重启服务器就能解决问题。如果由于某些原因不能重启，可以重新加载kernel mod。思路是先unload kernel mod，再reload kernel mod. 详见解决Driver/library version mismatchnvidia-smi可以正常运行后，问题就解决了。 123&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True 参考链接 Torch.cuda.is_available() returns False, nvidia-smi is working How can I update the NVIDIA drivers to version 390.77?","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"nvidia-smi","slug":"nvidia-smi","permalink":"http://yoursite.com/tags/nvidia-smi/"},{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"},{"name":"cuda","slug":"cuda","permalink":"http://yoursite.com/tags/cuda/"}]},{"title":"nvidia-smi返回错误信息‘Failed to initialize NVML: Driver/library version mismatch’","slug":"nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’","date":"2019-03-29T11:47:03.000Z","updated":"2019-07-21T15:23:36.000Z","comments":true,"path":"2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/","link":"","permalink":"http://yoursite.com/2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/","excerpt":"Ubuntu运行命令nvidia-smi出错。","text":"Ubuntu运行命令nvidia-smi出错。 问题描述在Ubuntu18.04的命令行中运行命令nvidia-smi，返回错误信息 1Failed to initialize NVML: Driver/library version mismatch 方法1：重启解决大部分问题博客解决Driver/library version mismatch讲述的很清楚，这里就不再赘述。或者参考大型交友网站stack overflow的问题NVIDIA NVML Driver/library version mismatch 方法2：重装驱动看返回的错误信息，这个问题出现的原因是NVIDIA Driver的版本不匹配。如果重启不能解决问题，我们需要卸载重装NVIDIA driver。 查看驱动程序版本 dpkg -l | grep nvidia 可以看到驱动版本是390.116 cat /proc/driver/nvidia/version 这里NVRM的版本是390.87。错误信息就是这两个版本不匹配造成的。接下来先卸载NVIDIA driver，再重新安装。 卸载旧版本的NVIDIA driver 1sudo apt-get remove --purge nvidia-\\* 添加NVIDIA的ppa源 1sudo add-apt-repository ppa:graphics-drivers/ppa 重新安装NVIDIA的驱动 1sudo apt-get update &amp;&amp; sudo apt-get install nvidia-390 用你自己的版本号替换390。 这时再用cat /proc/driver/nvidia/version查看NVIDIA driver的驱动。 可以看到NVRM的版本是390.116，这时版本就匹配了。再次执行nvidia-smi,终于看到 在nvidia driver各版本总览可以看到NVIDIA driver的各个版本。 额外的：update 与 upgrade记录下sudo apt-get update与sudo apt-get upgrade的区别。在windows系统中安装软件，只需要有exe文件，双击即可安装了。Linux系统中则不同，Linux会维护一个自己的软件仓库，几乎所有软件都在这个仓库里，而且里面的软件完全安全，绝对可以安装。我们自己的Ubuntu服务器上，维护一个软件源列表文件/etc/apt/sources.list,里面都是一些网址信息，每个网址就是一个软件源，这个地址指向的数据标识着有哪些软件可以安装使用。 查看源列表： 1sudo vim /etc/apt/sources.list 更新软件列表 1sudo apt-get update 这个命令对访问源列表里的每个网址，读取软件列表，保存到本地电脑。 更新软件 1sudo apt-get upgrade 这个命令会把本地已安装的软件，与软件列表里对应软件做对比，如果有可更新版本就更新软件。总的来说，sudo apt-get update是更新软件列表，sudo apt-get upgrade是更新软件。 参考链接 解决Driver/library version mismatch Ubuntu配置GPU+CUDA+CAFFE ubuntu下安装安装CUDA、cuDNN和tensotflow-gpu版本流程和问题总结 NVIDIA的wiki （原）Ubuntu16中安装nvidia的显卡驱动","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://yoursite.com/tags/ubuntu/"},{"name":"nvidia-smi","slug":"nvidia-smi","permalink":"http://yoursite.com/tags/nvidia-smi/"}]},{"title":"条件随机场CRF","slug":"条件随机场CRF","date":"2019-03-23T05:50:20.000Z","updated":"2019-07-22T01:26:35.000Z","comments":true,"path":"2019/03/23/条件随机场CRF/","link":"","permalink":"http://yoursite.com/2019/03/23/条件随机场CRF/","excerpt":"最近学习了条件随机场CRF，做下总结。主要参考BiLSTM+CRF模型中的CRF层为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。","text":"最近学习了条件随机场CRF，做下总结。主要参考BiLSTM+CRF模型中的CRF层为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。 从马尔科夫随机场到线性链条件随机场 概率图模型 概率图模型是用图G = (V,E)来表示概率分布。设有联合概率分布P(Y)，Y是一组随机变量。我们可以用无向图G = (V,E)来表示联合概率分布P(Y),节点$v \\in V$表示随机变量$Y_v$，边$e \\in E$表示随机变量之间的概率依赖关系。 概率无向图模型，即马尔科夫随机场 设有联合概率分布P(Y),由无向图G=(V,E)表示。如果概率分布P(Y)满足成对、局部、全局马尔科夫性，那么称 这个联合概率分布p(Y)为概率无向图模型，或马尔科夫随机场(Markov random field)。 成对马尔科夫性、局部马尔可夫性、全部马尔科夫性要表达的就是：在无向图中，没有边连接的节点之间没有概率依赖关系，也就是没有边连接的节点代表的随机变量之间是条件独立的。 条件随机场 设X和Y是随机变量，P(Y|X)是给定X的条件下Y的条件概率分布。若给定X的条件下，Y构成一个马尔科夫随机场。则称条件概率分布P(Y|X)为条件随机场。 我们可以看到，马尔科夫随机场是联合概率分布P(Y),而条件随机场是条件概率分布P(Y|X)。这是一点不同。 线性链条件随机场 设$X =(X_1,X_2,…,X_n), Y =(Y_1,Y_2,…,Y_n)$是线性链表示的随机变量序列。若在给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：$$P(Y_i|X,Y_1,Y_2,…,Y_{i-1},Y_{i+1},…,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})$$ 。则称条件概率分布p(Y|X)为线性链条件随机场。 线性链条件随机场和隐马尔可夫模型都是序列模型，可以用于标注问题。这时，条件概率模型P(Y|X)中，X是输入变量序列，表示需要标注的观测序列；Y是输出变量，表示标记序列，或称状态序列。 BiLSTM+CRF模型BiLSTM+CRF模型是命名实体识别任务的常用模型。假设我们训练集中有个由五个词组成的句子$X = (w_0,w_1,w_2,w_3,w_4)$,对应标签为$Y = [B-Person，I-Person,O,B-Organization,O]$。数据集中有五类标签： 类别 B-Person I-Person B-Organization I-Organization O 含义 人名的开始部分 人名的中间部分 组织机构的开始部分 组织机构的中间部分 非实体信息 先简单介绍下BiLSTM+CRF模型的结构。LSTM层的输入一般为每个词的Word-embedding，输出为每个词word在类别空间tag_space上的非归一化概率，也就是在单词对应每个类别的得分score。这些score作为CRF层的输入。 CRF的损失函数条件随机场中有两个重要的矩阵，转移概率矩阵和状态概率矩阵，分别对应转移特征和状态特征。 状态概率矩阵。就是LSTM层的输出，作为CRF层的输入。矩阵的形状为[N,M],N为句子长度，M为可能状态数。 转移概率矩阵。矩阵形状为[M,M]，M为可能状态数。转移矩阵是模型参数，是随机初始化的，在训练过程中不断更新优化。 给定转移矩阵T，随机变量序列X取值为x的条件下，随机变量序列取值为y的似然函数为：$$Likelihood(y|x,T) = \\frac{ \\sum_{i=0}^{n} P(x_i|y_i)T(y_i|y_{i-1})}{\\sum_{y^}\\biggl(\\sum_{i=0}^{n}P(x_i|y_i^)T(y_i^|y_{i-1}^)\\biggr)} \\cdots\\cdots\\cdots\\cdots\\cdots (1)$$上式中, $P(x_i|y_i)$表示当前状态为$y_i$时，产生观测值$x_i$的概率。对应状态分数。 $T(y_i|y_{i-1})$表示从上一个状态$y_{i-1}$转换到当前状态$y_i$的概率。对应转移分数。我们可以从转移矩阵中读出这个概率。 分子表示了单条路径y=[y_0,y_1,…,y_n]的分数score或概率。 分母表示了所有可能路径$y^*$的总分。注意计算分母时，我们要计算所有可能路径并求和。若序列长度为N,状态可能数为M,则所有可能路径数为$M^N$,这个数量是指数级的，非常大。我们的秘密武器是前向后向算法来高效地计算分母。 进一步地，负对数似然函数为：$$NegLogLikelihood(y|x,T) = \\sum_{y^}\\biggl( \\sum_{i=0}^{n} log(P(x_i|y_i^)T(y^_i|y^{i-1}))\\biggr) - \\sum{i=0}^{n}log(P(x_i|y_i)T(y_i|y_{i-1})) \\cdots\\cdots\\cdots\\cdots (2)$$ 从式子(1)到式子(2)，直接对式子(1)取负对数是得到式子(2)可能比较令人费解。需要留意的是：转移概率矩阵和状态概率矩阵中的概率都是对数概率（这很重要），这样计算路径概率时都是加法。对对数概率加上exp()运算我们能得到正常概率。《统计学习方法》书中说，线性链条件随机场是对数线性模型，在对数空间中，对数概率可以直接相加，带来很大的方便。接下来，我们来看看：真实路径的分数和所有路径的总分是怎么计算的？ 真实路径分数数据集中有五类标签，再引入start和end作为序列开始和结束标志。 类别 B-Person I-Person B-Organization I-Organization O start end 索引 0 1 2 3 4 5 6 长度为5的序列，$X = (w_0,w_1,w_2,w_3,w_4)$,对应类别为$Y = [B-Person，I-Person,O,B-Organization,O]$，标注序列，也就是真实路径为为y = [0,1,4,3,4]。真实路径的分数由两部分组成，状态分数和转移分数。状态矩阵就是LSTM层的输出。转移矩阵是模型参数，为$$[t_{ij}],i,j\\in [0,6];i\\neq 6,j\\neq 5$$其中$t_ij$表示从上一状态转换到当前状态的概率。转移时，不能转移到start，不能从end转移。 则真实路径的分数 = 转移分数 + 状态分数 = $1.5+0.4+0.1+0.2+0.5+t_{51}+t_{01}+t_{14}+t_{42}+t_{24}+t_{46}$ 所有路径分数-前向后向算法计算所有路径的总分面对的难题是要不要穷举所有路径。对于一个长度为N的序列，可能状态数为M，所有可能路径数为$M^N$，这是一个指数级的计算量。计算每条路径分数的计算量是$O(N)$,直接用穷举法计算所有路径总分的计算量是$O(N\\cdot M^N)$。这个计算量是无法接受的。《统计学习方法》p176写，前向算法是基于“路径结构”递推计算所有路径分数。前向算法高效的关键是局部计算前向概率，再递推到全局。前向算法的计算量是$O(N\\cdot M^2)$，前向算法减少计算量的原因是：每一次递推计算直接利用了前一个时刻的计算结果，避免了重复计算。 对于$w_0 \\to w_1$的局部路径。先计算$w_0$所有状态到$w_1$单个状态0的分数之和，并更新$w_1$的状态0的状态分数。有M条局部路径，计算量是$O(M)$ 用同样的方法更新$w_1$所有状态的状态分数，这就是所有局部路径的分数。要计算M个状态，计算量是$O(M^2)$ 依次递推到全局。序列长度为N,总的计算量是$O(N \\cdot M^2)$ 从图的角度解释了前向算法，我们再从数学计算的角度来看前向算法。简化一下问题，假设句子长度为3，$X = [w_0,w_1,w_2]$,只有2个类别[1,2]我们引入两个变量previous和obs。previous存储前一时刻的计算结果，obs存储当前状态分数。对于$w_0$:$$obs = [x_{01},x_{02}];previous = none$$对于$w_0 \\to w_1:$,$$previous = [x_{01},x_{02}],obs = [x_{11},x_{12}]$$先扩展previous和obs：$$previous = \\begin{pmatrix} x_{01}&amp;x_{01}\\x_{02}&amp;x_{02} \\end{pmatrix} \\quad$$$$obs = \\begin{pmatrix} x_{11}&amp;x_{12}\\x_{11}&amp;x_{12} \\end{pmatrix} \\quad$$将previous和obs和转移矩阵相加：$$score =\\begin{pmatrix} x_{01}&amp;x_{01}\\x_{02}&amp;x_{02} \\end{pmatrix} +\\begin{pmatrix} x_{11}&amp;x_{12}\\x_{11}&amp;x_{12} \\end{pmatrix}+\\begin{pmatrix} t_{11}&amp;t_{12}\\t_{21}&amp;t_{22} \\end{pmatrix}$$$$ = \\begin{pmatrix} x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\\x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22} \\end{pmatrix}$$score同列相加，更新previous:$$previous = [x_{01}+x_{11}+t_{11}+x_{02}+x_{11}+t_{21},x_{01}+x_{12}+t_{12}+x_{02}+x_{12}+t_{22}]$$这样第二次迭代就完成了。用图来表示到目前为止的计算： 用同样的方法迭代递推，就可以得到所有路径的分数。 这样我们就计算出了负对数似然函数，也就是CRF模型的损失函数。条件随机场的第二个基本问题是学习问题，给定训练集估计条件随机场的模型参数（转移矩阵）。我们可以通过最小化对数似然函数来求参数模型。可以用梯度下降法来实现。 维特比算法解码条件随机场的第三个基本问题是预测问题，给出条件随机场的模型 和 输入序列x，求条件概率最大输出序列$y^*$。 也就是找出所有路径中得分最高的那条路径作为标注路径。与计算所有路径总分一样，我们面对的难题是要不要求出所有路径的分数。当然是不用的，我们用维特比算法来解码。通信专业的同学一定知道大名鼎鼎的维特比算法，卷积码的译码就是用的维特比算法。对于 $w_0 \\to w_1$:先计算$w_0$到$w_1$的状态1五条路径的分数，找出分数最大的一条保留下来，其他全都丢弃掉。计算量为$O(M)$ 同样的找出$w_1$每个状态分数最大的一条路径，要计算$w_1$的5个状态，计算量为$O(M^2)$ 依次递推到全局。序列长度为N,计算量为$O(N \\cdot M^2)$ 比较下前向算法与维特比算法的异同：相同的地方在他们都面临要不要计算所有路径分数的问题，都是基于路径结构，用局部递推到全局。不同的地方在于前向算法在更新previous的单个状态时是做求和sum运算，而维特比算法是做max运算，只保留分数最大的，丢弃掉其他路径。此外，维特比算法找到分数最大的路径后，还要反向递推 参考链接 CRF Layer on the Top of BiLSTM 最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 如何直观地理解条件随机场，并通过PyTorch简单地实现 条件随机场CRF—刘建平 《统计学习方法》—李航","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"条件随机场","slug":"条件随机场","permalink":"http://yoursite.com/tags/条件随机场/"},{"name":"前向后向算法","slug":"前向后向算法","permalink":"http://yoursite.com/tags/前向后向算法/"},{"name":"维特比算法","slug":"维特比算法","permalink":"http://yoursite.com/tags/维特比算法/"}]},{"title":"pytorch实现基于LSTM的循环神经网络","slug":"pytorch实现基于LSTM的循环神经网络","date":"2019-03-20T14:41:10.000Z","updated":"2019-07-22T01:01:59.000Z","comments":true,"path":"2019/03/20/pytorch实现基于LSTM的循环神经网络/","link":"","permalink":"http://yoursite.com/2019/03/20/pytorch实现基于LSTM的循环神经网络/","excerpt":"用pytorch实现基于LSTM的循环神经网络。","text":"用pytorch实现基于LSTM的循环神经网络。 涉及函数详解class torch.nn.LSTM(args,*kwargs) 参数说明： input_size: 输入的特征维度 output_size: 输出的特征维度 num_layers: 层数（注意与时序展开区分） bidirectional: 如果为True，为双向LSTM。默认为False LSTM的输入：input,$(h_0,c_0)$ input(seq_len,batch,input_size): 包含输入特征的tensor,注意输入是tensor。 $h_0$(num_layers $\\cdot$ num_directions,batch,hidden_size): 保存初始化隐藏层状态的tensor $c_0$(num_layers $\\cdot$ num_directions,batch,hidden_size): 保存初始化细胞状态的tensor LSTM的输出： output,$(h_n,c_n)$ output(seq_len, batch, hidden_size * num_directions): 保存RNN最后一层输出的tensor $h_n$(num_layers * num_directions,batch,hidden_size): 保存RNN最后一个时间步隐藏状态的tensor $c_n$(num_layers * num_directions,batch,hidden_size): 保存RNN最后一个时间步细胞状态的tensor 1234567import torch.nn import torchlstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，该单元输入维度embedding_dim,输出维度为hidden_diminput = Variable(torch.randn(seq_len,1,embedding_dim)) # 输入input应该是三维的，第一维度是seq-length,也就是多个词构成的一句话；第二维度为1，不用管；第三个维度是一个词的词嵌入维度，即embedding_dimh0 = Variable(torch.randn(1,1,hidden_dim)) c0 = Variable(torch.randn(1,1,hidden_dim))lstm_out,hidden = lstm(input,(h0,c0)) class torch.nn.Linear()1class torch.nn.Linear(in_features,out_features,bias = True) 作用：对输入数据做线性变换。$y = Ax+b$ 参数： in_features：每个输入样本的大小 out_features: 每个输出样本的大小 bias: 默认值为True。是否学习偏置。 形状： 输入： (N,in_features) 输出： (N,out_features) 变量： weights: 可学习的权重，形状为(in_features,out_features) bias: 可学习的偏置，形状为(out_features) 1234m = nn.Linear(20,30)input = torch.randn(128,20)output = m(input)print(output) 先看个小例子用pytorch实现LSTM，先实例化一个LSTM单元，再给出tensor类型的输入数据inputs及初始隐藏状态hidden = $(h_0,c_0)$。值得注意的是，LSTM单元的输入inputs必须是三维的，第一维是seq-length，即一句话，元素是词。第二维是mini-batch,从来不用，设为1即可。第三维是embedding-size,即一个词向量。 123456import torch import torch.nn as nn lstm = nn.LSTM(4,3) #实例化一个LSTM单元，单元输入维度是4，输出维度是3inputs = [torch.randn(1,5) for _ in range(5)] #产生输入inputs。为tensor序列。hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #初始化隐藏状态 做好三步准备：实例化一个LSTM单元，准备好inputs，初始化隐藏状态hidden。我们就可以计算LSTM单元的输出了。我们有两种选择，将序列一个元素一个元素地送入LSTM单元，或是将整个序列一下子全送入LSTM单元。先看看第一种： 123for x in inputs: lstm_out,hidden = lstm(x.view(1,1,-1),hidden) #x.view(1,1,-1)将tensor整形为三维。前面说过LSTM单元的输入必须是三维的。print(lstm_out,hidden) 接下来，将整个序列送入LSTM单元： 1234inputs = torch.cat(inputs).view(len(inputs),1,-1) #将整个序列连接为tensor，并整形为三维。hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #清楚隐藏状态lstm_out,hidden = lstm(inputs,hidden)print(lstm_out,hidden) 我们可以看到： lstm_out 中包含了序列所有的隐藏状态。 hidden 中包含了最后一个时间步的隐藏状态和细胞状态。可以作为下个时间步LSTM单元的输入参数，继续输入序列或反向传播。 用lstm做词性标注先准备训练数据： 123456789101112131415train_data = [ (&quot;The dog ate the apple&quot;.split(), [&quot;DET&quot;, &quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;]), (&quot;Everybody read that book&quot;.split(), [&quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;])]# 词汇表字典word_to_ix = &#123;&#125;for sent,tags in train_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix)# 标签集字典tag_to_ix = &#123;&quot;DET&quot;: 0, &quot;NN&quot;: 1, &quot;V&quot;: 2&#125;EMBEDDING_DIM = 6HIDDEN_DIM = 6 构建LSTM模型: 123456789101112class LSTMtagger(nn.Module): def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size): super(LSTMtagger,self).__init__() self.hidden_dim = hidden_dim self.word_embeddings = nn.Embedding(vocab_size,embedding_dim) #随机初始化词向量表，是神经网络的参数 self.lstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，单元输入维度是embedding_dim，输出维度是hidden_dim self.hidden2tag = torch.Linear(hidden_dim,tagset_size) #线性层从隐藏状态空间映射到标签空间 def forward(self,sentence): embeds = self.word_embeddings(sentence) #查询句子的词向量表示。输入应该是二维tensor。 lstm_out,hidden = self.lstm(embeds.view(len(sentence),1,-1)) tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1)) tag_scores = F.log_softmax(tag_space) 训练模型 12345678910111213141516171819202122232425262728293031323334353637model = LSTMtagger(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))loss_function = nn.NLLLoss()optimizer = optim.SGD(model.parameters(),lr = 0.1)def prepare_sequence(seq,to_ix): idxs = [to_ix[w] for w in seq] return torch.tensor(idxs,dtype = torch.long)# 在训练模型之前，看看模型预测结果with torch.no_grad(): inputs = prepare_sequence(train_data[0][0],word_to_ix) tag_scores = model(inputs) print(tag_scores) predict = np.argmax(tag_scores,axis = 1) print(predict)for epoch in range(300): for sentence,tags in train_data: # step 1:pytorch会累积梯度，要清楚所有variable的梯度。 model.zero_grad() # step 2:准备好数据，变成tensor sentence_in = prepare_sequence(sentence,word_to_ix) targets = prepare_sequence(tags,tag_to_ix) # step 3:得到输出 tag_scores = model(sentence_in) # step4: 计算loss loss = loss_function(tag_scores,targets) # step5: 计算loss对所有variable的梯度 loss.backward() # step6： 单步优化，根据梯度更新参数 optimizer.step()# 模型训练后，看看预测结果with torch.no_grad(): inputs = prepare_sequence(train_data[0][0],word_to_ix) tag_scores = model(inputs) print(tag_scores) predict = np.argmax(tag_scores,axis = 1) print(predict) 输出结果为： 我们可以看到，训练之后的预测序列为 [0,1,2,0,1]也就是[“DET”, “NN”, “V”, “DET”, “NN”] 参考链接 序列模型和基于LSTM的循环神经网络 Sequence Models and Long-Short Term Memory Networks-官方 Understanding LSTM Networks The Unreasonable Effectiveness of Recurrent Neural Networks","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"},{"name":"循环神经网络","slug":"循环神经网络","permalink":"http://yoursite.com/tags/循环神经网络/"}]},{"title":"pytorch实现Word embedding","slug":"pytorch实现Word-embedding","date":"2019-03-20T12:16:21.000Z","updated":"2019-07-07T07:11:01.000Z","comments":true,"path":"2019/03/20/pytorch实现Word-embedding/","link":"","permalink":"http://yoursite.com/2019/03/20/pytorch实现Word-embedding/","excerpt":"word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。","text":"word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。 one-hot编码在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。举个例子，我们想要构建一个语言模型。有以下三个句子 数学家待在实验室里。 物理学家待在实验室里。 数学家解决了一个难题。 我们又看到一个新的句子： 物理学家解决了一个难题。 我们希望语言模型可以学习到以下特点： 数学家和物理学家在一个句子中同样的位置出现。这两个词之间有某种语义上的联系 数学家曾经出现在我们看到的这个新句子中物理学家出现的位置。 这就是语义相似性想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为distributional hypothesis值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。 word embedding我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码物理学家和数学家。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \\frac{q_物\\cdot q_数}{|q_物| \\cdot |q_数|}=cos(\\phi) 其中\\phi是两个向量之间的夹角。$$但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。 PyTorch实现word embedding代码如下： 12345678import torch import torch.nn as nnfrom torch.autograd import Variable# 词汇表字典word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;vocab_size = len(word_to_ix) embedding_dim = 15word_embeddings = nn.Embedding(vocab_size,embedding_dim) nn.Embedding()随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。接下来我们查询”dog”这个词的向量表示。 1234dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。dog_idx = Variable(dog_idx)dog_embed = word_embeddings(dog_idx) #注意不是索引print(dog_embed) 上述代码中，要访问dog的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。接下来，我们查询一句话的向量表示。 123456sent = &apos;The dog ate the apple&apos;.split()sent_idxs = [word_to_ix[w] for w in sent]sent_idxs = torch.LongTensor(sent_idxs)sent_idxs = Variable(sent_idxs)sent_embeds = embeds(sent_idxs) print(sent_embeds) pytorch加载预训练词向量之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。方式一： 1234import torch word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵pretrain_embedding = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中 方式二： 12word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding)) 涉及函数详解numpy()与from_numpy()1torch.from_numpy(ndarray) $\\to$ tensor 作用：numpy桥，将numpy.ndarray转换为pytorch的tensor.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。 1tensor.numpy() 作用：numpy桥，将pytorch的tensor转换为numpy.ndarray.二者共享同一内存空间，修改一个另一个也会被修改。 举个例子： 123a = np.arange(5)b = torch.from_numpy(a)c = b.numpy() tensor.copy_(src) 作用：将src中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。 举个例子： 123a = torch.randn(1,5)b = torch.randn(1,5)b.copy_(a) 参考链接 Word Embeddings: Encoding Lexical Semantics PyTorch快速入门教程七（RNN做自然语言处理）","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"word embedding","slug":"word-embedding","permalink":"http://yoursite.com/tags/word-embedding/"}]},{"title":"sublime插件","slug":"sublime插件","date":"2019-03-11T11:41:12.000Z","updated":"2019-03-11T13:04:43.000Z","comments":true,"path":"2019/03/11/sublime插件/","link":"","permalink":"http://yoursite.com/2019/03/11/sublime插件/","excerpt":"记录sublime的一些插件。","text":"记录sublime的一些插件。 OmniMarkupPreviewer作用：插件OmniMarkupPreviewer支持将markdown语言渲染为html并且在浏览器上实时预览，也就是将markdown内容实时显示为网页，效果之好令人惊叹。 安装可以使用Package Control的Insatll Package来安装，也可以直接从OmniMarkupPreviewer的github主页下载压缩包，解压到目录\\Sublime Text 3\\Packages\\下。 快捷键Ctrl + shift + p打开Package Control 输入install选择Package Control: Install Package 从列表中选择OmniMarkupPreviewer安装。 使用方法：对于window和Linux： Ctrl+Alt+O 在浏览器中预览 Ctrl+Alt+X 输出为html文件 Ctrl+Alt+C 复制为HTML文件 插件配置修改插件的配置，点击菜单栏的Preferences - Packages Settings - OmniMarkdownPreviwer - Setting-User。 12345678910111213141516171819202122&#123; &quot;server_host&quot;: &quot;127.0.0.1&quot;, //默认为localhost,修改为你电脑的ip，可以实现远程访问。也就是从其他电脑预览网页效果 &quot;server_port&quot;: 51004, &quot;refresh_on_modified&quot;: true, &quot;refresh_on_modified_delay&quot;: 500, &quot;refresh_on_saved&quot;: true, &quot;browser_command&quot;: [], &quot;html_template_name&quot;: &quot;github&quot;, &quot;ajax_polling_interval&quot;: 500, &quot;ignored_renderers&quot;: [&quot;LiterateHaskellRenderer&quot;], &quot;mathjax_enabled&quot;: true, //渲染数学公式要用到MathJax库，将值设为true,mathjax会在后端自动下载。 &quot;export_options&quot; : &#123; &quot;template_name&quot;: &quot;github-export&quot;, &quot;target_folder&quot;: &quot;.&quot;, &quot;timestamp_format&quot; : &quot;_%y%m%d%H%M%S&quot;, &quot;copy_to_clipboard&quot;: false, &quot;open_after_exporting&quot;: false &#125;, &quot;renderer_options-MarkdownRenderer&quot;: &#123; &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;] &#125;&#125; 遇到的错误预览文本时报错： 1234567Error: 404 Not FoundSorry, the requested URL &apos;http://127.0.0.1:51004/view/593&apos; caused an error:&apos;buffer_id(593) is not valid (closed or unsupported file format)&apos;**NOTE:** If you run multiple instances of Sublime Text, you may want to adjustthe `server_port` option in order to get this plugin work again. 解决办法是修改配置文件Sublime Text &gt; Preferences &gt; Package Settings &gt; OmniMarkupPreviewer &gt; Settings - User粘贴下面的代码： 12345&#123; &quot;renderer_options-MarkdownRenderer&quot;: &#123; &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;] &#125;&#125; 参考链接 OmniMarkupPreviewer的github主页 近乎完美的Markdown写作体验 - SublimeText3 + OmniMarkupPreviewer OmniMarkupPreviewer + MathJaxOmniMarkupPreviewerx渲染markdown内容为网页，MathJax对LATEX编辑的数学公式进行渲染。 下载mathjax 下载mathjax，解压到目录Sublime Text 3\\Packages\\OmniMarkupPreviewer\\public下。 在目录Sublime Text3\\Packages\\OmniMarkupPreviewer\\创建空文件MATHJAX.DOWNLOADED。这样就安装好了。 验证新建markdown文件输入内容： 123This expression $\\sqrt&#123;3x-1&#125;+(1+x)^2$ is an example of a $\\LaTeX$ inline equation.he Lorenz Equations:$$\\begin&#123;aligned&#125;\\dot&#123;x&#125; &amp; = \\sigma(y-x) \\\\\\dot&#123;y&#125; &amp; = \\rho x - y - xz \\\\\\dot&#123;z&#125; &amp; = -\\beta z + xy\\end&#123;aligned&#125;$$ 在sublime中用Ctrl+Alt+O预览，显示效果如下： 参考链接 使用Markdown的时候需要插入LaTeX公式方法 关于LATEX: 一份其实很短的 LaTeX 入门文档 一份其实很短的 LaTeX 入门文档 常用数学符号的 LaTeX 表示方法","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"sublime","slug":"sublime","permalink":"http://yoursite.com/tags/sublime/"}]},{"title":"熵、交叉熵与KL散度","slug":"熵、交叉熵与KL散度","date":"2019-03-11T06:31:33.000Z","updated":"2019-07-22T00:57:32.000Z","comments":true,"path":"2019/03/11/熵、交叉熵与KL散度/","link":"","permalink":"http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/","excerpt":"介绍交叉熵和KL散度。","text":"介绍交叉熵和KL散度。 从信息量到信源熵 信息量是通信专业的名词。一个变量的主要特征就是不确定性，也就是发生的概率。信息量用来衡量不确定性的大小。一个事情发生的概率越小，使人越感到意外，则这件事的信息量越大；反之，概率越大，越不意外，信息量越小。举个例子，有一架波音747飞机失事，发生的概率很小，让人很意外，带给人的信息量很大。 信息量函数应满足两个特性：1）随着概率的增大而减小，即是概率的减函数；2）信息量函数满足可加性，即两个统计独立的消息提供的信息量等于他们分别提供的信息量之和。同时满足递减性和可加性的函数是对数函数，即 $$ I[p(x_i)] = log \\frac{1}{p(x_i)} = -log p(x_i)$$ 信源熵定义为信源输出的平均信息量，即信息量的数学期望。$$ H(X) = E(I[p(x_i)]) = E(-log p(x_i)) = - \\sum_{i=1}^{n}p(x_i)log p(x_i)$$信源实际上是一个概率分布，信源熵可以解释为表示这个概率分布至少需要的信息量。 交叉熵对于一个随机事件，真实概率分布是$p(x_i)$ 是未知的，从数据中得到概率分布为$q(x_i)$。我们用概率分布$q(x_i)$来近似和逼近真实的概率分布$p(x_i)$ 。交叉熵定义为：$$H(p,q) = \\sum_{i=1}^{n}p(x_i) I[q(x_i)] =- \\sum_{i=1}^{n}p(x_i)log(x_i) $$交叉熵$H(p,q)$是用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量。上面我们说过，信源熵$H(X)$是表示真实概率分布$p(x_i)$需要的最小信息量。可以得到结论：$$H(p,q) \\ge H(p)$$由吉布斯不等式可以证明，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。这个不等式的意义是：用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量一定大于等于概率分布$p(x_i)$本身的信源熵。交叉熵比信源熵多出来的这部分，就是冗余信息量，我们称为KL散度（相对熵）。$$KL(p||q)= H(p,q) - H(p) \\ge 0$$容易看出交叉熵并不是一个对称量，即$ H(p,q) \\not=H(q,p)$。同样的,KL散度也不是一个对称量，即$KL(p||q) \\not =KL(q||p) $给定概率分布$p(x_i)$,信源熵$H(p)$就是固定不变的。在机器学习中，交叉熵常用作分类问题的损失函数。交叉熵刻画了预测概率分布$q(x_i)$与真实概率分布$p(x_i)$之间的距离。通过减小交叉熵$H(p,q)$,我们可以使得预测概率分布$q(x_i)$不断逼近真实概率分布$p(x_i)$ 相对熵真实的概率分布为$p(x_i)$，我们用预测概率分布$q(x_i)$对它进行建模和近似。我们需要的平均附加量，也就是冗余量是：$$KL(p,q) = H(p,q) - H(q) = -\\sum_{i=1}^{n}p(x_i)logq(x_i) - \\biggl(-\\sum_{i=1}^{n}p(x_i)logp(x_i)\\biggr) = -\\sum_{i=1}^{n}p(x_i)log{\\frac{q(x_i)}{p(x_i)}}$$KL散度有以下几个特性： KL散度不是一个对称量，即$KL(p||q) \\not =KL(q||p) $ $KL(p||q)\\ge 0$，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。 KL散度可以看做两个分布之间不相似程度的度量。KL散度越小，两个分布的不相似程度越小，分布$q(x_i)$越适合来近似$p(x_i)$。 tensorflow用交叉熵做损失函数在机器学习中交叉熵常常用作分类问题的损失函数。这里有个问题，交叉熵用于概率分布，但神经网络的输出并不一定是一个概率分布。概率分布应满足2个条件:1) $0 \\le p(X =x) \\le 1$2) $\\sum_{x}{} p(X=x) = 1$如何把神经网络的输出变成概率分布呢？这里就要用到softmax回归。假设输出层的输出为$y_0,y_1,y_2 \\dots y_n$,则softmax函数的形式为：$$softmax(y_i) = \\frac{exp(y_i)}{\\sum_{j}exp(y_j)}$$由于交叉熵一般会与softmax回归一起使用，TensorFlow对这两个功能进行了统一，可以直接用函数tf.nn.softmax_cross_entropy_with_logits来计算softmax后的交叉熵函数。对于只有一个正确答案的分类问题，可以用函数tf.nn.sparse_nn.softmax_cross_entropy_with_logits来加速计算过程。 pytorch中交叉熵损失函数的实现在多分类问题中，实际概率分布是 $y = [y_0,y_1,…,y_{C-1}]$,其中C为类别数;y是样本标签的one-hot表示，当样本属于第$i$类时$y_i=1$,否则$y_i=0$。预测概率分布为$p = [p_0,p_1,p_2,…,p_{C-1}]$。$c$是样本标签。此时，交叉熵损失函数为$$loss = -\\sum_{i=0}^{C-1}y_i log(p_i) = - y_c \\cdot log(p_c) = - log(p_c)$$接下来介绍pytorch中具体实现这个数学式子的函数。 torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()1torch.nn.functional.log_softmax() 作用：先做softmax运算，再做log运算。在数学上等价于$log(softmax(x))$ 1class torch.nn.NLLLoss(weight = None) 作用：这是neg log likelihood loss（NLLLoss），即负对数似然函数。 参数： weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。 输入： input(N,C): C是类别个数。为log_probabilities形式，即概率分布再取log。可以在最后一层加log_softmax,这就要用到函数torch.nn.functional.log_softmax() targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为2,而不是[0,0,1,0,0] loss可以表示为：$$loss(x,class) = -x[class]$$如果指定了weight，可以表示为：$$loss(x,class) = - weight[class]*x[class]$$举个例子: 1234567import torch log_m = torch.nn.functional.log_softmax()loss_function = torch.nn.NLLLoss()inputs = torch.randn(3,5) #batch_size * num_classes = 3 * 5target = torch.LongTensor([1,0,4])loss = loss_function(log_m(inputs),target) # inputs要先做log_softmax，再送入loss_functionloss.backward() class torch.nn.CrossEntropyLoss(weight = None) 作用：将函数log_softmax和NLLLoss集成到一起。在多分类问题中非常有用。 参数： weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。 输入： input(N,C): C是类别个数。每个类别的分数，不用过softmax层。 targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为2,而不是[0,0,1,0,0]。 loss可以表示为：$$loss(x,class) = - \\text{log}\\frac{e^{x[class]}}{ \\sum_{j=0}^{C-1}e^{x[j]}} = -x[class] + \\text{log}(\\sum_{j=0}^{C-1}e^{x[j]}) $$当指定了weight时，loss计算公式为： $$ loss(x, class) = weights[class] \\cdot (-x[class] + \\text{log}(\\sum_{j=0}^{C-1}e^{x[j]})) $$参见： PyTorch学习笔记——多分类交叉熵损失函数 pytorch官方手册参考链接 信息熵，相对熵，交叉熵的理解 Tensorflow基础知识—损失函数详解","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"交叉熵","slug":"交叉熵","permalink":"http://yoursite.com/tags/交叉熵/"},{"name":"相对熵","slug":"相对熵","permalink":"http://yoursite.com/tags/相对熵/"}]},{"title":"python的一些函数","slug":"python的一些函数","date":"2019-03-10T08:12:51.000Z","updated":"2019-07-07T07:07:46.000Z","comments":true,"path":"2019/03/10/python的一些函数/","link":"","permalink":"http://yoursite.com/2019/03/10/python的一些函数/","excerpt":"记录python的一些函数，实现某些功能。","text":"记录python的一些函数，实现某些功能。 求最大/小值的索引对于list12345import numpy as npa = range(100)np.random.shuffle(a)index_max = a.index(max(a)) #求最大值的索引index_min = a.index(min(a)) #求最小值的索引 对于numpy的数组ndarray1234567a = np.array(a)index_max = np.argmax(a) #求最大值的索引index_min = np.argmin(a) #求最小值的索引# 对于二维的数组b = np.arange(100).reshape(10,-1)row_max_list = np.argmax(b,axis = 1) #按行计算最大值在行中的索引line_max_list = np.argmin(b,axis = 0) #按列计算最小值在列中的索引 sort与sorted1sorted(iterable,key,reverse) iterable: 可迭代对象 key：用来进行比较的元素。常用函数： lambda x:x[i] reverse：排序规则。reverse=True按降序排列，reverse=False按升序排列（默认） 比较sort与sorted: 作用对象:sort()只能作用于list,sorted()可以作用于所有可迭代对象。 返回值：sort()没有返回值；sorted()返回一个新的list 字典的items()方法1dict.items() 返回可遍历的元素为（键，值）元组的数组。 模块collections–容器数据类型collections模块是python内建的一个集合模块，提供了许多有用的集成类。提供了list,dict,set,tuple的替代选择，相当于这几个数据类型的加强版。 collections.Counter(iterable)Counter是一个计数器，用于计数可哈希对象，统计元素出现的个数。它是一个集合，元素-计数像键-值对一样存储。 123456&gt;&gt; import collections&gt;&gt; a = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;a&quot;]&gt;&gt; counter = collections.Counter(a)&gt;&gt; print(counter)Counter(&#123;&apos;a&apos;: 3, &apos;b&apos;: 2, &apos;c&apos;: 1&#125;)","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"全文搜索引擎-Elasticsearch入门","slug":"全文搜索引擎-Elasticsearch入门","date":"2019-03-10T07:11:17.000Z","updated":"2019-07-22T00:38:38.000Z","comments":true,"path":"2019/03/10/全文搜索引擎-Elasticsearch入门/","link":"","permalink":"http://yoursite.com/2019/03/10/全文搜索引擎-Elasticsearch入门/","excerpt":"Elasticsearch是一个开源的搜索引擎框架。","text":"Elasticsearch是一个开源的搜索引擎框架。 Elasticsearch安装和启动安装前提：Elasticsearch需要Java7或以上的版本。 下载压缩包并解压： 123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.zipunzip elasticsearch-6.6.1.zipcd elasticsearch-6.6.1 进入解压后的文件目录，启动elasticsearch： 1./bin/elasticsearch 如果一切正常，elasticsearch默认在本机9200端口运行。打开另一个命令行窗口，执行以下命令，检查elasticsearch是否运行成功： 1curl localhost:9200 如果输出以下信息，则运行正常。 默认情况下，elasticsearch只允许本机访问。要想其他电脑可以访问，也就是实现远程访问，需要修改文件/config/elasticsearch.yml,取消字段network.host的注释，把该字段的值改为0.0.0.0。这样的话所有的电脑都能访问，实际情况中，最好不要这样。 如果启动遇到错误“Native controller process has stopped - no new native processes can be started”或“max virtual memory areas vm.maxmapcount [65530] is too low”。解决方法是执行以下命令： 1sudo sysctl -w vm.max_map_count=262144 在python中使用elasticsearch要先安装elasticsearch包。在python中使用elasticsearch要先启动elasticsearch。 1pip install elasticsearch 创建index12345from elasticsearch import Elasticsearch es = Elasticsearch() #创建实例，默认localhost:9200# es = Elasticsearch([&#123;&apos;host&apos;:&apos;10.112.1.109&apos;,&apos;port&apos;:&apos;9200&apos;&#125;]) #远程访问result = es.indices.create(index = &apos;news&apos;,ignore = 400)print(result) 如果创建成功，会返回以下信息 1&#123;&apos;acknowledged&apos;: True, &apos;shards_acknowledged&apos;: True, &apos;index&apos;: &apos;test_es&apos;&#125; 如果再次创建，就会返回以下信息： 1&#123;&apos;error&apos;: &#123;&apos;root_cause&apos;: [&#123;&apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;], &apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;, &apos;status&apos;: 400&#125; 表示创建失败，失败的原因是要创建的index已经存在了。status状态码是400。 插入数据1234567891011datas = [ &#123;&apos;title&apos;:&quot;算法导论（原书第2版）&quot;, &apos;url&apos;:&quot;https://book.douban.com/subject/1885170/&quot;, &apos;introduction&apos;:&quot;这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。&quot;&#125;, &#123;&apos;title&apos;:&quot;计算机程序的构造和解释&quot;, &apos;url&apos;:&quot;https://book.douban.com/subject/1148282/&quot;, &apos;introduction&apos;:&quot;《计算机程序的构造和解释(原书第2版)》1984年出版，成型于美国麻省理工学院(MIT)多年使用的一本教材，1996年修订为第2版。在过去的二十多年里，《计算机程序的构造和解释(原书第2版)》对于计算机科学的教育计划产生了深刻的影响。第2版中大部分重要程序设计系统都重新修改并做过测试，包括各种解释器和编译器。作者根据其后十余年的教学实践，还对其他许多细节做了相应的修改。&quot;&#125;, ]for i in range(len(datas)): es.index(index = &apos;book&apos;,doc_type = &apos;computer&apos;,id = i+1,body = datas[i]) index()方法可以完成两个操作，如果数据不存在，那就插入数据；如果数据已经存在，那就更新数据。 get()按ID查询12result= es.get(index=&apos;book&apos;,doc_type=&apos;computer&apos;,id =1)print(result[&apos;_source&apos;]) search()实现全文检索对于中文，我们要安装一个中文分词插件elasticsearch-analysis-ik。可以使用elastic的另一个命令行工具elastisearch-plugin来安装，要确保版本号一致。进入elastic的目录下，执行： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip 注意将6.3.0替换为自己的版本号。安装成功后，重启elasticsearch，就会自动加载这个中文分词插件。 1234567891011es = Elasticsearch()mapping = &#123; &apos;properties&apos;: &#123; &apos;title&apos;: &#123; &apos;type&apos;: &apos;text&apos;, &apos;analyzer&apos;: &apos;ik_max_word&apos;, &apos;search_analyzer&apos;: &apos;ik_max_word&apos; &#125; &#125;&#125;result = es.indices.put_mapping(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=mapping) 指定使用中文分词器，如果不指定默认使用英文分词器。 12345678dsl = &#123; &apos;query&apos;: &#123; &apos;match&apos;: &#123; &apos;introduction&apos;: &apos;计算机&apos; &#125; &#125;&#125;result = es.search(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=dsl) 参考链接 全文搜索引擎 Elasticsearch 入门教程—–阮一峰 Python Elasticsearch文档 Elasticsearch官方文档 Elasticsearch基本介绍及其与Python的对接实现–崔庆才 Elasticsearch搜索中文分词优化","categories":[{"name":"web搜索","slug":"web搜索","permalink":"http://yoursite.com/categories/web搜索/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Ubuntu服务器遇到的一些问题","slug":"Ubuntu服务器遇到的一些问题","date":"2019-03-07T11:53:00.000Z","updated":"2019-03-12T06:13:31.000Z","comments":true,"path":"2019/03/07/Ubuntu服务器遇到的一些问题/","link":"","permalink":"http://yoursite.com/2019/03/07/Ubuntu服务器遇到的一些问题/","excerpt":"记录Ubuntu服务器遇到的一些问题。","text":"记录Ubuntu服务器遇到的一些问题。 linux服务器连不上网 先检查网线是否插好了，若网线口发亮才是插好。检查电脑是否能ping通局域网的其他电脑。 查看其他电脑的ip地址ifconfig | grep inet ping其他电脑的IP地址ping 10.112.0.1如果可以ping通其他电脑，再检查下一步。 可以ping通其他电脑，但ping 10.3.8.211校园网网关失败。这时可能是路由出错，查看服务器的路由是否正确。 查看比较服务器与可以正常联网的电脑的路由。route -n 添加正确的默认路由。route add default gw 10.112.0.1 检查能否连接到校园网。ping 10.3.8.211Ubuntu配置路由参见: ubuntu配置静态路由及重启生效 连接到校园网，但是ping www.baidu.com失败。服务器ping不通域名，但可以ping通百度的ip地址112.34.112.40。这是服务器的DNS配置出错了，无法解析域名。 修改文件/etc/resolv.conf，必须有sudo权限。sudo vim /etc/resolv.conf 添加以下内容nameserver 8.8.8.8 重启网络使修改立即生效。sudo /etc/init.d/networking restart 这时应该能ping通百度了。 重启电源后，以上方法会被清除而失效，导致开机后需要重新配置。有效的方法是卸载开机重写/etc/resolv.conf的resolvconf，执行命令sudo apt-get autoremove resolvconf配置域名解析参见：Ubuntu 无法解析域名 不能通过Xshell或ssh命令连接到服务器 检查是否安装了ssh-server服务。ps -e | grep ssh 若没有安装，使用以下命令安装：sudo apt-get install openssh-server 若安装了ssh-server服务，检查ssh服务是否打开。需要sudo权限 检查ssh服务状态service sshd status或/etc/init.d/ssh status 开启ssh服务service sshd start或/etc/init.d/ssh start","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"}]},{"title":"pytorch学习笔记","slug":"pytorch学习笔记","date":"2019-03-05T01:27:14.000Z","updated":"2019-07-07T07:09:35.000Z","comments":true,"path":"2019/03/05/pytorch学习笔记/","link":"","permalink":"http://yoursite.com/2019/03/05/pytorch学习笔记/","excerpt":"这里是pytorch学习笔记。","text":"这里是pytorch学习笔记。 创建操作torch.randn()1torch.randn(*size,out = None) 输入： size(int)：指定了输出张量的形状 输出：输出结果为张量 作用：返回一个张量，从标准正态分布中抽取一组随机数。形状由*size决定 张量维度操作转置：transpose1torch.transpose(input,dim0,dim1) 参数： input: 输入张量，可以是二维及二维以上的张量 dim0,dim1: 要转置的两个维度。 作用： 返回输入矩阵的转置。一次只能转置张量的两个维度。输出张量与输入张量共享内存，同步改变。 1torch.t(tensor) 输入一个二维张量（矩阵），并转置0,1维。可以看做函数torch.transpose(input,0,1)的简写函数。比较下transpose与view这两个函数： 1234567891011121314151617181920212223242526272829303132333435&gt;&gt; a = torch.randn(2,3,5)&gt;&gt; b = torch.transpose(a,1,2)&gt;&gt; c = a.view(2,5,3)&gt;&gt; print(a)tensor([[[ 0.9926, -0.1669, -1.6571, -0.2730, -0.1313], [ 0.9811, -1.9854, 1.5519, 0.1383, 1.4571], [ 0.8221, -1.1283, -0.7675, -2.0497, 0.4748]], [[ 0.1594, 0.7166, -0.2603, 1.1027, 1.5283], [-0.7652, -1.4711, 0.5077, 0.6639, 0.0374], [ 1.8121, -1.4864, -2.9863, -0.5769, -0.2915]]]) &gt;&gt; print(b)tensor([[[ 0.9926, 0.9811, 0.8221], [-0.1669, -1.9854, -1.1283], [-1.6571, 1.5519, -0.7675], [-0.2730, 0.1383, -2.0497], [-0.1313, 1.4571, 0.4748]], [[ 0.1594, -0.7652, 1.8121], [ 0.7166, -1.4711, -1.4864], [-0.2603, 0.5077, -2.9863], [ 1.1027, 0.6639, -0.5769], [ 1.5283, 0.0374, -0.2915]]]) &gt;&gt; print(c)tensor([[[ 0.9926, -0.1669, -1.6571], [-0.2730, -0.1313, 0.9811], [-1.9854, 1.5519, 0.1383], [ 1.4571, 0.8221, -1.1283], [-0.7675, -2.0497, 0.4748]], [[ 0.1594, 0.7166, -0.2603], [ 1.1027, 1.5283, -0.7652], [-1.4711, 0.5077, 0.6639], [ 0.0374, 1.8121, -1.4864], [-2.9863, -0.5769, -0.2915]]]) 可以看到:二者得到的结果并不相同。transpose是进行转置操作。view对张量整形时，张量中元素的顺序保持不变。相当于将这个三维张量按顺序 torch.Tensortorch.manual_seed()1torch.manual_seed(seed) 输入： seed(int or long)：设定种子，为int类型或long类型 作用：设定生成随机数的种子。种子相同，生成的随机数就是相同的，实验结果就可以复现。 参见：利用随机数种子来使pytorch中的结果可以复现 .view() 整形1tensor.view(*size) 输入： *size(int)：指定了输出张量的形状 输出：输出结果为张量 作用：整形，只改变原张量的形状，形状由*size指定。 例子： 123456789&gt;&gt; x = torch.randn(5,4)&gt;&gt; x.size()torch.Size([5,4]&gt;&gt; x.view(30)&gt;&gt; x.size()torch.Size([20])&gt;&gt; x.view(1,1,-1) # -1表示该维度由其他的维度推断。&gt;&gt; x.size()torch.Size([1, 1, 20]) torch.cat() 连接1torch.cat(inputs,dimension = 0) 输入： inputs(sequence of Tensors)： 多个Tensor的python序列。 如[tensor1,tensor2…]或(Tensor1，tensor2) dimension(int,optional): 沿着该维连接张量序列。默认为0。 作用：在指定维度上，对输入张量序列进行连接操作。 举个例子： 12345678910&gt;&gt; impotr torch &gt;&gt; x = torch.randn(4,3)&gt;&gt; x.size()torch.Size([4, 3])&gt;&gt; y = torch.cat((x,x,x),0)&gt;&gt; y.size()torch.Size([12, 3])&gt;&gt; z = torch.cat((x,x,x),1)&gt;&gt; z.size() torch.Size([4, 9]) torch.optimclass torch.optim.SGD(params,lr=,momentum=0,weight_decay=0) 参数： params： 待优化参数的iterable lr(float): 学习率 momentum(float,可选)： 动量因子，默认为0 weight_decay(float,可选): 权重衰减，默认为0 作用：实现随机梯度下降算法。 如何使用optimizer? 123456789import torch.optim as optim optimizer = optim.SGD(model.parameters(),lr = 0.01) #构建一个optimizer,model.parameters()给出了所有要优化的参数for input,target in dataset: optimizer.zero_grad() #清空所有被优化过的Variable的梯度 output = model(input) loss = loss_fn(output,target) loss.backward() #反向传播算法，计算好所有要优化Variable的梯度。 optimizer.step() #单步优化，基于计算得到的梯度进行参数更新。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"}]},{"title":"Numpy学习笔记","slug":"Numpy学习笔记","date":"2019-02-27T12:33:15.000Z","updated":"2019-07-22T00:37:14.000Z","comments":true,"path":"2019/02/27/Numpy学习笔记/","link":"","permalink":"http://yoursite.com/2019/02/27/Numpy学习笔记/","excerpt":"这里是numpy学习笔记。","text":"这里是numpy学习笔记。 np.copy1np.copy(a) a为ndarray数组。np.copy复制一个与a完全相同的dnarray数组。来看看=与np.copy的区别。 12345678&gt;&gt; x = np.array([1,2,3])&gt;&gt; y = x&gt;&gt; z = x.copy(x)&gt;&gt; x[0] = 10&gt;&gt; x[0] == y[0]True&gt;&gt; x[0] == z[0]False 对于=，x与y共享同一内存，数据同步改变。一个改变另一个跟着改变。对于np.copy,x与z在不同的内存，数据改变互不影响。 np.load与np.savenumpy可以读写磁盘上的二进制数据。为将ndarray数组对象保存到文件，引入了文件格式npy。数组对象ndarray以未压缩的原始二进制格式保存在扩展名为.npy的文件中。 np.save(file,array) 作用： 将数组以二进制格式保存到扩展名为npy的文件中。 np.load(file) 作用： 从.npy文件中读取二进制数据还原为数组。举个例子：1234import numpy as np a = arange(5)np.save(&apos;a.npy&apos;,a)b = np.load(&apos;a.npy&apos;) np.full1np.full(shape,fill_value,dtype) 作用： 返回一个 给定形状为shape，数据类型为dtype，全都由fill_value填充后的ndarray。 np.foat32np.float32(x)作用：变换数据类型为float32 np.pad参数解释 1numpy.pad(array, pad_width, mode, **kwargs) 输入 array 为待填充的数组 pad_width 为((before_1,after_1),(before_2,after_2),….,(before_N,after_N))在每个维度前后填充的个数 mode 常用constant,用常数填充。 返回值： 填充后的ndarray 举个例子 123456import numpy as np a = range(5)# 在一维数组前后分别填充2,3位数字；填充的数字分别为0,2ndarray = np.pad(a,(2,3),&apos;constant&apos;,constant_values=(0,2))print(a)print(ndarray) 执行结果为 12[0, 1, 2, 3, 4][0 0 0 1 2 3 4 2 2 2] np.randomnp.random.uniformnp.random.uniform(low = 0,high = 1,size = None)作用： 生成一个形状为size的随机数矩阵，每个数从均匀分布的半开半闭区间[low,high)中抽样得到。 参考链接 Numpy官方手册","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"2018-群星陨落","slug":"2018-群星陨落","date":"2019-01-16T12:56:00.000Z","updated":"2019-07-22T00:35:37.000Z","comments":true,"path":"2019/01/16/2018-群星陨落/","link":"","permalink":"http://yoursite.com/2019/01/16/2018-群星陨落/","excerpt":"游龙当归海，海不迎我自来也！","text":"游龙当归海，海不迎我自来也！ 今天是农历12月十一，明天就是爸爸的生日。阳历的2018年已经过去了，但春节还没来呢！研究生一年级的学期期末，我想着回顾一下这一年，看看自己这一年是怎么度过的。2018年，许多了不起的人去世了，因此选了这个小标题。2018年，我大学毕业，顺利通过了研究生复试。考研，是与高考同等级别的考试，不同的是考研没有老师和家长的督促，只得一个人奋战。备战考研初试已经是2017年的往事了，但还是值得一提的。准备初试的小半年里基本上每天泡在图书馆里，坐在某个固定的座位。虽然有时候也会懈怠懒惰，但大部分时间都是在认真地学习。晚上九点半从图书馆出来，我常要绕着学校跑上五公里；回宿舍经过一楼的镜子，我看着自己嘴里念叨着学校的名字。如愿以偿，2018年年初，我查自己的考研初试成绩409，是比高考还令人满意的了。查到成绩后，我欢欣鼓舞了一阵子。 上半年 - 本科最后的日子年初，我因为考驾照的缘故住在我姨家里，有了跟亲戚一起生活半个月的机会。亲戚走动的机会不多，平时没有机会去了解他们促进感情，全凭着血缘的纽带联系着。但这次难得地坐在亲戚家里有了不一样的感受，感受到日久生情（可能用词不当，但意思到了呢）。有天中午，我姨有事外出，我跟表妹一块做了午饭，手忙脚乱，做出来的饭味道竟还不错！考过了科二，我便回了家。每年过年都得下一场雪，有到脚踝那么厚，到处都白茫茫的一片。我拿着铲子去屋顶铲雪，花了两个半天，把屋顶上的雪收拾到了院子里。只是那时候竟没有兴致堆个雪人，今天冬天北京一丝雪都没有落，真想回到家能看一场雪啊。高中的班主任还在带着高三的学生，过了年没几天高三的学生便开学了。班主任想着我给高三的同学们聊聊天，说“现在的学生都没有你高三下苦”，我勉强算是个刻苦的笨孩子吧。在老班家里吃了午饭，下午便回到曾经的教室里，跟高三的同学们聊天。时间在一天一天地度过，我们在一步一步地向前走呢！ 大四下，已经没课了，只剩下做毕业设计。我在明德楼B座的人才办找了一份勤工俭学的差事，平常的工作是给老师整理文件，打印，拿快递和打扫卫生。这是段闲适又想着充实的日子，想着好好完成毕业设计，想着去做大学里想做还没做的事，想着去看几本书，想着跟朋友们告别。我每天在老师的办公室里做毕设，中午便在沙发上睡一会，算不上特别努力，只是想在大学最后的几个月里把事情做好一些。办公室的窗口有一些灰尘，窗台上有爬山虎的脚，窗外有许多树。阳光照进来，树影婆娑的，像极了大学最后的这段日子。大四下，我去圣昆仑音乐厅听了音乐会，只听得一半便溜了出来；也在圣昆仑音乐厅看了话剧《蜀道难》，虽然有一些瑕疵，但依然十分精彩，震撼人心，让我领略到了现场话剧的魅力；我常去文理馆三楼找文学类的书，在做毕设的闲暇翻看。读了王小波写给李银河的信，知道爱情会让人牵肠挂肚；读了杨绛先生的散文集，我原先以为散文集是无趣的，但读了杨绛先生的散文集，才体会到朴实文字的动人之处。尤其当读到杨绛先生写文革期间女婿被红卫兵打死，父亲去世却奔丧不成，自己被分配到厕所刷马桶这些情节，就好像在讲述一些生活中的小事。也看到杨绛先生去菜园找钱钟书时流露出的不难察觉的爱意和在苦难中的幽默感。 杨绛先生和她的女儿钱瑗、丈夫钱钟书 车协是我大学四年最重要的地方吧，也是我大学归属感的来源。在最后这半年里，在周二周四的晚上参加了许多次的体训，周一周三周五也偶尔跟着大家骑车去怪坡。最后还参加了几次小假期的拉练：“放火烧山，牢底坐穿”，黄巢的篝火没有烧起来，但心里的篝火不曾熄灭，\"聚是一团火，散作满天星\"；“清明拉练”去看了遍野的油菜花，大一第一次参加拉练也是清明拉练呢；五一是药乡选拔，新的远征队又将踩着单车用车轮丈量祖国的大好河山！快要离开的时候，想着要做一些事，比平时更用心些。婷婷提出要办一个车协的\"考研、保研、工作交流会\"，但她因为要工作提前离校了，我接过这个活，跟车协的伙伴一起办了这个交流会。在北京比赛之前，我想起了前两年给京赛队员加油的火腿肠，今年该是我了吧！大学的许多生活都与车协相关，大学许多朋友都是车协的伙伴。我似乎没给留下些什么，但一同经历就是意义啊！ 16年黄巢拉练的篝火 大学最后的尾声是在拍毕业照和送老会中进行的。终于穿上那一身幼时憧憬的毕业服，拍学院里的毕业照，拍班级里的毕业照。也拍协会那一群人的毕业照，小树林，南门外，臧克家和闻一多的雕像旁，曾经朝夕相处，今日便要离之而去。在车协的送老会上，我以为我不会哭的，两年远征都没有哭，只是未曾坦开心扉，在送老会上，平时话不多的我也不知哪来那么多的话和眼泪。嘿，毕业快乐！！！ 暑期 - 目的地在尼泊尔的新藏线大学毕业后，趁着年轻力壮去骑了新藏线。时间跨度是7月21日到8月28日近四十天的时间。先坐火车从北京到叶城，中途在吐鲁番转车，从祖国的东边到最西边。骑车的路段是“喀什-萨嘎县-吉隆口岸-加德满都”。返程是费尽周折的，先坐吉普车从加德满都回国到基隆口岸，再做大巴车到日喀则，再坐火车“日喀则-青海西宁-陕西西安-山西临汾”，从陆地回家的成本虽然只有机票钱的一半，但花了整整五天。之前有过骑车出国门的想法，但不是十分笃定的。旺哥在论坛上发帖征新藏线的队友，我毅然地回了贴。原先计划的新藏线小队有我、旺哥和宇哥三个人，后来宇哥因为入职没能成行。新藏线是一条比川藏线要难的路线，我每天晚上跑步五公里来做体力储备，有时候也偷懒；在美骑网上看别人的路书和骑行游记，做了自己的路书和攻略。特地回老家办了护照和边防证，办边防证的过程费了一些周折，办边防证需要一份小领导写个名字，但他又不来上班，花了几天才终于办好了。又在北京的尼泊尔大使馆办了签证，准备地差不多只等出发了。 7月21日，坐火车从北京出发，坐了38个小时的硬座到了吐鲁番，再坐19个小时的硬座到新疆喀什地区喀什市跟旺哥会合。全国用的都是北京时间，因此到晚上九点，喀什的天还是亮的。略作修整，便到了开始每天的骑行。第一天的骑行就差点要了老命，一是第一天的路程远，又有很大的逆风，晚上到宾馆已经累地快虚脱了。新藏线前几天的路是比较平坦的，海拔上升不太大。骑车的第四天我遇到一个致命的问题，差点导致我的新藏线骑行半路夭折。我自行车后轮的花鼓在一个小村子阿克美其特村里断了，小村子里连去城里的公交车都没有，完全不可能修理。我跟旺哥商议，旺哥继续骑车到下个地点补给更方便的库地达坂等我，我返回叶城去修车。我拆了自行车的后轮，村里的村干部开车送我到了镇上，我在镇上坐了个出租车返回了叶城。为了节省时间，我没去更远更大的喀什市去修理，只在叶城找人修理，这给我后来的不幸埋下了隐患。在叶城找了一个不靠谱的单车修理店给我修理后轮，信誓旦旦地坐车回到阿克美其特村，结果骑了不到二公里，自行车的后轮又坏了，是根本没有修理好。悔不该杀那华佗！我终于下定决心老老实实地去喀什找捷安特专卖店买了轮子，又是艰难的交通，先在路边搭回城里的顺风车，再坐火车到喀什，买好轮子，再坐火车返回叶城，最后搭了许多车（私家车，大卡车）终于回到阿克美其特村。我修理好车，战战兢兢的出发了，小心翼翼地，生怕轮子再出幺蛾子。所幸后面一路上都没再出什么大问题，这一次就够我折腾的了，反反复复弄弄三天。以致于我想要不车子也不要了，买张票回家吧！现在回过头来看，所幸坚持了下来。 骑行在新藏线上 新藏线广袤荒芜，很少有高的树，当你看到高大的杨树和绿色的植物，就说明附近有水源，此处有人烟。新藏线上是一条连接新疆与西藏的纽带，有许多跑新藏线的大卡车司机。搭车的时候跟几位司机聊天，司机师傅大多是山西河南的，汉族人多，藏族少。在新藏线上跑大卡车，就是在拿命挣钱，高原反应和疲劳驾驶，一个不小心就是车毁人亡。新藏线路边会常看得到翻倒在路边的大卡车，有的男人带着妻子来跑车，出了事一家子就没了，剩个孩子跟着爷爷奶奶。新藏线有除了踩单车的，还有骑摩托车的，自驾游的。常看得到自驾游的大妈披着丝巾拍照。封路的时候，自驾游的车夹杂着拉水泥拉货的大卡车长长地排着队。一路上的比较大的客栈旅店，大都是汉族人开的，四川人很多，四川人勤劳敢拼，新疆西藏到处可见到他们的身影。新藏线这条长长的路上，整整齐齐训练有素的军车是一道靓丽的风景，让人感叹“好男儿就要当兵，保家卫国”。新藏线上，有人的地方就有众生相。到了萨嘎县之后，我们四人要分道扬镳了，我一路出吉隆口岸到加德满都，他们三人一路。旺哥先去珠峰大本营再去拉萨，邓翱和贤弟先去珠峰大本营再去加德满都。萨嘎县到吉隆县的路是非常难走的，没有硬实的柏油路，只有下雨后泥泞的泥路。十米宽的水坑，我脱了鞋子挂在车把上，推着车趟过去。泥泞的路，一脚一脚的踩踏着，车轮上滚满了泥巴，像是炸酥肉前给肉裹上厚厚的面。顶着下午刮个不停的巨大的逆风，长长的上坡，我再也没有了绝不推车的坚持。遇到难爬的上坡，就下来推着车走一段再骑车。大风和泥巴路，骑了一整天只走了五十公里，眼看就要露宿野外而帐篷早寄走了，终于看到一个大大的施工营地，像是溺水的人抓住一根救命稻草。营地里七八个西安的爷们收留了我一宿，度过了危机。到了基隆口岸，我换好尼币，经过中国海关，出了国门。尼泊尔的路才是真的烂路，石头路，泥路，山体滑坡封路都是家常便饭，偶尔能看到一段柏油路便开心地不行。尼泊尔的路是不推荐骑自行车的。尼泊尔是山地里的国家，较大的城镇都分布在平坦的河谷地。我买了尼泊尔的电话卡，但是用不了网络，不能看谷歌地图。我只能一边走，一边用蹩脚的英语问路“这条路是到加德满都吗？”；尼泊尔年轻人大多会说英语，偶尔几个会说汉语。在烂路里走了两天，我他娘的终于到了加德满都，在泰米尔区见到了中国人和中国店铺，激动地不行。只要有出发的勇气，就一定会到达！我终于到了加德满都！ 在山坡上俯瞰加德满都 新藏线一路上盖了一些邮戳，但并不多。骑车的间隙，抽空给15和16远征队的队友和一些老友寄去了明信片，现在有些明信片寄到了，有些寄丢了。我寄给自己的丢在路上了，就让他代替我在路上飘荡吧！骑完新藏线是一件了不起的事，是一件有意义的事。 下半年 - 研究生的日子暑假，在去新藏线之前来了读研的学校上了三周的暑期课程，《机器学习》和《凸优化》。9月初开学正式开始了研究生生活。之前我有一个错误的认识，认为研究生生活应该完全舍弃掉自行车，因为之前骑车太多了吧。后来，我认识到生活需要balance，不能只是学习，要让骑车和娱乐成为生活里积极的一部分。来了北京之后，出去玩的次数并不多。训超骑车从济南到北京，来我宿舍住了一晚，我跟他一块去了故宫。国庆节那天，旺哥来北京转车，我和训超早早地去看升旗，给旺哥接风洗尘。后来约着在北京读书工作的高中同学聚了一次。周末骑了两次车，去了卢沟桥和香山公园旁的西山国家森林公园。研一上，还有一些课要修。除了上课，其他时间是待在实验室里，看论文，写代码。前期，感觉有许多需要学习的东西，总能早早地起床。后来感觉不到自己的进步，有些陷入迷茫，不知道该做些什么，就失去了劲头，起床时间晚了许多。我找了一个辅导考研专业课的差事，每周末辅导一个半小时，《通信原理》已经大半年没看过了，但上手简单看一遍就能想起来那些知识点。妈妈生日那天，我买了一只天鹅项链做生日礼物。弟弟生日那天，我给他发了红包，他没领说自己有钱。爸爸今天生日，我鼓动身边的朋友给老爸发了祝福的短信。2019年，希望可以更加勇敢和更加积极地面对生活里的变化。你好，2019！","categories":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/categories/年度总结/"}],"tags":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/tags/年度总结/"},{"name":"生活记录","slug":"生活记录","permalink":"http://yoursite.com/tags/生活记录/"}]},{"title":"用Hexo+Next+github page搭建个人博客","slug":"用hexo搭建个人博客","date":"2019-01-13T02:08:31.000Z","updated":"2019-07-22T00:33:50.000Z","comments":true,"path":"2019/01/13/用hexo搭建个人博客/","link":"","permalink":"http://yoursite.com/2019/01/13/用hexo搭建个人博客/","excerpt":"Hexo是一个简洁漂亮的博客框架，用来生成静态网页。本文介绍使用hexo搭建博客。","text":"Hexo是一个简洁漂亮的博客框架，用来生成静态网页。本文介绍使用hexo搭建博客。 准备：安装Hexo安装前提在安装Hexo之前，应先安装以下应用程序： Node.js Git 安装Git对Linux(ubuntu,debian) 1sudo apt-get install git-core 安装Node.js1wget -qO- https://raw.github.com/creationix/nvm/v0.33.11/install.sh | sh 安装完成后，需要重启终端再执行以下命令： 1nvm install stable 安装Hexo安装好Git和Node.js后，使用npm安装Hexo: 1npm install -g hexo-cli hexo常用命令 hexo init #用于新建一个网站 hexo new #在站点根目录下执行，以/scaffolds/post.md为模板，创建一篇新的文章。新建文章存放在/source/_posts下。 hexo generate 简写为 hexo g #生成静态网页 hexo server 简写为 hexo s #启动服务器，默认访问网址为http://localhost:4000/ hexo clean #用于清楚缓存文件和已经生成的静态文件。当对站点的修改无论如何不能生效时，运行该命令。 hexo deploy 简写为 hexo d #将生成的静态文件部署到github等远程服务器。常用的套路是先使用hexo g和hexo s在本地编辑和调试博客，当调试无误后，再用hexo d部署到GitHub。 本地建站选好路径，以下命令会自动创建文件夹来存放博客内容。 1hexo init &lt;folder&gt; #生成hexo模板 上述命令生成了以下文件,生成文件夹的目录如下。 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 上述文件中，只需要重点关注和了解这几个文件： _config.yml 是站点配置文件。用来配置博客网站的各种信息。 /source/_posts 用来存放我们自己之后写的博客。 themes 用来存放博客的主题。再执行以下命令:123cd &lt;folder&gt;npm installhexo server 执行完以上命令，可以访问 http://localhost:4000 看到博客已经顺利搭建。 修改站点配置文件hexo中有两个重要的配置文件，名称都是 _config.yml： 站点配置文件，位于站点根目录下，用于站点本身的配置。 主题配置文件，位于主题目录下，用于主题相关的配置。 通过修改站点配置文件的下述字段，来分别修改网站名称、副标题、个性签名(或网站简介)、作者和语言设置。 12345title: subtitle:description: author: language: zh-Hans #设置语言为中文 将博客关联到GitHub1、在github创建仓库 &lt;your-user-name&gt;.git.io,&lt;your-user-name&gt;必须与你的github用户名相同。2、编辑站点配置文件，找到字段deploy，作如下的修改。 1234deploy: type: git repository: https://github.com/&lt;your-user-name&gt;/&lt;your-user-name&gt;.github.io.git branch: master 3、使用命令npm install --save安装插件。 1npm install hexo-deployer-git --save 4、生成静态网页 1hexo g 5、部署到GitHub 1hexo d 运行上述命令后，访问 http://&lt;your-user-name&gt;.git.io ‘hexo d’部署时不输密码执行hexo d将博客部署到github时，每次都要输入github的账号和密码，非常麻烦。我们利用ssh的公钥登录免去输入密码的步骤。 ssh的公钥登录简介所谓公钥登录。本地主机生成一个公钥和一个秘钥，把公钥告诉给远程主机（github），而秘钥只有自己知道。当本地主机要登录远程主机时，远程主机（github）发送一个消息序列给本地主机，本地主机用秘钥将消息序列加密后再发送给远程主机；远程主机收到后用公钥解密，若与原消息序列相同，则可以登录。通过这种“公钥登录”的方式保证了安全性，不需要再用账户和密码来验证身份。 本机主机生成公钥和秘钥在命令行执行： 1ssh-keygen 生成的文件有id_rsa.pub和id_rsa，分别为公钥和私钥。Linux系统下，生成文件在home/ssh/目录下。windows系统,在C:/user/.ssh下。 把公钥告诉给github在Linux系统下，执行以下命令查看公钥，然后复制。在windows系统下，将id_rsa.pub用文本格式打开来复制。 12cd ~/.sshcat id_rsa.pub 在github的settings&gt;SSH and GPG keys&gt;New SSH keys新建一个ssh key，将复制的公钥粘贴保存即可。 修改站点配置文件用ssh方式来访问仓库，而不是用https方式打开站点配置文件，修改字段deploy下的repository的值。 12345deploy: type: git # repository: https://github.com/spring-quan/spring-quan.github.io repository: git@github.com:spring-quan/spring-quan.github.io.git branch: master 参见SSH远程登录Hexo免输入密码部署到Github 绑定域名购买并注册域名在阿里云的万网购买一个域名。常用的域名后缀有.com/.cn/.me/.top。不同后缀的域名价格不同，可以按照喜好选择一个。购买好域名后，需要完成 域名实名认证，才能正常进行域名解析，该域名才能使用。 注意：注册域名后，需要域名实名认证，不需要备案，域名就可以正常使用。 给GitHub添加域名在博客对应的github仓库下，新建一个文件CNAME，文件名要大写。内容如下，注意要为顶级域名，不能包括www或http。 1example.com 在用hexo d命令将博客部署到GitHub上时，文件CNAME会被覆盖删除掉。为了解决这个问题，还需要在站点目录下/source，新建同样的文件CNAME。 设置域名解析设置域名解析就是将注册到的域名指向一个IP地址，首先我们要查询github.io域名对应的IP地址。在命令行中ping自己的github.io域名，就可以得到IP地址。 1ping &lt;your-user-name&gt;.github.io 接着，在阿里云万网的管理控制台&gt;域名服务&gt;域名列表中设置域名解析即可。这时域名就绑定好了，可以通过buptccq.top 和www.buptccq.top 来访问博客。 注意：1、设置域名解析不是立即生效的，需要等10分钟左右。2、若用自定义的域名访问失败后，经过修改，避免浏览器的DNS缓存，可以使用浏览器的无痕模式。 写作博客+技巧在站点根目录下，执行以下命令创建一篇新文章： 1hexo new &lt;title&gt; 上述命令在站点根目录下的\\source_posts 文件夹生成一个 .md文件。按照markdown规则，编辑该文件。接下来介绍编辑md文件的几个技巧。 块引用1、一种方法。 12&gt; 只是块引用&gt; 是的啊 效果图如下： 只是块引用是的啊 markdown语法 参考链接markdown中文文档Markdown 入门参考MarkDown语法简介Markdown 书写风格指南 绘制表格用|来分隔不同的单元格，用-来分隔表头和其他行。关于对齐： :--- 左对齐，默认情况 :---: 居中 ----: 右对齐 使用方式举例： 1234|主公|刘备|曹操|孙权||----|----|----|----||政权|蜀|魏|吴||相关|关羽、张飞|曹丕、曹植|周瑜、鲁肃| 实现效果： 主公 刘备 曹操 孙权 政权 蜀 魏 吴 相关 关羽、张飞 曹丕、曹植 周瑜、鲁肃 删除线使用方式： 1~~删除线~~ 使用效果为：删除线 markdown其他用法12345678910111213141516171819202122&lt;!-- more --&gt; #在首页显示摘要 content #块引用&lt;www.baidu.com&gt; #链接[百度](www.baidu.com) #内嵌链接![](/path/to/image &apos;image name&apos;)有序列表1. 2.3.无序列表- - - **粗体字***斜体字*``代码``## 二级标题### 三级标题 文字居左，居中，居右居中 1&lt;center&gt;诶嘿&lt;/center&gt; 居左 1&lt;p align=&quot;left&quot;&gt;诶嘿&lt;/p&gt; 居右 1&lt;p align=&quot;right&quot;&gt;诶嘿&lt;/p&gt; 插入表情😀😇👹 给博客选用主题这里要介绍一下“主题”这个概念。主题也就是博客的样式。给博客安装一个好的主题，就好像给人选一件漂亮的衣服穿。hexo有多种不同的主题，可以改hexo选择不同的样式和布局。主题列表有详细的介绍。使用最多的主题是Next,接下来介绍主题的安装。 安装主题Next 先下载主题Next，在站点根目录下执行： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题，打开站点配置文件，找到theme字段，把它的值修改为 next 1theme: next 验证主题。运行’hexo server’,访问http://localhost:4000。 选择Scheme通过修改主题配置文件完成。打开主题配置文件，找到字段scheme，将不要的主题注释掉，要用的主题不注释。 123#scheme: Muse#scheme: Mistscheme: Pisces 设置语言为中文通过修改站点配置文件完成。打开站点配置文件，找到字段language，把它值修改为 zh-Hans 1language: zh-Hans 主题美化-1设置上部的菜单栏 设置菜单内容。通过修改主题配置文件完成。打开主题配置文件，找到字段menu，不用的菜单栏选项注释掉，要用的不注释。 1234567menu: home: / archives: /archives # 冒号右边为主题目录下/source下的文件夹。 #about: /about #categories: /categories tags: /tags #commonweal: /404.html 设置菜单项的显示文本。编辑主题目录下的/languages/zh-Hans.yml。 12345678menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 commonweal: 公益404 设置菜单项的图标。编辑主题配置文件，找到字段menu_icons,修改为： 12menu_icons: enable: true 设置头像打开主题配置文件，找到字段avatar，把它的值修改为头像照片的路径。将头像照片放在主题目录/source/images(如果不存在就创建)。 1avatar: /images/avatar.png 添加‘标签’页面首先要在菜单栏显示‘标签’链接。在站点目录下，新建一个页面： 1hexo new page tags 编辑刚刚生成的页面,修改字段 title 和 type 123title: 标签date: 2019-01-13 12:22:58type: &quot;tags&quot; 在菜单栏中添加链接。修改主题配置文件，找到字段menu下的字段tags，把它值修改为 /tags 1234menu: home: / archives: /archives tags: /tags 添加‘分类’页面与添加‘标签’页面基本相同。首先要在菜单栏显示‘分类’链接。在站点目录下，新建一个页面： 1hexo new page categories 编辑刚刚生成的页面,修改字段 title 和 type 123title: 分类date: 2019-01-13 12:22:58type: &quot;categories&quot; 在菜单栏中添加链接。修改主题配置文件，找到字段menu下的字段tags，把它值修改为 /tags 12345menu: home: / archives: /archives tags: /tags categories: /categories 新建一个友链页面 在菜单栏中添加链接和链接图标。编辑站点配置文件，找到字段menu，添加以下内容： 12menu: links: /links/ || link # ||左边代表文件夹/source/links,右边代表图标 修改链接的显示文本，编辑文件/themes/next/languages/zh-Hans.yml,找到字段menu。 12menu: links: 友链 在站点目录下执行以下命令,会生成/source/links/index.md文件。通过编辑该文件可以控制友链页面的显示内容。 1hexo new page links 关闭’标签’、’分类’页的评论分别编辑站点目录下的/source/tags/index.md和/source/categories/index.md,添加字段comments,把它的值修改为false。 1comments: false 侧边栏添加社交链接通过修改主题配置文件完成。打开 主题配置文件 ，修改两个部分，链接和链接图标。1、添加链接。找到字段 social ，一行为一个链接。格式为 显示文本：链接 1234# Social linkssocial: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 2、修改链接图标。找到字段 social_icons 。键值对格式为 匹配键：图标名称 。在图标库找到合适的图标，将名字作为图标名称。 123456# Social Iconssocial_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 侧边栏添加友情链接打开 主题配置文件 ，找到字段 links 。键值对格式为 显示文本：链接 12345678# Blog rollslinks_icon: link #链接的图标links_title: 推荐阅读 #链接的名称links_layout: block #链接的布局，一行一个链接#links_layout: inline #一行多个链接links: # Title: http://example.com/ 廖雪峰: https://www.liaoxuefeng.com/ 开启打赏功能通过修改主题配置文件完成。打开 主题配置文件 ，找到以下字段。将收款二维码放到站点目录下的/source/images/(没有该文件夹，就新建)中。 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 修改打赏字体不闪动修改文件MyBlog/themes/next/source/css/_common/components/post/post-reward.styl，注释文字闪动的函数。 123456789101112/*注释文字闪动的函数#wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;#alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/ 订阅微信公众号在每篇文章的末尾显示微信公众号。把微信公众号二维码放在MyBlog/themes/next/source/images/下。编辑 主题配置文件 ，找到字段wechat_subscriber，修改它的值。 12345# Wechat Subscriberwechat_subscriber: enabled: true qcode: /path/to/your/wechatqcode description: 欢迎关注我 设置腾讯公益404界面1、在目录MyBlog/themes/next/source/下新建404.html页面，内容如下： 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 2、修改 主题配置文件 ,找到字段 menu 下的字段 commonweal，把它值修改为 /404.html 12menu: commonweal: /404.html || heartbeat 设置背景动画编辑 主题配置文件。以下四个字段为不同的背景动画效果，要想启用，将false改为true。 1234567891011# Canvas-nestcanvas_nest: false# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false 添加评论系统‘网易云跟帖’评论已经不提供服务。我们采用’来必力’评论系统时，需要获取uid。1、先在来必力的官方网站注册账号，在&gt;安装下安装city版本，再在&gt;管理页面&gt;代码管理下获取uid。data-uid字段的值就是我们需要的uid。2、编辑 主题配置文件 ，找到字段’livere_uid’,把它的值改为data-uid字段的值。 1livere_uid: #your uid 添加搜索功能通过Local Search搜索服务实现搜索功能。1、安装插件hexo-generator-searchdb，在站点的根目录下执行 1npm install hexo-generator-searchdb --save 2、编辑 站点配置文件,在文末新增以下内容： 12345search: path: search.xml field: post format: html limit: 10000 3、编辑主题配置文件,找到字段”local_search”，修改值为true。 12local_search: enable: true 主题美化-2设置【阅读全文】在首页只显示文章的部分内容，通过按钮【阅读全文】来实现跳转。有三种方式可以实现该功能，这里介绍最方便的一种。在文章中使用&lt;!-- more --&gt;来进行截断，&lt;!-- more --&gt;之前的内容显示在首页，后面的内容不显示。 1&lt;!-- more --&gt; 分页-设置页面文章的篇数为网站首页、归档页和标签页设置不同的文章篇数。1、使用npm install --save安装插件。 123npm install --save hexo-generator-indexnpm install --save hexo-generator-archivenpm install --save hexo-generator-tag 2、编辑站点配置文件,找到以下字段并修改。 12345678910index_generator: per_page: 10 archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 设置网站的图标在EasyIcon中找一张中意的图标。修改图标名称为favicon.ico，将图标放在路径/next/source/images/下。编辑主题配置文件，找到字段favicon，修改它的值。 123favicon: small: /images/favicon.ico medium: /images/favicon.ico 添加顶部的加载条修改主题配置文件,找到字段pace,把值设为true。还可以选择不同风格的加载条。 12345678910111213141516pace: true # Themes list:#pace-theme-big-counter#pace-theme-bounce#pace-theme-barber-shop#pace-theme-center-atom#pace-theme-center-circle#pace-theme-center-radar#pace-theme-center-simple#pace-theme-corner-indicator#pace-theme-fill-left#pace-theme-flash#pace-theme-loading-bar#pace-theme-mac-osx#pace-theme-minimalpace_theme: pace-theme-flash 修改文章底部的标签#为修改文件 /themes/next/layout/_macro/post.swig,搜索rel=&quot;tags&quot;&gt;#，将#换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;。也可以在图标库找自己中意的图标。 修改网页底部的桃心编辑/themes/next/layout/_partials/footer.swig，找到以下代码,将第二个class的值并修改为&quot;fa fa-heart&quot;，在图标库找自己中意的图标。 123&lt;span class=&quot;with-love&quot;&gt; &lt;i class=&quot;fa fa-&#123;&#123; theme.footer.icon &#125;&#125;&quot;&gt;&lt;/i&gt;&lt;/span&gt; 实现统计功能使用npm install --save安装插件 1npm install hexo-wordcount --save 网站底部字数统计编辑/themes/next/layout/_partials/footer.swig，在文末添加以下代码： 1234&lt;div class=&quot;theme-info&quot;&gt; &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt; &lt;span class=&quot;post-count&quot;&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 文章字数和阅读时长编辑主题配置文件，找到字段post_wordcount,并修改。 123456post_wordcount: item_text: true wordcount: true #字数统计 min2read: true #阅读时长统计 totalcount: true #站点总字数 separated_meta: true 统计访问量和阅读量我们用busuanzi实现统计功能。1、全局配置。编辑主题配置文件，找到字段busuanzi_count,将enable的值改为true。2、编辑字段site_uv,统计本站访客数。3、编辑字段site_pv,统计本站总访问量。4、编辑字段page_pv,统计每篇文章的阅读量。 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;本站访客数 site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt;本站总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt;本文总阅读量 page_pv_footer: 次 问题：busuanzi统计不显示数字（2019.1.15）。原因是buxuanzi的域名变更，由&lt;dn-lbstatics.qbox.me&gt;变为&lt;busuanzi.ibruce.info&gt;,导致统计功能不能实现。解决方案如下： 编辑文件themes\\next\\layout\\_third-party\\analytics\\busuanzi-counter.swig,找到以下代码 1&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 将其修改为： 1&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 重新推送后，可以看到统计功能正常。 实现点击出现桃心1、在路径/theme/next/source/js/src/下新建 love.js2、编辑文件/theme/next/layout/_layout.swig文件，在文末添加代码： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 修改链接文本样式修改文件themes\\next\\source\\css\\_common\\components\\post\\post.styl,在文末添加如下代码。关于颜色的代码见详情 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 置顶某篇文章修改文件MyBlog/node_modules/hexo-generator-index/lib/generator.js,将全部代码替换为 12345678910111213141516171819202122232425262728&apos;use strict&apos;;var pagination = require(&apos;hexo-pagination&apos;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &apos;page&apos;; return pagination(&apos;&apos;, posts, &#123; perPage: config.index_generator.per_page, layout: [&apos;index&apos;, &apos;archive&apos;], format: paginationDir + &apos;/%d/&apos;, data: &#123; __index: true &#125; &#125;);&#125;; 在文章front-matter中添加字段top，该字段的数值越大文章越靠前。 12title: 第三篇top: 10 文章加密访问编辑文件MyBlog/themes/next/layout/_partials/head.swig,在第五行插入以下代码： 1234567891011121314&lt;script&gt; (function () &#123; if (&apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; alert(&apos;密码错误！&apos;); if (history.length === 1) &#123; location.replace(&quot;http://xxxxxxx.xxx&quot;); // 这里替换成你的首页 &#125; else &#123; history.back(); &#125; &#125; &#125; &#125;)();&lt;/script&gt; 在文章front-matter中添加字段password，该字段的值不为空则文章加密，若字段的值为空则文章没有加密。 12title: 第三篇password: 自定义新建文章的md文件修改/scaffolds/post.md文件为： 123456789---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags: # 标签categories: # 分类copyright: # true添加版权信息top: # 数值越大，文章在首页越靠前。用于文章置顶password: # 为空则文章不加密。用于文章加密访问--- 显示浏览进度编辑主题配置文件，修改字段scrollpercent的值为true 12# Scroll percent label in b2t button.scrollpercent: true 使得HEXO的next主题渲染LATEX数学公式编辑主题配置文件，修改字段mathjax的值为true 123# MathJax Supportmathjax: enable: true 主题美化-3修改代码块自定义样式编辑\\themes\\next\\source\\css\\_custom\\custom.styl,在文末添加如下代码。关于颜色的代码见详情 123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125; 设置代码高亮主题Next使用 Tomorrow Theme作为代码高亮。有五个主题可供选择。1.编辑 主题配置文件，找到字段highlight_theme,修改它的值。 12345# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal 实现代码变成彩色。修改站点配置文件，找到字段highlight下的字段auto_detect的值为true1234highlight: enable: true line_number: true auto_detect: true 添加代码块复制功能通过第三方插件clipboard.js来实现复制功能。1.目录/themes/next/source/js/src下新建两个 .js文件。 clipboard.min.js。下载第三方插件clipboard.min.js clipboard-use.js，文件内容如下： 1234567891011121314151617/*页面载入完成后，创建复制按钮*/!function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = &apos;&apos;; copyHtml += &apos;&lt;button class=&quot;btn-copy&quot; data-clipboard-snippet=&quot;&quot;&gt;&apos;; copyHtml += &apos; &lt;i class=&quot;fa fa-copy&quot;&gt;&lt;/i&gt;&lt;span&gt;复制&lt;/span&gt;&apos;; #复制按钮的图标和显示文本 copyHtml += &apos;&lt;/button&gt;&apos;; $(&quot;.highlight .code pre&quot;).before(copyHtml); new ClipboardJS(&apos;.btn-copy&apos;, &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode();&#125;(window, document); 2.在文件\\themes\\next\\source\\css\\_custom\\custom.styl文末添加代码： 123456789101112131415161718192021222324252627282930313233343536//代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 3.在.\\themes\\next\\layout\\_layout.swig文末添加代码： 123&lt;!-- 代码块复制功能 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clipboard-use.js&quot;&gt;&lt;/script&gt; 参见：Hexo NexT主题代码块添加复制功能Hexo next博客添加折叠块功能添加折叠代码块 文章摘要图片让图片出现在网站首页的文章摘要中，但不出现在文章的正文中。 编辑主题配置文件，修改字段expert_description的值为false 1234excerpt_description: falseauto_excerpt: enable: false 编辑文件themes/next/layout/_macro/post.swig，找到代码： 12&#123;\\% elif post.excerpt %\\&#125; &#123;&#123; post.excerpt &#125;&#125; 在它后面添加如下代码： 12345&#123;\\% if post.image %\\&#125;&lt;div class=&quot;out-img-topic&quot;&gt; &lt;img src=&#123;&#123; post.image &#125;&#125; class=&quot;img-topic&quot; /&gt;&lt;/div&gt;&#123;\\% endif %\\&#125; 编辑文件/themes/next/source/css/_custom/custom.styl,在文末添加如下代码：1234// 自定义的文章摘要图片样式img.img-topic &#123; width: 100%;&#125; 使用方式是，在文章的.md文件的front-matter加上一行image: path/to/image： 12345678910title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;keywords: # 用于SEO搜索引擎优化tags: # 标签categories: # 分类copyright: # true添加版权信息top: # 数值越大，文章在首页越靠前。用于文章置顶password: # 为空则文章不加密。用于文章加密访问description:# 用于设置【阅读全文】image: # 文章摘要图片 设置背景图片方法一通过jquery-backstretch,编辑文件/themes/next/layout/_layout.swig,在最后的&lt;/body&gt;之前添加 123&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;body&quot;).backstretch(&quot;https://背景图.jpg&quot;); 方法二编辑文件/themes/next/source/css/_custom/custom.styl,添加代码： 12345// Custom styles.body &#123; background:url(/images/background.jpg); #图片放在/images文件中 background-attachment: fixed; #固定背景图&#125; 搜索引擎优化（SEO）-提高网站排名我们搭建好博客之后，在搜索引擎（百度、Google）中搜索博客，会发现根本搜索不到。这是因为我们的博客网站没有没搜索引擎收录，也就搜索不到。因此我们需要进行搜索引擎优化。搜索引擎优化是为提高我们的博客在搜索结果中的排名，来增大网站的流量。通俗地说，就是让我们的博客更容易被搜索到。 让搜索引擎收录我们的网站。 提高网站在搜索结果中的排名。 hexo优化准备编辑配置文件站点配置文件站点配置文件中的这四项一定要写 1234title: subtitle: description: url: 打开Next主题自带的SEO配置编辑主题配置文件,找到这四个字段，并设置为true。 1234canonical: trueseo: trueindex_with_subtitle: truebaidu_push: true 设置关键字 设置博客的关键字。编辑站点配置文件，添加字段keywords，值用逗号隔开。 设置文章的关键字。在文章中，添加字段keywords，值用逗号隔开。1keywords: word1,word2,word3 网站首页title优化我们知道读文章时，标题和摘要往往比段落中的大段文字重要。同样在搜索引擎爬取收录网页时，title起到更重要的作用。关键词出现在网站title中，可以提高网站被搜索到的可能。网站的title一般不超过80字符。我们把网站首页的title改为网站名称-关键词-网站描述的形式。编辑文件\\themes\\next\\layout\\index.swig，将以下代码 1&#123;\\% block title %\\&#125;&#123;&#123; config.title &#125;&#125;&#123;\\% if theme.index_with_subtitle and config.subtitle %\\&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;\\% endif %\\&#125;&#123;\\% endblock %\\&#125; 修改为，可以看到通过插入和来添加关键词和网站描述。 1&#123;\\% block title %\\&#125;&#123;&#123; config.title &#125;&#125; - &#123;&#123; config.keywords &#125;&#125; - &#123;&#123; theme.description &#125;&#125;&#123;\\% if theme.index_with_subtitle and config.subtitle %\\&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;\\% endif %\\&#125;&#123;\\% endblock %\\&#125; 网站名称、关键词、网站描述的显示文本分别对应站点配置文件中的字段title，keywords和description的值。 1234567title: spring&apos;Blog #网站标题subtitle: description: #对应网站描述keywords: #关键词author: spring-quanlanguage: zh-Hanstimezone: 给所有外部链接添加nofollow标签 使用npm install --save安装插件hexo-autonofollow 1npm install hexo-autonofollow --save 编辑站点配置文件,在文末添加如下代码。不想添加的链接写在字段exclude后。 12345nofollow: enable: true exclude: - exclude1.com - exclude2.com 提交链接和验证网站这一步是让搜索引擎收录我们的网站。我们需要在搜索引擎提交我们的链接，提交链接时需要验证这个网址是我们的，不是别人的。以下为搜索引擎的网站管理工具 google的网站管理工具Search Console 百度的搜索资源平台 搜索引擎给出了网站验证的多种方式，百度和谷歌进行网站验证的原理和步骤差不多。这里介绍两种验证方式：html文件验证和CNAME验证html文件验证 下载验证文件.html，放到站点目录/source下。 hexo会自动对HTML文件进行渲染，但我们不希望验证文件被渲染，需要做一些配置。修改站点配置文件，编辑字段skip_render的值为 验证文件名.html。该字段后可以有多个值，以逗号分隔。该字段后的文件名会跳过渲染。 1skip_render: google6af533d85b9bfd8c.html,baidu_verify_mreXNGCYRV.html 清除缓存并将修改同步到github. 12hexo cleanhexo g -d 点击完成验证。 CNAME验证 CNAME验证是设置域名解析。需要登录阿里云万网。 选择控制台&gt;产品与服务&gt;域名，管理域名的解析。添加CNAME类型的解析。 站点地图sitemap和爬虫协议robot.txt 站点地图是一个文件。告诉Google和百度应该抓取哪些网页。robot.txt也是一个文件。告诉Google和百度不应该抓取哪些网页。 站点地图sitemap先添加站点地图文件，再提交给Google和百度。告诉Google和百度应该抓取哪些网页。 使用命令安装自动生成站点地图sitemap的插件。 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 编辑站点配置文件，在文末添加以下代码： 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 执行命令，会在站点目录下的public文件夹生成站点地图文件sitemap.xml和baidusitemap.xml文件，前者提交给Google，后者提交给百度。 1hexo g 把站点地图文件提交给Google和百度。 在Google的旧版Search Console提交站点地图文件https://buptccq.top/sitemap.xml(修改为你自己的域名)。 在百度的网站管理工具提交站点地图文件https://buptccq.top/baidusitemap.xml(修改为你自己的域名)。 爬虫协议robot.txt，又成蜘蛛协议添加爬虫协议文件robot.txt，再提交给Google和百度。告诉Google和百度不要抓取哪些网页。 在站点目录source下新建文件robot.txt。内容如下，注意修改为自己的域名。 12345678910111213141516#hexo robots.txtUser-agent: *Allow: /Allow: /archives/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: http://blog.tangxiaozhu.com/search.xml #注意修改为你自己的域名Sitemap: http://blog.tangxiaozhu.com/sitemap.xmlSitemap: http://blog.tangxiaozhu.com/baidusitemap.xml 执行命令hexo d -g部署到github上。 1hexo d -g 再把爬虫协议文件提交给Google和百度。 Google的旧版Search Console。 百度的网站管理工具。 谷歌收录和百度收录查看网站是否被收录在百度或Google浏览器中，以site:&lt;your url&gt;形式搜索你的博客。如果能搜到，就是被收录了。搜不到就是没有收录。 谷歌收录百度收录我们在前面已经把博客首页的网址收录到百度搜索引擎了，但当我们写出新的博客，产生新的链接后，需要把这些新的链接收录到百度。这里介绍两种实现自动收录的方式：主动推送和自动推送主动推送主动推送将新产生的链接立即推送给百度搜索引擎，从而保证新链接被尽早收录。 使用命令npm install --save安装自动百度推送插件。 1npm install hexo-baidu-url-submit --save 编辑站点配置文件，在文末添加以下代码： 12345baidu_url_submit: count: 3 ## 比如3, 代表提交最新的三个链接 host: &lt;www.henvyluk.com&gt; ## 在百度站长平台中注册的域名 token: &lt;your_token&gt; ## 请注意这是您的秘钥, 请不要发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址, 新链接会保存在此文本文档里 其中&lt;your_token&gt;可以在百度网址管理工具中找到： 需要注意的是，编辑站点配置文件，确保字段url的值为你的域名。 123# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://buptccq.top #http://yoursite.com 编辑站点配置文件，找到字段deploy,添加一个新的deploy类型-type: baidu_url_submitter。不同的deploy类型用-type表示。 123456deploy:- type: baidu_url_submitter- type: git # repository: https://github.com/spring-quan/spring-quan.github.io repository: git@github.com:spring-quan/spring-quan.github.io.git branch: master 主动推送的原理如下： 产生新链接。hexo g会生成文本文件baidu_urls.txt，里面包含最新的链接。 推送新链接。hexo d会从文本文件baidu_urls.txt中读取新链接，再推送给百度。 自动推送编辑主题配置文件，修改字段baidu_push的值为true。 12# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 参见：基于Hexo搭建个人博客——进阶篇(从入门到入土)Hexo Seo优化让你的博客在google搜索排名第一 ##踩过的坑 “hexo server”本地启动时，报错如下： 1Unhandled rejection Error: ENOENT: no such file or directory, open &apos;/MyBlog/themes/next/layout/_scripts/schemes/.swig&apos; 错误原因处在主题配置文件_config.yml存在不规范的内容。要仔细检查，规范化内容就行。修改过来之后，要先执行hexo clean,再hexo server就可以了。 参考文章 Hexo官方教程 Next官方教程 markdown中文文档 打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化 【推荐，整理很系统】 基于Hexo搭建个人博客——进阶篇(从入门到入土) hexo的next主题个性化教程:打造炫酷网站 Hexo搭建博客教程 手把手教你使用Hexo + Github Pages搭建个人独立博客 从 0 开始搭建 hexo 博客","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"}]}]}