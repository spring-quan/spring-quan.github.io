{"meta":{"title":"spring's Blog","subtitle":null,"description":"游龙当归海，海不迎我自来也。","author":"spring","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-07-07T08:07:36.000Z","updated":"2019-07-07T08:23:22.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-07-07T08:07:24.000Z","updated":"2019-07-07T08:23:01.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"pip安装python模块报错","slug":"pip安装python模块报错","date":"2019-07-12T01:51:58.000Z","updated":"2019-07-12T02:11:58.000Z","comments":true,"path":"2019/07/12/pip安装python模块报错/","link":"","permalink":"http://yoursite.com/2019/07/12/pip安装python模块报错/","excerpt":"在使用pip install命令安装python模块时，报错： 1Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.","text":"在使用pip install命令安装python模块时，报错： 1Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. 错误分析报错信息告诉我们：“不能卸载‘pyyaml’模块，因为这个模块是distutils方式安装的，不能确定哪些文件属于这个模块，因此不能完整地卸载这个模块。” distutils是python最初的模块安装和分发系统，distutils不会保留哪些文件属于哪个安装包的信息，甚至不会保留安装包之间的依赖关系。直接使用distutils的方式已经被淘汰，取而代之的是setuptools. 所谓模块的分发，就是开发者打包并发布自己的模块，供其他人使用。 这样我们就知道了，因为pyyaml模块时通过distutils方式安装的，因此不能明确文件与包之间的隶属关系，不能正确卸载。 解决办法使用下面的命令忽略已安装的模块，强制安装和更新 1pip3 install &lt;package-name&gt; --ignore-installed &lt;pyyaml&gt; --upgrade 参考链接 强制安装和更新 如何在Windows操作系统中升级/卸载distutils软件包（PyYAML）？ python官方手册-安装python模块 setuptools与distutils","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"NAACL2019-对话系统","slug":"NAACL2019-对话系统","date":"2019-07-10T12:55:17.000Z","updated":"2019-07-10T13:10:29.000Z","comments":true,"path":"2019/07/10/NAACL2019-对话系统/","link":"","permalink":"http://yoursite.com/2019/07/10/NAACL2019-对话系统/","excerpt":"记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。NAACL2019的会议列表链接：https://naacl2019.org/program/accepted/","text":"记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。NAACL2019的会议列表链接：https://naacl2019.org/program/accepted/ 《Evaluating Coherence in Dialogue Systems using Entailment》【链接】https://arxiv.org/abs/1904.03371【代码】https://github.com/nouhadziri/DialogEntailment 加拿大阿尔伯塔大学发表的论文。论文提出了一种评估对话系统生成回复好坏的指标。这篇论文的想法来源于：发表在ACL2019上的论文《Dialogue Natural Language Inference》提出利用NLI(natural language inference)任务来提高对话系统生成回复的一致性。本文的作者则想到用NLI任务来评估对话系统生成回复的好坏。具体地，论文用了BERT[Devlin et al., 2018]和The Enhanced Sequential Inference Model(ESIM)[Chen et al., 2016] 这两种方法来训练NLI模型。另外论文还公开了一个用于NLI任务的数据集。","categories":[],"tags":[]},{"title":"ACL2019-对话系统","slug":"ACL2019-对话系统","date":"2019-07-07T11:19:27.000Z","updated":"2019-07-20T07:59:54.000Z","comments":true,"path":"2019/07/07/ACL2019-对话系统/","link":"","permalink":"http://yoursite.com/2019/07/07/ACL2019-对话系统/","excerpt":"记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml","text":"记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml 《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》【链接】：https://arxiv.org/abs/1906.01788【源码】：无 中科院自动化所发表的短论文。在多轮对话中，对话历史（context information）对回复（response）的生成有重要作用。任务型对话中的管道模型分为4个模块：NLU、对话状态追踪、对话策略学习 及NLG。对话状态追踪又包含任务：domain classification、intent detection和slot filling。domain classification和intent detection任务当做分类任务来处理，常采用SVM或深度神经网络的方法；slot filling任务被当做序列标注任务来处理，常采用BiLSTM+CRF模型。NLU能否充分利用context information，对这三个下游任务有很大影响。为了更好的利用context information，本文提出了对话逻辑推断任务（DLI,dialog logic inference），任务定义为：将打乱顺序的多轮对话重新排序；输入之前的对话，从剩余的utterance candidates中选中下一句对话。NLU任务采用了所谓的memory network，其实就是采用多个encoder对context information进行编码，再用attention机制或别的方法得到context information总的向量化表示。本文联合训练DLI任务和NLU任务，通过两个任务共享encoder和memory retrieve模块，来让NLU任务更好地利用context information。其实是得到context information更合理的向量化表示，来作为下游domain classification、intent detection和slot filling任务的输入。 论文提出的将打乱顺序的对话重新排序的DLI任务，可以进一步深入，将句子切分为几段打乱顺序再重新排序；可以应用到闲聊式对话系统中。 《Dialogue Natural Language Inference》【链接】：https://arxiv.org/abs/1811.00671【代码】：无【数据集】：https://wellecks.github.io/dialogue_nli/ 加利福尼亚大学、Facebook AI Lab发表的论文。核心是提出用NLI(natural language inference)任务来提高persona-based dialog system的一致性。这里就要先搞清楚NLI任务和一致性问题两个概念。 先从问题出发，所谓对话的一致性问题。可以分为两类： logical contradiction，逻辑矛盾。比如同一个人的两句话:”我有一只狗”，”我没养过狗”。就是逻辑矛盾的。 比较模糊的非逻辑矛盾。同一个人不可能说出的两句话：“我从来不运动”，“我去篮球了”。就是这种非逻辑矛盾。真香警告。 至于persona一致性问题，就是回复的utterance不能与说话人的persona矛盾，也不能与之前的回复有矛盾。 具体介绍NLI任务。这其实是一个分类问题。论文公开了一个自己标注的NLI数据集。 训练阶段：训练集形式是 {$（s_1,s_2）$,label }，对应labels $\\in$（一致、无关、矛盾）。 在test阶段，给定一个句子对（句子1，句子2）来判断对应的label。 论文的最终目的是通过NLI任务训练的模型来提高persona dialog system的一致性。这是如何来实现的呢？对于一个dialog system，给定对话历史$（u_1,u_2,…,u_t）$ 及说话人的persona文本描述$（p_1,p_2,…,p_n）$,从response candidates$（y_1,y_2,…,y_m）$中选择一个$u_{t+1}$（如何生成多个responses不是这篇论文要解决的）。用NLI任务的模型来预测$(y_i,u_j),(y_i,p_k)其中：i\\in [1,m],j\\in [1,t],k \\in [1,n]$对应的label，如果句子之间是矛盾的，则添加惩罚项。从而得到一致性最好的utterance作为response。 《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》【链接】：https://arxiv.org/abs/1907.05339【数据集】：English Ubuntu dialogue corpus【代码】：https://github.com/zhanghainan/ReCoSa 中科院发表的论文。在多轮对话中，生成response时，对话历史中最相关的部分起着重要的作用。论文要解决的问题：如何更准确地找到并利用relevant context来生成response。多轮对话中广泛使用的HRED模型,[(Serban et al.,2016;,Sordoni et al., 2015]无差别地利用context information，忽略了relevant context。虽然有利用relevant context的相关工作，但这些工作都有各自的问题。[Tian et al., 2017]提出计算context 与post之间的cosine similarity来衡量context relevance，其假设是context与response之间的relevance等价于post与response之间的relevance，这个假设是站不住脚的。[Xing et al., 2018]向HRED模型引入了attention机制，但attention机制定位relevant context时会产生偏差，因为基于RNN的attention机制倾向于最靠近的context（close context）。论文提出了自己的解决办法，用self-attention机制来衡量context于response之间的relevance。self-attention机制的优点是可以有效捕捉到长距离的依赖关系。 模型分为三个部分：context包含N轮对话： ${s_1,s_2,…,s_N}$其中，$s_i = {x_1,x_2,…,x_M}$，M为句子长度。response为$Y = {y_1,y_2,…,y_M}$ context representation encoder： 将context encode为vector。 word-level encoder： 用LSTM对sentence编码，将LSTM最后一个时间步的hidden state作为sentence representation: $h^{s_i}$； 由于self-attention机制不能区分word位置信息，还需要添加position embedding: $p^{s_i}$, 把两个向量做concatenate操作，得到总得sentence representation:$(h^{s_i},p^{s_i})$。 对于context中的N个句子有${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$ context self-attention: 采用multi-head self-attention机制，将${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$经过不同的线性变换作为query、keys、values matrix,由N个sentence representation得到总的context representation $O_s$。 response representation encoder 同样用multi-head self-attention机制,将response的word embedding及position embedding ${(w_1,p_1),…,(w_{t-1},p_{t-1})}$经过不同的线性变换作为query、keys、values matrix，得到response representation $O_r$。 在train阶段 采用mask操作，在时间步t对于word $y_t$，mask掉${y_t,y_{t+1},…,y_M}$，只保留${y_1,y_2,…,y_{t-1}}$来计算response representation。 在infer阶段 在生成response的时间步t，将生成的response ${g_1,…,g_{t-1}}$，来作为response representation。 context-response attention decoder 采用multi-head self-attention机制，将context attention representation $O_s$作为keys、values matrix，将response hidden representation $O_r$作为query matrix。得到输出$O_d$.","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"ACL2019","slug":"ACL2019","permalink":"http://yoursite.com/tags/ACL2019/"},{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"}]},{"title":"自然语言处理---会议列表","slug":"自然语言处理-会议列表","date":"2019-04-24T06:55:43.000Z","updated":"2019-07-07T09:00:06.000Z","comments":true,"path":"2019/04/24/自然语言处理-会议列表/","link":"","permalink":"http://yoursite.com/2019/04/24/自然语言处理-会议列表/","excerpt":"记录自然语言处理方向的国际会议列表。","text":"记录自然语言处理方向的国际会议列表。 A类会议AAAI，Association for the Advancement of Artificial Intelligence 其他ICLR，The International Conference on Learning Representations，国际学习表征会议2013年才刚刚成立了第一届。这个一年一度的会议已经被学术研究者们广泛认可，被认为「深度学习的顶级会议」。这个会议的来头不小，由位列深度学习三大巨头之二的 Yoshua Bengio 和 Yann LeCun 牵头创办。Yoshua Bengio 是蒙特利尔大学教授，深度学习三巨头之一，他领导蒙特利尔大学的人工智能实验室（MILA）进行 AI 技术的学术研究。MILA 是世界上最大的人工智能研究中心之一，与谷歌也有着密切的合作。而 Yann LeCun 就自不用提，同为深度学习三巨头之一的他现任 Facebook 人工智能研究院（FAIR）院长、纽约大学教授。作为卷积神经网络之父，他为深度学习的发展和创新作出了重要贡献。 参考链接 中国计算机学会推荐国际学术会议和期刊目录","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"会议列表","slug":"会议列表","permalink":"http://yoursite.com/tags/会议列表/"}]},{"title":"python读写csv文件","slug":"python读写csv文件","date":"2019-04-19T10:32:22.000Z","updated":"2019-04-20T10:56:58.000Z","comments":true,"path":"2019/04/19/python读写csv文件/","link":"","permalink":"http://yoursite.com/2019/04/19/python读写csv文件/","excerpt":"介绍csv文件的读写。","text":"介绍csv文件的读写。 csv模块csv.writer(csvfile)12345678910import csv row = [&apos;Symbol&apos;,&apos;Price&apos;,&apos;Date&apos;,&apos;Time&apos;,&apos;Change&apos;,&apos;Volume&apos;]rows = [(&apos;AA&apos;, 39.48, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.18, 181800), (&apos;AIG&apos;, 71.38, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.15, 195500), (&apos;AXP&apos;, 62.58, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.46, 935000), ]with open(&apos;name.csv&apos;,&apos;w&apos;) as csvfile: writer = csv.writer(csvfile,delimiter = &apos;\\t&apos;,lineterminator = &apos;\\n&apos;) #delimiter和lineterminator分别是分隔符，行结束符 writer.writerow(row) #写入单行 writer.writerows(rows) #写入多行 csv.reader(csvfile)该函数接收一个可迭代对象，返回对象reader是一个生成器，不能直接用下标访问。可以用for循环和next()函数访问。 12345with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile: reader = csv.reader(csvfile,delimiter = &apos;\\t&apos;) #迭代器 rows = [row for row in reader] #用for循环访问：for row in rows: print(row) 输出结果为：如果要读取csv文件的某列，可以看下面的例子 1234with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile: reader = csv.reader(csvfile,delimiter = &apos;\\t&apos;) #迭代器 column = [row[2] for row in reader] #用for循环访问：print(column) 输出结果为： csv.DictReader(csvfile)与csv.reader()函数相同，接收一个可迭代对象，返回一个生成器。不同之处是，返回的每个单元格放在字典的值中，字典的键就是这个单元格的列头。 12345with open(&apos;./name.csv&apos;,&apos;r&apos;) as f: reader = csv.DictReader(f,delimiter = &apos;\\t&apos;) rows = [row for row in reader]for row in rows: print(rows) 输出结果为：如果要读取csv文件的某列，可以看下面的例子: 1234with open(&apos;./name.csv&apos;,&apos;r&apos;) as f: reader = csv.DictReader(f,delimiter = &apos;\\t&apos;) column = [row[&apos;Time&apos;] for row in reader]print(column) 输出结果为： csv.DictWriter(csvfile)pandas读写csv也可以直接用pandas的函数read_csv()来读取csv文件的列。 1234import pandas as pdf = pd.read_csv(&apos;name.csv&apos;,delimiter = &apos;\\t&apos;)time = f.Timeprint(time) 输出结果为： 参考链接 python官方文档-csv模块 读写csv数据 使用python获取csv文本的某行或某列数据","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"csv","slug":"csv","permalink":"http://yoursite.com/tags/csv/"},{"name":"panda","slug":"panda","permalink":"http://yoursite.com/tags/panda/"}]},{"title":"torch.cuda.is_available()返回False,但nvidia-smi正常","slug":"torch-cuda-is-available-返回False-但nvidia-smi正常","date":"2019-04-16T09:04:44.000Z","updated":"2019-04-20T10:56:50.000Z","comments":true,"path":"2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/","link":"","permalink":"http://yoursite.com/2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/","excerpt":"torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。","text":"torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。 问题描述在pytorch用GPU来加速计算时发现。torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。 123&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()False 此时，nvidia-smi可以正常运行。 可能原因在nvidia-smi的运行结果中可以看到，driver version是390.xx。可能是driver version版本太低，造成了这个问题，实际上也是如此。driver version的常见版本是384.xx,390.xx,396.xx。接下来，把driver version升级到396.xx看能不能解决问题。 升级nvidia driver version 卸载旧版本的NVIDIA driver 1sudo apt-get remove --purge nvidia-\\* 添加NVIDIA的ppa源. 1sudo add-apt-repository ppa:graphics-drivers/ppa 安装新版本的NVIDIA driver 1sudo apt-get update &amp;&amp; sudo apt-get install nvidia-driver-396 此时，运行nvidia-smi，会报以下错误。 1Failed to initialize NVML: Driver/library version mismatch 这是更新NVIDIA driver版本后的常见问题。这个问题出现的原因是kernel mod的NVIDIA driver版本没有更新。执行以下命令可以查看nvidia kernel mod的version。 1cat /proc/driver/nvidia/version 可以看到已经加载的nvidia kernel mod的版本是还是旧版本390.xx。一般情况下，重启服务器就能解决问题。如果由于某些原因不能重启，可以重新加载kernel mod。思路是先unload kernel mod，再reload kernel mod. 详见解决Driver/library version mismatchnvidia-smi可以正常运行后，问题就解决了。 123&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True 参考链接 Torch.cuda.is_available() returns False, nvidia-smi is working How can I update the NVIDIA drivers to version 390.77?","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"nvidia-smi","slug":"nvidia-smi","permalink":"http://yoursite.com/tags/nvidia-smi/"},{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"},{"name":"cuda","slug":"cuda","permalink":"http://yoursite.com/tags/cuda/"}]},{"title":"nvidia-smi返回错误信息‘Failed to initialize NVML: Driver/library version mismatch’","slug":"nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’","date":"2019-03-29T11:47:03.000Z","updated":"2019-03-30T06:35:09.000Z","comments":true,"path":"2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/","link":"","permalink":"http://yoursite.com/2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/","excerpt":"Ubuntu运行命令nvidia-smi出错。","text":"Ubuntu运行命令nvidia-smi出错。 问题描述在Ubuntu18.04的命令行中运行命令nvidia-smi，返回错误信息 1Failed to initialize NVML: Driver/library version mismatch 方法1：重启解决大部分问题博客解决Driver/library version mismatch讲述的很清楚，这里就不再赘述。或者参考大型交友网站stack overflow的问题NVIDIA NVML Driver/library version mismatch 方法2：重装驱动看返回的错误信息，这个问题出现的原因是NVIDIA Driver的版本不匹配。如果重启不能解决问题，我们需要卸载重装NVIDIA driver。 查看驱动程序版本 dpkg -l | grep nvidia 可以看到驱动版本是390.116 cat /proc/driver/nvidia/version 这里NVRM的版本是390.87。错误信息就是这两个版本不匹配造成的。接下来先卸载NVIDIA driver，再重新安装。 卸载旧版本的NVIDIA driver 1sudo apt-get remove --purge nvidia-\\* 添加NVIDIA的ppa源 1sudo add-apt-repository ppa:graphics-drivers/ppa 重新安装NVIDIA的驱动 1sudo apt-get update &amp;&amp; sudo apt-get install nvidia-390 用你自己的版本号替换390。 这时再用cat /proc/driver/nvidia/version查看NVIDIA driver的驱动。 可以看到NVRM的版本是390.116，这时版本就匹配了。再次执行nvidia-smi,终于看到 在nvidia driver各版本总览可以看到NVIDIA driver的各个版本。 额外的：update 与 upgrade记录下sudo apt-get update与sudo apt-get upgrade的区别。在windows系统中安装软件，只需要有exe文件，双击即可安装了。Linux系统中则不同，Linux会维护一个自己的软件仓库，几乎所有软件都在这个仓库里，而且里面的软件完全安全，绝对可以安装。我们自己的Ubuntu服务器上，维护一个软件源列表文件/etc/apt/sources.list,里面都是一些网址信息，每个网址就是一个软件源，这个地址指向的数据标识着有哪些软件可以安装使用。 查看源列表： 1sudo vim /etc/apt/sources.list 更新软件列表 1sudo apt-get update 这个命令对访问源列表里的每个网址，读取软件列表，保存到本地电脑。 更新软件 1sudo apt-get upgrade 这个命令会把本地已安装的软件，与软件列表里对应软件做对比，如果有可更新版本就更新软件。总的来说，sudo apt-get update是更新软件列表，sudo apt-get upgrade是更新软件。 参考链接 解决Driver/library version mismatch Ubuntu配置GPU+CUDA+CAFFE ubuntu下安装安装CUDA、cuDNN和tensotflow-gpu版本流程和问题总结 NVIDIA的wiki （原）Ubuntu16中安装nvidia的显卡驱动","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://yoursite.com/tags/ubuntu/"},{"name":"nvidia-smi","slug":"nvidia-smi","permalink":"http://yoursite.com/tags/nvidia-smi/"}]},{"title":"条件随机场CRF","slug":"条件随机场CRF","date":"2019-03-23T05:50:20.000Z","updated":"2019-07-07T07:14:19.000Z","comments":true,"path":"2019/03/23/条件随机场CRF/","link":"","permalink":"http://yoursite.com/2019/03/23/条件随机场CRF/","excerpt":"最近学习了条件随机场CRF，做下总结。主要参考BiLSTM+CRF模型中的CRF层为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。","text":"最近学习了条件随机场CRF，做下总结。主要参考BiLSTM+CRF模型中的CRF层为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。 从马尔科夫随机场到线性链条件随机场 概率图模型 概率图模型是用图G = (V,E)来表示概率分布。设有联合概率分布P(Y)，Y是一组随机变量。我们可以用无向图G = (V,E)来表示联合概率分布P(Y),节点$v \\in V$表示随机变量$Y_v$，边$e \\in E$表示随机变量之间的概率依赖关系。 概率无向图模型，即马尔科夫随机场 设有联合概率分布P(Y),由无向图G=(V,E)表示。如果概率分布P(Y)满足成对、局部、全局马尔科夫性，那么称 这个联合概率分布p(Y)为概率无向图模型，或马尔科夫随机场(Markov random field)。 成对马尔科夫性、局部马尔可夫性、全部马尔科夫性要表达的就是：在无向图中，没有边连接的节点之间没有概率依赖关系，也就是没有边连接的节点代表的随机变量之间是条件独立的。 条件随机场 设X和Y是随机变量，P(Y|X)是给定X的条件下Y的条件概率分布。若给定X的条件下，Y构成一个马尔科夫随机场。则称条件概率分布P(Y|X)为条件随机场。 我们可以看到，马尔科夫随机场是联合概率分布P(Y),而条件随机场是条件概率分布P(Y|X)。这是一点不同。 线性链条件随机场 设$X =(X_1,X_2,…,X_n), Y =(Y_1,Y_2,…,Y_n)$是线性链表示的随机变量序列。若在给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：$$P(Y_i|X,Y_1,Y_2,…,Y_{i-1},Y_{i+1},…,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})$$ 。则称条件概率分布p(Y|X)为线性链条件随机场。线性链条件随机场 和 隐马尔可夫模型都是序列模型，可以用于标注问题。这时，条件概率模型P(Y|X)中，X是输入变量序列，表示需要标注的观测序列；Y是输出变量，表示标记序列，或称状态序列。 BiLSTM+CRF模型BiLSTM+CRF模型是命名实体识别任务的常用模型。假设我们训练集中有个由五个词组成的句子$X = (w_0,w_1,w_2,w_3,w_4)$,对应标签为$Y = [B-Person，I-Person,O,B-Organization,O]$。数据集中有五类标签： 类别 B-Person I-Person B-Organization I-Organization O 含义 人名的开始部分 人名的中间部分 组织机构的开始部分 组织机构的中间部分 非实体信息 先简单介绍下BiLSTM+CRF模型的结构。LSTM层的输入一般为每个词的Word-embedding，输出为每个词word在类别空间tag_space上的非归一化概率，也就是在单词对应每个类别的得分score。这些score作为CRF层的输入。 CRF的损失函数条件随机场中有两个重要的矩阵，转移概率矩阵和状态概率矩阵，分别对应转移特征和状态特征。 状态概率矩阵。就是LSTM层的输出，作为CRF层的输入。矩阵的形状为[N,M],N为句子长度，M为可能状态数。 转移概率矩阵。矩阵形状为[M,M]，M为可能状态数。转移矩阵是模型参数，是随机初始化的，在训练过程中不断更新优化。 给定转移矩阵T，随机变量序列X取值为x的条件下，随机变量序列取值为y的似然函数为：$$Likelihood(y|x,T) = \\frac{ \\sum_{i=0}^{n} P(x_i|y_i)T(y_i|y_{i-1})}{\\sum_{y^}\\biggl( \\sum_{i=0}^{n}P(x_i|y_i^)T(y_i^|y_{i-1}^)\\biggr)} \\cdots\\cdots\\cdots\\cdots\\cdots (1)$$上式中, $P(x_i|y_i)$表示当前状态为$y_i$时，产生观测值$x_i$的概率。对应状态分数。 $T(y_i|y_{i-1})$表示从上一个状态$y_{i-1}$转换到当前状态$y_i$的概率。对应转移分数。我们可以从转移矩阵中读出这个概率。 分子表示了单条路径y=[y_0,y_1,…,y_n]的分数score或概率。 分母表示了所有可能路径$y^*$的总分。注意计算分母时，我们要计算所有可能路径并求和。若序列长度为N,状态可能数为M,则所有可能路径数为$M^N$,这个数量是指数级的，非常大。我们的秘密武器是前向后向算法来高效地计算分母。 进一步地，负对数似然函数为：$$NegLogLikelihood(y|x,T) = \\sum_{y^}\\biggl( \\sum_{i=0}^{n} log(P(x_i|y_i^)T(y^_i|y^{i-1}))\\biggr) - \\sum{i=0}^{n}log(P(x_i|y_i)T(y_i|y_{i-1})) \\cdots\\cdots\\cdots\\cdots (2)$$ 从式子(1)到式子(2)，直接对式子(1)取负对数是得到式子(2)可能比较令人费解。需要留意的是：转移概率矩阵和状态概率矩阵中的概率都是对数概率（这很重要），这样计算路径概率时都是加法。对对数概率加上exp()运算我们能得到正常概率。《统计学习方法》书中说，线性链条件随机场是对数线性模型，在对数空间中，对数概率可以直接相加，带来很大的方便。接下来，我们来看看：真实路径的分数和所有路径的总分是怎么计算的？ 真实路径分数数据集中有五类标签，再引入start和end作为序列开始和结束标志。 类别|B-Person | I-Person | B-Organization | I-Organization|O|start|end—|—|—|—|—|—|—索引|0|1|2|3|4|5|6 长度为5的序列，$X = (w_0,w_1,w_2,w_3,w_4)$,对应类别为$Y = [B-Person，I-Person,O,B-Organization,O]$，标注序列，也就是真实路径为为y = [0,1,4,3,4]。真实路径的分数由两部分组成，状态分数和转移分数。状态矩阵就是LSTM层的输出。转移矩阵是模型参数，为$$[t_{ij}],i,j\\in [0,6] and i\\neq 6,j\\neq 5$$其中$t_ij$表示从上一状态转换到当前状态的概率。转移时，不能转移到start，不能从end转移。则真实路径的分数 = 转移分数 + 状态分数 = $1.5+0.4+0.1+0.2+0.5+t_{51}+t_{01}+t_{14}+t_{42}+t_{24}+t_{46}$ 所有路径分数-前向后向算法计算所有路径的总分面对的难题是要不要穷举所有路径。对于一个长度为N的序列，可能状态数为M，所有可能路径数为$M^N$，这是一个指数级的计算量。计算每条路径分数的计算量是$O(N)$,直接用穷举法计算所有路径总分的计算量是$O(N\\cdot M^N)$。这个计算量是无法接受的。《统计学习方法》p176写，前向算法是基于“路径结构”递推计算所有路径分数。前向算法高效的关键是局部计算前向概率，再递推到全局。前向算法的计算量是$O(N\\cdot M^2)$，前向算法减少计算量的原因是：每一次递推计算直接利用了前一个时刻的计算结果，避免了重复计算。 对于$w_0 \\to w_1$的局部路径。先计算$w_0$所有状态到$w_1$单个状态0的分数之和，并更新$w_1$的状态0的状态分数。有M条局部路径，计算量是$O(M)$用同样的方法更新$w_1$所有状态的状态分数，这就是所有局部路径的分数。要计算M个状态，计算量是$O(M^2)$依次递推到全局。序列长度为N,总的计算量是$O(N \\cdot M^2)$从图的角度解释了前向算法，我们再从数学计算的角度来看前向算法。简化一下问题，假设句子长度为3，$X = [w_0,w_1,w_2]$,只有2个类别[1,2]我们引入两个变量previous和obs。previous存储前一时刻的计算结果，obs存储当前状态分数。对于$w_0$:$$obs = [x_{01},x_{02}];previous = none$$对于$w_0 \\to w_1:$,$$previous = [x_{01},x_{02}],obs = [x_{11},x_{12}]$$先扩展previous和obs：$$previous = \\begin{pmatrix} x_{01}&amp;x_{01}\\x_{02}&amp;x_{02} \\end{pmatrix} \\quad$$$$obs = \\begin{pmatrix} x_{11}&amp;x_{12}\\x_{11}&amp;x_{12} \\end{pmatrix} \\quad$$将previous和obs和转移矩阵相加：$$score =\\begin{pmatrix} x_{01}&amp;x_{01}\\x_{02}&amp;x_{02} \\end{pmatrix} +\\begin{pmatrix} x_{11}&amp;x_{12}\\x_{11}&amp;x_{12} \\end{pmatrix}+\\begin{pmatrix} t_{11}&amp;t_{12}\\t_{21}&amp;t_{22} \\end{pmatrix}$$$$ = \\begin{pmatrix} x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\\x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22} \\end{pmatrix}$$score同列相加，更新previous:$$previous = [x_{01}+x_{11}+t_{11}+x_{02}+x_{11}+t_{21},x_{01}+x_{12}+t_{12}+x_{02}+x_{12}+t_{22}]$$这样第二次迭代就完成了。用图来表示到目前为止的计算：用同样的方法迭代递推，就可以得到所有路径的分数。 这样我们就计算出了负对数似然函数，也就是CRF模型的损失函数。条件随机场的第二个基本问题是学习问题，给定训练集估计条件随机场的模型参数（转移矩阵）。我们可以通过最小化对数似然函数来求参数模型。可以用梯度下降法来实现。 维特比算法解码条件随机场的第三个基本问题是预测问题，给出条件随机场的模型 和 输入序列x，求条件概率最大输出序列$y^*$。 也就是找出所有路径中得分最高的那条路径作为标注路径。与计算所有路径总分一样，我们面对的难题是要不要求出所有路径的分数。当然是不用的，我们用维特比算法来解码。通信专业的同学一定知道大名鼎鼎的维特比算法，卷积码的译码就是用的维特比算法。对于 $w_0 \\to w_1$:先计算$w_0$到$w_1$的状态1五条路径的分数，找出分数最大的一条保留下来，其他全都丢弃掉。计算量为$O(M)$同样的找出$w_1$每个状态分数最大的一条路径，要计算$w_1$的5个状态，计算量为$O(M^2)$依次递推到全局。序列长度为N,计算量为$O(N \\cdot M^2)$ 比较下前向算法与维特比算法的异同：相同的地方在他们都面临要不要计算所有路径分数的问题，都是基于路径结构，用局部递推到全局。不同的地方在于前向算法在更新previous的单个状态时是做求和sum运算，而维特比算法是做max运算，只保留分数最大的，丢弃掉其他路径。此外，维特比算法找到分数最大的路径后，还要反向递推 参考链接 CRF Layer on the Top of BiLSTM 最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 如何直观地理解条件随机场，并通过PyTorch简单地实现 条件随机场CRF—刘建平 《统计学习方法》—李航","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"条件随机场","slug":"条件随机场","permalink":"http://yoursite.com/tags/条件随机场/"},{"name":"前向后向算法","slug":"前向后向算法","permalink":"http://yoursite.com/tags/前向后向算法/"},{"name":"维特比算法","slug":"维特比算法","permalink":"http://yoursite.com/tags/维特比算法/"}]},{"title":"pytorch实现基于LSTM的循环神经网络","slug":"pytorch实现基于LSTM的循环神经网络","date":"2019-03-20T14:41:10.000Z","updated":"2019-07-07T07:10:30.000Z","comments":true,"path":"2019/03/20/pytorch实现基于LSTM的循环神经网络/","link":"","permalink":"http://yoursite.com/2019/03/20/pytorch实现基于LSTM的循环神经网络/","excerpt":"用pytorch实现基于LSTM的循环神经网络。","text":"用pytorch实现基于LSTM的循环神经网络。 涉及函数详解class torch.nn.LSTM(args,*kwargs) 参数说明： input_size: 输入的特征维度 output_size: 输出的特征维度 num_layers: 层数（注意与时序展开区分） bidirectional: 如果为Ture，为双向LSTM。默认为False LSTM的输入：input,$(h_0,c_0)$ input(seq_len,batch,input_size): 包含输入特征的tensor,注意输入是tensor。 $h_0$(num_layers \\cdot num_directions,batch,hidden_size): 保存初始化隐藏层状态的tensor $c_0$(num_layers \\cdot num_directions,batch,hidden_size): 保存初始化细胞状态的tensor LSTM的输出： output,$(h_n,c_n)$ output(seq_len, batch, hidden_size * num_directions): 保存RNN最后一层输出的tensor $h_n$(num_layers * num_directions,batch,hidden_size): 保存RNN最后一个时间步隐藏状态的tensor $c_n$(num_layers * num_directions,batch,hidden_size): 保存RNN最后一个时间步细胞状态的tensor 1234567import torch.nn import torchlstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，该单元输入维度embedding_dim,输出维度为hidden_diminput = Variable(torch.randn(seq_len,1,embedding_dim)) # 输入input应该是三维的，第一维度是seq-length,也就是多个词构成的一句话；第二维度为1，不用管；第三个维度是一个词的词嵌入维度，即embedding_dimh0 = Variable(torch.randn(1,1,hidden_dim)) c0 = Variable(torch.randn(1,1,hidden_dim))lstm_out,hidden = lstm(input,(h0,c0)) class torch.nn.Linear()1class torch.nn.Linear(in_features,out_features,bias = True) 作用：对输入数据做线性变换。$y = Ax+b$ 参数： in_features：每个输入样本的大小 out_features: 每个输出样本的大小 bias: 默认值为True。是否学习偏置。 形状： 输入： (N,in_features) 输出： (N,out_features) 变量： weights: 可学习的权重，形状为(in_features,out_features) bias: 可学习的偏置，形状为(out_features) 1234m = nn.Linear(20,30)input = torch.randn(128,20)output = m(input)print(output) 先看个小例子用pytorch实现LSTM，先实例化一个LSTM单元，再给出tensor类型的输入数据inputs及初始隐藏状态hidden = $(h_0,c_0)$。值得注意的是，LSTM单元的输入inputs必须是三维的，第一维是seq-length，即一句话，元素是词。第二维是mini-batch,从来不用，设为1即可。第三维是embedding-size,即一个词向量。 123456import torch import torch.nn as nn lstm = nn.LSTM(4,3) #实例化一个LSTM单元，单元输入维度是4，输出维度是3inputs = [torch.randn(1,5) for _ in range(5)] #产生输入inputs。为tensor序列。hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #初始化隐藏状态 做好三步准备：实例化一个LSTM单元，准备好inputs，初始化隐藏状态hidden。我们就可以计算LSTM单元的输出了。我们有两种选择，将序列一个元素一个元素地送入LSTM单元，或是将整个序列一下子全送入LSTM单元。先看看第一种： 123for x in inputs: lstm_out,hidden = lstm(x.view(1,1,-1),hidden) #x.view(1,1,-1)将tensor整形为三维。前面说过LSTM单元的输入必须是三维的。print(lstm_out,hidden) 接下来，将整个序列送入LSTM单元： 1234inputs = torch.cat(inputs).view(len(inputs),1,-1) #将整个序列连接为tensor，并整形为三维。hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #清楚隐藏状态lstm_out,hidden = lstm(inputs,hidden)print(lstm_out,hidden) 我们可以看到： lstm_out 中包含了序列所有的隐藏状态。 hidden 中包含了最后一个时间步的隐藏状态和细胞状态。可以作为下个时间步LSTM单元的输入参数，继续输入序列或反向传播。 用lstm做词性标注先准备训练数据： 123456789101112131415train_data = [ (&quot;The dog ate the apple&quot;.split(), [&quot;DET&quot;, &quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;]), (&quot;Everybody read that book&quot;.split(), [&quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;])]# 词汇表字典word_to_ix = &#123;&#125;for sent,tags in train_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix)# 标签集字典tag_to_ix = &#123;&quot;DET&quot;: 0, &quot;NN&quot;: 1, &quot;V&quot;: 2&#125;EMBEDDING_DIM = 6HIDDEN_DIM = 6 构建LSTM模型: 123456789101112class LSTMtagger(nn.Module): def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size): super(LSTMtagger,self).__init__() self.hidden_dim = hidden_dim self.word_embeddings = nn.Embedding(vocab_size,embedding_dim) #随机初始化词向量表，是神经网络的参数 self.lstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，单元输入维度是embedding_dim，输出维度是hidden_dim self.hidden2tag = torch.Linear(hidden_dim,tagset_size) #线性层从隐藏状态空间映射到标签空间 def forward(self,sentence): embeds = self.word_embeddings(sentence) #查询句子的词向量表示。输入应该是二维tensor。 lstm_out,hidden = self.lstm(embeds.view(len(sentence),1,-1)) tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1)) tag_scores = F.log_softmax(tag_space) 训练模型 12345678910111213141516171819202122232425262728293031323334353637model = LSTMtagger(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))loss_function = nn.NLLLoss()optimizer = optim.SGD(model.parameters(),lr = 0.1)def prepare_sequence(seq,to_ix): idxs = [to_ix[w] for w in seq] return torch.tensor(idxs,dtype = torch.long)# 在训练模型之前，看看模型预测结果with torch.no_grad(): inputs = prepare_sequence(train_data[0][0],word_to_ix) tag_scores = model(inputs) print(tag_scores) predict = np.argmax(tag_scores,axis = 1) print(predict)for epoch in range(300): for sentence,tags in train_data: # step 1:pytorch会累积梯度，要清楚所有variable的梯度。 model.zero_grad() # step 2:准备好数据，变成tensor sentence_in = prepare_sequence(sentence,word_to_ix) targets = prepare_sequence(tags,tag_to_ix) # step 3:得到输出 tag_scores = model(sentence_in) # step4: 计算loss loss = loss_function(tag_scores,targets) # step5: 计算loss对所有variable的梯度 loss.backward() # step6： 单步优化，根据梯度更新参数 optimizer.step()# 模型训练后，看看预测结果with torch.no_grad(): inputs = prepare_sequence(train_data[0][0],word_to_ix) tag_scores = model(inputs) print(tag_scores) predict = np.argmax(tag_scores,axis = 1) print(predict) 输出结果为：我们可以看到，训练之后的预测序列为 [0,1,2,0,1]也就是[“DET”, “NN”, “V”, “DET”, “NN”] 参考链接 序列模型和基于LSTM的循环神经网络 Sequence Models and Long-Short Term Memory Networks-官方 Understanding LSTM Networks The Unreasonable Effectiveness of Recurrent Neural Networks","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"},{"name":"循环神经网络","slug":"循环神经网络","permalink":"http://yoursite.com/tags/循环神经网络/"}]},{"title":"pytorch实现Word embedding","slug":"pytorch实现Word-embedding","date":"2019-03-20T12:16:21.000Z","updated":"2019-07-07T07:11:01.000Z","comments":true,"path":"2019/03/20/pytorch实现Word-embedding/","link":"","permalink":"http://yoursite.com/2019/03/20/pytorch实现Word-embedding/","excerpt":"word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。","text":"word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。 one-hot编码在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。举个例子，我们想要构建一个语言模型。有以下三个句子 数学家待在实验室里。 物理学家待在实验室里。 数学家解决了一个难题。 我们又看到一个新的句子： 物理学家解决了一个难题。 我们希望语言模型可以学习到以下特点： 数学家和物理学家在一个句子中同样的位置出现。这两个词之间有某种语义上的联系 数学家曾经出现在我们看到的这个新句子中物理学家出现的位置。 这就是语义相似性想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为distributional hypothesis值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。 word embedding我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码物理学家和数学家。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \\frac{q_物\\cdot q_数}{|q_物| \\cdot |q_数|}=cos(\\phi) 其中\\phi是两个向量之间的夹角。$$但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。 PyTorch实现word embedding代码如下： 12345678import torch import torch.nn as nnfrom torch.autograd import Variable# 词汇表字典word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;vocab_size = len(word_to_ix) embedding_dim = 15word_embeddings = nn.Embedding(vocab_size,embedding_dim) nn.Embedding()随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。接下来我们查询”dog”这个词的向量表示。 1234dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。dog_idx = Variable(dog_idx)dog_embed = word_embeddings(dog_idx) #注意不是索引print(dog_embed) 上述代码中，要访问dog的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。接下来，我们查询一句话的向量表示。 123456sent = &apos;The dog ate the apple&apos;.split()sent_idxs = [word_to_ix[w] for w in sent]sent_idxs = torch.LongTensor(sent_idxs)sent_idxs = Variable(sent_idxs)sent_embeds = embeds(sent_idxs) print(sent_embeds) pytorch加载预训练词向量之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。方式一： 1234import torch word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵pretrain_embedding = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中 方式二： 12word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding)) 涉及函数详解numpy()与from_numpy()1torch.from_numpy(ndarray) $\\to$ tensor 作用：numpy桥，将numpy.ndarray转换为pytorch的tensor.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。 1tensor.numpy() 作用：numpy桥，将pytorch的tensor转换为numpy.ndarray.二者共享同一内存空间，修改一个另一个也会被修改。 举个例子： 123a = np.arange(5)b = torch.from_numpy(a)c = b.numpy() tensor.copy_(src) 作用：将src中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。 举个例子： 123a = torch.randn(1,5)b = torch.randn(1,5)b.copy_(a) 参考链接 Word Embeddings: Encoding Lexical Semantics PyTorch快速入门教程七（RNN做自然语言处理）","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"word embedding","slug":"word-embedding","permalink":"http://yoursite.com/tags/word-embedding/"}]},{"title":"sublime插件","slug":"sublime插件","date":"2019-03-11T11:41:12.000Z","updated":"2019-03-11T13:04:43.000Z","comments":true,"path":"2019/03/11/sublime插件/","link":"","permalink":"http://yoursite.com/2019/03/11/sublime插件/","excerpt":"记录sublime的一些插件。","text":"记录sublime的一些插件。 OmniMarkupPreviewer作用：插件OmniMarkupPreviewer支持将markdown语言渲染为html并且在浏览器上实时预览，也就是将markdown内容实时显示为网页，效果之好令人惊叹。 安装可以使用Package Control的Insatll Package来安装，也可以直接从OmniMarkupPreviewer的github主页下载压缩包，解压到目录\\Sublime Text 3\\Packages\\下。 快捷键Ctrl + shift + p打开Package Control 输入install选择Package Control: Install Package 从列表中选择OmniMarkupPreviewer安装。 使用方法：对于window和Linux： Ctrl+Alt+O 在浏览器中预览 Ctrl+Alt+X 输出为html文件 Ctrl+Alt+C 复制为HTML文件 插件配置修改插件的配置，点击菜单栏的Preferences - Packages Settings - OmniMarkdownPreviwer - Setting-User。 12345678910111213141516171819202122&#123; &quot;server_host&quot;: &quot;127.0.0.1&quot;, //默认为localhost,修改为你电脑的ip，可以实现远程访问。也就是从其他电脑预览网页效果 &quot;server_port&quot;: 51004, &quot;refresh_on_modified&quot;: true, &quot;refresh_on_modified_delay&quot;: 500, &quot;refresh_on_saved&quot;: true, &quot;browser_command&quot;: [], &quot;html_template_name&quot;: &quot;github&quot;, &quot;ajax_polling_interval&quot;: 500, &quot;ignored_renderers&quot;: [&quot;LiterateHaskellRenderer&quot;], &quot;mathjax_enabled&quot;: true, //渲染数学公式要用到MathJax库，将值设为true,mathjax会在后端自动下载。 &quot;export_options&quot; : &#123; &quot;template_name&quot;: &quot;github-export&quot;, &quot;target_folder&quot;: &quot;.&quot;, &quot;timestamp_format&quot; : &quot;_%y%m%d%H%M%S&quot;, &quot;copy_to_clipboard&quot;: false, &quot;open_after_exporting&quot;: false &#125;, &quot;renderer_options-MarkdownRenderer&quot;: &#123; &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;] &#125;&#125; 遇到的错误预览文本时报错： 1234567Error: 404 Not FoundSorry, the requested URL &apos;http://127.0.0.1:51004/view/593&apos; caused an error:&apos;buffer_id(593) is not valid (closed or unsupported file format)&apos;**NOTE:** If you run multiple instances of Sublime Text, you may want to adjustthe `server_port` option in order to get this plugin work again. 解决办法是修改配置文件Sublime Text &gt; Preferences &gt; Package Settings &gt; OmniMarkupPreviewer &gt; Settings - User粘贴下面的代码： 12345&#123; &quot;renderer_options-MarkdownRenderer&quot;: &#123; &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;] &#125;&#125; 参考链接 OmniMarkupPreviewer的github主页 近乎完美的Markdown写作体验 - SublimeText3 + OmniMarkupPreviewer OmniMarkupPreviewer + MathJaxOmniMarkupPreviewerx渲染markdown内容为网页，MathJax对LATEX编辑的数学公式进行渲染。 下载mathjax 下载mathjax，解压到目录Sublime Text 3\\Packages\\OmniMarkupPreviewer\\public下。 在目录Sublime Text3\\Packages\\OmniMarkupPreviewer\\创建空文件MATHJAX.DOWNLOADED。这样就安装好了。 验证新建markdown文件输入内容： 123This expression $\\sqrt&#123;3x-1&#125;+(1+x)^2$ is an example of a $\\LaTeX$ inline equation.he Lorenz Equations:$$\\begin&#123;aligned&#125;\\dot&#123;x&#125; &amp; = \\sigma(y-x) \\\\\\dot&#123;y&#125; &amp; = \\rho x - y - xz \\\\\\dot&#123;z&#125; &amp; = -\\beta z + xy\\end&#123;aligned&#125;$$ 在sublime中用Ctrl+Alt+O预览，显示效果如下： 参考链接 使用Markdown的时候需要插入LaTeX公式方法 关于LATEX: 一份其实很短的 LaTeX 入门文档 一份其实很短的 LaTeX 入门文档 常用数学符号的 LaTeX 表示方法","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"sublime","slug":"sublime","permalink":"http://yoursite.com/tags/sublime/"}]},{"title":"熵、交叉熵与KL散度","slug":"熵、交叉熵与KL散度","date":"2019-03-11T06:31:33.000Z","updated":"2019-07-07T07:15:05.000Z","comments":true,"path":"2019/03/11/熵、交叉熵与KL散度/","link":"","permalink":"http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/","excerpt":"介绍交叉熵和KL散度。","text":"介绍交叉熵和KL散度。 从信息量到信源熵 信息量是通信专业的名词。一个变量的主要特征就是不确定性，也就是发生的概率。信息量用来衡量不确定性的大小。一个事情发生的概率越小，使人越感到意外，则这件事的信息量越大；反之，概率越大，越不意外，信息量越小。举个例子，有一架波音747飞机失事，发生的概率很小，让人很意外，带给人的信息量很大。 信息量函数应满足两个特性：1）随着概率的增大而减小，即是概率的减函数；2）信息量函数满足可加性，即两个统计独立的消息提供的信息量等于他们分别提供的信息量之和。同时满足递减性和可加性的函数是对数函数，即 $$ I[p(x_i)] = log \\frac{1}{p(x_i)} = -log p(x_i)$$ 信源熵定义为信源输出的平均信息量，即信息量的数学期望。$$ H(X) = E(I[p(x_i)]) = E(-log p(x_i)) = - \\sum_{i=1}^{n}p(x_i)log p(x_i)$$信源实际上是一个概率分布，信源熵可以解释为表示这个概率分布至少需要的信息量。 交叉熵对于一个随机事件，真实概率分布是$p(x_i)$ 是未知的，从数据中得到概率分布为$q(x_i)$。我们用概率分布$q(x_i)$来近似和逼近真实的概率分布$p(x_i)$ 。交叉熵定义为：$$H(p,q) = \\sum_{i=1}^{n}p(x_i) I[q(x_i)] =- \\sum_{i=1}^{n}p(x_i)log(x_i) $$交叉熵$H(p,q)$是用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量。上面我们说过，信源熵$H(X)$是表示真实概率分布$p(x_i)$需要的最小信息量。可以得到结论：$$H(p,q) \\ge H(p)$$由吉布斯不等式可以证明，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。这个不等式的意义是：用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量一定大于等于概率分布$p(x_i)$本身的信源熵。交叉熵比信源熵多出来的这部分，就是冗余信息量，我们称为KL散度（相对熵）。$$KL(p||q)= H(p,q) - H(p) \\ge 0$$容易看出交叉熵并不是一个对称量，即$ H(p,q) \\not=H(q,p)$。同样的,KL散度也不是一个对称量，即$KL(p||q) \\not =KL(q||p) $给定概率分布$p(x_i)$,信源熵$H(p)$就是固定不变的。在机器学习中，交叉熵常用作分类问题的损失函数。交叉熵刻画了预测概率分布$q(x_i)$与真实概率分布$p(x_i)$之间的距离。通过减小交叉熵$H(p,q)$,我们可以使得预测概率分布$q(x_i)$不断逼近真实概率分布$p(x_i)$ 相对熵真实的概率分布为$p(x_i)$，我们用预测概率分布$q(x_i)$对它进行建模和近似。我们需要的平均附加量，也就是冗余量是：$$KL(p,q) = H(p,q) - H(q) = -\\sum_{i=1}^{n}p(x_i)logq(x_i) - \\biggl(-\\sum_{i=1}^{n}p(x_i)logp(x_i)\\biggr) = -\\sum_{i=1}^{n}p(x_i)log{\\frac{q(x_i)}{p(x_i)}}$$KL散度有以下几个特性： KL散度不是一个对称量，即$KL(p||q) \\not =KL(q||p) $ $KL(p||q)\\ge 0$，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。 KL散度可以看做两个分布之间不相似程度的度量。KL散度越小，两个分布的不相似程度越小，分布$q(x_i)$越适合来近似$p(x_i)$。 tensorflow用交叉熵做损失函数在机器学习中交叉熵常常用作分类问题的损失函数。这里有个问题，交叉熵用于概率分布，但神经网络的输出并不一定是一个概率分布。概率分布应满足2个条件:1) $0 \\le p(X =x) \\le 1$2) $\\sum_{x}{} p(X=x) = 1$如何把神经网络的输出变成概率分布呢？这里就要用到softmax回归。假设输出层的输出为$y_0,y_1,y_2 \\dots y_n$,则softmax函数的形式为：$$softmax(y_i) = \\frac{exp(y_i)}{\\sum_{j}exp(y_j)}$$由于交叉熵一般会与softmax回归一起使用，TensorFlow对这两个功能进行了统一，可以直接用函数tf.nn.softmax_cross_entropy_with_logits来计算softmax后的交叉熵函数。对于只有一个正确答案的分类问题，可以用函数tf.nn.sparse_nn.softmax_cross_entropy_with_logits来加速计算过程。 pytorch中交叉熵损失函数的实现在多分类问题中，实际概率分布是 $y = [y_0,y_1,…,y_{C-1}]$,其中C为类别数;y是样本标签的one-hot表示，当样本属于第$i$类时$y_i=1$,否则$y_i=0$。预测概率分布为$p = [p_0,p_1,p_2,…,p_{C-1}]$。$c$是样本标签。此时，交叉熵损失函数为$$loss = -\\sum_{i=0}^{C-1}y_i log(p_i) = - y_c \\cdot log(p_c) = - log(p_c)$$接下来介绍pytorch中具体实现这个数学式子的函数。 torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()1torch.nn.functional.log_softmax() 作用：先做softmax运算，再做log运算。在数学上等价于$log(softmax(x))$ 1class torch.nn.NLLLoss(weight = None) 作用：这是neg log likelihood loss（NLLLoss），即负对数似然函数。 参数： weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。 输入： input(N,C): C是类别个数。为log_probabilities形式，即概率分布再取log。可以在最后一层加log_softmax,这就要用到函数torch.nn.functional.log_softmax() targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为2,而不是[0,0,1,0,0] loss可以表示为：$$loss(x,class) = -x[class]$$如果指定了weight，可以表示为：$$loss(x,class) = - weight[class]*x[class]$$举个例子: 1234567import torch log_m = torch.nn.functional.log_softmax()loss_function = torch.nn.NLLLoss()inputs = torch.randn(3,5) #batch_size * num_classes = 3 * 5target = torch.LongTensor([1,0,4])loss = loss_function(log_m(inputs),target) # inputs要先做log_softmax，再送入loss_functionloss.backward() class torch.nn.CrossEntropyLoss(weight = None) 作用：将函数log_softmax和NLLLoss集成到一起。在多分类问题中非常有用。 参数： weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。 输入： input(N,C): C是类别个数。每个类别的分数，不用过softmax层。 targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为2,而不是[0,0,1,0,0]。 loss可以表示为：$$loss(x,class) = - \\text{log}\\frac{e^{x[class]}}{ \\sum_{j=0}^{C-1}e^{x[j]}} = -x[class] + \\text{log}(\\sum_{j=0}^{C-1}e^{x[j]}) $$当指定了weight时，loss计算公式为： $$ loss(x, class) = weights[class] \\cdot (-x[class] + \\text{log}(\\sum_{j=0}^{C-1}e^{x[j]})) $$参见： PyTorch学习笔记——多分类交叉熵损失函数 pytorch官方手册参考链接 信息熵，相对熵，交叉熵的理解 Tensorflow基础知识—损失函数详解","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"交叉熵","slug":"交叉熵","permalink":"http://yoursite.com/tags/交叉熵/"},{"name":"相对熵","slug":"相对熵","permalink":"http://yoursite.com/tags/相对熵/"}]},{"title":"python的一些函数","slug":"python的一些函数","date":"2019-03-10T08:12:51.000Z","updated":"2019-07-07T07:07:46.000Z","comments":true,"path":"2019/03/10/python的一些函数/","link":"","permalink":"http://yoursite.com/2019/03/10/python的一些函数/","excerpt":"记录python的一些函数，实现某些功能。","text":"记录python的一些函数，实现某些功能。 求最大/小值的索引对于list12345import numpy as npa = range(100)np.random.shuffle(a)index_max = a.index(max(a)) #求最大值的索引index_min = a.index(min(a)) #求最小值的索引 对于numpy的数组ndarray1234567a = np.array(a)index_max = np.argmax(a) #求最大值的索引index_min = np.argmin(a) #求最小值的索引# 对于二维的数组b = np.arange(100).reshape(10,-1)row_max_list = np.argmax(b,axis = 1) #按行计算最大值在行中的索引line_max_list = np.argmin(b,axis = 0) #按列计算最小值在列中的索引 sort与sorted1sorted(iterable,key,reverse) iterable: 可迭代对象 key：用来进行比较的元素。常用函数： lambda x:x[i] reverse：排序规则。reverse=True按降序排列，reverse=False按升序排列（默认） 比较sort与sorted: 作用对象:sort()只能作用于list,sorted()可以作用于所有可迭代对象。 返回值：sort()没有返回值；sorted()返回一个新的list 字典的items()方法1dict.items() 返回可遍历的元素为（键，值）元组的数组。 模块collections–容器数据类型collections模块是python内建的一个集合模块，提供了许多有用的集成类。提供了list,dict,set,tuple的替代选择，相当于这几个数据类型的加强版。 collections.Counter(iterable)Counter是一个计数器，用于计数可哈希对象，统计元素出现的个数。它是一个集合，元素-计数像键-值对一样存储。 123456&gt;&gt; import collections&gt;&gt; a = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;a&quot;]&gt;&gt; counter = collections.Counter(a)&gt;&gt; print(counter)Counter(&#123;&apos;a&apos;: 3, &apos;b&apos;: 2, &apos;c&apos;: 1&#125;)","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"全文搜索引擎-Elasticsearch入门","slug":"全文搜索引擎-Elasticsearch入门","date":"2019-03-10T07:11:17.000Z","updated":"2019-03-15T10:31:33.000Z","comments":true,"path":"2019/03/10/全文搜索引擎-Elasticsearch入门/","link":"","permalink":"http://yoursite.com/2019/03/10/全文搜索引擎-Elasticsearch入门/","excerpt":"Elasticsearch是一个开源的搜索引擎框架。","text":"Elasticsearch是一个开源的搜索引擎框架。 Elasticsearch安装和启动安装前提：Elasticsearch需要Java7或以上的版本。 下载压缩包并解压： 123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.zipunzip elasticsearch-6.6.1.zipcd elasticsearch-6.6.1 进入解压后的文件目录，启动elasticsearch： 1./bin/elasticsearch 如果一切正常，elasticsearch默认在本机9200端口运行。打开另一个命令行窗口，执行以下命令，检查elasticsearch是否运行成功： 1curl localhost:9200 如果输出以下信息，则运行正常。 默认情况下，elasticsearch只允许本机访问。要想其他电脑可以访问，也就是实现远程访问，需要修改文件/config/elasticsearch.yml,取消字段network.host的注释，把该字段的值改为0.0.0.0。这样的话所有的电脑都能访问，实际情况中，最好不要这样。 如果启动遇到错误“Native controller process has stopped - no new native processes can be started”或“max virtual memory areas vm.maxmapcount [65530] is too low”。解决方法是执行以下命令： 1sudo sysctl -w vm.max_map_count=262144 在python中使用elasticsearch要先安装elasticsearch包。在python中使用elasticsearch要先启动elasticsearch。 1pip install elasticsearch 创建index12345from elasticsearch import Elasticsearch es = Elasticsearch() #创建实例，默认localhost:9200# es = Elasticsearch([&#123;&apos;host&apos;:&apos;10.112.1.109&apos;,&apos;port&apos;:&apos;9200&apos;&#125;]) #远程访问result = es.indices.create(index = &apos;news&apos;,ignore = 400)print(result) 如果创建成功，会返回以下信息 1&#123;&apos;acknowledged&apos;: True, &apos;shards_acknowledged&apos;: True, &apos;index&apos;: &apos;test_es&apos;&#125; 如果再次创建，就会返回以下信息： 1&#123;&apos;error&apos;: &#123;&apos;root_cause&apos;: [&#123;&apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;], &apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;, &apos;status&apos;: 400&#125; 表示创建失败，失败的原因是要创建的index已经存在了。status状态码是400。 插入数据1234567891011datas = [ &#123;&apos;title&apos;:&quot;算法导论（原书第2版）&quot;, &apos;url&apos;:&quot;https://book.douban.com/subject/1885170/&quot;, &apos;introduction&apos;:&quot;这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。&quot;&#125;, &#123;&apos;title&apos;:&quot;计算机程序的构造和解释&quot;, &apos;url&apos;:&quot;https://book.douban.com/subject/1148282/&quot;, &apos;introduction&apos;:&quot;《计算机程序的构造和解释(原书第2版)》1984年出版，成型于美国麻省理工学院(MIT)多年使用的一本教材，1996年修订为第2版。在过去的二十多年里，《计算机程序的构造和解释(原书第2版)》对于计算机科学的教育计划产生了深刻的影响。第2版中大部分重要程序设计系统都重新修改并做过测试，包括各种解释器和编译器。作者根据其后十余年的教学实践，还对其他许多细节做了相应的修改。&quot;&#125;, ]for i in range(len(datas)): es.index(index = &apos;book&apos;,doc_type = &apos;computer&apos;,id = i+1,body = datas[i]) index()方法可以完成两个操作，如果数据不存在，那就插入数据；如果数据已经存在，那就更新数据。 get()按ID查询12result= es.get(index=&apos;book&apos;,doc_type=&apos;computer&apos;,id =1)print(result[&apos;_source&apos;]) search()实现全文检索对于中文，我们要安装一个中文分词插件elasticsearch-analysis-ik。可以使用elastic的另一个命令行工具elastisearch-plugin来安装，要确保版本号一致。进入elastic的目录下，执行： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip 注意将6.3.0替换为自己的版本号。安装成功后，重启elasticsearch，就会自动加载这个中文分词插件。 1234567891011es = Elasticsearch()mapping = &#123; &apos;properties&apos;: &#123; &apos;title&apos;: &#123; &apos;type&apos;: &apos;text&apos;, &apos;analyzer&apos;: &apos;ik_max_word&apos;, &apos;search_analyzer&apos;: &apos;ik_max_word&apos; &#125; &#125;&#125;result = es.indices.put_mapping(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=mapping) 指定使用中文分词器，如果不指定默认使用英文分词器。 12345678dsl = &#123; &apos;query&apos;: &#123; &apos;match&apos;: &#123; &apos;introduction&apos;: &apos;计算机&apos; &#125; &#125;&#125;result = es.search(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=dsl) 参考链接 全文搜索引擎 Elasticsearch 入门教程—–阮一峰 Python Elasticsearch文档 Elasticsearch官方文档 Elasticsearch基本介绍及其与Python的对接实现–崔庆才 Elasticsearch搜索中文分词优化","categories":[{"name":"web搜索","slug":"web搜索","permalink":"http://yoursite.com/categories/web搜索/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Ubuntu服务器遇到的一些问题","slug":"Ubuntu服务器遇到的一些问题","date":"2019-03-07T11:53:00.000Z","updated":"2019-03-12T06:13:31.000Z","comments":true,"path":"2019/03/07/Ubuntu服务器遇到的一些问题/","link":"","permalink":"http://yoursite.com/2019/03/07/Ubuntu服务器遇到的一些问题/","excerpt":"记录Ubuntu服务器遇到的一些问题。","text":"记录Ubuntu服务器遇到的一些问题。 linux服务器连不上网 先检查网线是否插好了，若网线口发亮才是插好。检查电脑是否能ping通局域网的其他电脑。 查看其他电脑的ip地址ifconfig | grep inet ping其他电脑的IP地址ping 10.112.0.1如果可以ping通其他电脑，再检查下一步。 可以ping通其他电脑，但ping 10.3.8.211校园网网关失败。这时可能是路由出错，查看服务器的路由是否正确。 查看比较服务器与可以正常联网的电脑的路由。route -n 添加正确的默认路由。route add default gw 10.112.0.1 检查能否连接到校园网。ping 10.3.8.211Ubuntu配置路由参见: ubuntu配置静态路由及重启生效 连接到校园网，但是ping www.baidu.com失败。服务器ping不通域名，但可以ping通百度的ip地址112.34.112.40。这是服务器的DNS配置出错了，无法解析域名。 修改文件/etc/resolv.conf，必须有sudo权限。sudo vim /etc/resolv.conf 添加以下内容nameserver 8.8.8.8 重启网络使修改立即生效。sudo /etc/init.d/networking restart 这时应该能ping通百度了。 重启电源后，以上方法会被清除而失效，导致开机后需要重新配置。有效的方法是卸载开机重写/etc/resolv.conf的resolvconf，执行命令sudo apt-get autoremove resolvconf配置域名解析参见：Ubuntu 无法解析域名 不能通过Xshell或ssh命令连接到服务器 检查是否安装了ssh-server服务。ps -e | grep ssh 若没有安装，使用以下命令安装：sudo apt-get install openssh-server 若安装了ssh-server服务，检查ssh服务是否打开。需要sudo权限 检查ssh服务状态service sshd status或/etc/init.d/ssh status 开启ssh服务service sshd start或/etc/init.d/ssh start","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"}]},{"title":"pytorch学习笔记","slug":"pytorch学习笔记","date":"2019-03-05T01:27:14.000Z","updated":"2019-07-07T07:09:35.000Z","comments":true,"path":"2019/03/05/pytorch学习笔记/","link":"","permalink":"http://yoursite.com/2019/03/05/pytorch学习笔记/","excerpt":"这里是pytorch学习笔记。","text":"这里是pytorch学习笔记。 创建操作torch.randn()1torch.randn(*size,out = None) 输入： size(int)：指定了输出张量的形状 输出：输出结果为张量 作用：返回一个张量，从标准正态分布中抽取一组随机数。形状由*size决定 张量维度操作转置：transpose1torch.transpose(input,dim0,dim1) 参数： input: 输入张量，可以是二维及二维以上的张量 dim0,dim1: 要转置的两个维度。 作用： 返回输入矩阵的转置。一次只能转置张量的两个维度。输出张量与输入张量共享内存，同步改变。 1torch.t(tensor) 输入一个二维张量（矩阵），并转置0,1维。可以看做函数torch.transpose(input,0,1)的简写函数。比较下transpose与view这两个函数： 1234567891011121314151617181920212223242526272829303132333435&gt;&gt; a = torch.randn(2,3,5)&gt;&gt; b = torch.transpose(a,1,2)&gt;&gt; c = a.view(2,5,3)&gt;&gt; print(a)tensor([[[ 0.9926, -0.1669, -1.6571, -0.2730, -0.1313], [ 0.9811, -1.9854, 1.5519, 0.1383, 1.4571], [ 0.8221, -1.1283, -0.7675, -2.0497, 0.4748]], [[ 0.1594, 0.7166, -0.2603, 1.1027, 1.5283], [-0.7652, -1.4711, 0.5077, 0.6639, 0.0374], [ 1.8121, -1.4864, -2.9863, -0.5769, -0.2915]]]) &gt;&gt; print(b)tensor([[[ 0.9926, 0.9811, 0.8221], [-0.1669, -1.9854, -1.1283], [-1.6571, 1.5519, -0.7675], [-0.2730, 0.1383, -2.0497], [-0.1313, 1.4571, 0.4748]], [[ 0.1594, -0.7652, 1.8121], [ 0.7166, -1.4711, -1.4864], [-0.2603, 0.5077, -2.9863], [ 1.1027, 0.6639, -0.5769], [ 1.5283, 0.0374, -0.2915]]]) &gt;&gt; print(c)tensor([[[ 0.9926, -0.1669, -1.6571], [-0.2730, -0.1313, 0.9811], [-1.9854, 1.5519, 0.1383], [ 1.4571, 0.8221, -1.1283], [-0.7675, -2.0497, 0.4748]], [[ 0.1594, 0.7166, -0.2603], [ 1.1027, 1.5283, -0.7652], [-1.4711, 0.5077, 0.6639], [ 0.0374, 1.8121, -1.4864], [-2.9863, -0.5769, -0.2915]]]) 可以看到:二者得到的结果并不相同。transpose是进行转置操作。view对张量整形时，张量中元素的顺序保持不变。相当于将这个三维张量按顺序 torch.Tensortorch.manual_seed()1torch.manual_seed(seed) 输入： seed(int or long)：设定种子，为int类型或long类型 作用：设定生成随机数的种子。种子相同，生成的随机数就是相同的，实验结果就可以复现。 参见：利用随机数种子来使pytorch中的结果可以复现 .view() 整形1tensor.view(*size) 输入： *size(int)：指定了输出张量的形状 输出：输出结果为张量 作用：整形，只改变原张量的形状，形状由*size指定。 例子： 123456789&gt;&gt; x = torch.randn(5,4)&gt;&gt; x.size()torch.Size([5,4]&gt;&gt; x.view(30)&gt;&gt; x.size()torch.Size([20])&gt;&gt; x.view(1,1,-1) # -1表示该维度由其他的维度推断。&gt;&gt; x.size()torch.Size([1, 1, 20]) torch.cat() 连接1torch.cat(inputs,dimension = 0) 输入： inputs(sequence of Tensors)： 多个Tensor的python序列。 如[tensor1,tensor2…]或(Tensor1，tensor2) dimension(int,optional): 沿着该维连接张量序列。默认为0。 作用：在指定维度上，对输入张量序列进行连接操作。 举个例子： 12345678910&gt;&gt; impotr torch &gt;&gt; x = torch.randn(4,3)&gt;&gt; x.size()torch.Size([4, 3])&gt;&gt; y = torch.cat((x,x,x),0)&gt;&gt; y.size()torch.Size([12, 3])&gt;&gt; z = torch.cat((x,x,x),1)&gt;&gt; z.size() torch.Size([4, 9]) torch.optimclass torch.optim.SGD(params,lr=,momentum=0,weight_decay=0) 参数： params： 待优化参数的iterable lr(float): 学习率 momentum(float,可选)： 动量因子，默认为0 weight_decay(float,可选): 权重衰减，默认为0 作用：实现随机梯度下降算法。 如何使用optimizer? 123456789import torch.optim as optim optimizer = optim.SGD(model.parameters(),lr = 0.01) #构建一个optimizer,model.parameters()给出了所有要优化的参数for input,target in dataset: optimizer.zero_grad() #清空所有被优化过的Variable的梯度 output = model(input) loss = loss_fn(output,target) loss.backward() #反向传播算法，计算好所有要优化Variable的梯度。 optimizer.step() #单步优化，基于计算得到的梯度进行参数更新。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"}]},{"title":"Numpy学习笔记","slug":"Numpy学习笔记","date":"2019-02-27T12:33:15.000Z","updated":"2019-03-25T13:28:23.000Z","comments":true,"path":"2019/02/27/Numpy学习笔记/","link":"","permalink":"http://yoursite.com/2019/02/27/Numpy学习笔记/","excerpt":"这里是numpy学习笔记。","text":"这里是numpy学习笔记。 np.copy1np.copy(a) a为ndarray数组。np.copy复制一个与a完全相同的dnarray数组。来看看=与np.copy的区别。 12345678&gt;&gt; x = np.array([1,2,3])&gt;&gt; y = x&gt;&gt; z = x.copy(x)&gt;&gt; x[0] = 10&gt;&gt; x[0] == y[0]True&gt;&gt; x[0] == z[0]False 对于=，x与y共享同一内存，数据同步改变。一个改变另一个跟着改变。对于np.copy,x与z在不同的内存，数据改变互不影响。 np.load与np.savenumpy可以读写磁盘上的二进制数据。为将ndarray数组对象保存到文件，引入了文件格式npy。数组对象ndarray以未压缩的原始二进制格式保存在扩展名为.npy的文件中。 np.save(file,array) 作用： 将数组以二进制格式保存到扩展名为npy的文件中。 np.load(file) 作用： 从.npy文件中读取二进制数据还原为数组。举个例子：1234import numpy as np a = arange(5)np.save(&apos;a.npy&apos;,a)b = np.load(&apos;a.npy&apos;) np.full1np.full(shape,fill_value,dtype) 作用： 返回一个 给定形状为shape，数据类型为dtype，全都由fill_value填充后的ndarray。 np.foat32np.float32(x)作用：变换数据类型为float32 pad()参数解释 1numpy.pad(array, pad_width, mode, **kwargs) 输入 array 为待填充的数组 pad_width 为((before_1,after_1),(before_2,after_2),….,(before_N,after_N))在每个维度前后填充的个数 mode 常用constant,用常数填充。 返回值： 填充后的ndarray 举个例子 123456import numpy as np a = range(5)# 在一维数组前后分别填充2,3位数字；填充的数字分别为0,2ndarray = np.pad(a,(2,3),&apos;constant&apos;,constant_values=(0,2))print(a)print(ndarray) 执行结果为 12[0, 1, 2, 3, 4][0 0 0 1 2 3 4 2 2 2] np.randomnp.random.uniformnp.random.uniform(low = 0,high = 1,size = None)作用： 生成一个形状为size的随机数矩阵，每个数从均匀分布的半开半闭区间[low,high)中抽样得到。 参考链接 Numpy官方手册","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"2018-群星陨落","slug":"2018-群星陨落","date":"2019-01-16T12:56:00.000Z","updated":"2019-07-07T07:12:28.000Z","comments":true,"path":"2019/01/16/2018-群星陨落/","link":"","permalink":"http://yoursite.com/2019/01/16/2018-群星陨落/","excerpt":"游龙当归海，还不迎我自来也！","text":"游龙当归海，还不迎我自来也！ 今天是农历12月十一，明天就是爸爸的生日。阳历的2018年已经过去了，但春节还没来呢！研究生一年级的学期期末，我想着回顾一下这一年，看看自己这一年是怎么度过的。2018年，许多了不起的人去世了，因此选了这个小标题。2018年，我大学毕业，顺利通过了研究生复试。考研，是与高考同等级别的考试，不同的是考研没有老师和家长的督促，只得一个人奋战。备战考研初试已经是2017年的往事了，但还是值得一提的。准备初试的小半年里基本上每天泡在图书馆里，坐在某个固定的座位。虽然有时候也会懈怠懒惰，但大部分时间都是在认真地学习。晚上九点半从图书馆出来，我常要绕着学校跑上五公里；回宿舍经过一楼的镜子，我看着自己嘴里念叨着学校的名字。如愿以偿，2018年年初，我查自己的考研初试成绩409，是比高考还令人满意的了。查到成绩后，我欢欣鼓舞了一阵子。 上半年 - 本科最后的日子年初，我因为考驾照的缘故住在我姨家里，有了跟亲戚一起生活半个月的机会。亲戚走动的机会不多，平时没有机会去了解他们促进感情，全凭着血缘的纽带联系着。但这次难得地坐在亲戚家里有了不一样的感受，感受到日久生情（可能用词不当，但意思到了呢）。有天中午，我姨有事外出，我跟表妹一块做了午饭，手忙脚乱，做出来的饭味道竟还不错！考过了科二，我便回了家。每年过年都得下一场雪，有到脚踝那么厚，到处都白茫茫的一片。我拿着铲子去屋顶铲雪，花了两个半天，把屋顶上的雪收拾到了院子里。只是那时候竟没有兴致堆个雪人，今天冬天北京一丝雪都没有落，真想回到家能看一场雪啊。高中的班主任还在带着高三的学生，过了年没几天高三的学生便开学了。班主任想着我给高三的同学们聊聊天，说“现在的学生都没有你高三下苦”，我勉强算是个刻苦的笨孩子吧。在老班家里吃了午饭，下午便回到曾经的教室里，跟高三的同学们聊天。时间在一天一天地度过，我们在一步一步地向前走呢！ 大四下，已经没课了，只剩下做毕业设计。我在明德楼B座的人才办找了一份勤工俭学的差事，平常的工作是给老师整理文件，打印，拿快递和打扫卫生。这是段闲适又想着充实的日子，想着好好完成毕业设计，想着去做大学里想做还没做的事，想着去看几本书，想着跟朋友们告别。我每天在老师的办公室里做毕设，中午便在沙发上睡一会，算不上特别努力，只是想在大学最后的几个月里把事情做好一些。办公室的窗口有一些灰尘，窗台上有爬山虎的脚，窗外有许多树。阳光照进来，树影婆娑的，像极了大学最后的这段日子。大四下，我去圣昆仑音乐厅听了音乐会，只听得一半便溜了出来；也在圣昆仑音乐厅看了话剧《蜀道难》，虽然有一些瑕疵，但依然十分精彩，震撼人心，让我领略到了现场话剧的魅力；我常去文理馆三楼找文学类的书，在做毕设的闲暇翻看。读了王小波写给李银河的信，知道爱情会让人牵肠挂肚；读了杨绛先生的散文集，我原先以为散文集是无趣的，但读了杨绛先生的散文集，才体会到朴实文字的动人之处。尤其当读到杨绛先生写文革期间女婿被红卫兵打死，父亲去世却奔丧不成，自己被分配到厕所刷马桶这些情节，就好像在讲述一些生活中的小事。也看到杨绛先生去菜园找钱钟书时流露出的不难察觉的爱意和在苦难中的幽默感。车协是我大学四年最重要的地方吧，也是我大学归属感的来源。在最后这半年里，在周二周四的晚上参加了许多次的体训，周一周三周五也偶尔跟着大家骑车去怪坡。最后还参加了几次小假期的拉练：“放火烧山，牢底坐穿”，黄巢的篝火没有烧起来，但心里的篝火不曾熄灭，”聚是一团火，散作满天星”；“清明拉练”去看了遍野的油菜花，大一第一次参加拉练也是清明拉练呢；五一是药乡选拔，新的远征队又将踩着单车用车轮丈量祖国的大好河山！快要离开的时候，想着要做一些事，比平时更用心些。婷婷提出要办一个车协的”考研、保研、工作交流会”，但她因为要工作提前离校了，我接过这个活，跟车协的伙伴一起办了这个交流会。在北京比赛之前，我想起了前两年给京赛队员加油的火腿肠，今年该是我了吧！大学的许多生活都与车协相关，大学许多朋友都是车协的伙伴。我似乎没给留下些什么，但一同经历就是意义啊！大学最后的尾声是在拍毕业照和送老会中进行的。终于穿上那一身幼时憧憬的毕业服，拍学院里的毕业照，拍班级里的毕业照。也拍协会那一群人的毕业照，小树林，南门外，臧克家和闻一多的雕像旁，曾经朝夕相处，今日便要离之而去。在车协的送老会上，我以为我不会哭的，两年远征都没有哭，只是未曾坦开心扉，在送老会上，平时话不多的我也不知哪来那么多的话和眼泪。嘿，毕业快乐！！！ 暑期 - 目的地在尼泊尔的新藏线大学毕业后，趁着年轻力壮去骑了新藏线。时间跨度是7月21日到8月28日近四十天的时间。先坐火车从北京到叶城，中途在吐鲁番转车，从祖国的东边到最西边。骑车的路段是“喀什-萨嘎县-吉隆口岸-加德满都”。返程是费尽周折的，先坐吉普车从加德满都回国到基隆口岸，再做大巴车到日喀则，再坐火车“日喀则-青海西宁-陕西西安-山西临汾”，从陆地回家的成本虽然只有机票钱的一半，但花了整整五天。之前有过骑车出国门的想法，但不是十分笃定的。旺哥在论坛上发帖征新藏线的队友，我毅然地回了贴。原先计划的新藏线小队有我、旺哥和宇哥三个人，后来宇哥因为入职没能成行。新藏线是一条比川藏线要难的路线，我每天晚上跑步五公里来做体力储备，有时候也偷懒；在美骑网上看别人的路书和骑行游记，做了自己的路书和攻略。特地回老家办了护照和边防证，办边防证的过程费了一些周折，办边防证需要一份小领导写个名字，但他又不来上班，花了几天才终于办好了。又在北京的尼泊尔大使馆办了签证，准备地差不多只等出发了。 7月21日，坐火车从北京出发，坐了38个小时的硬座到了吐鲁番，再坐19个小时的硬座到新疆喀什地区喀什市跟旺哥会合。全国用的都是北京时间，因此到晚上九点，喀什的天还是亮的。略作修整，便到了开始每天的骑行。第一天的骑行就差点要了老命，一是第一天的路程远，又有很大的逆风，晚上到宾馆已经累地快虚脱了。新藏线前几天的路是比较平坦的，海拔上升不太大。骑车的第四天我遇到一个致命的问题，差点导致我的新藏线骑行半路夭折。我自行车后轮的花鼓在一个小村子阿克美其特村里断了，小村子里连去城里的公交车都没有，完全不可能修理。我跟旺哥商议，旺哥继续骑车到下个地点补给更方便的库地达坂等我，我返回叶城去修车。我拆了自行车的后轮，村里的村干部开车送我到了镇上，我在镇上坐了个出租车返回了叶城。为了节省时间，我没去更远更大的喀什市去修理，只在叶城找人修理，这给我后来的不幸埋下了隐患。在叶城找了一个不靠谱的单车修理店给我修理后轮，信誓旦旦地坐车回到阿克美其特村，结果骑了不到二公里，自行车的后轮又坏了，是根本没有修理好。悔不该杀那华佗！我终于下定决心老老实实地去喀什找捷安特专卖店买了轮子，又是艰难的交通，先在路边搭回城里的顺风车，再坐火车到喀什，买好轮子，再坐火车返回叶城，最后搭了许多车（私家车，大卡车）终于回到阿克美其特村。我修理好车，战战兢兢的出发了，小心翼翼地，生怕轮子再出幺蛾子。所幸后面一路上都没再出什么大问题，这一次就够我折腾的了，反反复复弄弄三天。以致于我想要不车子也不要了，买张票回家吧！现在回过头来看，所幸坚持了下来。因为修车我耽误了三天行程，旺哥还在前面等着我。为了追上旺哥，不让他多等，我搭了一个大卡车从麻扎兵站到四十里营方跟旺哥会合，这是一天的行程。在红柳滩我和旺哥找到了新的伙伴，贤弟和邓翱，我们变成了四人小队。从红柳滩往后的三百公里是无人区，但还有人烟，是一段可怕的路。骑友中流传着有些骑友没能赶到住宿点，在野外扎营被狼群吃掉的传说，不知真假。进无人区前一天的晚上，我如临大敌，把驼包收拾好，买好水和补给，第二天早早地就出发了。四人小队顺利地走过了无人区，无人区大多荒无人烟，是野生动物被不断蚕食的栖息地。新藏线比川藏线更难，因为新藏线整体海拔高，空气稀薄；而且人烟稀少，缺乏物资补给；再加上时不时的逆风和鬼见愁的上坡、高耸入云的达坂；新藏线这条路想着一辈子走一次就够了，打死不想再煎熬一次了。但新藏线有着她的魅力，荒芜广袤而静默无声的沙漠，精致如翡翠般的湖泊，奔跑的藏羚羊和黄羊，让人敬畏自然；世界上海拔最高的公路，让人想要去挑战和完成，走完新藏线后，有油然而生的骄傲。新藏线广袤荒芜，很少有高的树，当你看到高大的杨树和绿色的植物，就说明附近有水源，此处有人烟。新藏线上是一条连接新疆与西藏的纽带，有许多跑新藏线的大卡车司机。搭车的时候跟几位司机聊天，司机师傅大多是山西河南的，汉族人多，藏族少。在新藏线上跑大卡车，就是在拿命挣钱，高原反应和疲劳驾驶，一个不小心就是车毁人亡。新藏线路边会常看得到翻倒在路边的大卡车，有的男人带着妻子来跑车，出了事一家子就没了，剩个孩子跟着爷爷奶奶。新藏线有除了踩单车的，还有骑摩托车的，自驾游的。常看得到自驾游的大妈披着丝巾拍照。封路的时候，自驾游的车夹杂着拉水泥拉货的大卡车长长地排着队。一路上的比较大的客栈旅店，大都是汉族人开的，四川人很多，四川人勤劳敢拼，新疆西藏到处可见到他们的身影。新藏线这条长长的路上，整整齐齐训练有素的军车是一道靓丽的风景，让人感叹“好男儿就要当兵，保家卫国”。新藏线上，有人的地方就有众生相。到了萨嘎县之后，我们四人要分道扬镳了，我一路出吉隆口岸到加德满都，他们三人一路。旺哥先去珠峰大本营再去拉萨，邓翱和贤弟先去珠峰大本营再去加德满都。萨嘎县到吉隆县的路是非常难走的，没有硬实的柏油路，只有下雨后泥泞的泥路。十米宽的水坑，我脱了鞋子挂在车把上，推着车趟过去。泥泞的路，一脚一脚的踩踏着，车轮上滚满了泥巴，像是炸酥肉前给肉裹上厚厚的面。顶着下午刮个不停的巨大的逆风，长长的上坡，我再也没有了绝不推车的坚持。遇到难爬的上坡，就下来推着车走一段再骑车。大风和泥巴路，骑了一整天只走了五十公里，眼看就要露宿野外而帐篷早寄走了，终于看到一个大大的施工营地，像是溺水的人抓住一根救命稻草。营地里七八个西安的爷们收留了我一宿，度过了危机。到了基隆口岸，我换好尼币，经过中国海关，出了国门。尼泊尔的路才是真的烂路，石头路，泥路，山体滑坡封路都是家常便饭，偶尔能看到一段柏油路便开心地不行。尼泊尔的路是不推荐骑自行车的。尼泊尔是山地里的国家，较大的城镇都分布在平坦的河谷地。我买了尼泊尔的电话卡，但是用不了网络，不能看谷歌地图。我只能一边走，一边用蹩脚的英语问路“这条路是到加德满都吗？”；尼泊尔年轻人大多会说英语，偶尔几个会说汉语。在烂路里走了两天，我他娘的终于到了加德满都，在泰米尔区见到了中国人和中国店铺，激动地不行。只要有出发的勇气，就一定会到达！我终于到了加德满都！新藏线一路上盖了一些邮戳，但并不多。骑车的间隙，抽空给15和16远征队的队友和一些老友寄去了明信片，现在有些明信片寄到了，有些寄丢了。我寄给自己的丢在路上了，就让他代替我在路上飘荡吧！骑完新藏线是一件了不起的事，是一件有意义的事。 下半年 - 研究生的日子暑假，在去新藏线之前来了读研的学校上了三周的暑期课程，《机器学习》和《凸优化》。9月初开学正式开始了研究生生活。之前我有一个错误的认识，认为研究生生活应该完全舍弃掉自行车，因为之前骑车太多了吧。后来，我认识到生活需要balance，不能只是学习，要让骑车和娱乐成为生活里积极的一部分。来了北京之后，出去玩的次数并不多。训超骑车从济南到北京，来我宿舍住了一晚，我跟他一块去了故宫。国庆节那天，旺哥来北京转车，我和训超早早地去看升旗，给旺哥接风洗尘。后来约着在北京读书工作的高中同学聚了一次。周末骑了两次车，去了卢沟桥和香山公园旁的西山国家森林公园。研一上，还有一些课要修。除了上课，其他时间是待在实验室里，看论文，写代码。前期，感觉有许多需要学习的东西，总能早早地起床。后来感觉不到自己的进步，有些陷入迷茫，不知道该做些什么，就失去了劲头，起床时间晚了许多。我找了一个辅导考研专业课的差事，每周末辅导一个半小时，《通信原理》已经大半年没看过了，但上手简单看一遍就能想起来那些知识点。妈妈生日那天，我买了一只天鹅项链做生日礼物。弟弟生日那天，我给他发了红包，他没领说自己有钱。爸爸今天生日，我鼓动身边的朋友给老爸发了祝福的短信。2019年，希望可以更加勇敢和更加积极地面对生活里的变化。你好，2019！","categories":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/categories/年度总结/"}],"tags":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/tags/年度总结/"},{"name":"生活记录","slug":"生活记录","permalink":"http://yoursite.com/tags/生活记录/"}]},{"title":"用Hexo+Next+github page搭建个人博客","slug":"用hexo搭建个人博客","date":"2019-01-13T02:08:31.000Z","updated":"2019-07-07T07:21:22.000Z","comments":true,"path":"2019/01/13/用hexo搭建个人博客/","link":"","permalink":"http://yoursite.com/2019/01/13/用hexo搭建个人博客/","excerpt":"Hexo是一个简洁漂亮的博客框架，用来生成静态网页。本文介绍使用hexo搭建博客。","text":"Hexo是一个简洁漂亮的博客框架，用来生成静态网页。本文介绍使用hexo搭建博客。 准备：安装Hexo安装前提在安装Hexo之前，应先安装以下应用程序： Node.js Git 安装Git对Linux(ubuntu,debian) 1sudo apt-get install git-core 安装Node.js1wget -qO- https://raw.github.com/creationix/nvm/v0.33.11/install.sh | sh 安装完成后，需要重启终端再执行以下命令： 1nvm install stable 安装Hexo安装好Git和Node.js后，使用npm安装Hexo: 1npm install -g hexo-cli hexo常用命令 hexo init #用于新建一个网站 hexo new #在站点根目录下执行，以/scaffolds/post.md为模板，创建一篇新的文章。新建文章存放在/source/_posts下。 hexo generate 简写为 hexo g #生成静态网页 hexo server 简写为 hexo s #启动服务器，默认访问网址为http://localhost:4000/ hexo clean #用于清楚缓存文件和已经生成的静态文件。当对站点的修改无论如何不能生效时，运行该命令。 hexo deploy 简写为 hexo d #将生成的静态文件部署到github等远程服务器。常用的套路是先使用hexo g和hexo s在本地编辑和调试博客，当调试无误后，再用hexo d部署到GitHub。 本地建站选好路径，以下命令会自动创建文件夹来存放博客内容。 1hexo init &lt;folder&gt; #生成hexo模板 上述命令生成了以下文件,生成文件夹的目录如下。 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 上述文件中，只需要重点关注和了解这几个文件： _config.yml 是站点配置文件。用来配置博客网站的各种信息。 /source/_posts 用来存放我们自己之后写的博客。 themes 用来存放博客的主题。再执行以下命令:123cd &lt;folder&gt;npm installhexo server 执行完以上命令，可以访问 http://localhost:4000 看到博客已经顺利搭建。 修改站点配置文件hexo中有两个重要的配置文件，名称都是 _config.yml： 站点配置文件，位于站点根目录下，用于站点本身的配置。 主题配置文件，位于主题目录下，用于主题相关的配置。 通过修改站点配置文件的下述字段，来分别修改网站名称、副标题、个性签名(或网站简介)、作者和语言设置。 12345title: subtitle:description: author: language: zh-Hans #设置语言为中文 将博客关联到GitHub1、在github创建仓库 &lt;your-user-name&gt;.git.io,&lt;your-user-name&gt;必须与你的github用户名相同。2、编辑站点配置文件，找到字段deploy，作如下的修改。 1234deploy: type: git repository: https://github.com/&lt;your-user-name&gt;/&lt;your-user-name&gt;.github.io.git branch: master 3、使用命令npm install --save安装插件。 1npm install hexo-deployer-git --save 4、生成静态网页 1hexo g 5、部署到GitHub 1hexo d 运行上述命令后，访问 http://&lt;your-user-name&gt;.git.io ‘hexo d’部署时不输密码执行hexo d将博客部署到github时，每次都要输入github的账号和密码，非常麻烦。我们利用ssh的公钥登录免去输入密码的步骤。 ssh的公钥登录简介所谓公钥登录。本地主机生成一个公钥和一个秘钥，把公钥告诉给远程主机（github），而秘钥只有自己知道。当本地主机要登录远程主机时，远程主机（github）发送一个消息序列给本地主机，本地主机用秘钥将消息序列加密后再发送给远程主机；远程主机收到后用公钥解密，若与原消息序列相同，则可以登录。通过这种“公钥登录”的方式保证了安全性，不需要再用账户和密码来验证身份。 本机主机生成公钥和秘钥在命令行执行： 1ssh-keygen 生成的文件有id_rsa.pub和id_rsa，分别为公钥和私钥。Linux系统下，生成文件在home/ssh/目录下。windows系统,在C:/user/.ssh下。 把公钥告诉给github在Linux系统下，执行以下命令查看公钥，然后复制。在windows系统下，将id_rsa.pub用文本格式打开来复制。 12cd ~/.sshcat id_rsa.pub 在github的settings&gt;SSH and GPG keys&gt;New SSH keys新建一个ssh key，将复制的公钥粘贴保存即可。 修改站点配置文件用ssh方式来访问仓库，而不是用https方式打开站点配置文件，修改字段deploy下的repository的值。 12345deploy: type: git # repository: https://github.com/spring-quan/spring-quan.github.io repository: git@github.com:spring-quan/spring-quan.github.io.git branch: master 参见SSH远程登录Hexo免输入密码部署到Github 绑定域名购买并注册域名在阿里云的万网购买一个域名。常用的域名后缀有.com/.cn/.me/.top。不同后缀的域名价格不同，可以按照喜好选择一个。购买好域名后，需要完成 域名实名认证，才能正常进行域名解析，该域名才能使用。 注意：注册域名后，需要域名实名认证，不需要备案，域名就可以正常使用。 给GitHub添加域名在博客对应的github仓库下，新建一个文件CNAME，文件名要大写。内容如下，注意要为顶级域名，不能包括www或http。 1example.com 在用hexo d命令将博客部署到GitHub上时，文件CNAME会被覆盖删除掉。为了解决这个问题，还需要在站点目录下/source，新建同样的文件CNAME。 设置域名解析设置域名解析就是将注册到的域名指向一个IP地址，首先我们要查询github.io域名对应的IP地址。在命令行中ping自己的github.io域名，就可以得到IP地址。 1ping &lt;your-user-name&gt;.github.io 接着，在阿里云万网的管理控制台&gt;域名服务&gt;域名列表中设置域名解析即可。这时域名就绑定好了，可以通过buptccq.top 和www.buptccq.top 来访问博客。 注意：1、设置域名解析不是立即生效的，需要等10分钟左右。2、若用自定义的域名访问失败后，经过修改，避免浏览器的DNS缓存，可以使用浏览器的无痕模式。 写作博客+技巧在站点根目录下，执行以下命令创建一篇新文章： 1hexo new &lt;title&gt; 上述命令在站点根目录下的\\source_posts 文件夹生成一个 .md文件。按照markdown规则，编辑该文件。接下来介绍编辑md文件的几个技巧。 块引用1、一种方法。 12&gt; 只是块引用&gt; 是的啊 效果图如下： 只是块引用是的啊 markdown语法 参考链接markdown中文文档Markdown 入门参考MarkDown语法简介Markdown 书写风格指南 绘制表格用|来分隔不同的单元格，用-来分隔表头和其他行。关于对齐： :--- 左对齐，默认情况 :---: 居中 ----: 右对齐 使用方式举例： 1234|主公|刘备|曹操|孙权||----|----|----|----||政权|蜀|魏|吴||相关|关羽、张飞|曹丕、曹植|周瑜、鲁肃| 实现效果： 主公 刘备 曹操 孙权 政权 蜀 魏 吴 相关 关羽、张飞 曹丕、曹植 周瑜、鲁肃 #### 删除线 使用方式： 1~~删除线~~ 使用效果为：删除线 markdown其他用法12345678910111213141516171819202122&lt;!-- more --&gt; #在首页显示摘要 content #块引用&lt;www.baidu.com&gt; #链接[百度](www.baidu.com) #内嵌链接![](/path/to/image &apos;image name&apos;)有序列表1. 2.3.无序列表- - - **粗体字***斜体字*``代码``## 二级标题### 三级标题 文字居左，居中，居右居中 1&lt;center&gt;诶嘿&lt;/center&gt; 居左 1&lt;p align=&quot;left&quot;&gt;诶嘿&lt;/p&gt; 居右 1&lt;p align=&quot;right&quot;&gt;诶嘿&lt;/p&gt; 插入表情😀😇👹 给博客选用主题这里要介绍一下“主题”这个概念。主题也就是博客的样式。给博客安装一个好的主题，就好像给人选一件漂亮的衣服穿。hexo有多种不同的主题，可以改hexo选择不同的样式和布局。主题列表有详细的介绍。使用最多的主题是Next,接下来介绍主题的安装。 安装主题Next 先下载主题Next，在站点根目录下执行： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题，打开站点配置文件，找到theme字段，把它的值修改为 next 1theme: next 验证主题。运行’hexo server’,访问http://localhost:4000。 选择Scheme通过修改主题配置文件完成。打开主题配置文件，找到字段scheme，将不要的主题注释掉，要用的主题不注释。 123#scheme: Muse#scheme: Mistscheme: Pisces 设置语言为中文通过修改站点配置文件完成。打开站点配置文件，找到字段language，把它值修改为 zh-Hans 1language: zh-Hans 主题美化-1设置上部的菜单栏 设置菜单内容。通过修改主题配置文件完成。打开主题配置文件，找到字段menu，不用的菜单栏选项注释掉，要用的不注释。 1234567menu: home: / archives: /archives # 冒号右边为主题目录下/source下的文件夹。 #about: /about #categories: /categories tags: /tags #commonweal: /404.html 设置菜单项的显示文本。编辑主题目录下的/languages/zh-Hans.yml。 12345678menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 commonweal: 公益404 设置菜单项的图标。编辑主题配置文件，找到字段menu_icons,修改为： 12menu_icons: enable: true 设置头像打开主题配置文件，找到字段avatar，把它的值修改为头像照片的路径。将头像照片放在主题目录/source/images(如果不存在就创建)。 1avatar: /images/avatar.png 添加‘标签’页面首先要在菜单栏显示‘标签’链接。在站点目录下，新建一个页面： 1hexo new page tags 编辑刚刚生成的页面,修改字段 title 和 type 123title: 标签date: 2019-01-13 12:22:58type: &quot;tags&quot; 在菜单栏中添加链接。修改主题配置文件，找到字段menu下的字段tags，把它值修改为 /tags 1234menu: home: / archives: /archives tags: /tags 添加‘分类’页面与添加‘标签’页面基本相同。首先要在菜单栏显示‘分类’链接。在站点目录下，新建一个页面： 1hexo new page categories 编辑刚刚生成的页面,修改字段 title 和 type 123title: 分类date: 2019-01-13 12:22:58type: &quot;categories&quot; 在菜单栏中添加链接。修改主题配置文件，找到字段menu下的字段tags，把它值修改为 /tags 12345menu: home: / archives: /archives tags: /tags categories: /categories 新建一个友链页面 在菜单栏中添加链接和链接图标。编辑站点配置文件，找到字段menu，添加以下内容： 12menu: links: /links/ || link # ||左边代表文件夹/source/links,右边代表图标 修改链接的显示文本，编辑文件/themes/next/languages/zh-Hans.yml,找到字段menu。 12menu: links: 友链 在站点目录下执行以下命令,会生成/source/links/index.md文件。通过编辑该文件可以控制友链页面的显示内容。 1hexo new page links 关闭’标签’、’分类’页的评论分别编辑站点目录下的/source/tags/index.md和/source/categories/index.md,添加字段comments,把它的值修改为false。 1comments: false 侧边栏添加社交链接通过修改主题配置文件完成。打开 主题配置文件 ，修改两个部分，链接和链接图标。1、添加链接。找到字段 social ，一行为一个链接。格式为 显示文本：链接 1234# Social linkssocial: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 2、修改链接图标。找到字段 social_icons 。键值对格式为 匹配键：图标名称 。在图标库找到合适的图标，将名字作为图标名称。 123456# Social Iconssocial_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 侧边栏添加友情链接打开 主题配置文件 ，找到字段 links 。键值对格式为 显示文本：链接 12345678# Blog rollslinks_icon: link #链接的图标links_title: 推荐阅读 #链接的名称links_layout: block #链接的布局，一行一个链接#links_layout: inline #一行多个链接links: # Title: http://example.com/ 廖雪峰: https://www.liaoxuefeng.com/ 开启打赏功能通过修改主题配置文件完成。打开 主题配置文件 ，找到以下字段。将收款二维码放到站点目录下的/source/images/(没有该文件夹，就新建)中。 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 修改打赏字体不闪动修改文件MyBlog/themes/next/source/css/_common/components/post/post-reward.styl，注释文字闪动的函数。 123456789101112/*注释文字闪动的函数#wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;#alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/ 订阅微信公众号在每篇文章的末尾显示微信公众号。把微信公众号二维码放在MyBlog/themes/next/source/images/下。编辑 主题配置文件 ，找到字段wechat_subscriber，修改它的值。 12345# Wechat Subscriberwechat_subscriber: enabled: true qcode: /path/to/your/wechatqcode description: 欢迎关注我 设置腾讯公益404界面1、在目录MyBlog/themes/next/source/下新建404.html页面，内容如下： 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 2、修改 主题配置文件 ,找到字段 menu 下的字段 commonweal，把它值修改为 /404.html 12menu: commonweal: /404.html || heartbeat 设置背景动画编辑 主题配置文件。以下四个字段为不同的背景动画效果，要想启用，将false改为true。 1234567891011# Canvas-nestcanvas_nest: false# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false 添加评论系统‘网易云跟帖’评论已经不提供服务。我们采用’来必力’评论系统时，需要获取uid。1、先在来必力的官方网站注册账号，在&gt;安装下安装city版本，再在&gt;管理页面&gt;代码管理下获取uid。data-uid字段的值就是我们需要的uid。2、编辑 主题配置文件 ，找到字段’livere_uid’,把它的值改为data-uid字段的值。 1livere_uid: #your uid 添加搜索功能通过Local Search搜索服务实现搜索功能。1、安装插件hexo-generator-searchdb，在站点的根目录下执行 1npm install hexo-generator-searchdb --save 2、编辑 站点配置文件,在文末新增以下内容： 12345search: path: search.xml field: post format: html limit: 10000 3、编辑主题配置文件,找到字段”local_search”，修改值为true。 12local_search: enable: true 主题美化-2设置【阅读全文】在首页只显示文章的部分内容，通过按钮【阅读全文】来实现跳转。有三种方式可以实现该功能，这里介绍最方便的一种。在文章中使用&lt;!-- more --&gt;来进行截断，&lt;!-- more --&gt;之前的内容显示在首页，后面的内容不显示。 1&lt;!-- more --&gt; 分页-设置页面文章的篇数为网站首页、归档页和标签页设置不同的文章篇数。1、使用npm install --save安装插件。 123npm install --save hexo-generator-indexnpm install --save hexo-generator-archivenpm install --save hexo-generator-tag 2、编辑站点配置文件,找到以下字段并修改。 12345678910index_generator: per_page: 10 archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 设置网站的图标在EasyIcon中找一张中意的图标。修改图标名称为favicon.ico，将图标放在路径/next/source/images/下。编辑主题配置文件，找到字段favicon，修改它的值。 123favicon: small: /images/favicon.ico medium: /images/favicon.ico 添加顶部的加载条修改主题配置文件,找到字段pace,把值设为true。还可以选择不同风格的加载条。 12345678910111213141516pace: true # Themes list:#pace-theme-big-counter#pace-theme-bounce#pace-theme-barber-shop#pace-theme-center-atom#pace-theme-center-circle#pace-theme-center-radar#pace-theme-center-simple#pace-theme-corner-indicator#pace-theme-fill-left#pace-theme-flash#pace-theme-loading-bar#pace-theme-mac-osx#pace-theme-minimalpace_theme: pace-theme-flash 修改文章底部的标签#为修改文件 /themes/next/layout/_macro/post.swig,搜索rel=&quot;tags&quot;&gt;#，将#换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;。也可以在图标库找自己中意的图标。 修改网页底部的桃心编辑/themes/next/layout/_partials/footer.swig，找到以下代码,将第二个class的值并修改为&quot;fa fa-heart&quot;，在图标库找自己中意的图标。 123&lt;span class=&quot;with-love&quot;&gt; &lt;i class=&quot;fa fa-&#123;&#123; theme.footer.icon &#125;&#125;&quot;&gt;&lt;/i&gt;&lt;/span&gt; 实现统计功能使用npm install --save安装插件 1npm install hexo-wordcount --save 网站底部字数统计编辑/themes/next/layout/_partials/footer.swig，在文末添加以下代码： 1234&lt;div class=&quot;theme-info&quot;&gt; &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt; &lt;span class=&quot;post-count&quot;&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 文章字数和阅读时长编辑主题配置文件，找到字段post_wordcount,并修改。 123456post_wordcount: item_text: true wordcount: true #字数统计 min2read: true #阅读时长统计 totalcount: true #站点总字数 separated_meta: true 统计访问量和阅读量我们用busuanzi实现统计功能。1、全局配置。编辑主题配置文件，找到字段busuanzi_count,将enable的值改为true。2、编辑字段site_uv,统计本站访客数。3、编辑字段site_pv,统计本站总访问量。4、编辑字段page_pv,统计每篇文章的阅读量。 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;本站访客数 site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt;本站总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt;本文总阅读量 page_pv_footer: 次 问题：busuanzi统计不显示数字（2019.1.15）。原因是buxuanzi的域名变更，由&lt;dn-lbstatics.qbox.me&gt;变为&lt;busuanzi.ibruce.info&gt;,导致统计功能不能实现。解决方案如下： 编辑文件themes\\next\\layout\\_third-party\\analytics\\busuanzi-counter.swig,找到以下代码 1&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 将其修改为： 1&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 重新推送后，可以看到统计功能正常。 实现点击出现桃心1、在路径/theme/next/source/js/src/下新建 love.js2、编辑文件/theme/next/layout/_layout.swig文件，在文末添加代码： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 修改链接文本样式修改文件themes\\next\\source\\css\\_common\\components\\post\\post.styl,在文末添加如下代码。关于颜色的代码见详情 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 置顶某篇文章修改文件MyBlog/node_modules/hexo-generator-index/lib/generator.js,将全部代码替换为 12345678910111213141516171819202122232425262728&apos;use strict&apos;;var pagination = require(&apos;hexo-pagination&apos;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &apos;page&apos;; return pagination(&apos;&apos;, posts, &#123; perPage: config.index_generator.per_page, layout: [&apos;index&apos;, &apos;archive&apos;], format: paginationDir + &apos;/%d/&apos;, data: &#123; __index: true &#125; &#125;);&#125;; 在文章front-matter中添加字段top，该字段的数值越大文章越靠前。 12title: 第三篇top: 10 文章加密访问编辑文件MyBlog/themes/next/layout/_partials/head.swig,在第五行插入以下代码： 1234567891011121314&lt;script&gt; (function () &#123; if (&apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; alert(&apos;密码错误！&apos;); if (history.length === 1) &#123; location.replace(&quot;http://xxxxxxx.xxx&quot;); // 这里替换成你的首页 &#125; else &#123; history.back(); &#125; &#125; &#125; &#125;)();&lt;/script&gt; 在文章front-matter中添加字段password，该字段的值不为空则文章加密，若字段的值为空则文章没有加密。 12title: 第三篇password: 自定义新建文章的md文件修改/scaffolds/post.md文件为： 123456789---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags: # 标签categories: # 分类copyright: # true添加版权信息top: # 数值越大，文章在首页越靠前。用于文章置顶password: # 为空则文章不加密。用于文章加密访问--- 显示浏览进度编辑主题配置文件，修改字段scrollpercent的值为true 12# Scroll percent label in b2t button.scrollpercent: true 使得HEXO的next主题渲染LATEX数学公式编辑主题配置文件，修改字段mathjax的值为true 123# MathJax Supportmathjax: enable: true 主题美化-3修改代码块自定义样式编辑\\themes\\next\\source\\css\\_custom\\custom.styl,在文末添加如下代码。关于颜色的代码见详情 123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125; 设置代码高亮主题Next使用 Tomorrow Theme作为代码高亮。有五个主题可供选择。1.编辑 主题配置文件，找到字段highlight_theme,修改它的值。 12345# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal 实现代码变成彩色。修改站点配置文件，找到字段highlight下的字段auto_detect的值为true1234highlight: enable: true line_number: true auto_detect: true 添加代码块复制功能通过第三方插件clipboard.js来实现复制功能。1.目录/themes/next/source/js/src下新建两个 .js文件。 clipboard.min.js。下载第三方插件clipboard.min.js clipboard-use.js，文件内容如下： 1234567891011121314151617/*页面载入完成后，创建复制按钮*/!function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = &apos;&apos;; copyHtml += &apos;&lt;button class=&quot;btn-copy&quot; data-clipboard-snippet=&quot;&quot;&gt;&apos;; copyHtml += &apos; &lt;i class=&quot;fa fa-copy&quot;&gt;&lt;/i&gt;&lt;span&gt;复制&lt;/span&gt;&apos;; #复制按钮的图标和显示文本 copyHtml += &apos;&lt;/button&gt;&apos;; $(&quot;.highlight .code pre&quot;).before(copyHtml); new ClipboardJS(&apos;.btn-copy&apos;, &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode();&#125;(window, document); 2.在文件\\themes\\next\\source\\css\\_custom\\custom.styl文末添加代码： 123456789101112131415161718192021222324252627282930313233343536//代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 3.在.\\themes\\next\\layout\\_layout.swig文末添加代码： 123&lt;!-- 代码块复制功能 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clipboard-use.js&quot;&gt;&lt;/script&gt; 参见：Hexo NexT主题代码块添加复制功能Hexo next博客添加折叠块功能添加折叠代码块 文章摘要图片让图片出现在网站首页的文章摘要中，但不出现在文章的正文中。 编辑主题配置文件，修改字段expert_description的值为false 1234excerpt_description: falseauto_excerpt: enable: false 编辑文件themes/next/layout/_macro/post.swig，找到代码： 12&#123;\\% elif post.excerpt %\\&#125; &#123;&#123; post.excerpt &#125;&#125; 在它后面添加如下代码： 12345&#123;\\% if post.image %\\&#125;&lt;div class=&quot;out-img-topic&quot;&gt; &lt;img src=&#123;&#123; post.image &#125;&#125; class=&quot;img-topic&quot; /&gt;&lt;/div&gt;&#123;\\% endif %\\&#125; 编辑文件/themes/next/source/css/_custom/custom.styl,在文末添加如下代码：1234// 自定义的文章摘要图片样式img.img-topic &#123; width: 100%;&#125; 使用方式是，在文章的.md文件的front-matter加上一行image: path/to/image： 12345678910title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;keywords: # 用于SEO搜索引擎优化tags: # 标签categories: # 分类copyright: # true添加版权信息top: # 数值越大，文章在首页越靠前。用于文章置顶password: # 为空则文章不加密。用于文章加密访问description:# 用于设置【阅读全文】image: # 文章摘要图片 设置背景图片方法一通过jquery-backstretch,编辑文件/themes/next/layout/_layout.swig,在最后的&lt;/body&gt;之前添加 123&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;body&quot;).backstretch(&quot;https://背景图.jpg&quot;); 方法二编辑文件/themes/next/source/css/_custom/custom.styl,添加代码： 12345// Custom styles.body &#123; background:url(/images/background.jpg); #图片放在/images文件中 background-attachment: fixed; #固定背景图&#125; 搜索引擎优化（SEO）-提高网站排名我们搭建好博客之后，在搜索引擎（百度、Google）中搜索博客，会发现根本搜索不到。这是因为我们的博客网站没有没搜索引擎收录，也就搜索不到。因此我们需要进行搜索引擎优化。搜索引擎优化是为提高我们的博客在搜索结果中的排名，来增大网站的流量。通俗地说，就是让我们的博客更容易被搜索到。 让搜索引擎收录我们的网站。 提高网站在搜索结果中的排名。 hexo优化准备编辑配置文件站点配置文件站点配置文件中的这四项一定要写 1234title: subtitle: description: url: 打开Next主题自带的SEO配置编辑主题配置文件,找到这四个字段，并设置为true。 1234canonical: trueseo: trueindex_with_subtitle: truebaidu_push: true 设置关键字 设置博客的关键字。编辑站点配置文件，添加字段keywords，值用逗号隔开。 设置文章的关键字。在文章中，添加字段keywords，值用逗号隔开。1keywords: word1,word2,word3 网站首页title优化我们知道读文章时，标题和摘要往往比段落中的大段文字重要。同样在搜索引擎爬取收录网页时，title起到更重要的作用。关键词出现在网站title中，可以提高网站被搜索到的可能。网站的title一般不超过80字符。我们把网站首页的title改为网站名称-关键词-网站描述的形式。编辑文件\\themes\\next\\layout\\index.swig，将以下代码 1&#123;\\% block title %\\&#125;&#123;&#123; config.title &#125;&#125;&#123;\\% if theme.index_with_subtitle and config.subtitle %\\&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;\\% endif %\\&#125;&#123;\\% endblock %\\&#125; 修改为，可以看到通过插入和来添加关键词和网站描述。 1&#123;\\% block title %\\&#125;&#123;&#123; config.title &#125;&#125; - &#123;&#123; config.keywords &#125;&#125; - &#123;&#123; theme.description &#125;&#125;&#123;\\% if theme.index_with_subtitle and config.subtitle %\\&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;\\% endif %\\&#125;&#123;\\% endblock %\\&#125; 网站名称、关键词、网站描述的显示文本分别对应站点配置文件中的字段title，keywords和description的值。 1234567title: spring&apos;Blog #网站标题subtitle: description: #对应网站描述keywords: #关键词author: spring-quanlanguage: zh-Hanstimezone: 给所有外部链接添加nofollow标签 使用npm install --save安装插件hexo-autonofollow 1npm install hexo-autonofollow --save 编辑站点配置文件,在文末添加如下代码。不想添加的链接写在字段exclude后。 12345nofollow: enable: true exclude: - exclude1.com - exclude2.com 提交链接和验证网站这一步是让搜索引擎收录我们的网站。我们需要在搜索引擎提交我们的链接，提交链接时需要验证这个网址是我们的，不是别人的。以下为搜索引擎的网站管理工具 google的网站管理工具Search Console 百度的搜索资源平台 搜索引擎给出了网站验证的多种方式，百度和谷歌进行网站验证的原理和步骤差不多。这里介绍两种验证方式：html文件验证和CNAME验证html文件验证 下载验证文件.html，放到站点目录/source下。 hexo会自动对HTML文件进行渲染，但我们不希望验证文件被渲染，需要做一些配置。修改站点配置文件，编辑字段skip_render的值为 验证文件名.html。该字段后可以有多个值，以逗号分隔。该字段后的文件名会跳过渲染。 1skip_render: google6af533d85b9bfd8c.html,baidu_verify_mreXNGCYRV.html 清除缓存并将修改同步到github. 12hexo cleanhexo g -d 点击完成验证。 CNAME验证 CNAME验证是设置域名解析。需要登录阿里云万网。 选择控制台&gt;产品与服务&gt;域名，管理域名的解析。添加CNAME类型的解析。 站点地图sitemap和爬虫协议robot.txt 站点地图是一个文件。告诉Google和百度应该抓取哪些网页。robot.txt也是一个文件。告诉Google和百度不应该抓取哪些网页。 站点地图sitemap先添加站点地图文件，再提交给Google和百度。告诉Google和百度应该抓取哪些网页。 使用命令安装自动生成站点地图sitemap的插件。 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 编辑站点配置文件，在文末添加以下代码： 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 执行命令，会在站点目录下的public文件夹生成站点地图文件sitemap.xml和baidusitemap.xml文件，前者提交给Google，后者提交给百度。 1hexo g 把站点地图文件提交给Google和百度。 在Google的旧版Search Console提交站点地图文件https://buptccq.top/sitemap.xml(修改为你自己的域名)。 在百度的网站管理工具提交站点地图文件https://buptccq.top/baidusitemap.xml(修改为你自己的域名)。 爬虫协议robot.txt，又成蜘蛛协议添加爬虫协议文件robot.txt，再提交给Google和百度。告诉Google和百度不要抓取哪些网页。 在站点目录source下新建文件robot.txt。内容如下，注意修改为自己的域名。 12345678910111213141516#hexo robots.txtUser-agent: *Allow: /Allow: /archives/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: http://blog.tangxiaozhu.com/search.xml #注意修改为你自己的域名Sitemap: http://blog.tangxiaozhu.com/sitemap.xmlSitemap: http://blog.tangxiaozhu.com/baidusitemap.xml 执行命令hexo d -g部署到github上。 1hexo d -g 再把爬虫协议文件提交给Google和百度。 Google的旧版Search Console。 百度的网站管理工具。 谷歌收录和百度收录查看网站是否被收录在百度或Google浏览器中，以site:&lt;your url&gt;形式搜索你的博客。如果能搜到，就是被收录了。搜不到就是没有收录。 谷歌收录百度收录我们在前面已经把博客首页的网址收录到百度搜索引擎了，但当我们写出新的博客，产生新的链接后，需要把这些新的链接收录到百度。这里介绍两种实现自动收录的方式：主动推送和自动推送主动推送主动推送将新产生的链接立即推送给百度搜索引擎，从而保证新链接被尽早收录。 使用命令npm install --save安装自动百度推送插件。 1npm install hexo-baidu-url-submit --save 编辑站点配置文件，在文末添加以下代码： 12345baidu_url_submit: count: 3 ## 比如3, 代表提交最新的三个链接 host: &lt;www.henvyluk.com&gt; ## 在百度站长平台中注册的域名 token: &lt;your_token&gt; ## 请注意这是您的秘钥, 请不要发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址, 新链接会保存在此文本文档里 其中&lt;your_token&gt;可以在百度网址管理工具中找到： 需要注意的是，编辑站点配置文件，确保字段url的值为你的域名。 123# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://buptccq.top #http://yoursite.com 编辑站点配置文件，找到字段deploy,添加一个新的deploy类型-type: baidu_url_submitter。不同的deploy类型用-type表示。 123456deploy:- type: baidu_url_submitter- type: git # repository: https://github.com/spring-quan/spring-quan.github.io repository: git@github.com:spring-quan/spring-quan.github.io.git branch: master 主动推送的原理如下： 产生新链接。hexo g会生成文本文件baidu_urls.txt，里面包含最新的链接。 推送新链接。hexo d会从文本文件baidu_urls.txt中读取新链接，再推送给百度。 自动推送编辑主题配置文件，修改字段baidu_push的值为true。 12# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 参见：基于Hexo搭建个人博客——进阶篇(从入门到入土)Hexo Seo优化让你的博客在google搜索排名第一 ##踩过的坑 “hexo server”本地启动时，报错如下： 1Unhandled rejection Error: ENOENT: no such file or directory, open &apos;/MyBlog/themes/next/layout/_scripts/schemes/.swig&apos; 错误原因处在主题配置文件_config.yml存在不规范的内容。要仔细检查，规范化内容就行。修改过来之后，要先执行hexo clean,再hexo server就可以了。 参考文章 Hexo官方教程 Next官方教程 markdown中文文档 打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化 【推荐，整理很系统】 基于Hexo搭建个人博客——进阶篇(从入门到入土) hexo的next主题个性化教程:打造炫酷网站 Hexo搭建博客教程 手把手教你使用Hexo + Github Pages搭建个人独立博客 从 0 开始搭建 hexo 博客","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"}]}]}