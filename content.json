{"meta":{"title":"spring's Blog","subtitle":"游龙当归海，海不迎我自来也。","description":"游龙当归海，海不迎我自来也。","author":"spring","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-07-21T14:19:35.000Z","updated":"2019-07-21T14:19:56.000Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"收藏、资源、好文","date":"2019-07-28T03:02:05.000Z","updated":"2019-08-20T09:55:07.000Z","comments":true,"path":"collection/index.html","permalink":"http://yoursite.com/collection/index.html","excerpt":"","text":"文章：科研@card{ 初入NLP领域的一些小建议—香侬科技李纪为 机器学习科研的十年—陈天奇 } 文章：生活@card{ 在平凡的日子里激扬 } 工具@card{ 网址 简介 推荐指数 Font Awesome图标中文网 提供可缩放矢量图标 ⭐⭐⭐⭐ 正则表达式 正则表达式 ⭐⭐⭐⭐ Emoji Homepage markdown表情 ⭐⭐⭐⭐⭐ 常用数学符号的 LaTeX 表示方法 常用数学符号的 LaTeX 表示方法 ⭐⭐⭐⭐⭐ Linux命令大全 Linux命令大全 ⭐⭐⭐⭐⭐ }"},{"title":"友情链接","date":"2019-07-28T03:02:05.000Z","updated":"2019-08-09T08:53:28.000Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":"持续更新中…欢迎留言，互换友链~ 大牛的博客@card{ 名称 博主 语言 备注 colah's blog Christopher Olah 英文 Google Brain研究员，OpenAI成员 Jason Weston Jason Weston 英文 Facebook研究科学家，纽约大学教授。研究领域是：memory network，dialog system，QA system Richard Socher Richard Socher 英文 教授斯坦福cs224d课程 Lil'Log Lilian 英文 OpenAI成员 facebook research facebook research 英文 facebook research Harvard NLP Harvard NLP 英文 Harvard NLP主页 The BAIR Blog 伯克利学院 英文 伯克利人工智能研究院 Free MindFree Mind's old blog 张驰原 中文 本硕浙大计算机，现Google研究科学家 科学空间 苏剑林 中文 本科华东师范大学，中山大学研究生 赵天成的主页 赵天成 英文 卡耐基梅隆大学(CMU)Ph.D.，研究方向是深度对话系统 李纪为的主页 李纪为 中文 北大本科，斯坦福博士，香农科技创始人 黄民烈教授的主页 黄民烈教授 中文 清华大学计算机学院教授 }"},{"title":"tags","date":"2019-07-21T14:19:23.000Z","updated":"2019-07-21T14:20:13.000Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LDA主题模型","slug":"LDA主题模型","date":"2020-07-21T09:51:00.000Z","updated":"2020-07-24T02:01:48.000Z","comments":true,"path":"2020/07/21/LDA主题模型/","link":"","permalink":"http://yoursite.com/2020/07/21/LDA主题模型/","excerpt":"LDA(Latent Dirichlet Allocation, 隐含狄利克雷分布)是一种主题模型，将文档集中每篇文档的主题以概率分布的形式给出。LDA模型是一种词袋(bag-of-words)模型，也就是把一篇文档看作是一组词的集合，而不考虑词与词之间的先后顺序关系。","text":"LDA(Latent Dirichlet Allocation, 隐含狄利克雷分布)是一种主题模型，将文档集中每篇文档的主题以概率分布的形式给出。LDA模型是一种词袋(bag-of-words)模型，也就是把一篇文档看作是一组词的集合，而不考虑词与词之间的先后顺序关系。 数学知识 贝努利分布， 又称两点分布，0-1分布, 看作抛一次硬币的贝努利试验得到的离散概率分布。 二项分布: 独立地进行$n$次贝努利试验的离散概率分布。 多项分布：是二项分布的推广，每次试验变成抛一个有$K$个面的骰子。 Beta分布: 是二项分布的共轭先验。 Derichlet分布: 是多项分布的共轭先验。 共轭先验 Beta-binomial共轭 Dirichlet-multinomial共轭 文本建模日常生活中有大量的文档，文档可以表示为有序的词的序列：$document = \\lbrace{w_1, w_2, …, w_n}\\rbrace$。 Fig.1 包含m篇文档的语料库 src: LDA数学八卦 统计文本建模的目的是追问 文档中的词序列是如何生成的。文本可以看作是 上帝抛掷骰子生成的，我们要做的是猜测 上帝是如何抛掷骰子来生成文本的。这里的核心问题有两个： 上帝有什么样的骰子。 上帝是如何抛掷这些骰子的。 第一个问题是表示模型中有哪些参数，骰子每一面的概率对应于模型的参数。第二个问题表示了游戏规则是什么，上帝可能有各种不同种类的骰子，按照一定的规则来抛掷这些骰子来生成词序列。接下来，我们讨论几种猜测如何生成词序列文本的模型。 Unigram Model文本生成假设词表中有$V$个词$v_1, v_2, …, v_V$，那么最简单的Unigram Model认为上帝是按照以下的规则来抛掷骰子的。 上帝只有一个骰子，这个骰子有$V$面，每面对应一个词，每个面的概率是不同的。但每个面的概率是固定的。 每抛一次骰子，抛出的面对应生成一个词。如果一篇文档有$n$个词，那么依次独立地抛$n$次骰子，就生成了这$n$个词的序列。 Fig.2 上帝抛掷V个面的骰子 src: LDA数学八卦 在这个模型中，模型的参数为各个面的概率$\\vec{p} = (p_1, p_2, ..., p_V)$。独立地抛n次$V$个面的骰子，服从多项分布，记作$w \\backsim Mult(w|\\vec{p})$. 那么生成一篇文档$d = \\vec{w} = \\lbrace{w_1, w_2, ..., w_n}\\rbrace$的概率为 $$p(\\vec{w}) = p(w1,w_2,...,w_n) = p(w_1)p(w_2)\\cdots p(w_n)$$ 如果语料中有多篇文档$W = \\lbrace{\\vec{w_1}, \\vec{w_2}, ..., \\vec{w_m}}\\rbrace$，那么语料的概率为: $$p(W) = p(\\vec{w_1})p(\\vec{w_2})\\cdots p(\\vec{w_m})$$ 参数估计参数估计就是估计模型中的参数$\\vec{p}$，也就是要估计骰子每一面的概率是多少。采用统计学中 频率派的观点，采用极大似然估计，最大化语料概率$P(W)$，于是参数$p_i$的估计值为$$p_i = \\frac{n_i}{N}$$其中$N$是语料中总的词数，$n_i$是词$v_i$在语料中出现的次数。 贝叶斯观点下的Unigram Model文本生成对于Unigram Model, 贝叶斯统计学派认为上帝只有唯一一个固定的骰子是不合理的。在贝叶斯学派看来，一切参数都是随机变量，Unigram Model中的骰子$\\vec{p}$不是唯一固定的，它也是一个随机变量。按照贝叶斯学派的观点，上帝抛掷骰子的游戏规则，也就是文本生成的过程如下： 上帝有一个装着无穷多个骰子的坛子，每个骰子有$V$个面。每个骰子各个面的概率是固定的，不同骰子间每个面的概率是不同的。 上帝先从坛子中抽一个骰子出来，然后用这个骰子抛掷$n$次，就生成了有$n$个词的序列。 Fig.3 Dirichlet先验下的Unigram Model. src: LDA数学八卦 Fig.4 Dirichlet先验下的概率图模型. src: LDA数学八卦 坛子中有无穷多个骰子，骰子$\\vec{p}是一个随机变量，$从概率分布的角度来看，骰子$\\vec{p}$服从一个概率分布$p(\\vec{p})$，这个分布称为参数$\\vec{p}$的先验分布。 这个先验分布$p(\\vec{p})$有很多中选择，我们采用多项分布的共轭分布：Dirichlet分布，有$\\vec{p} \\backsim Dir(\\vec{p}|\\vec{\\alpha})$,其中$\\vec{\\alpha} = \\lbrace{\\alpha_1,\\alpha_2, …, \\alpha_V}\\rbrace$是Dirichlet分布的参数。多项分布及其共轭分布 Dirichlet分布有以下性质：$$Dirichlet先验分布 + 多项分布的数据 \\to 后验分布为Dirichlet分布$$ 加了贝叶斯观点后的区别原Unigram Model中，只有唯一一个骰子，骰子每个面的概率$\\vec{p}$是固定不变的。而贝叶斯观点下，骰子$\\vec{p}$不是固定不变的，而应该是一个随机变量，也就是骰子有无穷多个。$\\vec{p}$服从一个概率分布，称为$\\vec{p}$的先验分布。贝叶斯观点下，多了一个随机变量$\\vec{p}$的先验分布。 参数估计贝叶斯观点下Unigram Model的参数估计可以参见LDA数学八卦。 PLSA主题模型Unigram Model过于简单。一篇文章通常是由多个主题构成的，比如：一篇nlp相关的文章，可能30%谈论语言学，30%谈论概率统计，20%谈论神经网络，20%谈论计算机。主题模型中的主题指的是什么呢？一个主题可以用该主题相关的频率最高的一些词来描述。PLSA(Probalilistic Latent Senmantic Analysis)模型认为，一篇文档由多个主题(topics)混合而成，每个topic是在词汇上的概率分布，文章中的每个词都是由一个固定的topic生成的。 文本生成再回到上帝抛掷骰子上来，PLSA模型认为上帝是按照以下的游戏规则来生成文本的。 上帝有两个类型的骰子，一类是doc-topic骰子，每个doc-topic骰子有$K$个面，每个面对应一个topic；一类骰子是topic-word骰子，每个topic-word骰子有$V$个面，每个面对应一个词。 上帝一共有$K$个topic-word骰子，每个topic-word骰子对应一个topic。 在生成每篇文档前，上帝先为这篇文档制造一个特定的doc-topic骰子。然后重复以下过程生成文档中的词： 抛掷这个doc-topic骰子，得到一个topic； 选择这个topic对应topic-word骰子，抛掷这个topic-word骰子，得到一个词。 PLSA模型文本生成的过程可以图形化地表示为： Fig.5 PLSA模型的文本生成过程. src: LDA数学八卦 参数估计PLSA模型有两类参数，也就是doc-topic骰子每个面的概率$p(topic_k|doc_i)$，和$K$个topic-word骰子每个面的概率$p(word_j|topic_k)$。对于包含$M$篇文档的语料$C = ({d_1, d_2, …, d_M})$，每篇文档对应一个特定的doc-topic骰子，所有对应的骰子记为$(\\vec{\\theta_1}, \\vec{\\theta_2}, …, \\vec{\\theta_M})$; 游戏中总共有$K$个topic-word骰子，记为$(\\vec{\\varphi_1}, \\vec{\\varphi_2}, …, \\vec{\\varphi_K})$。 可以使用著名的EM算法来估计PLSA模型的参数。参数估计的求解参见：通俗理解LDA主题模型-CSDN估计模型参数的过程，实际上就是：根据文档中的词序列来反推其主题分布的过程。doc-topic骰子每个面的概率$p(topic_k|doc_i)$表征了文档的主题分布，topic-word骰子每个面的概率$p(word_j|topic_k)$表征了主题的词分布（PLSA模型认为 topic是在词汇上的概率分布）。 LDA主题模型对于上述的PLSA模型，doc-topic骰子$\\theta_m$和topic-word骰子$\\varphi_k$都是模型中的参数，贝叶斯学派认为 参数都是随机变量。类似于Unigram Model的贝叶斯改造，在两个骰子参数前加上先验分布，从而将PLSA的抛骰子游戏过程改造成一个贝叶斯的游戏过程。由于$\\varphi_k$和$\\theta_m$都对应多项分布，因此先验分布采用Dirichlet分布，于是就得到了LDA(Latent Dirichlet Allocation)模型。总的来说，LDA模型是对PLSA模型的贝叶斯版本。 文本生成在LDA模型中，上帝是按照以下的规则来玩文档生成的游戏的： 上帝有两大坛子的骰子，第一个坛子里装了无穷多个doc-topic骰子，第二个坛子里装了无穷多个topic-word骰子。 上帝随机从第二个坛子里独立地抽取$K$个topic-word骰子，标号$1-K$。 每次生成一篇文档前，上帝先从第一个坛子里随机抽取一个doc-topic骰子。然后重复以下过程生成文档中的词： 先抛掷这个doc-topic骰子，得到一个topic编号$z$； 从$K$个topic-word骰子中选择编号为$z$的那个，抛掷这个骰子，生成一个词。 PLSA模型与LDA模型的区别 PLSA模型中，doc-topic骰子和topic-word骰子是唯一确定的，也就是虽然doc-topic骰子$\\vec{\\theta_m}$和topic-word骰子$\\vec{\\varphi_k}$是未知的，但是确定的固定不变的。 LDA模型中，doc-topic骰子$\\vec{\\theta_m}$和topic-word骰子$\\vec{\\varphi_k}$是未知的，而且是随机变量，不是唯一确定的。这两类随机变量$\\vec{\\theta_m}$和$\\vec{\\varphi_k}$服从某个概率分布，我们采用与多项分布的共轭分布：Dirichlet分布。 Fig.6 LDA模型. src: LDA数学八卦 物理过程分解LDA模型的概率图表示如下图所示： Fig.7 LDA模型概率图表示. src: LDA数学八卦 这个概率图可以分解为两个主要的物理过程： $\\vec{\\alpha} \\to \\vec{\\theta_m} \\to z_{m,n}$: 这个过程表示在生成第$m$篇文档时，先从第一个坛子中抽取一个doc-topic骰子$\\vec{\\theta_m}$, 再抛掷这个骰子生成文档中的第$n$个词的topic编号$z_{m,n}$。 $\\vec{\\beta} \\to \\vec{\\varphi_k} \\to w_{m,n}|k = z_{m,n}$: 这个过程表示生成第$m$篇文档的第$n$个词时，从第二个坛子中抽取$K$个topic-word骰子$\\vec{\\varphi} = (\\varphi_1, \\varphi_2, …, \\varphi_K)$，挑选编号为$k = z_{m,n}$的骰子进行抛掷，生成第$n$个词$w_{m,n}$。 换一个角度来解释这两个物理过程： Fig.8 第一个物理过程. src: LDA数学八卦 $\\vec{\\alpha} \\to \\vec{\\theta_m} \\to z_{m,n}$表示生成文档中所有词对应的主题。$\\vec{\\alpha} \\to \\vec{\\theta_m}$对应Dirichlet分布，$\\vec{\\theta_m} \\to z_{m,n}$对应Multinomial分布，整体是一个Dirichlet-Multinomial共轭结构。第一步：从参数为$\\vec{\\alpha}$的Dirichlet先验分布中采样得到文档的主题分布$\\vec{\\theta_m}$，这是一个多项式分布；第二步：从参数为$\\vec{theta_m}$的多项式分布中采样一个主题$z_{m,n}$。 Fig.9 第二个物理过程. src: LDA数学八卦 $\\vec{\\beta} \\to \\vec{\\varphi_k} \\to w_{m,n}|k = z_{m,n}$表示生成文档中的词。$\\vec{\\beta} \\to \\vec{\\varphi_k}$对应Dirichlet分布，$\\vec{\\varphi_k} \\to w_{m,n}$对应多项式分布，整体是一个Dirichlet-Multinomial共轭结构。第一步：从参数为$\\vec{\\beta}$的Dirichlet先验分布中采样得到主题的词分布$\\vec{\\varphi_k}$，这是一个多项式分布；第二步：在这个参数为$\\vec{\\varphi_k}$的词分布中抽样一个词$w_k$。 参数估计接下来的问题是，LDA模型怎么由文档来反推每一篇文档的主题分布和每一个主题的词分布呢？一般有两种方法：Gibbs采样算法和变分推断EM算法。详见：LDA数学八卦 参考链接 LDA数学八卦 通俗理解LDA主题模型-CSDN 文本主题模型之LDA-博客园刘建平","categories":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/categories/nlp/"}],"tags":[{"name":"主题模型","slug":"主题模型","permalink":"http://yoursite.com/tags/主题模型/"},{"name":"PLSA","slug":"PLSA","permalink":"http://yoursite.com/tags/PLSA/"},{"name":"LDA","slug":"LDA","permalink":"http://yoursite.com/tags/LDA/"},{"name":"Unigram Model","slug":"Unigram-Model","permalink":"http://yoursite.com/tags/Unigram-Model/"}]},{"title":"NLP中的文本表示与词向量","slug":"NLP中的文本表示与词向量","date":"2020-07-20T09:23:05.000Z","updated":"2020-07-24T08:59:34.000Z","comments":true,"path":"2020/07/20/NLP中的文本表示与词向量/","link":"","permalink":"http://yoursite.com/2020/07/20/NLP中的文本表示与词向量/","excerpt":"文本表示是将文本中的字词进行数值化或向量化表示。文本表示是各种NLP任务的基础。如何将一篇文本用数学语言来表示呢？下面对不同的文本表示方法做一个归纳： 离散式表示(Discrete Representation): one-hot向量表示。 tf-idf 分布式表示(Distributed Representation):","text":"文本表示是将文本中的字词进行数值化或向量化表示。文本表示是各种NLP任务的基础。如何将一篇文本用数学语言来表示呢？下面对不同的文本表示方法做一个归纳： 离散式表示(Discrete Representation): one-hot向量表示。 tf-idf 分布式表示(Distributed Representation): one-hot向量表示先构建一个词汇表，词所在位置的值为1，其他值都是0。one-hot向量表示有两个缺点:（1）维度灾难。one-hot向量表示的维度等于词汇表的大小，且向量过于稀疏而浪费内存。（2）语义鸿沟。词汇表中任意两个词的one-hot向量表示都是正交的，不能通过余弦相似度方法来衡量两个词之间的相似性。 TF-IDFTF-IDF计算一个词对与一篇文档的重要程度。如果一个词在一篇文档中出现的次数越多，那么这个词对这篇文档的重要性越大。$$TF词频 = \\frac{一个词在文档中出现的次数}{这篇文档的总词数}$$另一方面，如果这个词出现在越多的文档中，说明这个词越常见，这个词包含的特征越少。$$IDF逆文档频率 = log({\\frac{语料库中文档的数量}{包含这个词的文档数 + 1}})$$综合考虑这两个方面，将TF与IDF相乘作为这个词的tfidf值: $$TF-IDF = 词频TF \\times 逆文档频率IDF$$参考链接：TF-IDF算法 TextRankTF-IDF算法从词的统计信息（词频、逆文档频率）出发，反映了一个词对一篇文档的重要程度。TFIDF并没有考虑到词与词之间的语义信息，而TextRank算法考虑到了词与词之间的语义关系，是一种基于图的关键词提取算法。另外，TFIDF需要一个语料库来计算词的逆文档频率，而TextRank算法脱离了语料库的背景，仅对单篇文档进行分析就可以提取关键词。TextRank算法的核心思想是：通过词之间的相邻共现关系来构建网络，然后用PageRank算法来计算每个节点(词)的rank值，排序rank值即可得到关键词。 PageRank算法PageRank算法是Google创始人拉里$\\cdot$佩奇和谢尔盖$\\cdot$布林提出的一种网页排序的链接分析算法。基本思想有两条： 链接数量。一个网页有越多的入链，也就是被越多的其他网页所链接，那么这个网页的重要性越高。 链接质量。一个网页被一个越高权重的网页所链接，那么这个网页的重要性越高。 如果把网页看作是一个个节点，把网页之间的链接看作节点之间的有向边，那么可以得到一个有向图$G = (V,E)$。计算节点$V_i$的PR(pagerank)值的迭代公式为：$$PR(V_i) = (1-d) + d \\times \\sum_{V_j \\in In(V_i)}\\frac{PR(V_j)}{|Out(V_j)|}$$ $PR(V_i)$表示节点$V_i$的pagerank值；$In(V_i)$表示节点$V_i$入链集合；$Out(V_i)$表示节点$V_i$的出链集合；$d$表示damping factor，阻尼系数，一般设置为0.85，便于快速收敛。 Fig.1 PageRank算法示意图 src: https://en.wikipedia.org/wiki/PageRank 网页之间的链接关系用图来表示很容易理解，接下来的问题是 如何把一篇文档构建成图呢？我们用窗口的概念来判断词与词之间的共现关系，设置一个长度为$K$的滑动窗口，当且仅当两个节点在长度为$K$的窗口内共现，则两个节点之间存在边。textrank构建的词图为无向图。textrank算法计算每个节点rank值的迭代公式为: $$WS(V_i) = (1-d) + d\\times\\sum_{V_j \\in In(V_i)} \\frac{w_{ij}}{\\sum_{V_k\\in In(V_j)}w_{jk}}WS(V_j)$$ 其中，$w_{ij}$表示节点$V_i$与节点$V_j$之间的共现次数，$\\sum_{V_j\\in In(V_j)}w_{jk}$表示节点$V_j$总的共现次数。与pagerank算法相比，textrank算法多了权重项，用来表示两个节点之间的边有着不同的重要程度。 TextRank算法提取关键词（1）文档$T$由多个句子组成$T = \\lbrace{S_1, S_2, …, S_m}\\rbrace$，对每个句子进行分词和词性标注，过滤停用词，只保留特定词性的词语（比如：名词、形容词、动词），第$i$个句子可表示为$S_i = \\lbrace{w_{i,1}, w_{i,2}, …, w_{i,n_i}}\\rbrace$。（2）构建词图$G = (V,E)$，其中$V$为节点集合。对于每个句子$S_i = \\lbrace{w_{i,1}, w_{i,2}, …, w_{i,n_i}}\\rbrace$，给定一个长度为$K$的滑动窗口，当且仅当两个词在滑动窗口内共现时，两个词之间存在边。两个词之间边的权重为两个词的共现次数。（3）随机初始化每个节点的rank值。（4）根据textrank算法的迭代公式，迭代计算每个节点的rank值，直至收敛。（5）按照节点的rank值倒序排列，排序靠前的候选词可以作为关键词。 Fig.2 TextRank算法的词图 src: https://www.cnblogs.com/en-heng/p/6626210.html 用textrank算法/TF-IDF算法 进行文本表示我们可以将 文档中关键词的textrank值（或 tfidf值）作为向量来表示文本。得到两个文本的向量表示之后，就可以用余弦相似度等度量方法来计算两个文本之间的相似度。计算两个文本之间相似度的步骤如下：（1）使用textrank算法（或 TF-IDF算法）来提取两个文本的关键词。（2）将两个文本的关键词合并起来，计算每个文本中关键词的textrank值。（3）生成两个文本各自的textrank值 向量表示。（4）计算两个文本的余弦相似度，值越大，表示这两个文本越相似。 参考链接：PageRank算法–从原理到实现关键词提取算法TextRank关键字提取算法TF-IDF和TextRank（python3）关键词提取和摘要算法TextRank详解与实战","categories":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/categories/nlp/"}],"tags":[{"name":"词向量","slug":"词向量","permalink":"http://yoursite.com/tags/词向量/"},{"name":"TextRank","slug":"TextRank","permalink":"http://yoursite.com/tags/TextRank/"}]},{"title":"文本相似度的计算方法","slug":"文本相似度的计算方法","date":"2020-07-13T07:23:46.000Z","updated":"2020-07-20T11:56:22.000Z","comments":true,"path":"2020/07/13/文本相似度的计算方法/","link":"","permalink":"http://yoursite.com/2020/07/13/文本相似度的计算方法/","excerpt":"在nlp任务中，我们常常需要判断两个文本的相似程度，计算这两个文本的相似度。比如，在文本聚类任务中，需要将相似度高的文本聚到同一个簇；在文本预处理过程中，基于文本相似度把重复的文本过滤掉；在检索式对话系统中，通过计算用户的query与数据库中的query的相似度，来选择回复。文本相似度计算 有2个关键组件：文本表示模型 和 相似度度量方法。文本表示模型 负责将文本表示为可计算的数值向量，也就是提供特征； 相似度度量方法负责基于数值向量计算文本之间的相似度。","text":"在nlp任务中，我们常常需要判断两个文本的相似程度，计算这两个文本的相似度。比如，在文本聚类任务中，需要将相似度高的文本聚到同一个簇；在文本预处理过程中，基于文本相似度把重复的文本过滤掉；在检索式对话系统中，通过计算用户的query与数据库中的query的相似度，来选择回复。文本相似度计算 有2个关键组件：文本表示模型 和 相似度度量方法。文本表示模型 负责将文本表示为可计算的数值向量，也就是提供特征； 相似度度量方法负责基于数值向量计算文本之间的相似度。 特征构建方式TF-IDFTF-IDF(term frequency-inverse document frequency) 是一种数学统计方法，反映了在一个语料集中，某一个词对一篇文档的重要程度，也就是这个词在多大程度上反映了这篇文档的特征。tf-idf分数常常作为信息检索中的权重因子，tf-idf是使用最广的词语加权方案之一。tf-idf分数与一个词出现在一篇文档中的次数成正比，与整个语料库中包含这个词的文档数成反比。给定一个用户查询(query)，搜索引擎根据相关度对文档进行打分排序，tf-idf加权方案就是搜索引擎实现这个过程的一个重要手段。在NLP任务的预处理过程中，我们经常会使用停用词表的方式过滤掉（的、是、在）这些没有实际意义的功能性词语。tf-idf给最常见的词(是、在、的)给予很小的权重分数，较常见的词分配较小的权重，较少见的词分配较大的权重。某种意义上，这个过程实现了跟停用词表类似的功能。 词频(term frequence)假设对于一个用户查询”中国的蜜蜂养殖”，要根据与query的相关性，对一个文档集合中的文档进行打分排序。我们把这个query分词为”中国”、”的”、”蜜蜂”、”养殖”。最简单的方法是先找出包含这四个词的文档(至少包含其中一个词)。为了进一步对这些文档进行排序，我们可以计算每个词出现在文档中的次数（词频，term frequency），一个词的权重与这个词的词频成正比。词频TF(term frequency)的计算方式有多种。常见的有两种: 一个词出现在文档中的次数。$$词频TF = 某个词出现在文档中的次数$$ 考虑到文档的长度不同，为了便于不同文章之间的比较，对‘词频’进行归一化。$$词频TF = \\frac{某个词出现在文档中的次数}{文档的总词数}$$ 或者 $$词频TF = \\frac{某个词出现在文档中的次数}{文档中出现次数最多的词的出现次数}$$ 逆文档频率(inverse document frequency)由于常见词 ‘的’在文档中的出现频率会比较大，词频(term frequency)会错误地给 使用”的”次数更多的文档更大的权重分数，而没有给 “中国”、”蜜蜂”、”养殖”给予足够的重视。像”的”这样的常用词没有实际的意义，不是适合区分文档相关度的好的关键词，而少见词”中国”、”蜜蜂”、”养殖”可以更好地反映文档的相关程度。因此，引入逆文档频率(inverse document frequency)来给常见词分配更小的权重，给少见词分配更大的权重。逆文档频率的计算方式如下: $$逆文档频率IDF = log(\\frac{语料库中的文档数}{包含该词的文档数 + 1})$$ 一个词越常见，那么分母越大，逆文档频率越小。分母之所以要加1，是为了避免分母为0。 词频-逆文档频率(TF-IDF)TF-IDF定义为 词频和逆文档频率这两个统计量的乘积。$$TF-IDF = 词频TF \\times 逆文档频率IDF$$ 一个词的tf-idf分数反映了这个词对文档的重要性，这个词在多大程度上反映了这篇文档的特征。回到”中国的蜜蜂养殖”这个检索问题上，对于每个文档，分别计算”中国”、”的”、”蜜蜂”、”养殖”这四个词的tf-idf分数，并求和作为这篇文档总的分数。用同样的方法对所有包含这四个词的文档打分，并排序。这样就得到了query”中国的蜜蜂养殖”的查询结果。 TF-IDF这种词语加权方法 常常与倒排索引方法联合使用，来实现文本检索。 参考链接：TF-IDF与余弦相似性的应用（一）：自动提取关键词TF-IDF_wikipedia 距离的度量方式闵可夫斯基距离闵可夫斯基距离(Minkowski distance)是衡量数值点之间距离的常见方法。闵可夫斯基距离可以看作是曼哈顿距离和欧式距离的推广。假设n维空间中的两个数值点：$X= {x_1, x_2, …, x_n}, Y= {y_1, y_2, …, y_n}$，那么闵可夫斯基距离的定义为: $$D_{p}(X,Y) = (\\sum_{i=1}^{n}|x_i - y_i|^p)^{\\frac{1}{p}}$$ 当$p = 2$时，闵可夫斯基距离转化为欧式距离(Euclidean_distance): $$\\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$ 当$p = 1$时，闵可夫斯基距离转化为曼哈顿距离(Manhattan_distance): $$\\sum_{i=1}^{n}|x_i - y_j|$$ 当$p \\to \\infty$时，闵可夫斯基距离转化为切比雪夫距离(Chebyshev_distance): $$lim_{p \\to \\infty}(\\sum_{i=1}^{n}|x_i - y_i|^p)^{\\frac{1}{p}} = max_{i=1}^{n}|x_i - y_i|$$ 参考链接：常用距离算法详解计算几何-距离 jaccard相似度jaccard相似度系数(jaccard similarity coefficient)，也称为并交比(intersection over union)，可以用来衡量两个有限样本集合之间的相似度。jaccard相似度定义为 两个集合交集大小 与并集大小的比例: $$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}$$jaccard相似度的思想很简单：如果两个集合共有的元素越多，那么这两个集合就越相似。如果集合$A$和集合$B$完全重合，则$J(A,B) = 1$。jaccard相似度的取值范围是: $0 \\le J(A,B) \\le 1$。jaccard距离(jaccard distance)用于衡量样本集之间的不相似度，定义为1减去jaccard相似度。$$d_{J}(A,B) = 1 - J(A,B) = \\frac{|A \\cup B| - |A \\cap B|}{|A \\cup B|}$$与欧式距离、余弦相似度等距离度量方式相比，jaccard相似度的优点在于：不需要把文本表示成数值化的向量表示，就可以计算两个文本之间的相似度。在文本聚类任务中，可以先通过jaccard相似度来计算数据集中文本之间的相似度，再用networkx来构建无向图；在无向图的基础上可以用louvain算法或DBSCAN算法来对文本进行聚类。 需要注意的是，louvain算法的无向图边的权重应该是 文本之间的jaccard相似度，来计算模块度。而DBSCAN算法的无向图边的权重应该是 文本之间的jaccard距离，来计算$\\epsilon$-邻域内样本点的数量。 参考链接：wikipedia-雅卡尔系数 参考链接 常见文本相似度计算方法简介–哈工大李鹏宇","categories":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/categories/nlp/"}],"tags":[{"name":"jaccard相似度","slug":"jaccard相似度","permalink":"http://yoursite.com/tags/jaccard相似度/"}]},{"title":"常见的文本聚类算法","slug":"常见的文本聚类算法","date":"2020-07-06T05:17:24.000Z","updated":"2020-07-13T07:48:22.000Z","comments":true,"path":"2020/07/06/常见的文本聚类算法/","link":"","permalink":"http://yoursite.com/2020/07/06/常见的文本聚类算法/","excerpt":"文本聚类就是把一些没有标签的，但有相同特征的数据聚在一起。聚类模型将样本划分为若干个簇(cluster)，每个簇对应一些潜在的概念或类别。","text":"文本聚类就是把一些没有标签的，但有相同特征的数据聚在一起。聚类模型将样本划分为若干个簇(cluster)，每个簇对应一些潜在的概念或类别。 K-means算法K-means聚类是最重要的聚类方法之一。我们假设已经将文档(document)表示为长度归一化的向量表示，也就是将文本数据映射到了欧式空间里。K-means聚类算法的主要思想是：对于每个簇(cluster)，选出一个中心点(cluster center)，使得该cluster内的点到该簇中心点的距离小于到其他簇的中心点的距离。我们将簇$w$的cluster center $\\vec{\\mu}$ 定义为簇的形心: $$\\vec{\\mu}(w) = \\frac{1}{|w|}\\sum_{\\vec{x} \\in w}\\vec{x} \\tag{1} \\label{1}$$ 其中，$|w|$表示簇$w$内 点 的个数。如何衡量cluster center是否很好地表示了整个cluster内的点呢？我们用cluster内每个点到cluster center的残差平方和(residual sum of squares, RSS)来衡量。对于第$k$个cluster：$$RSS_{k} = \\sum_{\\vec{x} \\in w_{k}}|\\vec{x} - \\vec{\\mu}(w_k)|^{2} \\tag{2}$$ 对于整个数据集来说: $$RSS = \\sum_{k=1}^{K}RSS_{k} = \\sum_{k=1}^{K} \\sum_{\\vec{x} \\in w_{k}} |\\vec{x} - \\vec{\\mu}(w_{k})|^{2} \\tag{3} \\label{RSS}$$ 上式$\\eqref{RSS}$就是K-means聚类算法的目标函数, 我们的目标是最小化该目标函数。接下来介绍K-means算法的优化迭代过程。第一步是随机选择$K$个点作为cluster center，接下来K-means模型迭代执行两步来最小化目标函数，直到满足停止条件：(1) 固定cluster center $\\vec{\\mu}(w_k)$，把数据集中的每个点重新分配到最近的cluster center所属的簇中；(2)根据重新分配后簇内的点，重新计算cluster center $\\vec{\\mu}(w_k)$。我们可以用以下停止条件: 固定的迭代次数$maxstep$。限制最大迭代次数 可以限制模型的迭代时间，但有可能由于模型迭代次数不够，导致聚类效果不好。 一直迭代到每个cluster内的点不再变化。这样会有很好的聚类效果，但可能导致迭代时间太长。 一直迭代到每个cluster的cluster center不再变化。 目标函数减小到某个阈值时停止迭代， 这个阈值保证了聚类模型有了不错的聚类效果。 目标函数的减少幅度小于某个很小的阈值，这表示聚类模型 接近于收敛了。 图1展示了K-means聚类算法的伪代码。 Fig.1 图2是一个K=2的K-means算法的例子。 Fig.2 目标函数的单调递减性接下来，我们证明: 每次迭代中，目标函数$RSS$会逐渐减小，从而K-means模型逐渐收敛。 在重新分配 点 的过程中，由于把每个点分配到 离它最近的 cluster center对应的簇中，从而目标函数$RSS$是减小的。 在重新计算cluster center的过程中，当cluster center为cluster的形心时，$RSS_k$达到它的最小值。证明如下: $$RSS_k(\\vec{v}) = \\sum_{\\vec{x} \\in w_k}|\\vec{x} - \\vec{v}|^2 = $\\sum_{\\vec{x} \\in w_k} \\sum_{m=1}^{M}(x_m - v_m)^2 \\tag{4} \\label{4}$$ 其中$\\vec{x}$和$\\vec{v}$是一个$M$维的向量，$x_m, v_m$分别是其对应向量的第$m$个值，令该式$\\eqref{4}$的偏导数等于0：$$\\frac{\\partial{RSS_k(\\vec{v})}}{\\partial{v_m}} = \\sum_{\\vec{x} \\in w_k}2(v_m - x_m) = 0$$ $$v_m = \\frac{1}{|w_k|}\\sum_{\\vec{x} \\in w_k}x_m \\tag{5} \\label{5}$$ 式子$\\eqref{5}$是形心公式$\\eqref{1}$的逐点表示形式。当偏导数=0时，即$\\vec{v} = \\vec{\\mu{w_k}}$时，式子$\\eqref{4}$取得最小值。 需要注意的是，K-means算法不能保证得到目标函数的最小值，这与初始中心点的选择有关。选择不同的初始中心点，可能会得到不同的聚类结果。如果数据集中包含多个离群点(远离其他的点，从而不适合分到任何一个簇中)，这样的离群点被选为初始中心点，在迭代过程中不会有其他的任何一个点被分配到这个簇中，迭代结束后我们会得到一个只有一个点的簇(singleton cluster)。即使还有其他的聚类方法，可以得到更小的目标函数值。因此，初始中心点的选择是个重要的问题。有以下几种方法： 把离群点从初始中心点的候选集中排除。 选择不同的初始中心点进行多次聚类，选择目标函数值最小的聚类结果。 K-means的时间复杂度假设数据集有$N$个点，每个点的向量表示的维度为$M$，聚类的簇的个数为$K$。K-means算法的大多数时间花在计算向量之间的距离，每次计算的时间复杂度是$\\Theta(M)$。在重新分配点的步骤中，需要进行$KN$次距离计算。每次迭代的时间复杂度为$\\Theta(KNM)$。进行$I$次迭代的时间复杂度为$\\Theta(IKNM)$。 可以看出: K-means算法的时间复杂度与 迭代次数、 簇的个数、样本数、向量空间的维度线性相关。 参考链接 K-means Louvain社区发现算法Louvain算法是基于模块度(modularity)的社区发现算法，能够发现层次性的社区结构，其优化目标是最大化整个社区网络的模块度。 模块度(Modularity)模块度是评估一个社区网络划分好坏的度量方法。模块度的物理意义是：模块度越大，同一个社区内的节点之间联系更加紧密，不同社区之间的节点联系更加松散。对于没有权重的无向图，模块度的取值范围是[-1/2,1]。如果 社区内 边的数量超过随机情况下边的数量，那么模块度是一个正值。 模块度的定义是 社区(group, or cluster, or community)内节点之间的实际连边数 与随机情况下连边数 的差值。如何理解随机情况下的连边数呢？对于一个有$n$个节点(nodes)，$m$条边(edges)的网络。其中对于节点$v$有$k_v$条边(我们把节点$v$的边数$k_v$成为节点$v$的度)。接下来我们要在满足网络内的节点数$n$、边数$m$、以及每个节点$v$的边数$k_v$保持不变的情况下，生成一个随机网络。具体地，把每条边分成两半(我们称每半条边为一个 stub)，每半条边 与 网络中其他的 任意半条边(stub)连接起来，这样我们就得到了一个完全随机的网络。考虑两个节点有$k_v$条边的$v$和$k_u$条边的$u$。在$2m$半条边中，一个半条边stub连接到另一个半条边的概率是$\\frac{1}{2m-1}$，则有$k_v$条半边的节点$v$和有$k_u$条半边的节点$u$的边数为$$\\frac{k_vk_u}{2m-1}\\approx\\frac{k_vk_u}{2m}$$当$m$很大时，可以取近似值。则两个节点之间的实际 连边数与随机情况下 连边数的差值为:$$A_{ij} - \\frac{k_i,k_j}{2m} \\tag{6} $$在$2m$个节点对上求和，可以得到整个网络的模块度$Q$： $$Q = \\frac{1}{2m}\\sum_{i,j}[A_{ij} - \\frac{k_ik_j}{2m}]\\delta(c_i,c_j) \\tag{7} \\label{7}$$$$ \\delta(x,y) =\\begin{cases}1; when x==y \\0; when x!= y\\end{cases} $$ $A_{ij}$表示节点$i$和节点$j$之间边的权重，当网络是无权图是，所有边的权重可以看作是1；$k_i = \\sum_jA_{ij}$表示节点$i$相连的边的权重之和(度数)；$c_i$表示节点$i$所属的社区；$m = \\frac{1}{2}\\sum_{ij}A_{ij}$表示所有边的权重之和（边的数目）。 对式子$\\eqref{7}$进行简化，$$Q = \\frac{1}{2m}\\sum_{ij}[A_{ij} - \\frac{k_ik_j}{2m}]\\delta(c_i,c_j) $$ $$ = \\frac{1}{2m}[\\sum_{ij}A_{ij} - \\frac{\\sum_ik_i\\sum_jk_j}{2m}]\\delta(c_i,c_j) $$ $$= \\frac{1}{2m}\\sum_{c}[\\sum_{in} - \\frac{(\\sum_{total})^2}{2m}] \\tag{8} \\label{8}$$ 其中$\\sum_{in}$表示社区$c$内部 边的权重之和(数目，对于无权图），$\\sum_{total}$表示所有与社区$c$内的节点相连的边的权重之和(数目，对于无权图)。对式子$\\eqref{8}$进一步简化: $$Q = \\sum_{c}[\\frac{\\sum_{in}}{2m} - (\\frac{\\sum_{total}}{2m})^2]$$ $$=\\sum_{c}[e_c - (a_c)^2]$$ 这样模块度可以理解为 社区内部边的权重和 减去 所有与社区内节点相连的边的权重和的平方。对于无权图，模块度为 社区内部的度数 减去 社区内节点的 总度数的平方。 参考链接 模块度与Louvain社区发现算法 维基百科-模块度 《Fast unfolding of communities in large networks》–louvain算法原论文，2008 DBSCAN密度聚类算法DBSCAN(Density-Based Spatial Cluster of Applications with Noise，有噪声的基于密度的聚类方法)是非常典型的基于密度的聚类方法。DBSCAN算法是一种基于样本分布密度的聚类算法，密度聚类算法一般假定可以类别通过样本分布的紧密程度来决定。把紧密相连的点划分到同一个簇。 一些基本概念设数据集为$D = \\lbrace{x_1, x_2, …, x_m\\rbrace}$。 1个核心思想：基于密度 DBSCAN算法可以找到 数据集中所有 样本紧密分布的区域，并分别将这些紧密的区域作为一个一个的簇。 2个算法参数 DBSCAN算法用 $\\epsilon$-邻域 的概念来衡量样本分布的紧密程度，也就是样本密度。对于样本点$x_j \\in D$，该样本点的$\\epsilon$-邻域内包含 与$x_j$距离不大于$\\epsilon$的子样本集，即$N_{\\epsilon}(x_j) = \\lbrace{x_i|distance(x_i,x_j) \\leq \\epsilon\\rbrace}$，这个子样本集中的样本数为$|N_{\\epsilon}(x_j)|$。 $\\epsilon$-邻域的半径$\\epsilon$。 $MinPoints$ 描述了某一样本半径为$\\epsilon$的$\\epsilon$-邻域中样本个数的阈值。 3种样本点 核心对象(core point)：对于一个样本点$x_j \\in D$，如果其$\\epsilon$-邻域中至少包含$MinPoints$个样本点，即$|N_{\\epsilon}(x_j)| \\geq MinPoints$，则$x_j$是核心对象。 边界点(boundary point)：对于一个样本点$x_j$，它不是核心对象，但它在某个核心对象的$\\epsilon$-邻域内，则$x_j$是边界点。 噪声点(noise point): 或称为离群点(outlier point)。对于一个样本点$x_j$，它不是核心对象，也不在任何一个核心对象的$\\epsilon$-邻域内，则$x_j$是噪声点。 样本点之间的4种关系 密度直达(directly reachable)：如果样本点$x_i$在核心对象$x_i$的$\\epsilon$-邻域内，则称$x_i$由$x_j$密度可达。注意密度直达关系不是对称的，此时不能说$x_j$由$x_i$密度直达。 密度可达(reachable)：对于$x_i$和$x_j$如果存在一个样本序列$p_1,p_2,…,p_T$，其中$p_1=x_i, p_T=x_j$，且$p_t+1$由$p_t$密度直达，则称$x_i$由$x_j$密度可达。密度可达具有传递性，此样本序列中的传递样本$p_1, p_2, …, p_{T-1}$都是核心对象，只有核心对象才能使得其他样本密度直达。 注意密度可达关系也不满足对称性，这可以由密度直达关系的不对称性得到。 密度相连(connectedness)：对于样本点$x_i$和$x_j$，如果存在核心对象$x_k$，使得$x_i$和$x_j$均可以由$x_k$密度可达，则称$x_i$与$x_j$密度相连。密度相连关系是满足对称性的。 密度不相连：不满足密度相连关系的两个样本点$x_i$和$x_j$，属于两个不同的簇，或者其中存在离群点。 如下图所示，图中$MinPoints$为5，红色的点都是核心对象，因为其$\\epsilon$-邻域内至少有5个样本点(包括核心对象本身)，圆圈内的黑点都是边界点，圆圈以外的黑点都是噪声点。对于某个红点来说，其对应的圈内的其他点都是密度直达的。途中用绿色箭头连起来的核心对象组成了密度可达的样本序列，这些密度可达的样本序列的$\\epsilon$-邻域内所有的样本点都是密度相连的。 Fig.3 source: https://www.cnblogs.com/pinard/p/6208966.html DBSCAN密度聚类思想DBSCAN的聚类定义为: 把所有密度相连的样本点划分到同一个簇。DBSCAN密度聚类得到的簇有以下特点: 簇内至少有一个核心对象，也就是至少有$MinPoints$个样本点。 簇内所有的点都是密度相连的。并且如果某个样本点与某个簇内的其他点密度相连，那么这个样本点属于这个簇。 离群点(噪音点)跟所有簇内的点都不是密度相连的，因此，噪音点不属于任何一个簇。 这里需要考虑一个问题，DBSCAN聚类算法会得到有重叠的簇吗？也就是说会有同一个点属于多个簇的情况吗？答案是不会。对于某个样本到两个核心对象的距离都小于$\\epsilon$，但这两个核心对象不是密度直达的，不属于同一个簇，怎么界定这个样本点的类别呢？DBSCAN会采取”先来后到”的方式，先进行聚类的类别簇会把这个样本标记为它的类别。 $\\tag{9} \\label{two}$ DBSCAN聚类算法输入：样本集$D = \\lbrace{x_1, x_2, …, x_m}\\rbrace$， 邻域参数($\\epsilon, MinPoints$)输出：簇的划分$C$ 遍历所有的样本点，找出所有的核心对象。也就是$\\epsilon$-邻域内样本数不小于$MinPoints$的样本点。 先忽略所有的非核心对象，只考虑核心对象，将所有密度可达的核心对象划分到一个簇。 再考虑非核心对象，将每个核心对象$\\epsilon$-邻域中的边界点划分到所属的簇中。噪音点不属于任何一个簇。 DBSCAN算法的优缺点DBSCAN算法有以下优点: 与传统的K-means算法相比，DBSCAN算法不需要输入聚类的类别数$K$； DBSCAN算法可以发现任意形状的聚类簇。传统的K-means算法一般只适用于 凸样本集的聚类，DBSCAN算法既可以用于凸样本集，也可以用于非凸样本集。 对于传统的K-means算法，离群点会影响聚类的结果。DBSCAN算法有噪音点的概念，可以在聚类过程中发现离群点，因此对离群点不敏感。 传统的K-means算法，K个随机初始中心点的选择会对聚类结果有很大的影响。相比之下，算法初始值的选择对DBSCAN的聚类结果影响很小(先来后到的例子)，DBSCAN的聚类结果是没有偏倚的。 DBSCAN算法的缺点: DBSCAN的聚类结果不完全是确定的。对于$\\eqref{two}$，聚类结果取决于数据处理的先后顺序。但对于核心对象和噪音点，聚类结果是确定的。对于边界点，聚类结果可能会有所不同。 如果数据集的密度不均匀的情况下，DBSCAN的聚类效果比较差。因为当簇之间的密度差别较大时，两个模型参数$\\epsilon$和$MinPoints$不可能适合于所有的簇。 需要对两个算法参数$\\epsilon$和$MinPoints$联合调参，不同的参数组合对聚类效果会有较大影响。 Fig.4 source: https://towardsdatascience.com/dbscan-clustering-best-practices-38de9cf57610 在上图中，我们可以看出DBSCAN算法和K-means算法的聚类结果之间的差别。K-means算法倾向于生成球形的簇，这主要是由于K-means算法的目标函数实质上是最小化样本点到簇中心点的欧氏距离平均值。DBSCAN算法倾向将样本之间联系紧密的区域作为一个簇，这是由于DBSCAN算法的思想是将所有密度相连的样本点作为一个簇。 总的来说，如果数据集是稠密的，并且数据集不是凸的，那么DBSCAN算法的聚类效果会比K-means算法的效果更好。 参考链接 DBSCAN密度聚类算法–刘建平 DBSCAN-wikipedia dbscan-clustering-best-practices","categories":[{"name":"nlp","slug":"nlp","permalink":"http://yoursite.com/categories/nlp/"}],"tags":[{"name":"文本聚类","slug":"文本聚类","permalink":"http://yoursite.com/tags/文本聚类/"},{"name":"k-means","slug":"k-means","permalink":"http://yoursite.com/tags/k-means/"},{"name":"DBSCAN","slug":"DBSCAN","permalink":"http://yoursite.com/tags/DBSCAN/"},{"name":"louvain","slug":"louvain","permalink":"http://yoursite.com/tags/louvain/"}]},{"title":"布尔检索","slug":"布尔检索","date":"2020-06-22T09:33:07.000Z","updated":"2020-07-06T05:01:33.000Z","comments":true,"path":"2020/06/22/布尔检索/","link":"","permalink":"http://yoursite.com/2020/06/22/布尔检索/","excerpt":"","text":"信息检索的含义是非常广泛的，在学术界将其定义为：在海量数据中找到符合信息需要的文档或文本。信息检索可以按照操作规模分为三类：网页搜索、个人信息搜索、企业或特定领域的搜索。 网页搜索： 提供了给 存放在百万台电脑上的百亿篇文档的搜索服务。网页搜索特有的问题是 需要爬取收集建索引的海量文档，并且在海量文档尺度上建立高效的搜索系统。另外，还需要处理一些网页特有的问题，比如超链接的爆炸性增长。 个人信息搜索： 消费者客户端提供了信息搜索功能。比如个人邮件系统不仅可以搜索，还可以实现垃圾邮件分类。个人信息搜索特有的问题是 搜索服务启动、运行计算、磁盘占用的轻量级。 企业或特定领域搜索：这个规模是介于 网页搜索和个人信息搜索之间的。 词-文档矩阵(term-document matrix)有一个需求，在《莎士比亚全集》中，我们要找到包含词”Brutus”、”Caesar”但不包含词”Calpurnia”的书。一种方式是从头到尾读一遍这些书，保留包含词”Brutus”、”Caesar”的书，排除掉包含词”Calpurnia”的书。对于计算机来说，找出包含某些词的文档 对应于linux系统下的grep操作。但实际情况中，我们需要的比grep操作更多： 在海量的数据上进行检索操作。 需要进行 更复杂的匹配操作。比如: 查询词”Brutus”接近 词”Caesar”的文档，这里接近 可以定义为在同一个句子内。 对搜索结果进行排序，我们希望 高质量的匹配文档排序更靠前。 避免每次query都进行线性检索(也就是从头到尾把文档都读一遍)的方法是：提前为每篇文档建立索引。假设我们为每篇文档 记录是否包含某个词，我们可以得到一个 二元词-文档关联矩阵(binary term-document incidence matrix)，如图1所示。对于 词-文档关联矩阵，它的行向量表示对于某个词，这个词出现在了哪些文档中；它的列向量表示 对于某篇文档，这篇文档包含了哪些词。 有了词-文档关联矩阵，就可以实现\"Brutus AND Caesar AND NOT Calpurnia\"这个查询(query)了。词\"Brutus\"、\"Caesar\"、\"Calpurnia\"对应的行向量分为为\"110100\",\"110111\"、\"010000\"。 110100 AND 110111 AND NOT 010000 = 110100 AND 110111 AND 101111 = 100100 那么查询的结果就是《Antony and Cleopatra》和《Hamlet》。布尔检索模型(boolean retrieval model)是可以把查询(query)表示成词的布尔表达式(词之间用与或非逻辑运算符连接)的一种信息检索模型，布尔检索模型把每篇文档看作是词袋（词的集合）。而 词-文档关联矩阵是进行布尔查询的基础。 倒排索引(inverted index)接下来我们考虑一种更现实的搜索场景。假设我们有 N = 1000,000篇文档(documents)，我们在这个文档集合(collections)上进行检索。假设每篇文档有1000个词，每个词占用内存大约6字节，那么整个文档集合占用的内存约为6GB。另外，整个文档集合大约有 500,000个不同的词。这种情况下，构建一个 $500K \\times 1M$ 的词-文档矩阵是不切实际的，这样的词-文档矩阵占用了太多的内存，并且这个矩阵是非常稀疏的(矩阵只有很少一部分元素是非零的)。更好的表示方法是只记录词出现在哪些文档中，这种想法是信息检索的一个重要概念：倒排索引(inverted index)。图2展示了一个倒排索引的例子，我们维护一个词表(dictionary)，对于每个词，对应一个包含这个词的文档列表。 每个词对应文档列表被称为”postings list”，所有文档列表的集合被称为”Postings”。 如何建立倒排索引为了提高检索效率，缩短检索时间，我们需要提前构建好索引。构建倒排索引需要四步： 收集需要建立索引的文档； 对文本进行分词处理，将文档转换为词的列表； 对词进行语言学预处理，得到标准化的词。比如 $Friends \\to friend; was \\to is$ 为包含每个词的文档建立倒排索引。倒排索引包含两个部分:dictionary 和postings。 图3左侧，我们为每篇文档分配一个唯一的文档号(docID),文档中的每个词对应它的文档号; 图3中间，按字母对词进行排序，图3右侧，按照词进行分组，合并文档号。这样就得到了倒排索引的dictionary 和 postings。dictionary不仅保存了每个词，还保存了其他一些统计信息(比如 包含每个词的文档数)，这些统计信息可以提高 加权排序检索模型的搜索效率。每个词的postings list中保存了包含这个词的文档列表，也可以保存一些统计信息(比如这个词在每篇文档中的出现次数，出现的位置)。 这里有两次排序和一次分组值得注意。两次排序是 按照字母对词进行了排序，按docID对每个词的postings list进行排序。一次分组是 按照词进行分组，合并了每个词对应的文档号，得到了每个词的postings list。 接下来讨论一下dictionary和 postings list的存储。dictionary保存在访问速度更快的内存中，postings list更大，通常保存在磁盘中。用什么数据结构来保存postings list呢？固定长度的数组比较浪费存储空间，因为高频词出现在更多的文档中，低频词则出现在很少的文档中。好的两种数据结构是 单链表 和 可变长度的数组。单链表的优点是方便进行postings list的更新操作，可以很快的插入新的docID。可变长度的数组 的优点是不需要存储单链表的指针，从而节省了磁盘空间；另外可变长度数组存储在连续的内存中从而提高了处理速度。 基于倒排索引进行boolean query如何基于倒排索引来进行布尔查询呢？考虑最简单的联合查询Brutus AND Calpurnia。如图4所示，可以分为以下两步: 从dictionary中取出词Brutus的postings list 和 词Calpurnia的postings list. 取这两个postings list的交集。这个交集就是包含两个词的文档。 Fig.4 可以按图5所示的算法，来取两个postings list的交集。为两个postings list分别维护一个指针，遍历两个数组。每一步，当两个指针指向的docID一致时，将这个docID添加到result list; 当不一致时，向前移动指向较小docID的指针。当两个指针都到达列表末尾时，循环结束。设两个postings list的长度分别为x和y，则取交集的时间负责度为$O(x+y)$，而之前线性检索的时间负责度为$\\Theta(N)$，通过建立倒排索引，把线性时间复杂度 降低到了常数时间复杂度。 参考链接 boolean-retrieval","categories":[{"name":"信息检索","slug":"信息检索","permalink":"http://yoursite.com/categories/信息检索/"}],"tags":[{"name":"信息检索","slug":"信息检索","permalink":"http://yoursite.com/tags/信息检索/"},{"name":"倒排索引","slug":"倒排索引","permalink":"http://yoursite.com/tags/倒排索引/"}]},{"title":"pySpark学习笔记","slug":"pySpark学习笔记","date":"2020-02-13T06:14:50.000Z","updated":"2020-02-13T10:30:34.529Z","comments":true,"path":"2020/02/13/pySpark学习笔记/","link":"","permalink":"http://yoursite.com/2020/02/13/pySpark学习笔记/","excerpt":"","text":"pyspark下载与环境设置前提： 安装pyspark之前，要检查电脑是否安装了JAVA环境，可以用命令java -version来查看。参考链接：Centos下JDK的安装与卸载 Centos下JDK的安装 查看yum库中有哪些可用的JDK版本：yum search java | grep jdk。 选择版本安装JDK，可以用yum install java-1.8.0-openjdk-devel.x86_64命令来安装JAVA环境。 Centos下JDK的卸载 先查看系统中安装了哪些rpm软件包,查看相关Java包的信息： rpm -qa | grep java 卸载已安装的JDK: yum -y remove java &lt;包名&gt;，比如yum -y remove java java-1.6.0-openjdk-1.6.0.38-1.13.10.0.el7_2.x86_64。 在清华大学镜像源http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz下载pyspark安装包。 遇到的问题 在./bin/pyspark启动pyspark时，报以下错误：1Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main : Unsupported major.minor version 52.0 出错原因是：pyspark 2.1需要Java 1.7以上的版本，而安装的Java版本是1.6的。 在python代码中调用pyspark报错：1ModuleNotFoundError: No module named &apos;py4j&apos; 这是因为~/.bashrc中py4j的版本与实际的版本不同。修改~/.bashrc中py4j的版本为实际的版本即可。参考链接：https://blog.csdn.net/skyejy/article/details/90690742 参考链接 编程字典-pyspark教程","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"pySpark","slug":"pySpark","permalink":"http://yoursite.com/tags/pySpark/"}]},{"title":"Hadoop学习笔记","slug":"Hadoop学习笔记","date":"2020-02-11T07:09:31.000Z","updated":"2020-02-11T09:36:22.143Z","comments":true,"path":"2020/02/11/Hadoop学习笔记/","link":"","permalink":"http://yoursite.com/2020/02/11/Hadoop学习笔记/","excerpt":"Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。","text":"Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。 Hadoop简介Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。随着技术发展，人类每天都会产生海量数据，用单一的机器来存储和处理这些数据已经不能满足需求。而Hadoop允许在从单一的机器扩展到上千台机器，从而在跨计算机的分布式环境中来存储和处理大数据。 Hadoop的架构如下图所示： Hadoop的架构图 HDFS: 分布式文件存储系统 YARN: 分布式资源管理 MapReduce: 分布式计算 Others: 利用YARN的资源管理来实现其他的数据处理方式 HDFSHDFS(Hadoop Distributed File System)是Hadoop应用主要的分布式文件系统。HDFS是基于“Master-Worker”架构的，一个HDFS集群由一个NameNode和多个DataNode组成。NameNode是一个中心服务器，管理文件系统的命名空间(NameSpace)，管理文件系统的元数据(MataData)，相当于是一个目录。DataNode负责存储实际的数据。 HDFS暴露了文件系统的命名空间，用户可以以文件的形式来存储数据。具体来看，一个文件被分为一个或者多个数据块(Block)，这些数据块存储在一组DataNode上；每个数据库对应NameNode上的一条记录。NameNode执行文件系统的命名空间操作，比如打开、关闭、重命名文件或目录；它也负责数据库与DataNode之间的映射。DataNode负责处理文件系统客户端的读写请求，在NameNode的统一管理下来进行数据块的创建、删除和复制。 HDFS的架构 Block 数据块block是基本存储单位，一般大小为64MB。 一个文件会被分成一个或多个数据块来存储。如果文件大小小于一个Block的大小，那么实际占用的空间为文件大小。 Block是基本的读写单位，相当于磁盘的页，每次都会读写一个Block。 每个Block会被存储到多个机器，默认是3个。防止机器故障造成数据丢失。 NameNode 存储文件的元数据，管理文件系统的命名空间。整个HDFS可存储文件的大小受限于NameNode的内存大小。 一个Block对应NameNode中的一条记录。 NameNode失效后，整个HDFS就都失效了。要保证NameNode的可用性。 DataNode 存储具体的block数据。 负责数据的读写操作和复制操作。 DataNode启动时会向NameNode汇报当前存储的block信息，随后也会定时向NameNode汇报修改信息。 DataNode之间会进行通信，复制block，保证数据的冗余性。 HDFS shell命令Hadoop包含了一系列类shell命令，可以直接和HDFS或hadoop支持的其他文件系统进行直接的交互。hadoop fs -help可以列出所有的shell命令。这些shell命令支持大部分普通文件系统的操作，比如复制、删除文件、更改文件权限等。 调用文件系统shell命令应该采用hadoop fs &lt;arg&gt;的形式，所有的FS shell命令都使用URI路径作为参数。URI路径的格式是’scheme://authority/path’，对于HDFS文件系统，scheme是hdfs；对于本地文件系统，scheme是file。其中scheme参数和authority参数是可选的，省略的话会使用默认的参数scheme。 ls 使用方式： hadoop fs -ls &lt;args&gt; 如果是文件，会显示文件信息；如果是目录，会显示目录下所有的文件。 test 使用方式： hadoop fs -test [edz] URI 选项： -e 检查文件是否存在，如果存在返回0； -z 检查文件是否是0字节，如果是返回0； -d 如果路径是个目录，返回1，否则返回0 参考链接 官方手册-Hadoop分布式文件系统：架构和设计 官方手册-Hadoop Shell命令 Hadoop简介 Hadoop教程-W3Cschool","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://yoursite.com/tags/HDFS/"}]},{"title":"2019-蓟门烟树","slug":"2019-蓟门烟树","date":"2020-01-13T15:01:16.000Z","updated":"2020-06-28T08:06:33.000Z","comments":true,"path":"2020/01/13/2019-蓟门烟树/","link":"","permalink":"http://yoursite.com/2020/01/13/2019-蓟门烟树/","excerpt":"这是在明光桥北度过的第二个冬天，已经下了两三场大雪，雪后的天空格外晴朗。又到了一年的末尾，没有经常写日记，只能从朋友圈、论坛发的骑行贴、日记本上不多的几篇日记，印象笔记上的记录来尽力回忆这一年是如何度过的。回首这一年，平凡普通，不惊心动魄，有些许遗憾，也有一些小的闪光和美好。 北京的雪","text":"这是在明光桥北度过的第二个冬天，已经下了两三场大雪，雪后的天空格外晴朗。又到了一年的末尾，没有经常写日记，只能从朋友圈、论坛发的骑行贴、日记本上不多的几篇日记，印象笔记上的记录来尽力回忆这一年是如何度过的。回首这一年，平凡普通，不惊心动魄，有些许遗憾，也有一些小的闪光和美好。 北京的雪 平淡的科研生活这一年大部分的时光是在实验室的座位上度过的，虽然有时会懈怠偷懒，但大部分的时间还是在学习与研究方向相关的内容。在一位北邮博士师兄的毕业论文中看到这样一段话：“不需要多么玩命，只要你能每天规律地作息且每天到实验室，学会控制情绪，在实验室的时间全部用来做与科研相关的事情就可以了。”研一的时候，我纠结着是否要读博士？慢慢地我打消了读博士的想法。虽然这一年待在实验室很长时间，但并没有取得太大的进展。虽然最近完成了一篇论文，但我逐渐明白我还不具备独立做科研的能力，也缺乏投身科研的热情，并且缺乏老师的指导。想要读博的动机是功利性的，想要更高的学位，进而毕业后可以获取到更好的工作机会和社会地位。我畏惧读博路上的艰难孤独，缺乏读博的足够动力，学位并不应该是目的，两种选择都不是错误，做好了抉择坚定自己的内心就可以了。 研究生的生活日常是这样的。给老师做一些项目，标注数据，给老师写一些琐碎无聊的项目书和PPT报告。当然老师的项目中，并不都是琐碎无聊，也有一些值得学习深入研究的内容。辩证法的角度来看，任何事物都是如此的，正面与反面是并存的。在找实习时投递的个人简历上，做项目是一个重要的经历。除了做项目之外，大部分的时间都在阅读论文，这一年来读了上百篇对话系统相关的论文。在量变积累的过程中，自然地对对话系统这个研究方向有了大致的了解，进而可以进一步地进行深入地研究和探索。阅读论文不是目的，阅读论文的数量也不应该成为追求，应该要做的事情是在阅读论文的过程中去发现问题并提出自己的解决方案。吸收知识，然后去应用知识和创造。创造的过程是充满的乐趣的。解决问题的过程就是自己产出的过程，我慢慢地才明白了这一点。 与导师、同学的相处也是研究生生活的一个重要议题。G老师不在学校在外面谋求仕途晋升之道，做了甩手掌柜，虽然一年中抽出时间开了几次电话会议和两次线下会议。每周一次的小组会是L老师负责的，研三的师兄师姐已经出去实习了，剩下博士学姐、研一研零的学弟学妹和研二的我们开组会。研一时的每次组会，我都会被L老师批评，那时我以为是自己的问题，并对自己产生了怀疑。每次跟着L老师开完组会，学习的积极性都会被打击到，需要花一两天时间来调整心态。如今研一研零的学弟学妹也被老师这样批评着，我渐渐明白了这是L老师的个人风格，并不完全是自己的原因。L老师和G老师不算是很坑的导师，但不同的导师性格不同，教导学生的方式也不同。要学会与不同性格的导师去相处，像水一样，去适应和配合来最大程度地让自己取得进步。跟我一届的几个女生都说”不想开小组会“，这是内心的写照了。大组会的氛围要好一些，实验室有时会请博士和一些已经毕业工作的师兄师姐来做展示交流。实验室的大boss有时也会到场，大boss有次说”要内心强大，要lold住。”，这句话我印象深刻，生活中的许多场景，内心强大，能hold住是非常重要的。 这个学期做了勤工俭学的工作，每周打扫实验室一次，一个月三百块的工资。我常常会压抑自己的欲望和需要，12个小时回家的火车票大多会买硬座票，很少买卧铺。这样的心理，有些凄惨。最近我慢慢地转变了这种心理和想法，想要穿羊毛绒的袜子，想坐卧铺车或者动车回家出行，想要烫一个帅气的发型，想要穿帅气的衣服。今年的收入除了实验室工资、奖学金和勤工俭学的工资之外，我还做了一份线上考研辅导的工作。这是第二次考研辅导，辅导对象是成都某大学的一个女生，辅导她通信原理专业课，薪水是三千块。通信原理是我考研时的专业课，研究生的专业与通信已没有关系了，离我考研已经过去了整整两年，但当我看一遍通信原理的教材，我还是可以理解书上的知识点和难点，我考研时应当是真的付出了很大的努力。我还记得蒋震图书馆前的那排高大的杨树，还记得考研复习时的心理活动：当树叶全部落完时，我就要考研了；当树叶再次长出来郁郁葱葱时，我就要在清风的吹拂中毕业了。如今，校园的主干道的两旁是梧桐树，每当夕阳落下，暮色昏沉，一大群乌鸦总会停在梧桐树枝头，喧闹地叫着，树下是呈正态分布的鸟屎。 研一下学期结束后的那个暑假，我找了一份中科院自动化所的实习。实习地点在知春路附近的自动化大厦，离学校三四公里远。这是我的第一份实习，实习工资并不高，做了一些文本纠错和文本摘要的任务。原本打算暑期在实习中度过，但还是抽出一周时间回了趟家。今年年初的时候，有个想法是带着爷爷奶奶来北京。爷爷奶奶应该从没有去过省外，也没有来过北京。但奶奶身体并不好，恐怕受不了舟车劳顿，这个想法只能作罢。 饭局少不了北京城，天子脚下，有不少同学会来，因此接待了不少朋友。有一块长大的发小。C已经毕业，在北京工作，在县城里买了房子，也与女方定了婚。年初我们一起去游了圆明园，圆明园离北大不远，历史课本中那副经典照片中的断垣残壁依旧挺立着。后来R来北京探亲，姐姐姐夫在北京工作，姐姐生下了孩子。在杏坛路上的一家店，吃了鱼。我们俩从小就一块打桌球，现在我已经不是他的对手了。 也有高中的同窗。高中同学有好几个在北京，有的读研，有的工作。年中组织过一次聚餐，亲切中带着一丝生疏，逛着北航的校园，吃了饭，看了部电影《无双》。S是高中班里的才子了，他在西安读研，暑期后不久来参加头条组织的一次夏令营活动。我们在杏坛路的一家店吃了串串香，在漫咖啡用玩了线上桌球。 更多的是大学的同窗。年初W和Y来北京参加考研复试，我们在小龙坎为他们接风洗尘，但后来Y虽然初试成绩不错但遗憾地没能通过复试。Y小侄女毕业前来北京玩，我们一起游了颐和园，颐和园真是风景宜人，值得一去。Z在准备考研的过程中，忙里偷闲来北京看了话剧。Z律师跟着领导来上北大的培训班。J从日本飞回来后，一块去爬了长城。真的是记成流水账了。 回头看这一年，还是有不少社交活动的。 骑车、跑步与越野这部分是我平淡生活中的一抹色彩。 在刚读研究生时，原本已经打算与骑行告别了，骑行的热情也消散了。但在实验室同桌X同学巨大骑行热情的影响下，我又重新燃起了骑行的热情，正确地认识了骑行在我生活中的位置。我并不应该将骑行从我生活中完全地剥离出去，而应该让骑行成为我生活中的一部分。我们组成了一个四人的骑行小队，这是一个慢慢培养了骑行默契的小队了。小队的第一次骑行是在五一假期，从北京到张北草原天路。途中经过了雄伟壮观的八达岭长城，蜿蜒的长城在青山的山脊上蔓延展开；也经过了巨大的官厅水库，流经卢沟桥的永定河就是从官厅水库流出的；张北草原天路只走了五公里，并且由于海拔较高五月份草还没长出来，最佳的观赏时节是七八月份，但从张北草原天路到张家口市的那段一路下坡的废弃国道的风景农田也是极美的。小队的第二次骑行是在十月份的一个周末，由北邮出发前往百里山水画廊，再返回北邮。里程有二百八十公里，天黑得又早，两天的行程是比较仓促的。第一天的骑行爬了很多上坡，天黑了才到达落脚地点千家店镇，难度确实太大了，两个女生骑得有些崩溃。第二天的路程虽然较长但是缓下平坦路面，轻松愉快地返回了学校。后来，我们四个人一块去学校附近的一家日料店吃了日料，顺便喝了一点青梅酒，彼此开诚布公袒露心扉，约定有机会的话一块去环海南岛。 2019年的跑量有1120公里，平均配速5分15，并且刷新了自己的十公里、半马和全马的配速。年初过完春节回来有些吃胖了，学期开始后晚上会去操场跑步。后来报名了11月3日举行的北京马拉松比赛，原本对中签没有报太大的期望，因此还报名了北马的志愿者，幸运的是竟然中签了。中签后，特定买了新的跑步鞋和运动裤，大部分时间都能保持较好的跑步训练，有时候也会偷懒几天。11月2号坐地铁去国际会展中心领取了参赛包，晚上早早地就上床睡了。3日凌晨不到5点就起床了，出了宿舍楼门，漆黑的夜里下着小雨，我骑车去了积水潭地铁站。北马赛事有三万五千人的规模，鸣枪开跑后十分钟才通过起跑线。刚开跑时，温度稍微有点低，但一公里后身体就发热了，太阳升起，温度也慢慢上来了。总的来说，11月的北京天气凉爽，并且北马补给充足，我用了3小时35分完赛，大大超出了自己的预期。这是我参加的第二个全马，相比于跑完东马的双腿疼痛，这次北马的准备要充足很多，双腿的酸痛感轻很多。年初我的体重在73公斤左右，托跑步的福，体重下降到了67公斤，这跟我远征完的体重差不多。 这一年还解锁了越野赛。G学长报名了香山21公里越野赛，但因工作原因不能参加。于是，我就得到了第一次参加山地越野赛赛的机会。不久前下的雪还没有融化，香山越野的21公里因此缩短为了18公里。虽然有段时间没有跑步了，但参加北马的体力基础还在，以七十多名的成绩完赛。山地越野比单纯的马拉松比赛要有趣很多，有柏油路，也有山间的小路，还有下坡的石头路。由于没有训练过如何快速地下坡，15公里时左腿的膝盖已经非常疼了，坚持了完成了比赛。除了山地越野赛之外，还接触到了定向越野赛。先是实验室组队参加了5公里西山定向越野，赢取了一份零食大礼包，在越野赛上遇到了一位女生S。后来，加了她的微信，慢慢有了接触，一起去看了电影，一起约了早饭，但最后并没有一个好的结果。 香山越野赛 身边有许多学长去登了雪山，希望以后有机会可以登上一座雪山。 2020年的主要基调是实习和找工作。希望这个寒假不要虚度，多刷几道算法题。2020的愿望是有更大信心去接纳自己,拥抱生活。","categories":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/categories/年度总结/"}],"tags":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/tags/年度总结/"},{"name":"生活记录","slug":"生活记录","permalink":"http://yoursite.com/tags/生活记录/"}]},{"title":"论文笔记《Large-Scale Transfer Learning for Natural Language Generation》","slug":"论文笔记《Large-Scale-Transfer-Learning-for-Natural-Language-Generation》","date":"2020-01-07T01:22:38.000Z","updated":"2020-01-07T02:29:27.650Z","comments":true,"path":"2020/01/07/论文笔记《Large-Scale-Transfer-Learning-for-Natural-Language-Generation》/","link":"","permalink":"http://yoursite.com/2020/01/07/论文笔记《Large-Scale-Transfer-Learning-for-Natural-Language-Generation》/","excerpt":"【来源】ACL2019【链接】https://www.aclweb.org/anthology/P19-1608.pdf【代码】未公布","text":"【来源】ACL2019【链接】https://www.aclweb.org/anthology/P19-1608.pdf【代码】未公布 这篇论文是huggingface发表在ACL2019的短文。 论文简介传统的有监督学习方法是：在特定的任务上，在有标签的数据集上，有监督地训练一个模型。有监督学习的局限在于许多NLP任务缺乏有标签的数据集，或者有标签数据集的规模比较小。迁移学习就派上用场了，迁移学习在许多NLP任务上取得了好的效果。迁移学习的思路是：先在大规模的未标注文本语料上无监督地预训练一个语言模型，再把预训练好的语言模型迁移到特定的任务上，对模型参数进行微调。目前迁移学习的大部分研究集中在文本分类和NLU(natural language understanding)任务上，迁移学习应用在NLG(natural language generation)任务上的研究比较少。论文认为NLG任务可以分为两类： high entropy任务。例如story generation，chit-chat dialog。输入文本包含的信息有限，可能不包含生成输出文本所需要的信息。 low entropy任务。例如文本摘要、机器翻译。特点是输入文本的信息量比较多，生成输出文本需要的信息包含在输入文本中。 论文主要研究了迁移学习在对话系统上的应用。 模型结构对话系统主要有三种输入：dialogue history,facts以及previous decoded tokens。论文研究单输入模型和多输入模型。单输入模型应用场景有限，主要关注下输入的部分。多输入模型基于encoder-decoder框架，关注下decoder部分的调整。 Fig.1. 单输入模型与多输入模型的结构图 单输入模型单输入模型把三种输入连接起来作为模型的输入。连接方式有三种： 用自然分隔符连接输入。论文中给每句对话添加双引号。 用空间分隔符连接。比如用’_SEP’把每个句子连接起来。 直接把句子连接起来，再用context-type embedding(CTE)来表示输入的类型。例如：$w_{CTE}^{info}$表示用户画像信息，$w_{CTE}^{p^1}$表示对话人1说的话，$w_{CTE}^{p^2}$表示对话人2说的话。 Fig.2. (a)单输入模型使用CTE方式的输入(b)多输入模型用起始分隔符连接的输入 多输入模型多输入模型基于encoder-decoder框架。用预训练的语言模型参数来初始化encoder和decoder。多输入模型的输入同样可以采用单输入模型的处理方式。将persona information和dialogue history分别送入encoder进行编码得到两个向量表示。重点在于decoder部分的调整。decoder的multi-head attention模块处理三种特征输入(personal information,dialogue history,previous decoded tokens)，再把三者的结果取平均值即可。 Fig.3. 基于transformer模型的多输入模型","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"http://yoursite.com/tags/Transfer-Learning/"},{"name":"NLG","slug":"NLG","permalink":"http://yoursite.com/tags/NLG/"}]},{"title":"论文笔记《A Pre-training Based Personalized Dialogue Generation Model with Persona-sparse Data》","slug":"论文笔记《A-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data》","date":"2020-01-06T03:30:13.000Z","updated":"2020-01-06T07:23:14.566Z","comments":true,"path":"2020/01/06/论文笔记《A-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data》/","link":"","permalink":"http://yoursite.com/2020/01/06/论文笔记《A-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data》/","excerpt":"【来源】AAAI2020【链接】https://arxiv.org/abs/1911.04700【代码】未公布","text":"【来源】AAAI2020【链接】https://arxiv.org/abs/1911.04700【代码】未公布 这篇论文是清华大学黄民烈教授组发表在AAAI2020的论文。 论文简介论文主要研究个性化的对话系统。论文中提出了“persona-dense”和“persona-sparse”的概念。 persona-dense: 像Persona-Chat数据集中，在收集语料的过程中，对话人要求在有限的轮数内交流彼此的个性化信息，对话内容是与persona是密切相关的。 persona-sparse：而在现实的对话中，只有少数的对话与persona是相关的，大多数的对话往往是与persona不相关的。直接在真实对话的语料上训练和微调模型，可能会让模型学习到大多数与persona无关的对话，而把少数与persona相关的对话当作是语料中的噪声。 为了解决这个问题，对话模型应该学习到哪些对话是与persona相关的，哪些对话是与persona不相关的。论文采用了基于encoder-decoder框架的transformer模型。预训练的方法在许多NLP任务上取得了好的效果，论文提出用预训练的语言模型参数来初始化encoder和decoder。将attribute embedding添加到了encoder的输入；用了attention route机制来建模dialogue history，target persona和previous tokens这三种不同的特征；另外，用了dynamic weight predictor来控制这三种不同的特征在解码生成回复时起到不同程度的作用。 模型结构任务可以描述为：给定对话历史$C$和回复者的target persona $T$，要求生成流畅的回复$Y$。 $$Y = argmax_{Y^{‘}}P(Y^{‘}|C,T)$$ 其中persona $T$由一系列属性来描述，$T = {t_1,t_2,…,t_N}$；每个属性用键值对来表示：$t_i = &lt;k_i,v_i&gt;$。对话历史扩展了说话人的信息，$C=\\lbrace{(U_1,T_1),(U_2,T_2),…,(U_M,T_M)}\\rbrace$ Fig.1. 模型结构的示意图 Encoding with Persona模型的输入包括两个部分对话历史$C$和回复者的target persona $T$。 先说对话历史$C=\\lbrace{(U_1,T_1),(U_2,T_2),…,(U_M,T_M)}\\rbrace$的的编码，将所有的句子用特定的字符’_SPE’连接起来，并且将对话人$T_i$属性$t_i$映射为embedding表示。具体地，论文中的用户属性包括性别、地址、爱好三个。前两个的属性只有一个值，直接经过embedding层就可以了。爱好可能有多个值，经过embedding层后再取平均值即可。最终将word embedding + positional embedding + attribute embedding作为transformer encoder的输入。 Fig.2. input representation of dialogue context 至于回复者的target persona $T$的编码，将所有的键值对属性连接起来作为一个序列，经过embedding层，直接作为transformer encoder的输入。 Attention Routing对于persona-sparse的对话语料，与persona无关的训练样例，在生成回复时不使用persona信息；与persona相关的训练样例，在生成回复时使用persona信息。论文为此设计了Attention routing模块来控制target persona $T$在生成回复时起到的作用。具体地，将previous decoded tokens的表示$E_{prev}$作为attention的query，在对话历史$E_C$、回复者的target persona $E_T$和previous decoded tokens $E_{prev}$这三种特征上使用multi-head attention机制： $$O_T = MultiHead(E_{prev},E_T,E_T)$$ $$O_C = MultiHead(E_{prev},E_C,E_C)$$ $$O_{prev} = MultiHead(E_{prev},E_{prev},E_{prev})$$ 计算$O_T$和$O_C$时使用unmasked self-attention机制，计算$O_{prev}$时使用masked self-attention机制为避免decoder看到未来时刻的信息。用一个persona weight $\\alpha$把三个attention route $O_T,O_C,O_{prev}$结合起来：$$O_{merge} = \\alpha O_T + (1-\\alpha)O_C + O_C + O_{prev}$$ persona weight $\\alpha$应该基于对话是否与persona有关，论文设计了dynamic weight predictor来预测$\\alpha$。具体地，这个predictor是一个以对话历史$E_C$为输入的二元分类器$P_{\\theta}(r|E_C)$：$$\\alpha = P_{\\theta}(r = 1|E_C)$$ 这个dynamic weight predictor的训练损失采用交叉熵函数：$$L_W(\\theta) = -\\sum_i r_i log P_{\\theta}(r_i|E_C) + (1-r_i)log(1-P_{\\theta}(r_i|E_C))$$ Pre-training and Fine-tuning我们先在大规模的文本语料上预训练一个语言模型，最小化负对数似然函数：$$L_{LM}(\\phi) = -\\sum_ilog P_\\phi(u_i|u_{i-k},…,u_{i-1})$$其中$\\phi$是语言模型的参数，$k$是窗口大小。接着用预训练好的语言模型的参数来初始化transformer模型的encoder和decoder，对于回复生成任务，最优化以下的目标函数：$$L_D(\\phi) = -\\sum_ilogP_{\\phi}(u_i|u_{i-k},…,u_{i-1},E_C,E_T)$$为了把预训练阶段和微调阶段联系起来，在微调阶段，也会在对话语料训练集上最小化语言模型的损失函数。也就是把语言模型的损失函数作为微调阶段的辅助损失函数。则微调阶段，模型总的损失函数为：$$L(\\phi,\\theta) = L_D(\\phi) + \\lambda_1L_{LM}(\\phi) + \\lambda_2L_W(\\theta)$$ 参考链接 https://github.com/silverriver/PersonalDilaog https://github.com/SpiderClub/weibospider","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"},{"name":"Transformer","slug":"Transformer","permalink":"http://yoursite.com/tags/Transformer/"}]},{"title":"论文笔记《Assigning Personality（Profile） to a Chatting Machine for Coherent Conversation Generation》","slug":"论文笔记《Assigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation》","date":"2020-01-05T03:10:26.000Z","updated":"2020-01-05T04:51:42.806Z","comments":true,"path":"2020/01/05/论文笔记《Assigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation》/","link":"","permalink":"http://yoursite.com/2020/01/05/论文笔记《Assigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation》/","excerpt":"【来源】ICJAI2018【链接】https://arxiv.org/abs/1706.02861【数据集】http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality【代码】未公布","text":"【来源】ICJAI2018【链接】https://arxiv.org/abs/1706.02861【数据集】http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality【代码】未公布 这篇论文是清华大学黄民烈教授组的2017年的工作，2018年发表在IJCAI。 论文简介论文的研究内容是赋予对话系统以个性化信息(personality/profile)来生成具有一致性的回复。具体来说，对话语料中用键值对属性值来描述用户画像。对话系统先使用一个profile detector来检测生成回复时是否使用个性化信息。如果要使用，从所有的键值对属性用户画像中选择一个键值对来生成回复。采用一个bidirectional decoder来生成回复，让键值对出现在生成的回复中。进一步地，为了提高bidirectional decoder的性能，采用了position detector来检测键值对在回复中的位置。 模型结构论文提出的对话模型包括了三个重要模块。profile detector检测是否使用用户画像并选择一个键值对属性。bidirectional decoder根据选中的键值对属性来生成回复。position detector检测键值对属性值在回复中出现的位置。 任务定义给定post $X = \\lbrace{x_1,x_2,…,x_n}\\rbrace$以及描述用户个性化信息的键值对属性$\\lbrace{&lt;k_i,v_i&gt;|i=1,2,…,K}\\rbrace$，目标是生成有一致性的回复$Y = \\lbrace{y_1,y_2,…,y_m}\\rbrace$。生成过程可以定义为: $$P(Y|X,\\lbrace&lt;k_i,v_i&gt;\\rbrace) = P(z=0|X) \\cdot P^{fr}(Y|X) + P(z=1|X) \\cdot P^{bi}(Y|X,\\lbrace&lt;k_i,v_i&gt;\\rbrace)$$ Fig.1. 模型结构的示意图 encoder采用GRU将post $X = \\lbrace{x_1,x_2,…,x_n}\\rbrace$编码为$\\lbrace{h_1,h_2,…,h_n}\\rbrace$。GRU的更新公式如下：$$h_t = GRU(h_{t-1},x_t)$$ profile detector作用有两个。一是检测是否使用profile，二是如果要使用，选择一个特定的键值对属性$&lt;key,value&gt;$来生成回复。第一个作用，是否使用profile是在有标签数据上训练的二元分类器$P(z|X)$，计算方式如下：$$P(z|X) = P(z|\\tilde{h}) = \\sigma(W_p\\tilde{h})$$ $$\\tilde{h} = \\sum_{j}h_j$$ 第二个作用，选择一个特定的键值对属性。生成一个在所有键值对上的概率分布：$$\\beta_i=MLP([\\tilde{h},k_i,v_i])=softmax(W\\cdot [\\tilde{h},k_i,v_i])$$其中$W$是可训练的模型参数，$\\tilde{h}$是post的表示。则取概率最大的键值对作为选中的键值对：$$\\tilde{v} = argmax_i(\\beta_i)$$ 进一步地，bidirectional decoder的解码过程可以定义为：$$P^{bi}(Y|X,&lt;\\lbrace{k_i,v_i}\\rbrace&gt;) = P^{bi}(Y|X,\\tilde{v})$$ bidirectional decoder让选中的键值对属性值$\\tilde{v}$出现在生成的回复中：$Y = (Y^b,\\tilde{v},Y^f) = (y_1^b,…,y_{t-1}^b,\\tilde{v},y_{t+1}^f,…,y_m^f)$。backward deocder生成$Y^b$，forward decoder生成$Y^f$。 解码过程可以定义为$$P^{bi}(Y|X,\\tilde{v}) = P^{b}(Y^b|X,\\tilde{v}) * P^f(Y^f|Y^b,X,\\tilde{v})$$ $$P^{b}(Y^b|X,\\tilde{v}) = \\prod_{j=t-1}^{1} P^b(y^b_j|y^b_{&lt;j},X,\\tilde{v})$$ $$P^{f}(Y^f|Y^b,X,\\tilde{v}) = \\prod_{j=t+1}^{m}P^f(y_j^f|y_{&lt;j}^f,Y^b,X,\\tilde{v})$$ 具体地，生成一个word的概率分布为：$$P^b(y^b_j|y^b_{&lt;j},X,\\tilde{v}) \\propto MLP([s_j^b;y^b_{j+1};c_j^b])$$ $$P^f(y_j^f|y_{&lt;j}^f,Y^b,X,\\tilde{v}) \\propto MLP([s_j^f;y_{j-1}^f;c_j^f])$$ 注意区分backWard decoder与forward decoder的差别，分别是逆序和顺序的，一个输入的是$y^b_{j+1}$，而另一个是$y^f_{j-1}$。$s_j^{*},{*} \\in \\lbrace{f,b}\\rbrace$ 是相应decoder的隐藏状态，$c_j^{*}$为相应的context vector。更新方式如下：$$s_j^{*} = GRU(s_{j+l}^{*}, [y_{j+l}^{*},c_j^{*}])$$ $$c_j^* = \\sum_{t=1}^n\\alpha_{j,t}^{*}h_t$$ $$\\alpha_{j,t} \\propto MLP([s_{j+l}^{*},h_t])$$ 对于backWard decoder，有$* = b,l = 1$，而对于forward decoder，有$* = f,l = -1$。 position detector为了检测属性值$v$在回复中出现的位置，在训练阶段，检测哪个词可以被profile中的属性值代替。也就是估计概率：$P(j|y_1y_2…y_m,&lt;k,v&gt;),j\\in[1,m]$，这个概率分布的意义是词$y_j$可以被属性值$v$替换的可能性大小。这里采用最大化$y_j$与属性值$v$之间的余弦相似度：$$P(j|Y,&lt;k,v&gt;) \\propto cos({y_j},{v})$$","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"},{"name":"Bidirectional Decoder","slug":"Bidirectional-Decoder","permalink":"http://yoursite.com/tags/Bidirectional-Decoder/"}]},{"title":"论文笔记《Learning Personalized End-to-End Goal-Oriented Dialog》","slug":"论文笔记《Learning-Personalized-End-to-End-Goal-Oriented-Dialog》","date":"2020-01-03T01:57:32.000Z","updated":"2020-01-03T04:08:45.359Z","comments":true,"path":"2020/01/03/论文笔记《Learning-Personalized-End-to-End-Goal-Oriented-Dialog》/","link":"","permalink":"http://yoursite.com/2020/01/03/论文笔记《Learning-Personalized-End-to-End-Goal-Oriented-Dialog》/","excerpt":"【来源】AAAI2019【链接】https://arxiv.org/pdf/1811.04604v1.pdf【代码】未公布","text":"【来源】AAAI2019【链接】https://arxiv.org/pdf/1811.04604v1.pdf【代码】未公布 这篇论文是北京大学于2019年初发表的。研究内容主要是将用户个性化信息(personalization)结合到端到端的任务型对话模型中，来调整回复的策略和语言风格，并消除歧义。 个性化对话系统的相关工作最早探索个性化对话系统的研究工作是 Li Jiwei于16年发表的论文《A Persona-Based Neural Conversation Model》。这个工作的具体来说是，给chatbot agent赋予特定的人格来生成一致性的回复。另一个思路，我们更希望chatbot agent可以感知到用户的身份和偏好，来提供个性化的对话。个性化的对话系统也是分为闲聊式对话和任务型对话。 个性化闲聊式对话的训练语料有Persona-Chat和在此基础上扩展得到的CONVAI2，另外还有Mazare et al.(2018)基于Reddit Corpus构建的个性化对话语料。 个性化的任务型对话系统的训练语料有personalized bAbI dialog corpus。 缺乏个性化的任务型对话系统的不足只基于对话历史而未考虑个性化的任务型对话系统存在以下不足： 不能根据用户的身份和偏好来动态地调整语言风格。 缺乏根据用户的信息来动态调整对话策略的能力。 难以处理用户请求中的歧义项。比如：“contact”可以理解为“电话”，也可以理解为“社交媒体”。个性化模型可以学习到年轻人倾向于社交媒体，而老年人倾向于电话这一事实，从而消除歧义。 个性化任务型对话系统的研究动机就是解决上述问题。个性化的任务型对话与chatbot有所区别。个性化的chatbot研究倾向于赋予chatbot以特定的用户画像，来生成一致性的回复。而个性化的任务型对话更多的是感知到用户的身份和偏好，从而根据不同的用户身份来调整回复的语言风格和对话策略，从而提高对话效率和用户满意度。 End-to-End Memory Network本文的个性化任务型对话模型是基于Memory Network的，因此对Memory Network做简单的介绍。Memory Network的相关工作有许多，本文中介绍的是发表在ICLR2017的《Learning end-to-end goal-oriented dialog》，这是基于检索的任务型对话。Memory Network包括了两个部分：context memory和next sentence prediction。 Memory Representationmemory中储存的是对话历史。在时间步t，对话历史为$t$句用户的对话$\\lbrace{c_1^u,c_2^u,…,c_t^u}\\rbrace$以及$t-1$句对话系统的回复$\\lbrace{u_1^r,u_2^r,…,u_{t-1}^r}\\rbrace$。直接采用了词袋方法，经过embedding层，将对话历史表示为向量。 $$m = (A\\Phi(c_1^u),A\\Phi(c_1^r),…,A\\Phi(c_{t-1}^u),A\\Phi(c_{t-1}^r))$$ 其中$\\Phi(\\cdot)$是one-hot向量表示，$A$是embedding table。用同样的方法，将上一句话$c_t^u$编码为attention机制的的query：$$q = A\\Phi(c_t^u)$$ Memory Operation采用attention机制对context memory进行读取。计算方式如下：$$o = R\\sum_i\\alpha_im_i$$ $$\\alpha_i = softmax(q^\\top m_i)$$ 其中$R$是线性层的权重矩阵。多跳机制将query按下式进行更新，再采用attention机制对context memory进行读取。$$q_2 = q + o$$ next sentence prediction设有C个候选句子$y_i$。先将候选句子进行向量化表示: $$r_i = W\\Phi(y_i)$$ 其中$W$是另一个embedding table。则最终的预测概率分布为：$$\\hat{r} = softmax({q_{N+1}}^\\top r_1,…,{q_{N+1}}^\\top r_C)$$ 模型结构模型包括profile model和preference model。profile model将用户的个性化信息结合到模型中。preference model建模用户信息与知识库之间的联系，来消除歧义。 Fig.1. 模型结构的示意图 profile modeluser embedding的计算对话语料中用$n$个键值对属性$\\lbrace(k_i,v_i)\\rbrace_{i=1}^n$来描述用户画像。第$i$个属性被表示为one-hot vector $a_i \\in R^{d_i}$，其中$d_i$表示第$i$个属性$k_i$可能的取值个数。则总的用户画像的one-hot表示为$$\\hat{a} = concat(a_1,a_2,…,a_n)$$ 有$\\hat{a}\\in R^{d_p}$，其中$d_p = \\sum_{i=1}^nd_i$进一步将用户画像表示为分布式向量$$p = P\\hat{a}$$ 其中$P$可以看作是embedding table。 将$p$结合到对话模型将user embedding $p$结合到模型中的两个地方。 结合到多跳机制的query更新公式中。query对于context memory的读取和预测概率的生成起着重要作用。 $$q_{i+1} = q_i + o_i + p$$ 根据user embedding对候选句子的向量表示进行修改。 $$r_i^* = \\sigma(p^\\top r_i) r_i$$ global memory除了context memory外，模型还有一个memory用来储存相似用户的对话历史。本文中将相似用户定义为有相同的属性值的用户。global memory的读取方式与context memory的读取方式相同。最后将两部分的query结合起来：$$q^+ = q + q^g$$ 参考链接 本文作者的个人主页 作者的英文论文简介 作者的中文论文简介 本篇论文的阅读笔记","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Memory Network","slug":"Memory-Network","permalink":"http://yoursite.com/tags/Memory-Network/"},{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"},{"name":"Goal-Oriented Dialog","slug":"Goal-Oriented-Dialog","permalink":"http://yoursite.com/tags/Goal-Oriented-Dialog/"}]},{"title":"论文笔记《A Dynamic Speaker Model for Conversational Interactions》","slug":"论文笔记《A-Dynamic-Speaker-Model-for-Conversational-Interactions》","date":"2020-01-02T12:31:22.000Z","updated":"2020-01-02T14:08:56.015Z","comments":true,"path":"2020/01/02/论文笔记《A-Dynamic-Speaker-Model-for-Conversational-Interactions》/","link":"","permalink":"http://yoursite.com/2020/01/02/论文笔记《A-Dynamic-Speaker-Model-for-Conversational-Interactions》/","excerpt":"【来源】：NAACL2019【链接】：https://www.aclweb.org/anthology/N19-1284/【代码】：https://github.com/hao-cheng/dynamic_speaker_model","text":"【来源】：NAACL2019【链接】：https://www.aclweb.org/anthology/N19-1284/【代码】：https://github.com/hao-cheng/dynamic_speaker_model 这篇论文是由华盛顿大学发表的。 个性化对话系统的研究现状近几年来，基于用户画像（personal information）来生成个性化的回复已经成为对话系统领域的一个研究热点。为什么要将persona结合到对话系统模型中呢？目的是提高生成回复的一致性，来获取用户信任，让用户在对话中更投入。生成回复的一致性具体指什么呢？举个例子，你问对话系统“你的职业是什么？”，它可能回答是出租车司机；当你再次问这个问题时，它又可能回答是老师。给定对话系统一个用户画像（persona），基于这个persona来生成回复，就可以提高生成回复的一致性，避免出现这种问题。基于persona来生成个性化的回复，也是对话系统可以通过图灵测试的必要条件。 现有的工作中，将persona结合到对话系统模型中的思路有两种。 学习一个潜在的向量user embedding（或user representation）来潜在地表示用户画像，再基于这个user embedding来生成个性化的回复。这样做的一个原因是现有的对话语料中没有相应的用户画像文本信息，随着个性化对话数据集Persona-Chat以及CONVAI2的提出，就有了第二种思路。 直接用键值对属性值信息或者是自由文本来明确地描述用户画像，再生成个性化的回复。 论文简介这篇论文属于第一种思路，主要内容是用神经网络模型从对话历史中学习一个动态更新的user embedding。并且将学习到的user embedding用到了两个下游任务（对话话题分类、dialog acts分类）中来评估user embedding的学习效果。论文中提到，学习一个动态更新的user embedding的动机有两个： 用户的对话反映了这个用户的对话意图、语言风格等特征。因此，可以从用户的对话中来学习user embedding来建模和表征用户的这些个性化特征。 随着对话的进行，用户的个人信息得到累积。因此，可以从对话中提炼和动态更新user embedding。 模型结构如下图所示，模型包括三个部分：Latent Mode Analyzer，Speaker State Tracker，Speaker Language Predictor。 Fig.1. 模型的整体框架图 Latent Mode AnalyzerLatent Mode Analyzer的作用是从输入的单轮对话中学到一个 local speaker vector。采用了Bi-LSTM + Attention机制。时间步t，输入为单轮对话$\\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}\\rbrace}$，经过embedding层后送入到Bi-LSTM层，将前向LSTM和后向LSTM的最后一个隐藏状态连接起来作为句子总的向量表示$s_t$: $$s_t=[e^F_{t,N_t},e^B_{t,1}]$$ 其中，$e^F_{t,N_t},e^B_{t,1}$分别表示前向LSTM和后向LSTM的最后一个隐藏状态。接着，再使用attention机制。将句子的向量表示作为attention机制的query，将 $K$ 个全局的mode vectors $\\lbrace{u_1,u_2,…,u_K\\rbrace}$作为attention机制的keys和values。这$K$个mode vectors也是模型参数，可以看作是用户在不同方面的个性化特征。那么可以通过attention机制计算得到local speaker vector $\\tilde{u_t}$ :$$\\tilde{u_t} = \\sum_{k=1}^Ka_{t,k}u_k$$ $$a_{t,k} = softmax(\\beta_{t,k})$$ $$\\beta_{t,k} = &lt;Ps_t,Qu_k&gt;$$ 其中$&lt; , &gt;$表示点乘操作。 Speaker State TrackerSpeaker State Tracker 的作用是动态更新speaker state vector。实质上是一个单向的LSTM。在时间步t，根据当前的local speaker vector $\\tilde{u_t}$ 和 上一个时间步的隐藏状态$h_{t-1}$来更新隐藏状态$h_t$。将$h_t$作为时间步t的speaker state vector。$$h_t = LSTM(\\tilde{u_t},h_{t-1})$$ Speaker Language PredictorSpeaker Language Predictor的作用是促进前两个模块的参数学习，根据speaker state vector来重构句子$\\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}}\\rbrace$。该模块实质上是一个条件语言模型，预测条件概率 $P(w_{t,n}|w_{t,&lt;n})$。采用了单向的LSTM，LSTM的隐藏状态更新公式为：$$d_{t,n} = LSTM(R^I(w_{t,n-1},h_t),d_{t,n-1})$$ 其中$R^I()$是一个线性层。则语言模型生成下一个词的条件概率为：$$P(w_{t,n}|w_{t,&lt;n}) = softmax(VR^O(h_t,d_{t,n}))$$ 其中$R^O()$是一个线性层。 损失函数模型最小化负对数似然函数来更新模型参数: $$L = -\\sum_{t}\\sum_{n}log P(w_{t,n}|w_{t,&lt;n})$$","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"}]},{"title":"论文笔记《Towards Knowledge-Based Personalized Product Description Generation in E-commerce》","slug":"论文笔记《Towards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce》","date":"2019-10-25T10:52:24.000Z","updated":"2019-10-25T14:49:12.478Z","comments":true,"path":"2019/10/25/论文笔记《Towards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce》/","link":"","permalink":"http://yoursite.com/2019/10/25/论文笔记《Towards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce》/","excerpt":"【来源】：ARXIV 2019【链接】：https://arxiv.org/abs/1903.12457v3【代码、数据集】：https://github.com/THUDM/KOBE","text":"【来源】：ARXIV 2019【链接】：https://arxiv.org/abs/1903.12457v3【代码、数据集】：https://github.com/THUDM/KOBE 清华大学和阿里巴巴发表的论文。论文的研究内容是给定商品名称，商品的属性特征和外部知识库，自动生成商品的描述。 数据集描述论文在淘宝收集了一个真实的商品描述数据集，包含了212,9187个商品名称和描述。数据集是公开的，下载地址为：https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717 任务定义给定商品名称$x = \\{x_1,…,x_n\\}$，要求生成个性化的、富有信息的商品描述$y = \\{y_1,…,y_m\\}$。引入两个附加信息：商品属性 和 外部知识： Attributes 每个商品名称$x$对应$l$个属性$a = \\{a_1,…,a_l\\}$。论文中包含两种属性，商品的某个方面（如质量、外观等）和用户类型（反映用户的兴趣）。 Knowledge 论文采用一个大规模的中文知识图谱CN-DBpedia作为外部knowledge。CN-DBpedia包含大量命名实体$V$索引的原始文本条目$W$。每个条目包含一个命名实体$v &ensp; \\in V$作为key，对应一个knowledge句子 $w = \\{w_1,…,w_u\\} \\in W$作为value。 最终的任务定义为：给定商品名称$x$、商品属性$a$和相关的knowledge $w$，要求生成个性化的，信息量丰富的回复$y$。 模型结构模型采用了基于Transformer的encoder-decoder框架，结合了两个模块：Attribute Fusion和knowledge Incorporation。 Fig.1. 模型结构的示意图 encoder-decoder框架Transformer是完全基于self-attention机制 和 前馈神经网络（FFN,feed-forward neural network）的。 encoder 对于输入$x = \\{x_1,…,x_n\\}$，先经过embedding层得到word embedding representation $e = \\{e_1,…,e_n\\}$，连同position embedding一起，作为encoder layers的输入，得到context representation $h = \\{h_1,…,h_n\\}$。 在embedding层之上，encoder layers由完全相同的6层堆叠组成，每层transformer包括multi-head self-attention和FFN两部分。 attention机制 attention机制根据queries $Q$和keys $K$计算在values $V$上的分布，进而得到attention的输出。$$C = \\alpha V$$ $$\\alpha = softmax(f(Q,K))$$其中$C$表示attention的输出，$\\alpha$表示attention的分布，$f$表示计算attention分数的函数。 uni-head attention 对于单头attention，queries $Q$，keys $K$和values $V$分别是输入context $e$的线性转换。即$Q = W_Qe$, $K = W_Ke$ 和 $V = W_Ve$。此时，uni-head attention可以表示为 $$C_{self} = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$ 其中$d_k$表示输入$e$的维度。 multi-head attention 对于多头attention，将$C^i_{self}, i\\in \\{1,2,…,c\\}$连接起来，作为FFN的输入。其中$c$表示heads的数量。再经过前馈神经网络（FFN），FFN的函数表示为：$$FFN(z) = W_2(Relu(W_1z + b_1)) + b_2$$ decoder 与encoder类似，decoder也是由完全相同的6层堆叠而成的，每层包含multi-head attention和FFN两部分。不同于encoder的”self-attenion”，decoder的multi-head attention是“context attention”。queries $Q$是decoder state的线性转换，keys $K$和values $V$是context states $h = \\{h_1,…,h_n\\}$的线性转换。 Training 模型的目标是最大化似然函数。模型的目标函数是：$$P(y|x) = \\prod_{t=1}^m P(y_t|y_{&lt;t},x)\\tag{1}$$ Attribute Fusion模块商品的属性$a = \\{a_1,a_2\\}$，包含商品方面 和 用户类型两个属性。先经过embedding层得到attribute representation $\\{e_{a_1},e_{a_2}\\}$，再做attribute average得到总的attribute representation $e_{attr}$： $$e_{attr} = \\frac{1}{2}\\sum_{i=1}^2e_{a_i}$$ 如何有效结合$e_{attr}$呢？在基于RNN的模型中，方法比较多，比如：用$e_{attr}$来attend context representation，或着作为decoder隐藏状态更新的输入等。但本文中的模型是基于Transformer的，直接将attribute embedding $e_{attr}$与word embedding $e_i$相加，来结合商品的属性信息。如Fig.2所示。 Fig.2. Attribute Fusion的示意图 此时，公式（1）中的目标函数变为$$ P(y|x,a) = \\prod_{t=1}^m P(y_t|y_{&lt;t},x,a)$$ Knowledge Incorporationknowledge incorporation包括三个部分，knowledge检索，knowledge编码，和knowledge结合。 knowledge retrieval 给定商品名称$x = \\{x_1,…,x_n\\}$，对于每个word $x_i$匹配对应的命名实体$v_i \\in V$。再根据命名实体$v_i$，从$W$中检索对应的knowledge $w_i$。对于每个商品，最多抽取5个匹配的knowledge，再用分隔符 “”连接起来。 knowledge encoding 类似于$x = \\{x_1,…,x_n\\}$通过encoder编码得到context representation $h = \\{h_1,….,h_n\\}$，将检索到的knowledge经过一个基于Transformer的knowledge encoder得到knowledge representation $u$。 knowledge combination 用BiDAF(bidirectional attention flow)来结合context representation $h$和knowledge representation $u$。BiDAF计算两个方向的attention：title-to-knowledge attention和knowledge-to-context attention。 相似度矩阵S 先计算一个context representation $h \\in R^{n \\times d}$和knowledge representation $u \\in R^{u \\times d}$之间的相似度矩阵$S \\in R^{n\\times u}$。其中$S_{ij}$衡量第i个title word和第j个knowledge word之间的相似度。$$S_{ij} = \\alpha(h_i,u_j) \\in R$$ 其中$\\alpha()$是计算两个向量之间相似度的函数。 $$\\alpha(h,u) = W_s^T[h;u;h \\cdot u], &emsp; W_s\\in R^{3d}$$ title-to-knowledge attention 表明了对于每个title word，哪个knowledge word是最相关的。 $a_i \\in R^u$表示对于第i个title word，在所有knowledge words上的attention权重分布。$$a_i = softmax(S_{i:}) &emsp; \\in R^u$$ 其中，$S_{i:}$表示相似度矩阵$S \\in R^{n\\times u}$的第i个行向量。对于所有的$i$，$a_i$满足：$$\\sum_ja_{ij} = 1$$ 对于第i个title word，attended knowledge vector为：$$\\widetilde{u_i} = \\sum_ja_{ij}u_j$$ 则对于所有的title words $\\{x_1,…,x_n\\}$，有$\\widetilde{u} \\in R^{n \\times d}$ knowledge-to-title attention 表明了对于每个knowledge word，哪个title word是最相似的。 计算在所有title words上的attention权重分布：$$b = softmax(max(S_{i:}))$$ 则attended title vector为$$\\widetilde{h} = \\sum_{k}b_kh_k$$ 把$\\widetilde{h}$在列上复制n次，得到$\\widetilde{h} \\in R^{n\\times d}$。 输出融合 做一个简单的连接操作，得到组合representation： $[h;\\widetilde{u};h\\circ \\widetilde{u};h\\circ \\widetilde{h}] \\in R^{4d \\times n}$ 参考链接 BiDAF(bidirectional attention flow)可以参考: 论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》 中文知识图谱CN-DBpedia可以参考： 《CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System》 CN-DBpedia Dump数据下载","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"},{"name":"BiDAF","slug":"BiDAF","permalink":"http://yoursite.com/tags/BiDAF/"},{"name":"Knowledge-Based","slug":"Knowledge-Based","permalink":"http://yoursite.com/tags/Knowledge-Based/"}]},{"title":"论文笔记《Automatic Generation of Personalized Comment Based on User Profile》","slug":"论文笔记《Automatic-Generation-of-Personalized-Comment-Based-on-User-Profile》","date":"2019-10-25T07:15:24.000Z","updated":"2019-10-25T09:09:29.504Z","comments":true,"path":"2019/10/25/论文笔记《Automatic-Generation-of-Personalized-Comment-Based-on-User-Profile》/","link":"","permalink":"http://yoursite.com/2019/10/25/论文笔记《Automatic-Generation-of-Personalized-Comment-Based-on-User-Profile》/","excerpt":"【来源】：ACL2019【链接】：https://arxiv.org/abs/1907.10371【代码、数据集】：https://github.com/Walleclipse/AGPC","text":"【来源】：ACL2019【链接】：https://arxiv.org/abs/1907.10371【代码、数据集】：https://github.com/Walleclipse/AGPC 北京大学发表在ACL2019的论文。论文的研究内容是基于User profile的评论生成。 数据集描述论文从微博收集了一个中文数据集，没有公开，但给出了部分样例数据。这个数据集可以看作基于persona的单轮对话数据集。将微博的博文看作对话历史，将用户的评论看作回复。用户画像包括两个部分，键值对形式的人口统计特征属性（年龄、性别、地区等）和 句子形式的个人描述（微博签名）。 Fig.1. 数据样例 任务定义给定博文 $X = \\lbrace{x_1,…,x_n}\\rbrace$和用户画像 $U = \\lbrace{F,D}\\rbrace$，其中$F = \\lbrace{f_1,…,f_k}\\rbrace$是用户的数值化属性特征，$D = \\lbrace{d_1,…,d_l}\\rbrace$是句子形式的个人描述。要求生成与personal profile一致的回复$Y = \\lbrace{y_1,…,y_m}\\rbrace$。$$Y^* = \\underset{Y}{argmax}(Y|X,U)$$ 模型结构 Fig.2.模型结构的示意图 encoder-decoder框架基本框架当然还是seq2seq模型 + attention机制。$X= \\{x_1,…,x_n\\}$经过encoder转换为向量表示$h^X = \\{h^X_1,…,h_n^X\\}$，encoder采用Bi-LSTM。$$h_t^X = LSTM_{enc}^X(h_{t-1}^X,x_t)$$ decoder采用单向LSTM，decoder的隐藏状态更新公式为：$$s_t = LSTM_{dec}(s_{t-1},[c_t^X;e(y_{t-1})]) \\tag{1}$$其中$c_t^X$表示时间步t，在所有encoder hidden states $h^X = \\{h^X_1,…,h_n^X\\}$上使用attention得到的context vector。则decoder生成词的概率分布为：$$p(y_t) = softmax(W_os_t)$$ 采用负对数似然函数作为目标函数，模型最大化真实回复 $Y^* = \\{y_1,…,y_m\\}$ 的似然函数：$$Loss = -\\sum_{t=1}^m log\\Bigl(p(y_t|y_{&lt;t},X,U)\\Bigr)$$ User Feature Embedding with Gated Memory将用户的数值化特征属性$F = \\{f_1,…,f_k\\}$经过一个全连接层，得到向量表示$v_u$。$v_u$可以看作是user feature embedding，表明用户的个人特征。如果user feature embedding是静态的，在decode过程中会影响生成回复的语法性。为了解决这个问题，设计了一个gated memory来动态地表达用户的个人特征。在decode过程中，保持一个Internal personal state$M_t$，在decode过程中$M_t$逐渐衰减，decode结束，$M_t$衰减为0，表示用户的个人特征完全表达了。$M_0$的初始值设为$v_u$。 $$g_t^u = sigmoid(W_g^us_t)$$ $$M_0 = v_u$$ $$M_t = g_t^u \\cdot M_{t-1}, &emsp; t&gt;0$$ 引入输出门机制$g_t^o$来充值persona信息的流动：$$g_t^o = sigmoid(W_g^o[s_{t-1};e(y_{t-1});c_t^X])$$ 则时间步t，personal information为：$$M_t^o = g_t^o\\cdot M_t$$ Blog-User Co-Attention实质上是在用户的个人描述$D = \\{d_1,…,d_l\\}$上使用attention机制。先用另一个persona encoder来编码$D = \\{d_1,…,d_l\\}$，得到向量表示$\\{h_1^D,…,h_l^D\\}$。persona encoder采用LSTM: $$h_t^D = LSTM_{enc}^D(h_{t-1}^D,d_t)$$ 在$\\{h_1^D,…,h_l^D\\}$上使用attention机制，得到总的persona context vector $c_t^D$ $$c_t^D = \\sum_{j=1}^k\\alpha_{tj}h_j^D$$ $$\\alpha_{tj} = softmax(\\beta_{tj})$$ $$\\beta_{tj} = score(s_{t-1},h_j^D) = s_{t-1}W_ah_j^D$$ 结合$c_t^D$和$c_t^X$作为时间步t总的context vector $c_t$: $$c_t = [c_t^X,c_t^D]$$ 则式（1）中decoder的隐藏状态更新公式变为：$$s_t = LSTM_{dec}(s_{t-1},[c_t;e(y_{t-1});M_t^o])$$ External Personal Expression通过将internal persona state$M_t$和persona context vector $c_t^D$作为decoder隐藏状态更新的输入，来结合persona信息，进而影响decode过程。这种影响是隐性的，为了更明确地利用用户信息来指导word的生成，将用户信息直接作为输出层的输入。先计算一个user representation $r_t^u$: $$r_t^u = W_r[v_u;c_t^D]$$ 将$r_t^u$作为输出层的输入，则生成词的概率分布为：$$p(y_t) = softmax(W_o[s_t;r_t^u])$$ 相似论文 《Personalized Dialogue Generation with Diversified Traits》 笔记链接 异同点比较： 不同点是：本篇论文用internal persona state $M_t$和personal context vector $c_t^D$来作为decoder隐藏状态$s_t$更新的输入，进而影响word的生成。而相似的这篇论文中，将personal vector $v_p$ 作为attention机制的query，来attend对话历史。含义是用persona vector $v_p$来选择context相关的信息。 相同点是：两篇论文都把persona information 作为输出层的输入，来明确地影响word的生成。","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"}]},{"title":"论文笔记《Personalized Dialogue Generation with Diversified Traits》","slug":"论文笔记《Personalized-Dialogue-Generation-with-Diversified-Traits》","date":"2019-10-25T02:51:12.000Z","updated":"2019-10-25T06:33:49.351Z","comments":true,"path":"2019/10/25/论文笔记《Personalized-Dialogue-Generation-with-Diversified-Traits》/","link":"","permalink":"http://yoursite.com/2019/10/25/论文笔记《Personalized-Dialogue-Generation-with-Diversified-Traits》/","excerpt":"【链接】：https://arxiv.org/abs/1901.09672【代码、数据集】：无","text":"【链接】：https://arxiv.org/abs/1901.09672【代码、数据集】：无 三星电子中国、清华大学黄明烈教授提交到ARXIV的论文。论文的研究内容是基于persona的单轮对话系统。数据集描述论文中构建了一个PersonaDialog的数据集，但数据集没有公开。数据是从微博上爬取，把用户的博文作为对话的post，把用户的评论作为回复。persona是用键值对来描述用户的属性（年龄、性别、地址、兴趣标签等）。而不是用几句话的文本来描述用户画像的。 任务定义给定单句的对话历史$X = \\lbrace{x_1,…,x_n\\rbrace}$，以及回复者的N个属性$T = \\lbrace{t_1,…,t_N\\rbrace}$，其中$t_i = &ensp; &lt;k_i,v_i&gt;$为键值对。要求生成与用户的画像相一致的回复$Y = \\lbrace{y_1,y_2,…,y_m\\rbrace}$。$$Y^* = \\underset{Y}{arg max} P(Y|X,T)$$ 模型结构基本框架当然是seq2seq模型+attention机制。将对话历史$X = \\lbrace{x_1,…,x_n}\\rbrace$经过encoder编码为$\\lbrace{h_1,…,h_n}\\rbrace$。设时间步t，decoder的上一个隐藏状态为$s_{t-1}$，计算attention得到总的context vector $c_t$：$$c_t = \\sum_{i=1}^n\\alpha_ih_i$$ $$alpha_i = softmax(\\beta_i)$$ $$\\beta_i = score(s_{t-1},h_i) = V^T\\cdot tanh(W^1_{\\alpha}s_{t-1} + W^2_{\\alpha}h_i) \\tag{1}$$decoder RNN的隐藏状态更新公式是：$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 生成$y_t$的概率分布为：$$p(y_t) = softmax(W_os_t + b_{out})$$ Fig.1. 模型结构的示意图 Personality Trait Fusion这个模块用来把整合persona得到persona representation $v_p$，并用$v_p$来影响decode过程。将回复者的属性描述$T = \\lbrace{t_1,…,t_N}\\rbrace$经过embedding层，得到对应的向量表示$\\lbrace{v_{t_1},…,v_{t_N}}\\rbrace$。论文提出了三种整合persona Trait的方法。 Trait Attention计算decoder的隐藏状态$s_{t-1}$在Trait representation $\\lbrace{v_{t_1},…,v_{t_N}}\\rbrace$上的attention，来整合persona Trait，得到persona representation $v_p$。$$v_p = \\sum_{i=1}^N\\alpha_i^pv_{t_i}$$ $$\\alpha_i^p = softmax(\\beta_i^p)$$ $$\\beta_i^p = score(s_{t-1},v_{t_i}) = V_p^T \\cdot tanh(W_p^1s_{t-1} + W_p^2v_{t_i})$$ Trait Average直接在Trait representation $\\lbrace{v_{t_1},…,v_{t_N}}\\rbrace$上取平均值，来作为persona representation $v_p$：$$v_p = \\frac{1}{N}\\sum_{i=1}^Nv_{t_i}$$ Trait Concatenation直接把Trait representation $\\lbrace{v_{t_1},…,v_{t_N}}\\rbrace$做连接操作，作为persona representation $v_p$。 使用persona representation $v_p$进行decodePersona-Aware Attention让persona representation $v_p$来影响式（1）中attention权重的计算。这种方法可以获得基于persona representation $v_p$的context vector $c_t$。$$\\beta_i = f(s_{t-1},h_i,v_p) = V^T \\cdot tanh(W^1_{\\alpha}s_{t-1} + W^2_{\\alpha}h_i + W^3_{\\alpha}v_p)$$ Persona-Aware Bias在输出层结合$v_p$，来影响decode过程。$$p(y_t) = softmax(a_t\\cdot W_o^1s_t + (1-s_t)\\cdot W_o^2v_p + b_{out})$$ $$a_t = \\sigma(V_o^T\\cdot s_t)$$ 其中$a_t\\in [0,1]$作为一个门机制，来控制生成persona相关的词，或者语义相关的词。","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Personalization","slug":"Personalization","permalink":"http://yoursite.com/tags/Personalization/"}]},{"title":"论文笔记《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》","slug":"论文笔记《DEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks》","date":"2019-10-24T08:19:48.000Z","updated":"2019-10-25T03:08:22.933Z","comments":true,"path":"2019/10/24/论文笔记《DEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks》/","link":"","permalink":"http://yoursite.com/2019/10/24/论文笔记《DEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks》/","excerpt":"【链接】：https://arxiv.org/abs/1908.10731【代码、数据集】：无","text":"【链接】：https://arxiv.org/abs/1908.10731【代码、数据集】：无 论文的研究内容是conditional text generation，基于knowledge facts的单轮对话。基于给定的对话历史和外部知识，生成合适的回复。论文中将K句话的个人描述作为knowledge facts。论文采用的模型是seq2seq模型 + attention机制 + 分级pointer network。论文的亮点是对比模型的思路，可以重点学习一下如何设计对比模型。 任务定义输入分为两部分：对话历史$X = (x_1,…,x_n)$和K个相关的knowledge facts，其中第i个knowledge fact为$f^i = (f^i_1,…,f^i_{n_i}),i\\in \\lbrack 1,K \\rbrack$。要求生成输出$Y = (y_1,…,y_m)$。 模型baseline: seq2seq模型 + attention机制seq2seq模型是基于encoder-decoder框架的。encoder包括embedding层和LSTM层，对于输入$X$，经过encoder得到相应的向量表示 $\\lbrace{h_1,…,h_n}\\rbrace$。decoder采用单向LSTM，设时间步t的隐藏状态为$s_t$，隐藏状态更新公式为：$$s_t = f(s_{t-1},y_t,c_t)$$ 其中$s_{t-1}$是上一个时间步decoder的隐藏状态，$y_{t-1}$是上一个时间步的输出。c_t为用attention机制计算得到的context vector。计算过程如下：$$c_t = \\sum_{i=1}^{n}\\alpha_ih_i$$ $$\\alpha_i = softmax(\\beta_i)$$ $$\\beta_i = score(s_{t-1},h_i)$$ 其中$score(s,h)$是计算$s$和$h$之间相似度的函数。在时间步t，decoder的隐藏状态$s_t$和对应的context vector $c_t$，经过线性层和softmax层，得到在固定词汇表上的概率分布。$$p_g(y_t) = softmax(W[h_t,c_t] + b)$$ 可以看出“seq2seq模型 + attention机制”可以用来完成”text-to-text”的text generation的任务。输入是text，没有其他的附加信息（比如knowledge，persona，context等），输出也是text。根据输入的不同，可以得到以下三个模型。 SEQ2SEQ + NOFACT 只把对话历史$X$作为encoder的输入。 SEQ2SEQ + BESTFACTCONTEXT 先从K个knowledge facts $\\lbrace{f^1,…,f^K}\\rbrace$中选择与dialog context $X$最相似的fact $f^c$ 再将$f^c$与dialog context $X$连接起来的$[X;f^c]$，作为encoder的输入。 SEQ2SEQ + BESTFACTCONTEXT 先从K个knowledge facts $\\lbrace{f^1,…,f^K}\\rbrace$中选择与truth response $Y$最相似的fact $f^r$ 再将$f^r$与dialog context $X$连接起来的$[X;f^r]$，作为encoder的输入。 从这三个对比试验，可以表明是否添加knowledge fact，以及knowledge fact的不同的选择，对回复生成的影响。 Memory Network“seq2seq模型 + attention机制”容易遇到 generic response的问题，需要添加附加信息作为额外的输入，来得到信息更丰富的回复。与直接把knowledge fact $f$与dialog context $X$的连接$[X;f]$作为encoder的输入不同，用Memory Network可以更好地结合knowledge facts这样的附加信息。 Memory Network的作用是可以更有效地结合knowledge facts，persona description，dialog context这样的附加信息。Memory Network的工作原理可以分成两个部分：Memory representation 和 read Memory。 Memory representation 实质上是对附加信息通过另一个facts encoder的向量化表示。$\\lbrace{f^1,…,f^K}\\rbrace$通过另外一个encoder来编码，经过线性变换分别得到key vectors $\\lbrace{k_1,…,k_K}\\rbrace$和value vectors$\\lbrace{m_1,m_2,…,m_K}\\rbrace$。 read Memory 实质上是计算在$\\lbrace{f^1,…,f^K}\\rbrace$的attention。论文中用context encoder的最后一个隐藏状态$u$作为query，计算得到总的memory representation： $$o = \\sum_{i=1}^K\\alpha_im_i$$ $$\\alpha_i = softmax(\\beta_i)$$ $$\\beta_i = score(u,k^i)$$ 最后把context encoder的最后一个隐藏状态$u$和总的memory representation $o$组合起来，$$\\hat{u} = u + o$$ 接着用$\\hat{u}$来初始化decoder的隐藏状态。 根据是否使用attention机制，可以得到以下四个模型： MEMNET 用Memory Network来结合附加信息knowledge facts，用$\\hat{u}$来初始化decoder的隐藏状态。 实际上相当于没有用attention机制的seq2seq模型。 MEMNET + CONTEXTATTENTION 用Memory Network来结合附加信息knowledge facts，用$\\hat{u}$来初始化decoder的隐藏状态。 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在context encoder的输出context representation $\\lbrace{h_1,…,h_n}\\rbrace$上的attention，得到总的context vector $c_t^{(c)}$。$$c_t^{(c)} = \\sum_{i=1}^{n}\\alpha_ih_i$$ $$\\alpha_i = softmax(\\beta_i)$$ $$\\beta_i = score(s_{t-1},h_i)$$ 将$c_t^{(c)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(c)})$$ MEMNET + FACTATTENTION 用Memory Network来结合附加信息knowledge facts，用$\\hat{u}$来初始化decoder的隐藏状态。 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在facts encoder的输出facts representation $\\lbrace{m_1,…,m_K}\\rbrace$上的attention，得到总的facts vector $c_t^{(f)}$。 $$c_t^{(f)} = \\sum_{i=1}^K\\alpha_im_i$$ $$\\alpha_i = softmax(\\beta_i)$$ $$\\beta_i = score(s_{t-1},k_i)$$ 将$c_t^{(f)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(f)})$$ MEMNET + FULLATTENTION 同时在context representation $\\lbrace{h_1,…,h_n}\\rbrace$和facts representation $\\lbrace{m_1,…,m_K}\\rbrace$上用attention，得到context vector $c_t^{(c)}$和facts vector $c_t^{(f)}$。把二者连接起来，作为decoder隐藏状态更新的输入。$$s_t = f(s_{t-1},y_{t-1},[c_t^{(c)},c_t^{(f)}])$$ seq2seq模型 + copy机制seq2seq模型只能从固定的词汇表中生成word。Pointer Network的作用是可以从source input中来复制word。用Pointer Network来实现copy机制也可以分为两个部分。 计算在所有input tokens $\\lbrace{x_1,…,x_n}\\rbrace$上的attention权重分布，作为在 $\\lbrace{x_1,…,x_n}\\rbrace$上的概率分布。 使用一个“soft switch”机制，在copy模式时，从source input $\\lbrace{x_1,…,x_n}\\rbrace$中复制word；当在generation模式时，从固定的词汇表中生成word。 单纯的“seq2seq模型 + attention机制”，再结合copy机制可以得到以下三种模型： SEQ2SEQ + NOFACT + COPY SEQ2SEQ + BESTFACTCONTEXT + COPY SEQ2SEQ + BESTFACTRESPONSE + COPY 分级Pointer Network单纯的Pointer Network可以从单句话中复制word，使用分级Pointer Network可以从K句话中复制word。 Fig.1. 系统的总体框架图 从dialog context中复制worddialog context $X = \\lbrace{x_1,…,x_n}\\rbrace$经过context encoder后的context representation为$\\lbrace{h_1,…,h_n}\\rbrace$，设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用attention机制：$$c_t^{(x)} = \\sum_{i=1}^n\\alpha_i^{c}h_i$$ $$\\alpha_i^{(x)} = softmax(\\beta_i^{(x)})$$ $$\\beta_i^{(x)} = score(s_{t-1},h_i)$$ 则从dialog context $X = \\lbrace{x_1,…,x_n}\\rbrace$中复制word的概率分布为$$p_{copy}^{(x)}(y_t) = \\alpha_i^{(x)} &emsp; i \\in [1,n]$$ 从knowledge facts中复制word Fig.2. 分级Pointer Network的示意图 knowledge facts包含K个句子$\\lbrace{f^1,…,f^K}\\rbrace$，其中$f^i = \\lbrace{f^i_1,…,f^i_{n_i}}\\rbrace$。经过facts encoder后，再经过线性转换，得到词级别的keys vector和values vector分别为$\\lbrace{k_1^{f^i},…,k_{n_i}^{f^i}}\\rbrace$和$\\lbrace{m_1^{f^i},…,m_{n_i}^{f^i}}\\rbrace$。设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用词级别的attention机制：$$c_t^{f^i} = \\sum_{j=1}^{n_i}\\alpha_j^{f^i}m_j^{f^i} &emsp; i\\in \\lbrack {1,K}\\rbrack $$ $$\\alpha_j^{f^i} = softmax(score(s_{t-1},k_j^{f^i})) &emsp; i\\in [1,K],j\\in [1,n_i]$$ 其中$c_t^{f^i}, i\\in [1,K]$是K个句子$\\lbrace{f^1,…,f^K}\\rbrace$句子级别的向量表示。使用句子级别的attention机制：$$c_t^{(f)} = \\sum_{i=1}^{K} = \\beta_i^fc_t^{f^i}$$ $$\\beta_i^f = softmax(score(s_{t-1},c_t^{f^i})) &emsp; i\\in [1,K]$$ 其中从$c_t^{(f)}$是knowledge facts总的向量表示。则从knowledge facts中复制word的概率分布为：$$p_{copy}^{(f)}(y_t) = \\beta_i^f\\alpha_j^{f^i} &emsp; i\\in [1,K],j\\in [1,n_i]$$ 总的复制word的概率分布为了把复制word的两个概率分布$p_{copy}^{(x)}$和$p_{copy}^{(f)}$结合起来，使用decoder的隐藏状态$s_t$在context representation $c_t^{(x)}$和总的facts representation $c_t^{(f)}$上用attention机制。得到attention权重分布为$\\lbrack{\\gamma,1- \\gamma}\\rbrack$。$$c_t = \\gamma c_t^{(x)} + (1 - \\gamma) c_t^{(f)}$$ $$\\lbrack{\\gamma,1-\\gamma}\\rbrack = softmax(\\lbrack{score(s_{t-1},c_t^{(x)}),score(s_{t-1},c_t^{(f)})\\rbrack})$$其中$c_t$既包含了dialog context的信息，也包含了knowledge facts的信息，可以用来更新decoder的隐藏状态。$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 则总的复制word的概率分布为$$p_{copy}(y_t) = \\gamma \\cdot p_{copy}^{(x)} + (1 - \\gamma) \\cdot p_{copy}^{(f)}$$ soft switchsoft switch可以把copy模式和generation这两种模式结合起来。用一个门机制$p_{gen}$来控制是从固定的词汇表中生成词，或者从dialog context和knowledge facts中复制词。$$p_{gen} = sigmoid(W\\lbrack{s_t,c_t}\\rbrack)$$ $$p(y_t) = p_{gen} \\cdot p_g(y_t) + (1 - p_{gen}) \\cdot p_{copy}(y_t)$$ 损失函数采用负对数似然函数作为优化的目标函数，对于单个word的loss函数为：$$loss(\\Theta) = -log(p(y_t|y_{&lt;t},X,\\lbrace{f^i}\\rbrace_{i=1}^K))$$其中$\\Theta$表示模型所有的可训练参数，$y_{&lt;t}$表示$y_t$之前所有的word。则对于一个训练样本的loss函数为：$$J_{loss}(\\Theta) = -\\frac{1}{|Y|}\\sum_{t=1}^{|Y|}log(p(y_t|y_{&lt;t},X,\\lbrace{f^i}\\rbrace_{i=1}^K))$$","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"Memory Network","slug":"Memory-Network","permalink":"http://yoursite.com/tags/Memory-Network/"},{"name":"Dialog System","slug":"Dialog-System","permalink":"http://yoursite.com/tags/Dialog-System/"},{"name":"Copy Mechanism","slug":"Copy-Mechanism","permalink":"http://yoursite.com/tags/Copy-Mechanism/"},{"name":"Pointer Network","slug":"Pointer-Network","permalink":"http://yoursite.com/tags/Pointer-Network/"}]},{"title":"论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》","slug":"论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》","date":"2019-10-23T01:48:41.000Z","updated":"2019-10-25T14:42:34.397Z","comments":true,"path":"2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/","link":"","permalink":"http://yoursite.com/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/","excerpt":"【来源】：ICLR2017【链接】：https://arxiv.org/abs/1611.01603【代码、数据集】： https://github.com/allenai/bi-att-flow","text":"【来源】：ICLR2017【链接】：https://arxiv.org/abs/1611.01603【代码、数据集】： https://github.com/allenai/bi-att-flow 这是由华盛顿大学和艾伦人工智能研究所发表的论文。艾伦人工智能研究所是大名鼎鼎的微软联合创始人保罗·艾伦创建的。这是一篇经典的论文，截至目前被引次数高达678次。论文最大的贡献是在阅读理解任务中提出了双向attention机制（BiDirectional attention flow, BiDAF），BiDAF也可以用在其他任务中。 阅读理解任务定义给定文章context $\\lbrace{x_1,x_2,…,x_T}\\rbrace$及query $\\lbrace{q_1,q_2,…,q_J}\\rbrace$，在文章context中找到某个段span作为query的答案。输出其实是这个span的起始坐标和结束坐标。 模型结构设encoder包含embedding层和Bi-LSTM层，经过encoder后的context representation为$H = \\lbrace{h_1,h_2,…,h_T}\\rbrace$，query representation为$U = \\lbrace{u_1,u_2,…,u_J}\\rbrace$。其中$H \\in R^{2d\\times T}, U \\in R^{2d\\times J}$。 传统attention机制的几个特征先介绍传统attention的计算方式。在时间步t计算传统attention时，需要用到上个时间步t-1的decoder RNN的隐藏状态$s_{t-1}$。decoder RNN的隐藏状态更新公式为：$$s_t = f(s_{t-1},y_{t-1},c_t)$$其中c_t为context vector，计算方式为：$$c_t = \\sum_{i=1}^{T}\\alpha_{t,i}h_i$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$\\beta_{t,i} = score(s_{t-1},h_t)$$其中score(s,h)函数计算s与t之间的相似度。 从传统attention的计算方式可以看出，传统attention有以下几个特征： attention权重用来将所有的context representation $\\lbrace{h_1,h_2,…,h_T}\\rbrace$总结为一个固定维度的向量$c_t$。这个过程不可避免地会带来信息丢失。 时间步t的attention权重$\\alpha_{t_i}$计算 依赖于上一个时间步的向量$s_{t-1}$。这里可以看出，attention权重的计算是有记忆的。 attention的计算是单向的。 Fig.1. 模型的整体框架图 双向attention机制论文中模型分为了6层，这里只介绍最关键的一层：attention flow layer。该层的输入是context representation $H$和query representation $U$，输出是意识到query的context words representation $G$,以及前一层的context representation。 相似度矩阵S先定义一个 $H\\in R^{2d\\times T}$和 $U\\in R^{2d\\times J}$之间的共享相似度矩阵$S\\in R^{T\\times J}$。其中$S_{tj}$衡量了第t个context word与第j个query word之间的相似度。$$S_{tj} = \\alpha(H_{:t},U_{:j}) \\in R$$ 其中$H_{:t}\\in R^{2d}$是H的第t个列向量，$U_{:j}\\in R^{2d}$是U的第j个列向量。$\\alpha()$是一个计算相似度的函数：$$\\alpha(h,u) = w_{(S)}[h;u;h·u]$$ context-to-query attention表示对于每个context word，哪个query word是最相关的。对于第t个context word，在所有query words $\\{q_1,q_2,…,q_J\\}$上的attention权重为$a_t\\in R^{J}$，有$$\\sum_{j}a_{tj} = 1$$attention权重$a_t$的计算方式为：$$a_t = softmax(S_{t:}) \\in R^{J}$$ 对于第t个context word的attended query vector为$$\\widetilde{U_{:t}} = \\sum_{j}a_{tj}U_{:j} \\in R^{2d}$$ 对于所有的context words $\\{x_1,x_2,…,x_T\\}$,则有$\\widetilde{U} \\in R^{2d\\times T}$ query-to-context attention表示对于每个query words，哪个context word是最相似的，对于回答query最重要。计算在所有context words ${x_1,…,x_T}$上的attention权重为 $$b = softmax(max_{col}(S)) \\in R^{T}$$其中$max_{col}(S) \\in R^{T}$函数表示在矩阵$S \\in R^{T\\times J}$的列上取最大值。则attended context vector为$$\\widetilde{h} = \\sum_{t}b_{t}H_{:t} \\in R^{2d}$$ 这个向量的含义是对于query所有重要的context words的加权和。把$\\widetilde{h}$在列上复制T次，得到了$\\widetilde{H} \\in R^{2d\\times T}$ 输出融合把上一层的context representation $H$和attended vector $\\widetilde{H}$和$\\widetilde{U}$总结组合起来得到意识到query的context words representation $G$，计算方式为：$$G_{:t} = \\beta(H_{:t},\\widetilde{U_{:t}},\\widetilde{H_{:t}}) \\in R^{d_{G}}$$ 其中$\\beta()$函数可以是任意神经网络，比如MLP多层感知机。论文中采用了简单的连接操作，将$\\beta()$函数定义为：$$\\beta(h,\\widetilde{h},\\widetilde{u}) = [h;\\widetilde{u};h \\circ \\widetilde{u};h \\circ \\widetilde{h}] \\in R^{8d\\times T}$$ 从双向attention机制的计算可以看出，双向attention机制有以下几个特征： 与传统attention将所有context representation $\\lbrace{h_1,h_2,…,h_T}\\rbrace$总结为一个固定维度的向量$c_t$不同。双向attention机制为每个时间步都计算attention，并将attended vector $\\widetilde{H}$、$\\widetilde{U}$和前一层的context representation $H$流动到下一层。这样减少了提前总结为固定维度的向量带来的信息损失。 这是无记忆的attention机制。当前时间步的attention计算只取决于当前的context representation $H$和query representation $U$，而不依赖于上一个时间步的attention。 这种无记忆的attention机制将attention layer和model layer分隔开，迫使attention layer专注于学习context与query之间的attention，而model layer专注于学习attention layer输出内部之间的联系。 双向attention机制是双向的，包含query-to-context attention和context-to-query attention，可以彼此之间相互补充。 参考链接 BiDAF 机器阅读理解之双向注意力流||Bidirectional Attention Flow for Machine Comprehension","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"attention","slug":"attention","permalink":"http://yoursite.com/tags/attention/"},{"name":"BiDAF","slug":"BiDAF","permalink":"http://yoursite.com/tags/BiDAF/"},{"name":"Machine Comprehension","slug":"Machine-Comprehension","permalink":"http://yoursite.com/tags/Machine-Comprehension/"}]},{"title":"对话系统的数据集","slug":"对话系统的数据集","date":"2019-09-06T02:07:32.000Z","updated":"2019-09-06T02:11:16.000Z","comments":true,"path":"2019/09/06/对话系统的数据集/","link":"","permalink":"http://yoursite.com/2019/09/06/对话系统的数据集/","excerpt":"在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。","text":"在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。","categories":[{"name":"对话系统","slug":"对话系统","permalink":"http://yoursite.com/categories/对话系统/"}],"tags":[{"name":"对话系统","slug":"对话系统","permalink":"http://yoursite.com/tags/对话系统/"},{"name":"数据集","slug":"数据集","permalink":"http://yoursite.com/tags/数据集/"}]},{"title":"论文笔记《Bridging the Gap between Training and Inference for Neural Machine Translation》","slug":"论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》","date":"2019-08-02T06:46:00.000Z","updated":"2019-08-05T05:14:53.000Z","comments":true,"path":"2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/","link":"","permalink":"http://yoursite.com/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/","excerpt":"【来源】：ACL2019【链接】：https://arxiv.org/abs/1906.02448【代码、数据集】： 无","text":"【来源】：ACL2019【链接】：https://arxiv.org/abs/1906.02448【代码、数据集】： 无 这篇论文由中科院发表，获得了ACL2019的 “best long paper”。 要解决的问题在Neural Machine Translation(NMT)任务中，模型通常采用encoder-decoder框架，基于RNN 或 CNN 或attention。假设输入为$X = \\lbrace{x_1,x_2,…,x_m}\\rbrace$，真实输出为$Y = \\lbrace{y_1^*,y_2^*,…,y_n^*}\\rbrace$，预测输出为$Y’ = \\lbrace {y_1’,y_2’,…,y_m’}\\rbrace$。 第一个问题是：decoder会一个词一个词地生成整个回复。在train阶段，在时间步t生成$y_t’$时，decoder会根据之前真实的词$\\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\\rbrace$来预测$y_t’$。在infer阶段，由于不可能知道真实输出，在时间步生成$y_t’$时，decoder会根据之前预测的词$\\lbrace{y_1’,y_2’,…,y_{t-1}’}\\rbrace$来预测$y_t’$。可以看到train阶段与infer阶段所依据的词是不同的，train阶段和infer阶段预测的词$y_t’$来自两个不同的概率分布，分别是数据分布(data distribution)和模型的分布(model distribution)，这种差别称为“爆炸偏差(exposure bias)”。随着预测序列的长度增加，错误会逐渐累积。为了解决第一个问题，消除train阶段和infer阶段的这种差别，一个可能的解决方法是：在train阶段，decoder同时根据真实的词$\\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\\rbrace$和预测的词$\\lbrace{y_1’,y_2’,…,y_{t-1}’}\\rbrace$来生成$y_t’$。 第二个问题是: NMT模型通常最优化$Y与Y’$之间的交叉熵目标函数来更新模型参数，但交叉熵函数会严格匹配预测输出$Y’$与真实的输出$Y$。但在NMT任务中，一句话可以有多个不同但合理的翻译。一旦预测输出$Y’$的某个词与$Y$不同，尽管它是合理的，也会被交叉熵函数纠正。这种情况称为“过度纠正的现象”。 为了消除train阶段与infer阶段的差别，论文提出了一种在train阶段做改进的解决方案。首先，从预测的词中选择oracle word $y_{j-1}^{oracle}$，设真实输出中上一个词为$y_{j-1}^{*}$。接着从$\\lbrace{y_{j-1}^{oracle},y_{j-1}^{*}}\\rbrace$中抽样一个词，抽中$y_{j-1}^{*}$的概率为$p$，抽中$y_{j-1}^{oracle}$的概率为$1-p$。最后，decoder根据抽样的这个词来预测$y_j$。 在train阶段刚开始时，抽中真实的词$y_{j-1}^{*}$的概率比较大，随着模型逐渐收敛，抽中预测的词$y_{j-1}^{oracle}$的概率变大，让模型有能力处理”过度纠正的问题”。 Fig.1 论文提出的方法的结构图 RNN-based NMT ModelNMT任务常采用encoder-decoder框架，可以基于RNN或CNN或纯attention。论文提出的消除train阶段和infer阶段差别的方法，可以用于任何NMT模型。论文以基于RNN的NMT模型为例，来介绍这种方法。这一节先介绍RNN-based NMT模型。下一节介绍NMT模型如何结合这种方法。 encoder记输入为$X = \\lbrace{x_1,x_2,…,x_m}\\rbrace$，真实输出为$Y = \\lbrace{y_1,y_2,…,y_n}\\rbrace$。encoder采用bi-GRU分别获取正向和反向的隐藏状态$\\overrightarrow{h_i},\\overleftarrow{h_i}$。$x_i$的embedding向量为$e_{x_i}$。$$\\overrightarrow{h_i} = GRU(e_{x_i},h_{i-1})$$ $$\\overleftarrow{h_i} = GRU(e_{x_i},h_{i+1})$$ 将$\\overrightarrow{h_i},\\overleftarrow{h_i}$连接起来，作为$x_i$对应的隐藏状态：$$h_i = [\\overrightarrow{h_i},\\overleftarrow{h_i}] \\tag{1}$$ attentionattention机制用来联系encoder和decoder，更好地捕捉source sequence的信息。也就是在时间步t,通过encoder所有的隐藏状态$\\lbrace h_1,h_2,…,h_m \\rbrace$来计算context vector $c_t$。记decoder上一时间步的隐藏状态为$s_{t-1}$。 $c_t$是encoder所有隐藏状态$\\lbrace h_1,h_2,…,h_m \\rbrace$的加权和：$$c_t = \\sum_{i=1}^{m}\\alpha_{ti}h_i \\tag{2}$$ 其中$\\alpha_{ti}$是attention权重，计算方式为:$$\\beta_{ti} = v_a^\\top tanh(W_as_{t-1} + U_ah_i) \\tag{3}$$ $$\\alpha_{ti} = softmax(\\beta_{ti}) = \\frac{exp(\\beta_{ti})}{\\sum_jexp(\\beta_{tj})}$$ decoderdecoder采用单向GRU的变体，隐藏状态更新公式为:$$s_t = GRU(s_{t-1},e_{y_{t-1}^*},c_t) \\tag{4}$$ 最后根据e_{y_{t-1}^*}，decoder的隐藏状态$s_t$，对应的context vector $c_t$来预测$y_t$。 $$o_t = W_og(e_{y_{t-1}^*},s_t,c_t) \\tag{5}$$ 在词汇表上的概率分布为：$$P_t(y_t = w) = softmax(o_t) \\tag{6}$$ 方法为了消除或减轻train阶段和infer阶段的差别，论文提出 从真实的词$y_{t-1}*$和$y_{t-1}^{oracle}$预测的词中抽样，decoder根据抽样的词来预测下一个词$y_t$。使用论文提出的方法，在时间步t预测$y_t$分为三步： 先从预测的词中选择$y_{t-1}^{oracle}$。 论文提出了两种方法来选择oracle word，分别是词级别的方法和句子级别的方法。 从$\\lbrace{y_{t-1}^{oracle},y_{t-1}*}\\rbrace$中抽样得到$y_{t-1}$，抽中$y_{t-1}*$的概率为$p$，抽中$y_{t-1}^{oracle}$的概率为$1-p$。 用抽样的词$y_{t-1}$来替换公式$(4)(5)$中的$y_{t-1}^*$来预测下一个词。 oracle word的选择传统的方法中，decoder会根据上一个时间步真实的$y_{t-1}^*$来预测$y_t$。为了消除train阶段的infer阶段的差别，可以从预测的词中选择oracle word $y_{t-1}^{oracle}$来代替$y_{t-1}^*$。一种方法是每个时间步采用词级别的greedy search来生成oracle word，称为word-level oracle(WO)，另一种方法是采用beam-search，扩大搜索空间，用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO). word-level oracle选择$y_{t-1}^{oracle}$最简单直观的方法是，在时间步t-1，选择公式$P_{t-1}$中概率最高的词作为$y_{t-1}^{oracle}$，如Fig.2所示。 为了获得更健壮的$y_{t-1}^{oracle}$，更好地选择是使用gumbel max技术来冲离散分布中进行抽样，如Fig.3所示。具体地讲，将gumbel noise $\\eta$作为正则化项加到公式(5)中的$o_{t-1}$，再进行softmax操作得到$y_{t-1}$的概率分布。$$\\eta = -log(-log(u)) $$ $$\\tilde{o_{t-1}} = \\frac{o_{t-1} + \\eta}{\\tau} \\tag{7}$$ $$\\tilde{P_{t-1}} = softmax(\\tilde{o_{t-1}}) \\tag{8}$$ 其中变量$u \\sim U(0,1)$服从均匀分布。$\\tau$为温度系数，当$\\tau \\to 0$时，公式(8)的softmax()逐渐相当于argmax()函数；当$\\tau \\to \\infty$时，softmax()函数逐渐相当于均匀分布。则$y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{WO} =argmax(\\tilde{P_{t-1}}) \\tag{9}$$需要注意的是gumbel noise $\\eta$只用来选择oracle word，而不会影响train阶段的目标函数。 Fig.2. word level oracle without gumbel noise Fig.3. word level oracle with gumbel noise sentence-level oracle为了选择sentence-level oracle word，首先要进行beam-search解码，设beam size为k，得到k个candidate句子。在beam-search解码的过程中，生成每个词时也应用gumbel max技术。接着，得到k个candidate句子后，用句子级衡量指标BLEU来给这k个句子打分，得分最高的句子为oracle sentence $Y^S = \\lbrace{y_1^S,y_2^S,..,y_{|y^S|}^S}\\rbrace$。则时间步t解码对应的oracle word $y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{SO} = y_{t-1}^{S} \\tag{10}$$ 当模型从真实输出$Y$和sentence oracle $Y^S$抽样，这有一个前提是，这两个序列的长度需要是一致的。但beam-search decode不能保证解码序列的长度。为了保证这两个序列长度一致，论文提出了force decoding的解决方法。 force decoding设真实输出$Y = \\lbrace{y_1,y_2,…,y_n}\\rbrace$的序列长度为n。force decoding需要解码得到长度同样为n的序列，以特殊字符”EOS”结束。设beam search decode时，时间步t对应的概率分布为$P_t$。 当$t&lt; n$时，对于概率分布$P_t$，即使字符”EOS”是概率最高的词，那么生成概率次高的词。 当$t = n+1$时，对于概率分布$P_{n+1}$，即使字符”EOS”不是概率最高的词，也要生成”EOS”。 果真是强制生成长度为n的序列。这样beam-search decode得到的序列与真实输出序列的长度就是一致的，都为n。 递减抽样根据公式(9)或(10)得到$y_{t-1}^{oracle}$后，下一步是从$\\lbrace{y_{t-1}^{oracel},y_{t-1}^*}\\rbrace$中抽样，抽中$y_{t-1}^*$的概率是p，抽中$y_{t-1}^{oracle}$的概率是1-p。在训练的初始阶段，如果过多地选择$y_{t-1}^{oracle}$，会导致模型收敛速度慢；在训练的后期阶段，如果过多地选择$y_{t-1}^*$，会导致模型在train阶段没有学习到如何处理infer阶段的差别。因此，好的选择是：在训练的初始阶段，更大概率地选择$y_{t-1}^*$来加快模型收敛，当模型逐渐收敛后，以更大概率选择$y_{t-1}^{oracle}$，来让模型学习到如何处理infer阶段的差别。从数学表示上，概率$p$先大后逐渐衰减，$p$随着训练轮数$e$的增大而逐渐变小。$$p = \\frac{\\mu}{\\mu + exp(\\frac{e}{\\mu})} \\tag{11}$$其中，$\\mu$是超参数。$p$是轮数$e$的单调递减函数。$e$从0开始，此时，$p=1$。 训练将采样得到的$y_{t-1}$代替公式(4)-(6)中的$y_{t-1}^*$来预测$y_t$在词汇表上的概率分布。采用最大似然估计，相当于最小化以下目标函数：$$L(\\theta) = -\\sum_{n=1}^{N}\\sum_{j=1}^{|y_n|}logP_j^n[y_j^n]$$","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"ACL2019","slug":"ACL2019","permalink":"http://yoursite.com/tags/ACL2019/"},{"name":"Neural Machine Translation","slug":"Neural-Machine-Translation","permalink":"http://yoursite.com/tags/Neural-Machine-Translation/"}]},{"title":"论文笔记《Multi-Level Memory for Task Oriented Dialogs》","slug":"论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》","date":"2019-08-01T06:14:37.000Z","updated":"2019-10-25T07:26:40.134Z","comments":true,"path":"2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/","link":"","permalink":"http://yoursite.com/2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/","excerpt":"【来源】：NAACL2019【链接】：https://arxiv.org/pdf/1810.10647.pdf【代码、数据集】：https://github.com/DineshRaghu/multi-level-memory-network","text":"【来源】：NAACL2019【链接】：https://arxiv.org/pdf/1810.10647.pdf【代码、数据集】：https://github.com/DineshRaghu/multi-level-memory-network 已有工作中，端到端的任务型对话系统采用memory network来结合外部的知识库(knowledgt base) 和 对话历史(context)。为了使用从跑一趟 network，通常将二者放在同一个memory中。这样带来的问题是：memory变得太大，模型在读取memory时需要区分外部知识库和对话历史，并且在memory上的推理变得很难。为了解决这个问题，论文将外部知识库和对话历史区分开，另外，将外部知识库保存为分层的memory。 模型结构模型主要包括三个部分。 分级encoder： 分别编码对话历史中的句子。 milti-level memory 保存了目前为止所有的query以及对应的知识库查询结果，是以分级的方式保存在memory中的。 copy机制增强的decoder： 从词汇表中生成词，或者从知识库multi-level memory中复制词，或者从对话历史(context)中复制词。 Fig.1. 模型的整体框架图来源:Revanth Reddy2019 分级encoder在第t轮，对话历史共有2t-1个句子$\\lbrace{c_1,c_2,…,c_{2t-1}}\\rbrace$，其中用户对话为t轮，回复对话为t-1轮。 每个句子$c_i$都是词序列$\\lbrace{w_{i1},w_{i2},…,w_{im}}\\rbrace$。每个句子$c_i$先经过embedding layer得到词向量表示，再经过单层bi-GRU得到句子的向量表示$\\varphi(c_i)$。$h_{ij}^e$表示词$w_{ij}$对应的隐藏状态。再将$\\varphi{c_i}$经过另一个单词GRU来得到context的向量表示$c$。 multi-level memorymemory的关键是分级的分为三级：query $\\to$ result $\\to$ result key和result value。见Fig.2。记本轮对话之前所有的知识库query为$q_1,…,q_k$。每个query $q_i$是一个(key,value)对，$q_i = \\lbrace{k_a^{q_i}:v_a^{q_i},0&lt; a&lt; n_{q_i}}\\rbrace $。其中key和value分别对应query的槽(slots)和槽值，$n_{q_i}$是query $q_i$的槽值个数。第j轮对话，用query $q_i$查询知识库的返回结果为result $r_{ij}$。$r_{ij}$也是一个key-value对，$r_{ij} = \\lbrace{k_a^{r_{ij}}:v_a^{r_{ij}},0&lt; a &lt; n_{r_{ij}}}\\rbrace$。其中$n_{r_{ij}}$是key-value对的个数。 Fig.2. multi memory来源:Revanth Reddy2019 第一级memory是query的向量表示。 $q_i$的向量表示为$q_i^v$，$q_i^v$为所有values $v_a^{q_i}$的词袋(bag of words)向量表示。 第二级memory是result的向量表示。同样地，$r_{ij}$的向量表示为$r_{ij}^v$，$r_{ij}^v$为所有values $v_a^{r_{ij}}$的词袋(BOW)向量表示。 第三级memory是result的key-value对，$(k_a^{r_{ij}}:v_a^{r_{ij}})$，其中value $v_a^{r_{ij}}$可能会被复制到回复中。 copy机制增强的decoderdecoder一个词一个词地生成回复。在时间步t生成词$y_t$时，可能从词汇表中生成，也能从两个分开的memory上复制。用门$g_1$来选择是从词汇表上生成，还是从memory中复制。如果是后者，用另一个门$g_2$来选择是从context中复制，还是从知识库复制。 从词汇表生成词 时间步t，decoder的隐藏状态$h_t$为$$h_t = GRU(y_{t-1},s_{t-1})$$用$h_t$计算在encoder的所有隐藏状态上的attention权重，采用”concat attention”机制：$$a_{ij} = softmax(w_1^\\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))) = \\frac{w_1^\\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}{\\sum_{ij}w_1^\\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}$$则context vector为$$d_t = \\sum_{ij}a_{ij}h^e_{ij}$$ $h_t$和$d_t$连接后经过线性层和softmax层得到在词汇表上的概率分布：$$P_g(y_t) = softmax(W_1[h_t,d_t] + b_1)$$ 从context memory中复制词 直接将计算context vector时的attention权重，作为在context所有词$w_{ij}$上的概率分布：$$P_{con}(y_t = w) = \\sum_{ij:w_{ij}=w}a_{ij}$$ 从KB memory中复制实体 时间步t的隐藏状态$h_t$和context vector $d_t$用来计算在所有query上的attention权重。第一级在所有query $q_1,q_2,…,q_k$的attention权重为$$\\alpha_i = softmax(w_2^\\top tanh(W_4[h_t,d_t,q_i^v])) = \\frac{w_2^\\top tanh(W_4[h_t,d_t,q_i^v])}{\\sum_{i}w_2^\\top tanh(W_4[h_t,d_t,q_i^v])}$$ 第二级$\\beta_i$在$q_i$对应的$r_i$上的attention权重为$$\\beta_{ij} = softmax(w_3^\\top tanh(W_5[h_t,d_t,r_{ij}^v])) = \\frac{w_3^\\top tanh(W_5[h_t,d_t,r_{ij}^v])}{\\sum_{j}w_3^\\top tanh(W_5[h_t,d_t,r_{ij}^v])}$$ 第一级attention和第二级attention的乘积是在所有result上的attention权重分布。则memory总的向量表示为$$m_t = \\sum_{i}\\sum_j\\alpha_i\\beta_{ij}r_{ij}^v$$ 第三级memory为result的key-value对$(k_a^{r_{ij}}:v_a^{r_{ij}})$，类似于(Eric and Manning, 2017)，用key $k_a^{r_{ij}}$来计算attention权重，将对应的value $v_a^{r_{ij}}$复制到回复中。在$r_{ij}$所有keys上的attention权重为$$\\gamma_{ijl} = softmax(w_4^\\top tanh(W_6[h_t,d_t,m_t,k_l^{r_{ij}}]))$$则在所有values $v_a^{r_{ij}}$的概率分布为:$$P_{kb}(y_t = w) = \\sum_{ijl:v_l^{r_{ij}}=w}\\alpha_i\\beta_{ij}\\gamma_{ijl}$$ decoding 我们用门机制$g_2$来来结合$P_{con}(y_t)$和$P_{kb}(y_t)$，得到memory上的copy概率分布$P_c(y_t)$。$$g_2 = sigmoid(W_7[h_t,d_t,m_t]+b_2)$$ $$P_c(y_t) = g_2P_{kb}(y_t) + (1-g_2)P_{con}(y_t)$$ 用门机制$g_1$来结合$P_{c}(y_t)$和$P_{g}(y_t)$来得到总的概率分布$P(y_t)$：$$g_1 = sigmoid(W_8[h_t,d_t,m_t]+b_3)$$ $$P(y_t) = g_1P_g(y_t) + (1-g_1)P_c(y_t)$$ 相似论文 《Multi-level Memory for Task Oriented Dialogs》 发表在NAACL2019 github: https://github.com/DineshRaghu/multi-level-memory-network 《Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems》 发表在ACL2018 github: https://github.com/HLTCHKUST/Mem2Seq 《Commonsense Knowledge Aware Conversation Generation with Graph Attention》 发表在IJCAI2018 github： https://github.com/tuxchow/ccm 《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》 发表于2019年 代码：无","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"},{"name":"NAACL2019","slug":"NAACL2019","permalink":"http://yoursite.com/tags/NAACL2019/"},{"name":"Memory Network","slug":"Memory-Network","permalink":"http://yoursite.com/tags/Memory-Network/"}]},{"title":"Neural Turing Machines与Memory Network","slug":"Neural-Turing-Machines与Memory-Network","date":"2019-07-26T03:03:15.000Z","updated":"2019-08-01T02:30:21.000Z","comments":true,"path":"2019/07/26/Neural-Turing-Machines与Memory-Network/","link":"","permalink":"http://yoursite.com/2019/07/26/Neural-Turing-Machines与Memory-Network/","excerpt":"介绍Memory Networks。","text":"介绍Memory Networks。 Neural Turing Machines-神经图灵机Google DeepMind团队在Alex Graves2014提出Neural Turing Machines，第一次提出用external memory来提高神经网络的记忆能力。这之后又出现了多篇关于Memory Networks的论文。我们先看看Turing Machines的概念。 Turing Machines-图灵机计算机先驱turing在1936年提出了Turing Machines这样一个计算模型。它由三个基本的组件： tape: 一个无限长的纸带作为memory，包含无数个symbols，每个symbol的值为0、1或”$\\space$”。 head: 读写头，对tape上的symbols进行读操作和写操作。 controller： 根据当前状态来控制head的操作。 理论上Turing Machines可以模拟任何一个计算算法，不管这个算法多么复杂。但现实中，计算机不可能有无限大的memory space，因此Turing Machines只是数学意义上的计算模型。 Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/) Neural Turing MachinesNeural Turing Machines(NTM,Alex Graves2014)用external memory来提高神经网络的记忆能力。LSTM(Long and short memory)通过门机制有效缓解了RNN的’梯度消失和梯度爆炸问题’，可以通过internal memory实现长期记忆。当LSTM的internal memory的记忆能力有限，需要用external memory来提高神经网络的记忆能力。 Neural Turing Machines包含两个基本组件：a neural network controller和memory bank。memory是一个 $N\\cdot M$阶的矩阵，包含N个向量，每个向量的维度是M。我们把每个memory vector称为memory location。controller控制heads对memory进行读写操作。 如何对memory matrix进行读写操作呢？关键问题是如何让读写操作是可微的，这样才能用梯度下降法来更新模型参数。具体来说，问题是让模型关于memory location是可微的，但memory locations是离散的。Neural Turing Machines用了一个很聪明的方法来解决这个问题：不是对单独某个memory location进行读写操作，而是对所有的memory locations进行不同程度的读写操作，这个程度是通过attention的权重分布来控制的。 Fig. 2. Neural Turing Machine Architecture 读操作记时间步t memory matrix为$N\\cdot M$阶矩阵$M_t$，$w_t$是在N个memory向量上的权重分布，是一个N维向量。则时间步t的read vector $r_t$为$$r_t = \\sum_{i=1}^{N}w_t(i)\\cdot M_t(i)$$ $$where: \\sum_{i=1}^{N}w_t(i) = 1; 0 \\le w_t(i) \\le 1,\\forall i $$其中，$w_t(i)$是$w_t$的第i个元素，$M_t(i)$是$M_t$的第i个行向量。 写操作受LSTM门机制的启发，将写操作分成两步：先erase，再add。先根据erase vector $e_t$擦去旧的内容，再根据add vector $a_t$添加新的内容。 先erase： 在时间步t，attention权重分布为$w_t$，erase vector $e_t$是一个M维向量，每个元素取值[0,1]，上一个时间步的memory vector为$M_{t-1}$。则erase操作为$$\\tilde{M_{t}}(i) = M_{t-1}(i)[\\vec{1}-w_t(i)e_t]$$ $\\vec{1}$是一个M维的全1向量。对memory vector的erase操作是逐点进行的。当$e_t$的元素和memory location对应权重$w_t(i)$的元素值都是1时，memory vector $M_t(i)$的元素值才会置为0。如果$e_t$或$w_t(i)$的元素值为0时，memory vector $M_t(i)$的元素值保持不变。 再add: 每个write head会产生一个M维的add vector a_t，则：$$M_t(i) = \\tilde{M_{t}}(i) + w_t(i)a_t$$至此，就完成了写操作。 寻址机制进行读写操作前，要搞清楚对哪个memory location进行读写呢？这就是寻址。为了让模型关于memory locatios可微，Neural Turing Machines不是对某个单独的memory location进行读写操作，而是对所有memory locations进行不同程度的读写操作，这个程度就是由权重分布$w_t$来控制的。模型结合并同时使用了content-based和location-based两种寻址方式来计算这个权重分布$w_t$。具体地，权重计算分为以下几步： content-based addressing 时间步t，每个head产出一个M维的key vector $k_t$，通过$k_t$与memory vectors $M_t(i)$之间的相似性来计算content-based attention权重分布$w_{t}^{c}$。相似性是通过余弦相似度来衡量的。$$w_{t}^{c} = softmax(\\beta_tK(k_t,M_t(i))) = \\frac{\\beta_tK(k_t,M_t(i))}{\\sum_{j}K(k_t,M_t(j))}$$ $$K(u,v) = \\frac{u\\cdot v}{|u|\\cdot |v|}$$ $\\beta_t$可以放大或缩小权重的精度。 内插法 每个head产生一个interpolation gate $g_t$，取值[0,1]。content-based attention权重分布为$w_t^{c}$，上一个时间步的attention权重分布为$w_{t-1}$。则门控制的权重分布$w_t^g$为：$$w_t^g = g_tw_t^c + (1-g_t)w_{t-1}$$当$g_t$为0时，采用上一个时间步的权重分布$w_{t-1}$，当$g_t$为1时，采用content-based attention权重分布$w_t^c$。 循环卷积 对经过插值后的权重分布$w_t^g$进行循环卷积，主要功能是对权重进行旋转位移。比如当权重分布关注某个memory location时，经过循环卷积就会扩展到附近的memory locations，也会对附近的memory locations进行少量的读写操作。每个head产生的转移权重为$s_t$,循环卷积的操作为:$$\\tilde{w_t(i)} = \\sum_{j=0}^{N-1}w_t^g(i)s_t(i-j)$$ 关于$s_t$的详细介绍可以见attention?attenion!; 循环卷积的详细介绍可以见Neural Turing Machines-NTM系列（一）简述 锐化 循环卷积往往会造成权重泄漏和分散，为了解决这个问题，需要最后进行锐化操作。$$w_t(i) = \\frac{\\tilde{w_t(i)^{\\gamma_t}}}{\\sum_j\\tilde{w_t(j)^{\\gamma_t}}}$$其中$\\gamma_t &gt;1$。至此，就得到了时间步t的权重分布$w_t$。可以根据这个权重分布$w_t$对memory matrix进行读写操作。 总结以下这4步操作。第一步content-based addressing根据输入得到关于memory locations的相似度；后三步实现了location-based addressing。第二步插值操作引入了上一个时间步的权重分布，对content-based 权重进行修正；第三步循环卷积将每个位置的权重向两边分散；第四步锐化操作将权重突出化，大的更大，小的更小。 Fig.3. 寻址机制的4步操作 Fig.4. 寻址机制的4步操作来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401) 参考链接 Attention and Augmented Recurrent Neural Networks 用动图直观地表现Neural Turing Machines的计算过程。推荐！👍 记忆网络之Neural Turing Machines，中文 attention?attenion! Memory Network在Neural Turing Machines提出仅仅五天后，Facebook研究员Jason Weston发表了MEMORY NETWORKS。在QA系统的领域，应用memory network。虽然RNN或LSTM可以通过hidden state和weights来进行短期记忆，但它们的记忆能力是有限的。要实现长期记忆，需要memory network。 Memory Network的一般框架memory network包括一个记忆单元memory，和四个基本组件： I(input feature map): 将input x进行向量化表示，编码为feature representation I(x)。 G(generalization): 对memory进行写操作。根据input 来更新memory $m_i$。$m_i = G(m_i,I(x),m)$ O(output feature map): 对memory进行读操作。根据input和memory生成output feature。$o = O(I(x),m)$ R(response): 根据output feature o来生成response。 Fig.5. memory network的框架图 memory network框架的实现–MemNNs在I模块将input $x_i$编码为$I(x_i)$后，G模块之间将$I(x_i)$保存到下一个空的memory slot中，而不更新旧的memory slots。真正实现inference的核心模块是O和R。 O模块在给定x的条件下，依次找到与x最相关的k个memory slots。论文中采用k = 2。先找到第一个最相关的memory slot：$$m_{o1} = \\mathop{argmax}\\limits_{i = 1,…,N} s_{o1}(x,m_i)$$其中$s_o()$是一个匹配函数，计算x与$m_i$之间的相关程度。接着，根据x和第一个memory找到下一个memory：$$m_{o2} = \\mathop{argmax}\\limits_{i = 1,…,N} s_{o2}([x,m_{o1}],m_i)$$将output feature o = $[x,m_{o1},m_{o2}]$作为R模块的输入。 R模块将词汇表中所有词与output feature进行匹配，选择匹配度最高的词作为response。这样生成的response只有一个词。$$r = \\mathop{argmax}\\limits_{w \\in W}s_R([x,m_{o1},m_{o2}],w)$$其中$s_R()$是一个匹配函数。 匹配函数$s_O$和$s_R$都采用以下函数：$$s(x,y) = \\Phi_x(x)^\\top U^\\top U \\Phi_y(y)$$其中$\\Phi_x(x),\\Phi_y(y)$分别将x/y编码为向量。目标函数在训练阶段采用最大边缘目标函数，设对于question x，真实的label为r，对应的memory为$m_{o1},m_{o2}$。则最大边缘目标函数为：$$\\sum_{m_i\\ne m_{o1}}max(0,\\gamma - s_{O1}(x,m_{o1}) + s_{O1}(x,m_i)) + $$ $$\\sum_{m_j\\ne m_{o2}}max(0,\\gamma - s_{O2}([x,m_{o1}],m_{o2}) + s_{O2}([x,m_{o1}],m_j)) + $$ $$\\sum_{r’ \\ne r}max(0,\\gamma - s_{R}([x,m_{o1},m_{o2}],r) + s_{R}([x,m_{o1},m_{o2}],r’))$$ 由于argmax()函数的存在，这个模型是不可微的。而且中间过程找到相关memory需要监督，这个模型不是端到端的。总的来说，这个memory network是一种普适性的架构，是很初级很简单的，很多部分还不完善，不足以应用具体的任务上。不过，通过多跳方式找到相关memory的思路是很值得学习的。 End-to-End Memory NetworkJason Weston作为三作的Sainbayar Sukhbaatar2015对Memory network工作的改进，主要改进是实现了端到端，减少了监督。End-to-End Memory Network采用soft attention而不是hard attention来read memory，因此是端到端的。另外不需要对相关memory进行监督。提高memory network的可用性。假设多个句子input $x_1,…,x_n$作为memory，对于query q，输出对应的answer a。给定query q，经过多跳找到相关的memory，并生成对应的answer a。 single layer给定input $x_1,x_2,…,x_n$，采用两个不同的embedding matrix A和C分别编码为向量$\\lbrace{m_1,…,m_n}\\rbrace$，$\\lbrace{c_1,…,c_n}\\rbrace$,分别对应attention机制的keys和values。将query q经过embedding matrix B编码为向量表示u。 采用dot-product attention计算权重：$$p_i = softmax(u^\\top m_i) = \\frac{exp(u^\\top m_i)}{\\sum_{j}exp(u^\\top m_j)}$$则memory representation为：$$o = \\sum_{i}p_i m_i$$根据u和o来进行预测：$$\\hat{a} = softmax(W (o + u))$$通过最小化a与$\\hat{a}$之间的交叉熵来训练模型参数A,B,C,W。这个single layer end-to-end Memory network是简单而直观的。核心是用soft attention来read memory，找到相关的memory，并进行inference。 Fig.6.左:single layer;右:multi layers来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895) multi layers将K层single layer进行stack得到K层memory network，进行K跳memory查询操作。具体地stack方式为： 将第k层的输入$u^k$和memory representation $o^k$相加作为第k+1层的输入:$$u^{k+1} = u^k + o^k$$ 每一层都有单独的embedding matrix $A^k$和$C^k$ 最后一层的预测输出为：$$\\hat{a} = softmax(W u^{K+1}) = softmax(W(u^K + o^K))$$ 为了减少参数量，有两种方法： adjacent: 让相邻层的embedding matrix A=C，共享参数。即：$C^k = A^{k+1}$，对第一层有$A^1 = B$，最后一层有：$C^K = W$。这样就减少了一半的参数量。 RNN-like: 跟RNN一样，采用完全参数共享的方法，$A^1 = A^2 = … = A^K$;$C^1 = C^2 = … = C^K$。参数数量大大减少导致模型效果变差，在层与层之间添加一个线性映射：$u^{k+1} = Hu^k + o^k$ key-value Memory NetworksJason Weston作为作者之一的Alexander Miller2016在End-to-End Memory networks的基础上继续推进，可以更好的通过memory来编码和利用先验知识，并且具体地应用到了QA系统中。 作为memory的先验知识可以是结构化的三元组知识库，也可以是非结构化的文本。 三元组知识库。三元组的形式是”实体-关系-实体”，或”主语-谓语-宾语”。三元组知识库的优点是结构化的，便于机器处理。但缺点是与一句完整的话比较，三元组缺少了一些信息。由于三元组知识库是人工构建的，难免会有覆盖不到的知识，对于某个问题可能知识库中根本就没有对应的知识。另外，三元组中的实体可以有多种不同的表达，比如知识库中有三元组”中国-首都-北京”。当问题是“中华人民共和国的首都是？”时，可能就不能很好地回答。 像“维基百科”这样的非结构化文本。优点时覆盖面广，几乎包含所有问题的知识。缺点是非结构化的，有歧义，需要经过复杂的推理才能找到答案。 作为先验知识的memory是(key,value)形式的。 key memory用于寻址(addressing/lookup)阶段，通过计算query与key memory的相关程度来计算attention权重，因此在设计key memory时，key memory的特征应该更好地匹配query。 value memory用于read阶段，将value memory的加权和作为memory总的向量表示，因此在涉及value memory时，value memory的特征应该更好地匹配response。 比较一下end-to-end memory network与key-value memory network的区别： 前者是将相同的输入经过两个不同的embedding matrix编码分为作为key memory和value memory。而后者可以将不同的知识(key,value)分别编码为key memory和value memory，可以更灵活地利用先验知识。 后者的每个hop之间添加了用$R_j$来进行线性映射。 模型结构在问答系统中，记memory slots为$(k_1,v_1),…,(k_M,v_M)$，问题query为x，真实回复为a，预测回复为$\\hat{a}$。$\\Phi_{X},\\Phi_{Y},\\Phi_{K},\\Phi_{V}$分别是x,a,key,value的embedding matrix，将文本编码为向量表示。 则单次memory的寻址和读取可以分为三步： key hashing: 当知识库很大时，这一步是非常必要的。根据query从知识库中检索筛选出相关的facts $ (k_{h_1},v_{h_1}),(k_{h_2},v_{h_2}),…,(k_{h_N},v_{h_N})$，筛选条件可以是key中至少包含query中一个相同的词（去除停用词）。这一步可以在数据预处理时进行，直接将query和相关的facts作为模型的输入。 key addressing(寻址阶段) 计算query与memory的相关程度来分配在memory上的概率分布：$$p_{h_i} = softmax(A\\Phi_{X}(x) \\cdot A\\Phi_{K}(k_{h_i}))$$其中$\\Phi$将文本编码为D维向量，A是一个$d\\times D$的可训练矩阵。 value reading： 将value的加权求和作为memory总的向量表示。$$o = \\sum_{i}p_{h_i}A\\Phi_{V}(v_{h_i})$$ memory的读取过程是由controller神经网络通过query $q = A\\Phi_{X}(x)$来控制的。模型会利用query $q$与上一跳(hop)的$o$来更新query，进而迭代地寻址和读取memory，这个迭代的过程称为多跳(hops)。用多跳方式来迭代地寻址和读取memory，可以这样来理解：浅层神经网络可以学习到低级的特征，随着神经网络层数增多就可以学习到更高级的特征。类比CNN处理人脸图片时，第一层可以学习到一些边缘特征，第二层可以学习到眼睛、鼻子、嘴巴这样的特征，最后一层得到整个人脸的特征。同样地，用多跳方式来寻址和读取memory，可以得到更相关更突出的memory，同时可以起到推理的作用。 query的更新公式为:$$q_2 = R_1(q + o)$$其中R是一个$d\\times d$的可训练矩阵。每一跳使用不同的矩阵$R_j$。则第j跳更新query后，寻址阶段的计算公式为$$p_{h_i} = softmax(q_{j+1}^\\top \\cdot A\\Phi_{K}(k_{h_i}))$$在经过H跳之后，用controller神经网络的最终状态进行预测:$$\\hat{a} = argmax_{i=1,…,C}softmax(q_{H+1}B\\Phi_{y}(y_i))$$其中B是一个$d\\times D$的可训练矩阵，形状跟A一样。$y_i$可以是知识库中的实体，或者候选句子。 模型的目标函数为预测回复$\\hat{a}$与真实回复$a$之间的交叉熵，用梯度下降的方法来更新模型参数：$A,B,R_1,…,R_H$ Fig.7. 问答系统key-value memory networks的模型框架来源:Alexander Miller2016 key-value的选择与编码方式论文根据不同形式的先验知识，提出了key-value不同的编码方式： 知识库三元组。三元组形式为”subject-relation-object”，将”subject-relation”作为寻址的key，将”object”作为记忆的value。 sentence level。直接将句子的词袋向量表示作为key和value，key和value是一样的。每个memory slot存一个句子。 window level。以大小为W的窗口对文档进行分割（只保留中心词为实体的窗口），将单个窗口内的词作为寻址的key，将窗口的中心词作为value。 参考链接 记忆网络-Memory Network Memory network (MemNN) &amp; End to end memory network (MemN2N), Dynamic memory network Memory Networks for Language Understanding, ICML Tutorial 2016","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"Neural Turing Machines","slug":"Neural-Turing-Machines","permalink":"http://yoursite.com/tags/Neural-Turing-Machines/"},{"name":"Memory Network","slug":"Memory-Network","permalink":"http://yoursite.com/tags/Memory-Network/"}]},{"title":"attention? attention!","slug":"attention-attention","date":"2019-07-23T08:20:18.000Z","updated":"2019-08-28T09:38:20.000Z","comments":true,"path":"2019/07/23/attention-attention/","link":"","permalink":"http://yoursite.com/2019/07/23/attention-attention/","excerpt":"读了博主Weng, Lilian的文章attention? attention!，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。","text":"读了博主Weng, Lilian的文章attention? attention!，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。 why we need attention?从seq2seq模型谈起seq2seq模型与14年提出(Sutskever, et al. 2014)，实现输入序列(source sequence)到输出序列(target sequence)的映射，这两个序列的长度都是可变的。序列到序列映射的任务包括机器翻译、问答系统、对话系统、摘要生成等。 用数学语言来定义序列到序列的任务，给定输入序列(source sequence) $X = \\lbrace{ x_1,x_2,…,x_n \\rbrace}$，需要生成输出序列(target sequence) $Y = \\lbrace{ y_1,y_2,…,y_m \\rbrace}$，其中source sequence长度为$n$,target sequence长度为$m$。 seq2seq模型基于encoder-decoder框架，包括2个部分： encoder将source sequence编码（映射）为一个固定维度的向量表示(context vector,或称为sentence embedding)，我们希望这个向量表示可以很好的表示source sequence的意思。 encoder可以采用卷积神经网络CNN，也可以采用循环神经网络RNN，但用的更多的效果也更好的还是RNN。通常使用LSTM 或 GRU。 encoder RNN的隐藏状态更新公式为：$$\\begin{gather}h_t = f(h_{t-1},x_t)\\end{gather}$$其中$h_t$为RNN在时间步t的隐藏状态，f为LSTM 或GRU. 对于长度为n的source sequence，一个词接一个词地输入RNN后，可以得到n个隐藏状态$(h_1,h_2,…,h_n)$，通常将最后一个时间步最后一个词对应的隐藏状态$h_t$作为source sequence的向量表示，也就是context vector，记为$c$。 decoder根据source sequence的向量表示context vector，来一个词一个词的生成target sequence。 decoder采用单向RNN，decoder RNN隐藏状态的更新公式为:$$\\begin{gather}s_t = f(s_{t-1},y_{t-1},c)\\end{gather}$$其中$s_t$为decoder在时间步t的隐藏状态，$y_{t-1}$为target sequence中的上一个词，在train阶段，$y_{n-1}$为真实target sequence中的上一个词，在infer阶段，$y_{t-1}$为预测输出的上一个词；c为context vector。 时间步t，隐藏状态$s_t$再经过线性层和softmax得到在词表上的概率分布，将概率最大的词作为prediction word $y_t$。迭代循环直到输出整个target sequence。 Fig.1. seq2seq模型的框架图 我们可以看到当生成不同的$y_t$时，所依据的context vector都是固定不变的。固定的context vector有一个缺点是：当encoder编码完整个source sequence时，会偏向于最近的词，而遗忘了距离更远的最开始的一些词。(Bahdanau et al., 2015)提出了attention机制来解决这个问题。 attention机制:born for Translationattention机制最先在机器翻译(neural machine translation,NMT)任务上提出。从解决长期依赖问题的角度，attention可以实现长距离的记忆；从注意力的角度，attention机制可以实现对齐(alignment)，用更多的注意力关注到相关的部分，而忽略或低注意力关注到不相关的部分。 上文中提到，在生成不同的$y_t$时，直接将encoder最后一个时间步的隐藏状态$h_n$作为固定context vector。不同于这种方法，attention机制将所有encoder隐藏状态$\\lbrace{ h_1,h_2,…,h_n }\\rbrace$的加权和作为context vector，这样在每个时间步t生成$y_t$时，所依据的context vector都是专门针对于$y_t$的。一方面，context vector可以获取到所有隐藏状态，也就是整个source sequence的信息，这样就可以实现长距离的记忆。另一方面，source sequence 与target sequence之间的语义对齐(aligenment)是也是通过context vector实现的。在计算时间步t生成$y_t$对应的context vector $c_t$的计算需要三个部分的信息： 所有的encoder隐藏状态： $\\lbrace{ h_1,h_2,…,h_n }\\rbrace$ 上个时间步t-1的decoder 隐藏状态： $s_{t-1}$ source与target之间的alignment. Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf) attention的数学定义在计算时间步t生成$y_t$对应的context vector $c_t$时，encoder的所有隐藏状态为 $\\lbrace{ h_1,h_2,…,h_n }\\rbrace$，时间步t-1的decoder隐藏状态为 $s_{t-1}$，decoder RNN的隐藏状态更新公式变为：$$\\begin{gather}s_t = f(s_{t-1},y_{t-1},c_t)\\end{gather}$$ context vector $c_t$为encoder hidden state的加权和：$$c_t = \\sum_{i=1}^{n}\\alpha_{t,i}h_i$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i}) = \\frac{exp(\\beta_{t,i})}{\\sum_{j = 1}^{n}exp(\\beta_{t,j})}$$ $$\\beta_{t,i} = score(s_{t-1},h_i)$$其中权重$\\alpha_{t,i}$是时间步t生成$y_t$与隐藏状态$h_i$之间的score，从某种意义上说，$h_i$可以看作是$x_i$的表示，也可以看作是$\\lbrace{x_1,x_2,…,x_{i}}\\rbrace$的表示。因此，$\\alpha_{t,i}$可以看作是$y_t$与$x_i$之间联系（相关性）的score。所有权重$\\lbrace{\\alpha_{t,1},\\alpha_{t,2},…,\\alpha_{t,n}}\\rbrace$衡量了生成$y_t$时应该如何关注到所有的encoder hidden state。 score()为打分函数，有多种计算方法，下文会详细介绍。在Bahdanau et al., 2015.中，score()采用前馈神经网络，采用非线性激活函数$tanh()$,score()的数学形式为：$$score(s_{t},h_i) = v_a^\\top tanh(W_a[s_t;h_i])$$其中$v_a,W_a$是可训练参数。attention权重可视化矩阵很直观地表明了source words与target words之间的关联关系: Fig.3.来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf) 各种attention机制汇总下表总结了使用比较广泛的attention机制，及其对应的alignment score function。 名字 alignment score funtion 来源 content-based attention $score(s_t,h_i) = cosine(s_t,h_i)$ Graves2014 concat/additive $score(s_{t},h_i) = v_a^\\top tanh(W_a[s_t;h_i])$ Bahdanau2015 location-based $\\alpha_{t,i} = softmax(W_as_t)$将alignment简化为只依赖于target position Luong2015 general $score(s_{t},h_i) = s_t^\\top W_ah_i$ Luong2015 dot-product $score(s_{t},h_i) = s_t^\\top h_i$note:当general attention的$W_a$为单位矩阵时，就退出为dot-product attention Luong2015 scaled dot-product(*) $score(s_{t},h_i) = \\frac{s_t^\\top h_i}{\\sqrt{n}}$note:跟dot-product attention很像，n是encoder hidden state $h_i$的维度 Vaswani2017 (*)scaled dot-product attention机制添加了比例因子$/frac{1}{/sqrt{n}}$，动机是：对于softmax()函数，当输入很大时，对应的梯度很小（梯度逐渐消失），难以进行高效的优化和学习。因此，添加比例因子可以减小$score(s_t,h_i)$。 下表列出了更广范畴上的attention机制。 名字 定义 来源 self attention(&) 将input sequence的不同部分联系起来，只用到input sequence本身，而不用target sequence。可以使用上表中的所有score function，只要将target sequence替换为input sequence即可。 Cheng2016 global/soft attention context vector是整个input sequence的加权和，注意到整个input sequence Xu2015 local/hard attention context vector是局部input sequence的加权和，注意到局部input sequence Xu2015，Luong2015 (&)self-attention在一些论文中也被称为intra-attention. self-attentionself-attention,最先在Cheng2016提出称为”intra-attention”，后来在大作attention is all you need中发挥了更大的影响力。self-attention将同一个sequence的不同位置的tokens联系起来，建模tokens之间的关系，计算这个sequence的向量表示。[Cheng2016]提出self-attention的动机是什么呢？ 我们先看以下LSTM的局限。LSTM在编码sequence的向量表示时，隐藏状态更新公式为：$$h_t = f(h_{t-1},x_t)$$从这个更新公式可以看到：在给定$h_t$的条件下，$h_{t+1}$与之前的状态$\\lbrace{h_1,h_2,…,h_{t-1}}\\rbrace$及之前的tokens $\\lbrace{x_1,x_2,…,x_t}\\rbrace$是条件独立的。LSTM的潜在假设是当前状态$h_t$包含了之前所有tokens的信息，这相当于假设LSTM有无限大的memory，这个假设实际上是不成立的。实际上LSTM会偏向于更近的tokens，而逐渐遗忘距离更远的tokens。另一方面，LSTM在编码token的隐藏状态时，没有建模tokens之间的关系。而这恰恰就是self-attention要解决的问题，也就是self-attention的核心思想：在计算sequence的向量表示时，引入tokens之间的关系。 接下来看self-attention的数学表示。对于sequence $\\lbrace{x_1,x_2,…,x_n}\\rbrace$，每个token $x_t$分别对应一个hidden vector 和memory vector。当前的memory tape $C_{t-1} = \\lbrace{c_1,c_2,…,c_{t-1}}\\rbrace$，hidden state tape为$H_{t-1} = \\lbrace{h_1,h_2,…,h_{t-1}}\\rbrace$。self-attention计算$x_t$与$\\lbrace{x_1,x_2,…,x_{t-1}}\\rbrace$之间的关系：$$\\beta_{t,i} = score(x_t,h_i) = v^\\top tanh(W_hh_i,W_xx_t,W_{\\tilde{h}}\\tilde{h_{t-1}})$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i}) ;i\\in[1,t-1]$$ attention权重$\\alpha_{t,i}$是t时间步x_t在之前的tokens $\\lbrace{x_1,x_2,…,x_{t-1}}\\rbrace$对应的hidden vector上的概率分布。 Fig.4.红色表示当前token，蓝色的深浅表示相关程度。来源:[Cheng2016](https://arxiv.org/pdf/1601.06733.pdf) 比较一下self-attention机制与传统attention机制的区别： 传统的attention机制是将target sequence与source sequence联系起来，attention权重$\\lbrace{\\alpha_{t,1},\\alpha_{t,2},…,\\alpha_{t,n}}\\rbrace$是在encoder hidden states $\\lbrace{h_1,h_2,…,h_n}\\rbrace$上的概率分布。而self-attention是将同个sequence不同位置的tokens联系起来，attention权重$\\alpha_{t,i}$是t时间步$x_t$在之前的tokens $\\lbrace{x_1,x_2,…,x_{t-1}}\\rbrace$对应的hidden vector上的概率分布。 传统的attention机制常与RNN联合使用，在transformer中self-attention可以与RNN解耦开（也就是分开使用），单独用self-attention也可以编码sequence的表示向量。 soft vs hard attentionShow, Attend and Tell,Kelvin Xu2015将attention机制用到了”给图片生成描述”的任务，第一次明确区分了hard attention与soft attention，区分的依据是attention是关注到整张图片，还是图片的局部。 soft attention：attention关注到整张图片，或者是整个序列。alignment 权重$\\alpha_{t,i}$是在整个序列上的概率分布。就像普通的attention一样。 好处：模型是可微的。 坏处：计算量比较大。 hard attention：attention关注到图片的局部，或者是序列的一部分。 好处：减少了计算量。 坏处：模型不可微，需要用更复杂的技术，比如强化学习或者方差缩减来训练模型。Luong2015 global vs local attentionLuong2015在NMT任务上提出了global 和local attention的概念。区分的依据是attention是关注到整个序列，还是关注到序列的一部分。 global attention。 类似于soft attention，关注到整个序列。这里比较下Luong2015的global attention与Bahdanau2015中attention的区别。 Bahdanau2015中attention的计算路径是：$s_{t-1} \\to \\alpha_{t} \\to c_t \\to s_t$$$\\beta_{t,i} = score(s_{t-1},h_i)$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$c_t = \\sum_{i = 1}^{n}\\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = g(y_{t-1},c_t,s_t)$$ Luong2015的global attention的计算路径是：$s_t \\to \\alpha_{t} \\to c_t \\to \\tilde{s_t}$$$\\beta_{t,i} = score(s_{t},h_i)$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$c_t = \\sum_{i = 1}^{n}\\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$\\tilde{s_t} = tanh(W_c[c_t,s_t])$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = softmax(W_s\\tilde{s_t})$$ local attention。是soft 与hard attention的结合，关注到序列的一部分。对hard attention进行改进，使得模型可微，训练和计算变得更容易。改进的方法如下： 对于时间步t的target token $y_t$先用模型预测，生成一个对齐的位置$p_t$， 再根据固定窗口大小内$[p_t - D,p_t + D]$的encoder hidden state来计算context vector。D是窗口大小，是按经验定义好的。 Fig.5. global and local attenion.来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf) pointer network对于输出序列的类别数依赖于输入序列的长度的问题，seq2seq模型或神经图灵机不能解决。因为这类问题中，输出的类别数是可变的，而seq2seq模型的decoder只能在固定数目的类别上生成一个概率分布。Vinyals2017提出了pointer network（Pr_Net）来解决输出词表可变的问题。pointer network实际上是以attention为基础的。 我们比较下attention机制与pointer network的区别。记输入序列$X = \\lbrace{x_1,…,x_n}\\rbrace$,输出序列$Y = {y_1,…,y_m}$，$y_j$是X的位置索引，$y_i \\in [1,n] $。encoder的所有hidden state为$\\lbrace{h_1,h_2,…,h_n}\\rbrace$，decoder在时间步t的隐藏状态为$s_t$，则： attention机制用alignment权重来计算context vector： $$\\beta_{t,i} = score(s_t,h_i) = v^\\top tanh(W_ss_t,W_hh_i); i \\in [1,n]$$ $$\\alpha_{t,i} = softmax(\\beta_{t,i})$$ $$c_t = \\sum_{i=1}^{n}\\alpha_{t,i}h_i$$ pointer network则用alignment权重在作为在输入序列上的概率分布，将输入序列中的token直接复制到输出序列中： $$\\beta_{t,i} = score(s_t,h_i) = v^\\top tanh(W_ss_t,W_hh_i); i \\in [1,n]$$ $$p(y_i|y_{&lt; i},X) = softmax(\\beta_{t,i})$$ Fig.6.Pointer Network model来源:[Vinyals2017](https://arxiv.org/abs/1506.03134) pointer network解决OOV问题什么是OOV（out of vocabulary）问题？在序列（source sequence）到序列（target sequence）的映射问题（对话系统，问答系统）中，会根据训练集语料来构建词表，根据完成$word \\to index \\to embedding$的向量化表示。而在测试集的source sequence中难免会出现一些词表中没有的词，通常会将这些out of vocabulary的词映射到一个特定的字符”UNK”，而decoder在生成response时也可能生成”UNK”这个特殊字符。这就是OOV问题。 pointer network是解决OOV问题的有效方法。当source sequence中出现不在词表中的词时，pointer network可以直接将这个生词从输入序列复制到输出序列中。Abigail See2017就用了pointer network来解决OOV问题。 记时间步t decoder的隐藏状态为$s_t$,对应的context vector为$c_t$,alignment权重为$\\alpha_{t,i},i \\in [1,n]$ 在词汇表上的概率分布为:$p_{vocab} = softmax(W[s_t,c_t] + b)$ 在输入序列的概率分布为:$p_{ptr} = \\alpha_{t,i} = softmax(\\beta_{t_i})$ 选择开关为: $p_{gen} = sigmoid(W_ss_t + W_cc_t + W_xx_t + b)$为逻辑回归，取值为[0,1] 最终在extend vocabulary上的概率分布为:$p(w) = p_{gen}p_{vocab} + (1-p_{gen})p_{ptr}$.当$p_{gen}$为1时，从词汇表中生成word；当$p_{gen}$为0时，将输入序列的词复制到输出序列中。 Fig.7.Pointer-generator model.来源：[Abigail See2017](https://arxiv.org/abs/1704.04368) 类似的论文还有：CopyNet,Jiatao Gu2016 transformerattention is all you need!(Vaswani, et al., 2017)提出了transformer。transformer也是基于encoder-decoder框架的，也可以看作是一个seq2seq模型。但不同于encoder和decoder都采用RNN的seq2seq模型，transformer完全依赖self-attention机制来计算input和output的向量表示，而不使用RNN或CNN。一般来说，attention机制是与RNN联合使用的，transformer把attention和RNN解耦开了，只使用attention机制。 为什么transformer用self-attention来编码input和output，而不使用RNN呢？一方面，由RNN的更新公式$s_t = f(s_{t-1},x_t)$可以看到，RNN处理序列时是串行计算的，尤其是处理长序列时更费时间。不利于并行化，计算效率低。而transformer采用attention来编码计算向量，可以进行并行化计算，提高计算效率。另一方面，RNN在编码长序列时，随着距离的增大，往往会偏向于最近的部分，而学习不到长期依赖。但self-attention机制不受距离的限制，可以有效地学习到长期依赖。 scaled dot-product attention与key,value,querytransformer的主要组件是multi-heads self-attenion mechanism，这个组件用到了scaled dot-product attention机制。一般地，attention机制将query和(key,value)映射为output，其中query,key,value,output都是vector。output是所有values的加权和，权重是通过计算query与对应key之间的关联度得到。 具体来说，将什么作为key,value,query？分两种情况，从框图可以直观的看到： 在encoder-decoder框架中，联系encoder与decoder的attention机制通常将encoder的所有hidden states乘以两个不同的矩阵$W^Q,W^K$分别作为keys和values。将decoder上一个时间步的hidden state作为query。 在encoder模块，self-attention机制将input词级别的向量表示乘以三个不同的矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算input总的句子级别的向量表示。同样地，在decoder模块中self-attention机制将output词级别的向量表示分别乘以三个不同矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算output总的句子级别的向量表示。 有了具体的key,value,query后，scaled dot-product attention机制怎么来计算output呢？记query,key,value的矩阵形式分别为Q,K,V。query和key维度为$d_k$,value维度为$d_v$。则output的计算方式为：$$Attention(Q,K,V) = softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$$ 多种attention机制中，transformer为什么选择采用scale dot-product attention机制呢？最常用的两种attention机制是dot-product和additive attention机制。dot-product attention用点乘来做打分函数，additive attenion将有一层隐藏层的前馈网络作为打分函数。理论上来说，这两种attention的计算复杂度是一样的；但实际上，dot-product attention计算更快，占用内存更小。因为dot-product attention机制可以采用高度优化的矩阵乘法代码。当维度$d_k$较小时，这两种attention机制的效果是差不多的。当维度$d_k$更大时，additive attention的效果要好于dot-product attention。这可能是因为当维度$d_k$变大时，点乘的值变得过大，而softmax()函数在值过大的范围梯度是很小的，类似于梯度消失问题。因此，添加比例因子$\\frac{1}{\\sqrt{d_k}}$来减小点乘的值。 multi-head attention并不是只用一次attention机制，将维度为$d_{model}$的key,value,query映射为output。而是将query,key,value映射到维度为$d_k,d_k,d_v$不同的子向量空间，并行的计算$h$次，分别得到output做concat操作，得到总的output。$h$为head的个数，也就是并行attention layer的层数。有关系$d_{model} = d_v \\cdot h$，论文中采用$d_{model} =512,h=8,d_k = d_v = \\frac{d_{model}}{h} = 64$ $$MultiHead(Q,K,V) = concat(head_1,head_2,…,head_h)W^O$$ $$where \\quad head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$ 其中$W_i^Q \\in R^{d_{model} \\cdot d_k},W_i^K \\in R^{d_{model} \\cdot d_k},W_i^V \\in R^{d_{model} \\cdot d_v},W^O \\in R^{hd_{v} \\cdot d_{model}}$ Fig.7.multi-head scaled dot-product attention来源:Vaswani, et al., 2017 encoderencoder将input text编码为基于attention的包含位置信息的向量表示。 由6个完全相同的层堆叠起来。 每一层包含两个子层。第一子层是multi-head attention层，第二层是一个简单的全连接层。 两个子层之间采用残差连接，并进行归一化。这样所有子层的输出都有相同的维度$d_{model} = 512$ Fig.9. transformer encoder来源:Vaswani, et al., 2017 decoder从encoder output得到总的context vector，并据此生成response。 与encoder相同，由6个完全相同的层堆叠起来。 每一层除了encoder中的两个子层外，还插入了一个multi-head layer来在所有encoder output上进行attention操作。 两个子层之间采用残差连接，并进行归一化。 第一个multi-head attention sub-layer进行mask操作，mask掉output当前时间步后所有的tokens。防止attention机制看到未来的信息。 Fig.10. transformer decoder来源:Vaswani, et al., 2017 transformer的总体结构 input和output都先经过一个embedding layer得到各自的向量表示，维度为$d_{model} = 512$ 由于self-attention不能像RNN一样自动地编码位置信息，因此需要额外地将位置信息添加到输入。 在最后decoder的输出外接一个线性层和softmax层。 Fig.11. transformer的整体框架来源:Vaswani, et al., 2017 参考链接 Attention? Attention! by Weng, Lilian","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/深度学习/"}],"tags":[{"name":"rnn","slug":"rnn","permalink":"http://yoursite.com/tags/rnn/"},{"name":"attention","slug":"attention","permalink":"http://yoursite.com/tags/attention/"}]},{"title":"pip安装python模块报错","slug":"pip安装python模块报错","date":"2019-07-12T01:51:58.000Z","updated":"2019-07-12T02:11:58.000Z","comments":true,"path":"2019/07/12/pip安装python模块报错/","link":"","permalink":"http://yoursite.com/2019/07/12/pip安装python模块报错/","excerpt":"在使用pip install命令安装python模块时，报错： 1Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.","text":"在使用pip install命令安装python模块时，报错： 1Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. 错误分析报错信息告诉我们：“不能卸载‘pyyaml’模块，因为这个模块是distutils方式安装的，不能确定哪些文件属于这个模块，因此不能完整地卸载这个模块。” distutils是python最初的模块安装和分发系统，distutils不会保留哪些文件属于哪个安装包的信息，甚至不会保留安装包之间的依赖关系。直接使用distutils的方式已经被淘汰，取而代之的是setuptools. 所谓模块的分发，就是开发者打包并发布自己的模块，供其他人使用。 这样我们就知道了，因为pyyaml模块时通过distutils方式安装的，因此不能明确文件与包之间的隶属关系，不能正确卸载。 解决办法使用下面的命令忽略已安装的模块，强制安装和更新 1pip3 install &lt;package-name&gt; --ignore-installed &lt;pyyaml&gt; --upgrade 参考链接 强制安装和更新 如何在Windows操作系统中升级/卸载distutils软件包（PyYAML）？ python官方手册-安装python模块 setuptools与distutils","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"NAACL2019-对话系统","slug":"NAACL2019-对话系统","date":"2019-07-10T12:55:17.000Z","updated":"2019-07-21T15:03:16.000Z","comments":true,"path":"2019/07/10/NAACL2019-对话系统/","link":"","permalink":"http://yoursite.com/2019/07/10/NAACL2019-对话系统/","excerpt":"记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。NAACL2019的会议列表链接：https://naacl2019.org/program/accepted/","text":"记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。NAACL2019的会议列表链接：https://naacl2019.org/program/accepted/ 《Evaluating Coherence in Dialogue Systems using Entailment》【链接】https://arxiv.org/abs/1904.03371【代码】https://github.com/nouhadziri/DialogEntailment 加拿大阿尔伯塔大学发表的论文。论文提出了一种评估对话系统生成回复好坏的指标。这篇论文的想法来源于：发表在ACL2019上的论文《Dialogue Natural Language Inference》提出利用NLI(natural language inference)任务来提高对话系统生成回复的一致性。本文的作者则想到用NLI任务来评估对话系统生成回复的好坏。具体地，论文用了BERT[Devlin et al., 2018]和The Enhanced Sequential Inference Model(ESIM)[Chen et al., 2016] 这两种方法来训练NLI模型。另外论文还公开了一个用于NLI任务的数据集。","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"},{"name":"NAACL2019","slug":"NAACL2019","permalink":"http://yoursite.com/tags/NAACL2019/"}]},{"title":"ACL2019-对话系统","slug":"ACL2019-对话系统","date":"2019-07-07T11:19:27.000Z","updated":"2019-07-31T03:40:18.000Z","comments":true,"path":"2019/07/07/ACL2019-对话系统/","link":"","permalink":"http://yoursite.com/2019/07/07/ACL2019-对话系统/","excerpt":"记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml","text":"记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。ACL2019的会议列表链接：http://www.acl2019.org/EN/program.xhtml 《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》【链接】：https://arxiv.org/abs/1906.01788【源码】：无 中科院自动化所发表的短论文。在多轮对话中，对话历史（context information）对回复（response）的生成有重要作用。任务型对话中的管道模型分为4个模块：NLU、对话状态追踪、对话策略学习 及NLG。对话状态追踪又包含任务：domain classification、intent detection和slot filling。domain classification和intent detection任务当做分类任务来处理，常采用SVM或深度神经网络的方法；slot filling任务被当做序列标注任务来处理，常采用BiLSTM+CRF模型。NLU能否充分利用context information，对这三个下游任务有很大影响。为了更好的利用context information，本文提出了对话逻辑推断任务（DLI,dialog logic inference），任务定义为：将打乱顺序的多轮对话重新排序；输入之前的对话，从剩余的utterance candidates中选中下一句对话。NLU任务采用了所谓的memory network，其实就是采用多个encoder对context information进行编码，再用attention机制或别的方法得到context information总的向量化表示。本文联合训练DLI任务和NLU任务，通过两个任务共享encoder和memory retrieve模块，来让NLU任务更好地利用context information。其实是得到context information更合理的向量化表示，来作为下游domain classification、intent detection和slot filling任务的输入。 论文提出的将打乱顺序的对话重新排序的DLI任务，可以进一步深入，将句子切分为几段打乱顺序再重新排序；可以应用到闲聊式对话系统中。 《Dialogue Natural Language Inference》【链接】：https://arxiv.org/abs/1811.00671【代码】：无【数据集】：https://wellecks.github.io/dialogue_nli/ 加利福尼亚大学、Facebook AI Lab发表的论文。核心是提出用NLI(natural language inference)任务来提高persona-based dialog system的一致性。这里就要先搞清楚NLI任务和一致性问题两个概念。 先从问题出发，所谓对话的一致性问题。可以分为两类： logical contradiction，逻辑矛盾。比如同一个人的两句话:”我有一只狗”，”我没养过狗”。就是逻辑矛盾的。 比较模糊的非逻辑矛盾。同一个人不可能说出的两句话：“我从来不运动”，“我去篮球了”。就是这种非逻辑矛盾。真香警告。 至于persona一致性问题，就是回复的utterance不能与说话人的persona矛盾，也不能与之前的回复有矛盾。 具体介绍NLI任务。这其实是一个分类问题。论文公开了一个自己标注的NLI数据集。 训练阶段：训练集形式是 {$（s_1,s_2）$,label }，对应labels $\\in$（一致、无关、矛盾）。 在test阶段，给定一个句子对（句子1，句子2）来判断对应的label。 论文的最终目的是通过NLI任务训练的模型来提高persona dialog system的一致性。这是如何来实现的呢？对于一个dialog system，给定对话历史$（u_1,u_2,…,u_t）$ 及说话人的persona文本描述$（p_1,p_2,…,p_n）$,从response candidates$（y_1,y_2,…,y_m）$中选择一个$u_{t+1}$（如何生成多个responses不是这篇论文要解决的）。用NLI任务的模型来预测$(y_i,u_j),(y_i,p_k)其中：i\\in [1,m],j\\in [1,t],k \\in [1,n]$对应的label，如果句子之间是矛盾的，则添加惩罚项。从而得到一致性最好的utterance作为response。 《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》【链接】：https://arxiv.org/abs/1907.05339【数据集】：English Ubuntu dialogue corpus【代码】：https://github.com/zhanghainan/ReCoSa 中科院发表的论文。在多轮对话中，生成response时，对话历史中最相关的部分起着重要的作用。论文要解决的问题：如何更准确地找到并利用relevant context来生成response。多轮对话中广泛使用的HRED模型,[(Serban et al.,2016;,Sordoni et al., 2015]无差别地利用context information，忽略了relevant context。虽然有利用relevant context的相关工作，但这些工作都有各自的问题。[Tian et al., 2017]提出计算context 与post之间的cosine similarity来衡量context relevance，其假设是context与response之间的relevance等价于post与response之间的relevance，这个假设是站不住脚的。[Xing et al., 2018]向HRED模型引入了attention机制，但attention机制定位relevant context时会产生偏差，因为基于RNN的attention机制倾向于最靠近的context（close context）。论文提出了自己的解决办法，用self-attention机制来衡量context于response之间的relevance。self-attention机制的优点是可以有效捕捉到长距离的依赖关系。 模型分为三个部分：context包含N轮对话： ${s_1,s_2,…,s_N}$其中，$s_i = {x_1,x_2,…,x_M}$，M为句子长度。response为$Y = {y_1,y_2,…,y_M}$ context representation encoder： 将context encode为vector。 word-level encoder： 用LSTM对sentence编码，将LSTM最后一个时间步的hidden state作为sentence representation: $h^{s_i}$； 由于self-attention机制不能区分word位置信息，还需要添加position embedding: $p^{s_i}$, 把两个向量做concatenate操作，得到总得sentence representation:$(h^{s_i},p^{s_i})$。 对于context中的N个句子有${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$ context self-attention: 采用multi-head self-attention机制，将${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$经过不同的线性变换作为query、keys、values matrix,由N个sentence representation得到总的context representation $O_s$。 response representation encoder 同样用multi-head self-attention机制,将response的word embedding及position embedding ${(w_1,p_1),…,(w_{t-1},p_{t-1})}$经过不同的线性变换作为query、keys、values matrix，得到response representation $O_r$。 在train阶段 采用mask操作，在时间步t对于word $y_t$，mask掉${y_t,y_{t+1},…,y_M}$，只保留${y_1,y_2,…,y_{t-1}}$来计算response representation。 在infer阶段 在生成response的时间步t，将生成的response ${g_1,…,g_{t-1}}$，来作为response representation。 context-response attention decoder 采用multi-head self-attention机制，将context attention representation $O_s$作为keys、values matrix，将response hidden representation $O_r$作为query matrix。得到输出$O_d$. 模型框架图 《Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good》【链接】：https://arxiv.org/abs/1906.06725【代码、数据集】：https://gitlab.com/ucdavisnlp/persuasionforgood/tree/master 浙江大学、加利福尼亚大学发表。获得ACL2019 best paper提名。论文的主要贡献是公开了一个包含说话人个人信息的劝说数据集，在子集上标注了十种不同的劝说策略。并训练了用于分类不同劝说策略的分类器。 《Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study》【链接】：https://arxiv.org/abs/1906.01603【代码】：https://github.com/chinnadhurai/ParlAI/ 论文获得ACL2019 best short paper提名。论文的研究点是：生成式对话系统是否有效利用或正确理解了对话历史？论文通过向对话历史中引入不同类型的扰动，来研究生成式对话系统生成回复的困惑度变化。这个方法的一个前提是如果生成式对话系统对某种信息的扰动不敏感，那么它没有有效利用这种信息。 论文在比较了三种模型。 基于LSTM的seq2seq模型。 基于LSTM的seq2seq模型 + attention机制。 基于transformer的seq2seq模型。 论文在四个多轮对话数据集上进行实验。 bAbI dialog。(Bordes and Weston, 2016) Persona Chat。(Zhang et al., 2018) Dailydialog。 (Li et al., 2017) MutualFriends。(He et al., 2017) 论文向对话历史引入了不同的扰动。 句子级别的扰动： 随机打乱对话历史中句子的顺序。 倒序对话历史中句子的顺序。 随机去掉对话历史中特定的句子。 对话历史中有n个句子，只保留最近的k个句子$(k \\le n)$。 词级别的扰动： 随机打乱一个句子中词的顺序。 倒序一个句子中词的顺序。 随机去掉对话历史中30%的词。 去掉所有的名词。 去掉所有的动词。","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"ACL2019","slug":"ACL2019","permalink":"http://yoursite.com/tags/ACL2019/"},{"name":"dialog system","slug":"dialog-system","permalink":"http://yoursite.com/tags/dialog-system/"}]},{"title":"自然语言处理---会议列表","slug":"自然语言处理-会议列表","date":"2019-04-24T06:55:43.000Z","updated":"2019-07-07T09:00:06.000Z","comments":true,"path":"2019/04/24/自然语言处理-会议列表/","link":"","permalink":"http://yoursite.com/2019/04/24/自然语言处理-会议列表/","excerpt":"记录自然语言处理方向的国际会议列表。","text":"记录自然语言处理方向的国际会议列表。 A类会议AAAI，Association for the Advancement of Artificial Intelligence 其他ICLR，The International Conference on Learning Representations，国际学习表征会议2013年才刚刚成立了第一届。这个一年一度的会议已经被学术研究者们广泛认可，被认为「深度学习的顶级会议」。这个会议的来头不小，由位列深度学习三大巨头之二的 Yoshua Bengio 和 Yann LeCun 牵头创办。Yoshua Bengio 是蒙特利尔大学教授，深度学习三巨头之一，他领导蒙特利尔大学的人工智能实验室（MILA）进行 AI 技术的学术研究。MILA 是世界上最大的人工智能研究中心之一，与谷歌也有着密切的合作。而 Yann LeCun 就自不用提，同为深度学习三巨头之一的他现任 Facebook 人工智能研究院（FAIR）院长、纽约大学教授。作为卷积神经网络之父，他为深度学习的发展和创新作出了重要贡献。 参考链接 中国计算机学会推荐国际学术会议和期刊目录","categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/论文/"}],"tags":[{"name":"会议列表","slug":"会议列表","permalink":"http://yoursite.com/tags/会议列表/"}]},{"title":"python读写csv文件","slug":"python读写csv文件","date":"2019-04-19T10:32:22.000Z","updated":"2019-07-21T15:22:55.000Z","comments":true,"path":"2019/04/19/python读写csv文件/","link":"","permalink":"http://yoursite.com/2019/04/19/python读写csv文件/","excerpt":"介绍csv文件的读写。","text":"介绍csv文件的读写。 csv模块csv.writer(csvfile)12345678910import csv row = [&apos;Symbol&apos;,&apos;Price&apos;,&apos;Date&apos;,&apos;Time&apos;,&apos;Change&apos;,&apos;Volume&apos;]rows = [(&apos;AA&apos;, 39.48, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.18, 181800), (&apos;AIG&apos;, 71.38, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.15, 195500), (&apos;AXP&apos;, 62.58, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.46, 935000), ]with open(&apos;name.csv&apos;,&apos;w&apos;) as csvfile: writer = csv.writer(csvfile,delimiter = &apos;\\t&apos;,lineterminator = &apos;\\n&apos;) #delimiter和lineterminator分别是分隔符，行结束符 writer.writerow(row) #写入单行 writer.writerows(rows) #写入多行 csv.reader(csvfile)该函数接收一个可迭代对象，返回对象reader是一个生成器，不能直接用下标访问。可以用for循环和next()函数访问。 12345with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile: reader = csv.reader(csvfile,delimiter = &apos;\\t&apos;) #迭代器 rows = [row for row in reader] #用for循环访问：for row in rows: print(row) 输出结果为： 如果要读取csv文件的某列，可以看下面的例子 1234with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile: reader = csv.reader(csvfile,delimiter = &apos;\\t&apos;) #迭代器 column = [row[2] for row in reader] #用for循环访问：print(column) 输出结果为： csv.DictReader(csvfile)与csv.reader()函数相同，接收一个可迭代对象，返回一个生成器。不同之处是，返回的每个单元格放在字典的值中，字典的键就是这个单元格的列头。 12345with open(&apos;./name.csv&apos;,&apos;r&apos;) as f: reader = csv.DictReader(f,delimiter = &apos;\\t&apos;) rows = [row for row in reader]for row in rows: print(rows) 输出结果为： 如果要读取csv文件的某列，可以看下面的例子: 1234with open(&apos;./name.csv&apos;,&apos;r&apos;) as f: reader = csv.DictReader(f,delimiter = &apos;\\t&apos;) column = [row[&apos;Time&apos;] for row in reader]print(column) 输出结果为： csv.DictWriter(csvfile)pandas读写csv也可以直接用pandas的函数read_csv()来读取csv文件的列。 1234import pandas as pdf = pd.read_csv(&apos;name.csv&apos;,delimiter = &apos;\\t&apos;)time = f.Timeprint(time) 输出结果为： 参考链接 python官方文档-csv模块 读写csv数据 使用python获取csv文本的某行或某列数据","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"},{"name":"csv","slug":"csv","permalink":"http://yoursite.com/tags/csv/"},{"name":"panda","slug":"panda","permalink":"http://yoursite.com/tags/panda/"}]},{"title":"torch.cuda.is_available()返回False,但nvidia-smi正常","slug":"torch-cuda-is-available-返回False-但nvidia-smi正常","date":"2019-04-16T09:04:44.000Z","updated":"2019-07-22T01:27:54.000Z","comments":true,"path":"2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/","link":"","permalink":"http://yoursite.com/2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/","excerpt":"torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。","text":"torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。 问题描述在pytorch用GPU来加速计算时发现。torch.cuda.is_available()返回False,但nvidia-smi可以正常运行。 123&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()False 此时，nvidia-smi可以正常运行。 可能原因在nvidia-smi的运行结果中可以看到，driver version是390.xx。可能是driver version版本太低，造成了这个问题，实际上也是如此。driver version的常见版本是384.xx,390.xx,396.xx。接下来，把driver version升级到396.xx看能不能解决问题。 升级nvidia driver version 卸载旧版本的NVIDIA driver 1sudo apt-get remove --purge nvidia-\\* 添加NVIDIA的ppa源. 1sudo add-apt-repository ppa:graphics-drivers/ppa 安装新版本的NVIDIA driver 1sudo apt-get update &amp;&amp; sudo apt-get install nvidia-driver-396 此时，运行nvidia-smi，会报以下错误。 1Failed to initialize NVML: Driver/library version mismatch 这是更新NVIDIA driver版本后的常见问题。这个问题出现的原因是kernel mod的NVIDIA driver版本没有更新。执行以下命令可以查看nvidia kernel mod的version。 1cat /proc/driver/nvidia/version 可以看到已经加载的nvidia kernel mod的版本是还是旧版本390.xx。一般情况下，重启服务器就能解决问题。如果由于某些原因不能重启，可以重新加载kernel mod。思路是先unload kernel mod，再reload kernel mod. 详见解决Driver/library version mismatchnvidia-smi可以正常运行后，问题就解决了。 123&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True 参考链接 Torch.cuda.is_available() returns False, nvidia-smi is working How can I update the NVIDIA drivers to version 390.77?","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"nvidia-smi","slug":"nvidia-smi","permalink":"http://yoursite.com/tags/nvidia-smi/"},{"name":"GPU","slug":"GPU","permalink":"http://yoursite.com/tags/GPU/"},{"name":"cuda","slug":"cuda","permalink":"http://yoursite.com/tags/cuda/"}]},{"title":"nvidia-smi返回错误信息‘Failed to initialize NVML: Driver/library version mismatch’","slug":"nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’","date":"2019-03-29T11:47:03.000Z","updated":"2019-07-21T15:23:36.000Z","comments":true,"path":"2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/","link":"","permalink":"http://yoursite.com/2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/","excerpt":"Ubuntu运行命令nvidia-smi出错。","text":"Ubuntu运行命令nvidia-smi出错。 问题描述在Ubuntu18.04的命令行中运行命令nvidia-smi，返回错误信息 1Failed to initialize NVML: Driver/library version mismatch 方法1：重启解决大部分问题博客解决Driver/library version mismatch讲述的很清楚，这里就不再赘述。或者参考大型交友网站stack overflow的问题NVIDIA NVML Driver/library version mismatch 方法2：重装驱动看返回的错误信息，这个问题出现的原因是NVIDIA Driver的版本不匹配。如果重启不能解决问题，我们需要卸载重装NVIDIA driver。 查看驱动程序版本 dpkg -l | grep nvidia 可以看到驱动版本是390.116 cat /proc/driver/nvidia/version 这里NVRM的版本是390.87。错误信息就是这两个版本不匹配造成的。接下来先卸载NVIDIA driver，再重新安装。 卸载旧版本的NVIDIA driver 1sudo apt-get remove --purge nvidia-\\* 添加NVIDIA的ppa源 1sudo add-apt-repository ppa:graphics-drivers/ppa 重新安装NVIDIA的驱动 1sudo apt-get update &amp;&amp; sudo apt-get install nvidia-390 用你自己的版本号替换390。 这时再用cat /proc/driver/nvidia/version查看NVIDIA driver的驱动。 可以看到NVRM的版本是390.116，这时版本就匹配了。再次执行nvidia-smi,终于看到 在nvidia driver各版本总览可以看到NVIDIA driver的各个版本。 额外的：update 与 upgrade记录下sudo apt-get update与sudo apt-get upgrade的区别。在windows系统中安装软件，只需要有exe文件，双击即可安装了。Linux系统中则不同，Linux会维护一个自己的软件仓库，几乎所有软件都在这个仓库里，而且里面的软件完全安全，绝对可以安装。我们自己的Ubuntu服务器上，维护一个软件源列表文件/etc/apt/sources.list,里面都是一些网址信息，每个网址就是一个软件源，这个地址指向的数据标识着有哪些软件可以安装使用。 查看源列表： 1sudo vim /etc/apt/sources.list 更新软件列表 1sudo apt-get update 这个命令对访问源列表里的每个网址，读取软件列表，保存到本地电脑。 更新软件 1sudo apt-get upgrade 这个命令会把本地已安装的软件，与软件列表里对应软件做对比，如果有可更新版本就更新软件。总的来说，sudo apt-get update是更新软件列表，sudo apt-get upgrade是更新软件。 参考链接 解决Driver/library version mismatch Ubuntu配置GPU+CUDA+CAFFE ubuntu下安装安装CUDA、cuDNN和tensotflow-gpu版本流程和问题总结 NVIDIA的wiki （原）Ubuntu16中安装nvidia的显卡驱动","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://yoursite.com/tags/ubuntu/"},{"name":"nvidia-smi","slug":"nvidia-smi","permalink":"http://yoursite.com/tags/nvidia-smi/"}]},{"title":"条件随机场CRF","slug":"条件随机场CRF","date":"2019-03-23T05:50:20.000Z","updated":"2019-07-22T01:26:35.000Z","comments":true,"path":"2019/03/23/条件随机场CRF/","link":"","permalink":"http://yoursite.com/2019/03/23/条件随机场CRF/","excerpt":"最近学习了条件随机场CRF，做下总结。主要参考BiLSTM+CRF模型中的CRF层为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。","text":"最近学习了条件随机场CRF，做下总结。主要参考BiLSTM+CRF模型中的CRF层为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。 从马尔科夫随机场到线性链条件随机场 概率图模型 概率图模型是用图G = (V,E)来表示概率分布。设有联合概率分布P(Y)，Y是一组随机变量。我们可以用无向图G = (V,E)来表示联合概率分布P(Y),节点$v \\in V$表示随机变量$Y_v$，边$e \\in E$表示随机变量之间的概率依赖关系。 概率无向图模型，即马尔科夫随机场 设有联合概率分布P(Y),由无向图G=(V,E)表示。如果概率分布P(Y)满足成对、局部、全局马尔科夫性，那么称 这个联合概率分布p(Y)为概率无向图模型，或马尔科夫随机场(Markov random field)。 成对马尔科夫性、局部马尔可夫性、全部马尔科夫性要表达的就是：在无向图中，没有边连接的节点之间没有概率依赖关系，也就是没有边连接的节点代表的随机变量之间是条件独立的。 条件随机场 设X和Y是随机变量，P(Y|X)是给定X的条件下Y的条件概率分布。若给定X的条件下，Y构成一个马尔科夫随机场。则称条件概率分布P(Y|X)为条件随机场。 我们可以看到，马尔科夫随机场是联合概率分布P(Y),而条件随机场是条件概率分布P(Y|X)。这是一点不同。 线性链条件随机场 设$X =(X_1,X_2,…,X_n), Y =(Y_1,Y_2,…,Y_n)$是线性链表示的随机变量序列。若在给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：$$P(Y_i|X,Y_1,Y_2,…,Y_{i-1},Y_{i+1},…,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})$$ 。则称条件概率分布p(Y|X)为线性链条件随机场。 线性链条件随机场和隐马尔可夫模型都是序列模型，可以用于标注问题。这时，条件概率模型P(Y|X)中，X是输入变量序列，表示需要标注的观测序列；Y是输出变量，表示标记序列，或称状态序列。 BiLSTM+CRF模型BiLSTM+CRF模型是命名实体识别任务的常用模型。假设我们训练集中有个由五个词组成的句子$X = (w_0,w_1,w_2,w_3,w_4)$,对应标签为$Y = [B-Person，I-Person,O,B-Organization,O]$。数据集中有五类标签： 类别 B-Person I-Person B-Organization I-Organization O 含义 人名的开始部分 人名的中间部分 组织机构的开始部分 组织机构的中间部分 非实体信息 先简单介绍下BiLSTM+CRF模型的结构。LSTM层的输入一般为每个词的Word-embedding，输出为每个词word在类别空间tag_space上的非归一化概率，也就是在单词对应每个类别的得分score。这些score作为CRF层的输入。 CRF的损失函数条件随机场中有两个重要的矩阵，转移概率矩阵和状态概率矩阵，分别对应转移特征和状态特征。 状态概率矩阵。就是LSTM层的输出，作为CRF层的输入。矩阵的形状为[N,M],N为句子长度，M为可能状态数。 转移概率矩阵。矩阵形状为[M,M]，M为可能状态数。转移矩阵是模型参数，是随机初始化的，在训练过程中不断更新优化。 给定转移矩阵T，随机变量序列X取值为x的条件下，随机变量序列取值为y的似然函数为：$$Likelihood(y|x,T) = \\frac{ \\sum_{i=0}^{n} P(x_i|y_i)T(y_i|y_{i-1})}{\\sum_{y^}\\biggl(\\sum_{i=0}^{n}P(x_i|y_i^)T(y_i^|y_{i-1}^)\\biggr)} \\cdots\\cdots\\cdots\\cdots\\cdots (1)$$上式中, $P(x_i|y_i)$表示当前状态为$y_i$时，产生观测值$x_i$的概率。对应状态分数。 $T(y_i|y_{i-1})$表示从上一个状态$y_{i-1}$转换到当前状态$y_i$的概率。对应转移分数。我们可以从转移矩阵中读出这个概率。 分子表示了单条路径y=[y_0,y_1,…,y_n]的分数score或概率。 分母表示了所有可能路径$y^*$的总分。注意计算分母时，我们要计算所有可能路径并求和。若序列长度为N,状态可能数为M,则所有可能路径数为$M^N$,这个数量是指数级的，非常大。我们的秘密武器是前向后向算法来高效地计算分母。 进一步地，负对数似然函数为：$$NegLogLikelihood(y|x,T) = \\sum_{y^}\\biggl( \\sum_{i=0}^{n} log(P(x_i|y_i^)T(y^_i|y^{i-1}))\\biggr) - \\sum{i=0}^{n}log(P(x_i|y_i)T(y_i|y_{i-1})) \\cdots\\cdots\\cdots\\cdots (2)$$ 从式子(1)到式子(2)，直接对式子(1)取负对数是得到式子(2)可能比较令人费解。需要留意的是：转移概率矩阵和状态概率矩阵中的概率都是对数概率（这很重要），这样计算路径概率时都是加法。对对数概率加上exp()运算我们能得到正常概率。《统计学习方法》书中说，线性链条件随机场是对数线性模型，在对数空间中，对数概率可以直接相加，带来很大的方便。接下来，我们来看看：真实路径的分数和所有路径的总分是怎么计算的？ 真实路径分数数据集中有五类标签，再引入start和end作为序列开始和结束标志。 类别 B-Person I-Person B-Organization I-Organization O start end 索引 0 1 2 3 4 5 6 长度为5的序列，$X = (w_0,w_1,w_2,w_3,w_4)$,对应类别为$Y = [B-Person，I-Person,O,B-Organization,O]$，标注序列，也就是真实路径为为y = [0,1,4,3,4]。真实路径的分数由两部分组成，状态分数和转移分数。状态矩阵就是LSTM层的输出。转移矩阵是模型参数，为$$[t_{ij}],i,j\\in [0,6];i\\neq 6,j\\neq 5$$其中$t_ij$表示从上一状态转换到当前状态的概率。转移时，不能转移到start，不能从end转移。 则真实路径的分数 = 转移分数 + 状态分数 = $1.5+0.4+0.1+0.2+0.5+t_{51}+t_{01}+t_{14}+t_{42}+t_{24}+t_{46}$ 所有路径分数-前向后向算法计算所有路径的总分面对的难题是要不要穷举所有路径。对于一个长度为N的序列，可能状态数为M，所有可能路径数为$M^N$，这是一个指数级的计算量。计算每条路径分数的计算量是$O(N)$,直接用穷举法计算所有路径总分的计算量是$O(N\\cdot M^N)$。这个计算量是无法接受的。《统计学习方法》p176写，前向算法是基于“路径结构”递推计算所有路径分数。前向算法高效的关键是局部计算前向概率，再递推到全局。前向算法的计算量是$O(N\\cdot M^2)$，前向算法减少计算量的原因是：每一次递推计算直接利用了前一个时刻的计算结果，避免了重复计算。 对于$w_0 \\to w_1$的局部路径。先计算$w_0$所有状态到$w_1$单个状态0的分数之和，并更新$w_1$的状态0的状态分数。有M条局部路径，计算量是$O(M)$ 用同样的方法更新$w_1$所有状态的状态分数，这就是所有局部路径的分数。要计算M个状态，计算量是$O(M^2)$ 依次递推到全局。序列长度为N,总的计算量是$O(N \\cdot M^2)$ 从图的角度解释了前向算法，我们再从数学计算的角度来看前向算法。简化一下问题，假设句子长度为3，$X = [w_0,w_1,w_2]$,只有2个类别[1,2]我们引入两个变量previous和obs。previous存储前一时刻的计算结果，obs存储当前状态分数。对于$w_0$:$$obs = [x_{01},x_{02}];previous = none$$对于$w_0 \\to w_1:$,$$previous = [x_{01},x_{02}],obs = [x_{11},x_{12}]$$先扩展previous和obs：$$previous = \\begin{pmatrix} x_{01}&amp;x_{01}\\x_{02}&amp;x_{02} \\end{pmatrix} \\quad$$$$obs = \\begin{pmatrix} x_{11}&amp;x_{12}\\x_{11}&amp;x_{12} \\end{pmatrix} \\quad$$将previous和obs和转移矩阵相加：$$score =\\begin{pmatrix} x_{01}&amp;x_{01}\\x_{02}&amp;x_{02} \\end{pmatrix} +\\begin{pmatrix} x_{11}&amp;x_{12}\\x_{11}&amp;x_{12} \\end{pmatrix}+\\begin{pmatrix} t_{11}&amp;t_{12}\\t_{21}&amp;t_{22} \\end{pmatrix}$$$$ = \\begin{pmatrix} x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\\x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22} \\end{pmatrix}$$score同列相加，更新previous:$$previous = [x_{01}+x_{11}+t_{11}+x_{02}+x_{11}+t_{21},x_{01}+x_{12}+t_{12}+x_{02}+x_{12}+t_{22}]$$这样第二次迭代就完成了。用图来表示到目前为止的计算： 用同样的方法迭代递推，就可以得到所有路径的分数。 这样我们就计算出了负对数似然函数，也就是CRF模型的损失函数。条件随机场的第二个基本问题是学习问题，给定训练集估计条件随机场的模型参数（转移矩阵）。我们可以通过最小化对数似然函数来求参数模型。可以用梯度下降法来实现。 维特比算法解码条件随机场的第三个基本问题是预测问题，给出条件随机场的模型 和 输入序列x，求条件概率最大输出序列$y^*$。 也就是找出所有路径中得分最高的那条路径作为标注路径。与计算所有路径总分一样，我们面对的难题是要不要求出所有路径的分数。当然是不用的，我们用维特比算法来解码。通信专业的同学一定知道大名鼎鼎的维特比算法，卷积码的译码就是用的维特比算法。对于 $w_0 \\to w_1$:先计算$w_0$到$w_1$的状态1五条路径的分数，找出分数最大的一条保留下来，其他全都丢弃掉。计算量为$O(M)$ 同样的找出$w_1$每个状态分数最大的一条路径，要计算$w_1$的5个状态，计算量为$O(M^2)$ 依次递推到全局。序列长度为N,计算量为$O(N \\cdot M^2)$ 比较下前向算法与维特比算法的异同：相同的地方在他们都面临要不要计算所有路径分数的问题，都是基于路径结构，用局部递推到全局。不同的地方在于前向算法在更新previous的单个状态时是做求和sum运算，而维特比算法是做max运算，只保留分数最大的，丢弃掉其他路径。此外，维特比算法找到分数最大的路径后，还要反向递推 参考链接 CRF Layer on the Top of BiLSTM 最通俗易懂的BiLSTM-CRF模型中的CRF层介绍 如何直观地理解条件随机场，并通过PyTorch简单地实现 条件随机场CRF—刘建平 《统计学习方法》—李航","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"条件随机场","slug":"条件随机场","permalink":"http://yoursite.com/tags/条件随机场/"},{"name":"前向后向算法","slug":"前向后向算法","permalink":"http://yoursite.com/tags/前向后向算法/"},{"name":"维特比算法","slug":"维特比算法","permalink":"http://yoursite.com/tags/维特比算法/"}]},{"title":"pytorch实现基于LSTM的循环神经网络","slug":"pytorch实现基于LSTM的循环神经网络","date":"2019-03-20T14:41:10.000Z","updated":"2019-07-22T01:01:59.000Z","comments":true,"path":"2019/03/20/pytorch实现基于LSTM的循环神经网络/","link":"","permalink":"http://yoursite.com/2019/03/20/pytorch实现基于LSTM的循环神经网络/","excerpt":"用pytorch实现基于LSTM的循环神经网络。","text":"用pytorch实现基于LSTM的循环神经网络。 涉及函数详解class torch.nn.LSTM(args,*kwargs) 参数说明： input_size: 输入的特征维度 output_size: 输出的特征维度 num_layers: 层数（注意与时序展开区分） bidirectional: 如果为True，为双向LSTM。默认为False LSTM的输入：input,$(h_0,c_0)$ input(seq_len,batch,input_size): 包含输入特征的tensor,注意输入是tensor。 $h_0$(num_layers $\\cdot$ num_directions,batch,hidden_size): 保存初始化隐藏层状态的tensor $c_0$(num_layers $\\cdot$ num_directions,batch,hidden_size): 保存初始化细胞状态的tensor LSTM的输出： output,$(h_n,c_n)$ output(seq_len, batch, hidden_size * num_directions): 保存RNN最后一层输出的tensor $h_n$(num_layers * num_directions,batch,hidden_size): 保存RNN最后一个时间步隐藏状态的tensor $c_n$(num_layers * num_directions,batch,hidden_size): 保存RNN最后一个时间步细胞状态的tensor 1234567import torch.nn import torchlstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，该单元输入维度embedding_dim,输出维度为hidden_diminput = Variable(torch.randn(seq_len,1,embedding_dim)) # 输入input应该是三维的，第一维度是seq-length,也就是多个词构成的一句话；第二维度为1，不用管；第三个维度是一个词的词嵌入维度，即embedding_dimh0 = Variable(torch.randn(1,1,hidden_dim)) c0 = Variable(torch.randn(1,1,hidden_dim))lstm_out,hidden = lstm(input,(h0,c0)) class torch.nn.Linear()1class torch.nn.Linear(in_features,out_features,bias = True) 作用：对输入数据做线性变换。$y = Ax+b$ 参数： in_features：每个输入样本的大小 out_features: 每个输出样本的大小 bias: 默认值为True。是否学习偏置。 形状： 输入： (N,in_features) 输出： (N,out_features) 变量： weights: 可学习的权重，形状为(in_features,out_features) bias: 可学习的偏置，形状为(out_features) 1234m = nn.Linear(20,30)input = torch.randn(128,20)output = m(input)print(output) 先看个小例子用pytorch实现LSTM，先实例化一个LSTM单元，再给出tensor类型的输入数据inputs及初始隐藏状态hidden = $(h_0,c_0)$。值得注意的是，LSTM单元的输入inputs必须是三维的，第一维是seq-length，即一句话，元素是词。第二维是mini-batch,从来不用，设为1即可。第三维是embedding-size,即一个词向量。 123456import torch import torch.nn as nn lstm = nn.LSTM(4,3) #实例化一个LSTM单元，单元输入维度是4，输出维度是3inputs = [torch.randn(1,5) for _ in range(5)] #产生输入inputs。为tensor序列。hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #初始化隐藏状态 做好三步准备：实例化一个LSTM单元，准备好inputs，初始化隐藏状态hidden。我们就可以计算LSTM单元的输出了。我们有两种选择，将序列一个元素一个元素地送入LSTM单元，或是将整个序列一下子全送入LSTM单元。先看看第一种： 123for x in inputs: lstm_out,hidden = lstm(x.view(1,1,-1),hidden) #x.view(1,1,-1)将tensor整形为三维。前面说过LSTM单元的输入必须是三维的。print(lstm_out,hidden) 接下来，将整个序列送入LSTM单元： 1234inputs = torch.cat(inputs).view(len(inputs),1,-1) #将整个序列连接为tensor，并整形为三维。hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #清楚隐藏状态lstm_out,hidden = lstm(inputs,hidden)print(lstm_out,hidden) 我们可以看到： lstm_out 中包含了序列所有的隐藏状态。 hidden 中包含了最后一个时间步的隐藏状态和细胞状态。可以作为下个时间步LSTM单元的输入参数，继续输入序列或反向传播。 用lstm做词性标注先准备训练数据： 123456789101112131415train_data = [ (&quot;The dog ate the apple&quot;.split(), [&quot;DET&quot;, &quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;]), (&quot;Everybody read that book&quot;.split(), [&quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;])]# 词汇表字典word_to_ix = &#123;&#125;for sent,tags in train_data: for word in sent: if word not in word_to_ix: word_to_ix[word] = len(word_to_ix)# 标签集字典tag_to_ix = &#123;&quot;DET&quot;: 0, &quot;NN&quot;: 1, &quot;V&quot;: 2&#125;EMBEDDING_DIM = 6HIDDEN_DIM = 6 构建LSTM模型: 123456789101112class LSTMtagger(nn.Module): def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size): super(LSTMtagger,self).__init__() self.hidden_dim = hidden_dim self.word_embeddings = nn.Embedding(vocab_size,embedding_dim) #随机初始化词向量表，是神经网络的参数 self.lstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，单元输入维度是embedding_dim，输出维度是hidden_dim self.hidden2tag = torch.Linear(hidden_dim,tagset_size) #线性层从隐藏状态空间映射到标签空间 def forward(self,sentence): embeds = self.word_embeddings(sentence) #查询句子的词向量表示。输入应该是二维tensor。 lstm_out,hidden = self.lstm(embeds.view(len(sentence),1,-1)) tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1)) tag_scores = F.log_softmax(tag_space) 训练模型 12345678910111213141516171819202122232425262728293031323334353637model = LSTMtagger(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))loss_function = nn.NLLLoss()optimizer = optim.SGD(model.parameters(),lr = 0.1)def prepare_sequence(seq,to_ix): idxs = [to_ix[w] for w in seq] return torch.tensor(idxs,dtype = torch.long)# 在训练模型之前，看看模型预测结果with torch.no_grad(): inputs = prepare_sequence(train_data[0][0],word_to_ix) tag_scores = model(inputs) print(tag_scores) predict = np.argmax(tag_scores,axis = 1) print(predict)for epoch in range(300): for sentence,tags in train_data: # step 1:pytorch会累积梯度，要清楚所有variable的梯度。 model.zero_grad() # step 2:准备好数据，变成tensor sentence_in = prepare_sequence(sentence,word_to_ix) targets = prepare_sequence(tags,tag_to_ix) # step 3:得到输出 tag_scores = model(sentence_in) # step4: 计算loss loss = loss_function(tag_scores,targets) # step5: 计算loss对所有variable的梯度 loss.backward() # step6： 单步优化，根据梯度更新参数 optimizer.step()# 模型训练后，看看预测结果with torch.no_grad(): inputs = prepare_sequence(train_data[0][0],word_to_ix) tag_scores = model(inputs) print(tag_scores) predict = np.argmax(tag_scores,axis = 1) print(predict) 输出结果为： 我们可以看到，训练之后的预测序列为 [0,1,2,0,1]也就是[“DET”, “NN”, “V”, “DET”, “NN”] 参考链接 序列模型和基于LSTM的循环神经网络 Sequence Models and Long-Short Term Memory Networks-官方 Understanding LSTM Networks The Unreasonable Effectiveness of Recurrent Neural Networks","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"LSTM","slug":"LSTM","permalink":"http://yoursite.com/tags/LSTM/"},{"name":"循环神经网络","slug":"循环神经网络","permalink":"http://yoursite.com/tags/循环神经网络/"}]},{"title":"pytorch实现Word embedding","slug":"pytorch实现Word-embedding","date":"2019-03-20T12:16:21.000Z","updated":"2019-07-07T07:11:01.000Z","comments":true,"path":"2019/03/20/pytorch实现Word-embedding/","link":"","permalink":"http://yoursite.com/2019/03/20/pytorch实现Word-embedding/","excerpt":"word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。","text":"word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。 one-hot编码在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。举个例子，我们想要构建一个语言模型。有以下三个句子 数学家待在实验室里。 物理学家待在实验室里。 数学家解决了一个难题。 我们又看到一个新的句子： 物理学家解决了一个难题。 我们希望语言模型可以学习到以下特点： 数学家和物理学家在一个句子中同样的位置出现。这两个词之间有某种语义上的联系 数学家曾经出现在我们看到的这个新句子中物理学家出现的位置。 这就是语义相似性想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为distributional hypothesis值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。 word embedding我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码物理学家和数学家。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \\frac{q_物\\cdot q_数}{|q_物| \\cdot |q_数|}=cos(\\phi) 其中\\phi是两个向量之间的夹角。$$但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。 PyTorch实现word embedding代码如下： 12345678import torch import torch.nn as nnfrom torch.autograd import Variable# 词汇表字典word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;vocab_size = len(word_to_ix) embedding_dim = 15word_embeddings = nn.Embedding(vocab_size,embedding_dim) nn.Embedding()随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。接下来我们查询”dog”这个词的向量表示。 1234dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。dog_idx = Variable(dog_idx)dog_embed = word_embeddings(dog_idx) #注意不是索引print(dog_embed) 上述代码中，要访问dog的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。接下来，我们查询一句话的向量表示。 123456sent = &apos;The dog ate the apple&apos;.split()sent_idxs = [word_to_ix[w] for w in sent]sent_idxs = torch.LongTensor(sent_idxs)sent_idxs = Variable(sent_idxs)sent_embeds = embeds(sent_idxs) print(sent_embeds) pytorch加载预训练词向量之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。方式一： 1234import torch word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵pretrain_embedding = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中 方式二： 12word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding)) 涉及函数详解numpy()与from_numpy()1torch.from_numpy(ndarray) $\\to$ tensor 作用：numpy桥，将numpy.ndarray转换为pytorch的tensor.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。 1tensor.numpy() 作用：numpy桥，将pytorch的tensor转换为numpy.ndarray.二者共享同一内存空间，修改一个另一个也会被修改。 举个例子： 123a = np.arange(5)b = torch.from_numpy(a)c = b.numpy() tensor.copy_(src) 作用：将src中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。 举个例子： 123a = torch.randn(1,5)b = torch.randn(1,5)b.copy_(a) 参考链接 Word Embeddings: Encoding Lexical Semantics PyTorch快速入门教程七（RNN做自然语言处理）","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/categories/pytorch/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"},{"name":"word embedding","slug":"word-embedding","permalink":"http://yoursite.com/tags/word-embedding/"}]},{"title":"sublime插件","slug":"sublime插件","date":"2019-03-11T11:41:12.000Z","updated":"2019-03-11T13:04:43.000Z","comments":true,"path":"2019/03/11/sublime插件/","link":"","permalink":"http://yoursite.com/2019/03/11/sublime插件/","excerpt":"记录sublime的一些插件。","text":"记录sublime的一些插件。 OmniMarkupPreviewer作用：插件OmniMarkupPreviewer支持将markdown语言渲染为html并且在浏览器上实时预览，也就是将markdown内容实时显示为网页，效果之好令人惊叹。 安装可以使用Package Control的Insatll Package来安装，也可以直接从OmniMarkupPreviewer的github主页下载压缩包，解压到目录\\Sublime Text 3\\Packages\\下。 快捷键Ctrl + shift + p打开Package Control 输入install选择Package Control: Install Package 从列表中选择OmniMarkupPreviewer安装。 使用方法：对于window和Linux： Ctrl+Alt+O 在浏览器中预览 Ctrl+Alt+X 输出为html文件 Ctrl+Alt+C 复制为HTML文件 插件配置修改插件的配置，点击菜单栏的Preferences - Packages Settings - OmniMarkdownPreviwer - Setting-User。 12345678910111213141516171819202122&#123; &quot;server_host&quot;: &quot;127.0.0.1&quot;, //默认为localhost,修改为你电脑的ip，可以实现远程访问。也就是从其他电脑预览网页效果 &quot;server_port&quot;: 51004, &quot;refresh_on_modified&quot;: true, &quot;refresh_on_modified_delay&quot;: 500, &quot;refresh_on_saved&quot;: true, &quot;browser_command&quot;: [], &quot;html_template_name&quot;: &quot;github&quot;, &quot;ajax_polling_interval&quot;: 500, &quot;ignored_renderers&quot;: [&quot;LiterateHaskellRenderer&quot;], &quot;mathjax_enabled&quot;: true, //渲染数学公式要用到MathJax库，将值设为true,mathjax会在后端自动下载。 &quot;export_options&quot; : &#123; &quot;template_name&quot;: &quot;github-export&quot;, &quot;target_folder&quot;: &quot;.&quot;, &quot;timestamp_format&quot; : &quot;_%y%m%d%H%M%S&quot;, &quot;copy_to_clipboard&quot;: false, &quot;open_after_exporting&quot;: false &#125;, &quot;renderer_options-MarkdownRenderer&quot;: &#123; &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;] &#125;&#125; 遇到的错误预览文本时报错： 1234567Error: 404 Not FoundSorry, the requested URL &apos;http://127.0.0.1:51004/view/593&apos; caused an error:&apos;buffer_id(593) is not valid (closed or unsupported file format)&apos;**NOTE:** If you run multiple instances of Sublime Text, you may want to adjustthe `server_port` option in order to get this plugin work again. 解决办法是修改配置文件Sublime Text &gt; Preferences &gt; Package Settings &gt; OmniMarkupPreviewer &gt; Settings - User粘贴下面的代码： 12345&#123; &quot;renderer_options-MarkdownRenderer&quot;: &#123; &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;] &#125;&#125; 参考链接 OmniMarkupPreviewer的github主页 近乎完美的Markdown写作体验 - SublimeText3 + OmniMarkupPreviewer OmniMarkupPreviewer + MathJaxOmniMarkupPreviewerx渲染markdown内容为网页，MathJax对LATEX编辑的数学公式进行渲染。 下载mathjax 下载mathjax，解压到目录Sublime Text 3\\Packages\\OmniMarkupPreviewer\\public下。 在目录Sublime Text3\\Packages\\OmniMarkupPreviewer\\创建空文件MATHJAX.DOWNLOADED。这样就安装好了。 验证新建markdown文件输入内容： 123This expression $\\sqrt&#123;3x-1&#125;+(1+x)^2$ is an example of a $\\LaTeX$ inline equation.he Lorenz Equations:$$\\begin&#123;aligned&#125;\\dot&#123;x&#125; &amp; = \\sigma(y-x) \\\\\\dot&#123;y&#125; &amp; = \\rho x - y - xz \\\\\\dot&#123;z&#125; &amp; = -\\beta z + xy\\end&#123;aligned&#125;$$ 在sublime中用Ctrl+Alt+O预览，显示效果如下： 参考链接 使用Markdown的时候需要插入LaTeX公式方法 关于LATEX: 一份其实很短的 LaTeX 入门文档 一份其实很短的 LaTeX 入门文档 常用数学符号的 LaTeX 表示方法","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"sublime","slug":"sublime","permalink":"http://yoursite.com/tags/sublime/"}]},{"title":"熵、交叉熵与KL散度","slug":"熵、交叉熵与KL散度","date":"2019-03-11T06:31:33.000Z","updated":"2019-07-22T00:57:32.000Z","comments":true,"path":"2019/03/11/熵、交叉熵与KL散度/","link":"","permalink":"http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/","excerpt":"介绍交叉熵和KL散度。","text":"介绍交叉熵和KL散度。 从信息量到信源熵 信息量是通信专业的名词。一个变量的主要特征就是不确定性，也就是发生的概率。信息量用来衡量不确定性的大小。一个事情发生的概率越小，使人越感到意外，则这件事的信息量越大；反之，概率越大，越不意外，信息量越小。举个例子，有一架波音747飞机失事，发生的概率很小，让人很意外，带给人的信息量很大。 信息量函数应满足两个特性：1）随着概率的增大而减小，即是概率的减函数；2）信息量函数满足可加性，即两个统计独立的消息提供的信息量等于他们分别提供的信息量之和。同时满足递减性和可加性的函数是对数函数，即 $$ I[p(x_i)] = log \\frac{1}{p(x_i)} = -log p(x_i)$$ 信源熵定义为信源输出的平均信息量，即信息量的数学期望。$$ H(X) = E(I[p(x_i)]) = E(-log p(x_i)) = - \\sum_{i=1}^{n}p(x_i)log p(x_i)$$信源实际上是一个概率分布，信源熵可以解释为表示这个概率分布至少需要的信息量。 交叉熵对于一个随机事件，真实概率分布是$p(x_i)$ 是未知的，从数据中得到概率分布为$q(x_i)$。我们用概率分布$q(x_i)$来近似和逼近真实的概率分布$p(x_i)$ 。交叉熵定义为：$$H(p,q) = \\sum_{i=1}^{n}p(x_i) I[q(x_i)] =- \\sum_{i=1}^{n}p(x_i)log(x_i) $$交叉熵$H(p,q)$是用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量。上面我们说过，信源熵$H(X)$是表示真实概率分布$p(x_i)$需要的最小信息量。可以得到结论：$$H(p,q) \\ge H(p)$$由吉布斯不等式可以证明，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。这个不等式的意义是：用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量一定大于等于概率分布$p(x_i)$本身的信源熵。交叉熵比信源熵多出来的这部分，就是冗余信息量，我们称为KL散度（相对熵）。$$KL(p||q)= H(p,q) - H(p) \\ge 0$$容易看出交叉熵并不是一个对称量，即$ H(p,q) \\not=H(q,p)$。同样的,KL散度也不是一个对称量，即$KL(p||q) \\not =KL(q||p) $给定概率分布$p(x_i)$,信源熵$H(p)$就是固定不变的。在机器学习中，交叉熵常用作分类问题的损失函数。交叉熵刻画了预测概率分布$q(x_i)$与真实概率分布$p(x_i)$之间的距离。通过减小交叉熵$H(p,q)$,我们可以使得预测概率分布$q(x_i)$不断逼近真实概率分布$p(x_i)$ 相对熵真实的概率分布为$p(x_i)$，我们用预测概率分布$q(x_i)$对它进行建模和近似。我们需要的平均附加量，也就是冗余量是：$$KL(p,q) = H(p,q) - H(q) = -\\sum_{i=1}^{n}p(x_i)logq(x_i) - \\biggl(-\\sum_{i=1}^{n}p(x_i)logp(x_i)\\biggr) = -\\sum_{i=1}^{n}p(x_i)log{\\frac{q(x_i)}{p(x_i)}}$$KL散度有以下几个特性： KL散度不是一个对称量，即$KL(p||q) \\not =KL(q||p) $ $KL(p||q)\\ge 0$，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。 KL散度可以看做两个分布之间不相似程度的度量。KL散度越小，两个分布的不相似程度越小，分布$q(x_i)$越适合来近似$p(x_i)$。 tensorflow用交叉熵做损失函数在机器学习中交叉熵常常用作分类问题的损失函数。这里有个问题，交叉熵用于概率分布，但神经网络的输出并不一定是一个概率分布。概率分布应满足2个条件:1) $0 \\le p(X =x) \\le 1$2) $\\sum_{x}{} p(X=x) = 1$如何把神经网络的输出变成概率分布呢？这里就要用到softmax回归。假设输出层的输出为$y_0,y_1,y_2 \\dots y_n$,则softmax函数的形式为：$$softmax(y_i) = \\frac{exp(y_i)}{\\sum_{j}exp(y_j)}$$由于交叉熵一般会与softmax回归一起使用，TensorFlow对这两个功能进行了统一，可以直接用函数tf.nn.softmax_cross_entropy_with_logits来计算softmax后的交叉熵函数。对于只有一个正确答案的分类问题，可以用函数tf.nn.sparse_nn.softmax_cross_entropy_with_logits来加速计算过程。 pytorch中交叉熵损失函数的实现在多分类问题中，实际概率分布是 $y = [y_0,y_1,…,y_{C-1}]$,其中C为类别数;y是样本标签的one-hot表示，当样本属于第$i$类时$y_i=1$,否则$y_i=0$。预测概率分布为$p = [p_0,p_1,p_2,…,p_{C-1}]$。$c$是样本标签。此时，交叉熵损失函数为$$loss = -\\sum_{i=0}^{C-1}y_i log(p_i) = - y_c \\cdot log(p_c) = - log(p_c)$$接下来介绍pytorch中具体实现这个数学式子的函数。 torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()1torch.nn.functional.log_softmax() 作用：先做softmax运算，再做log运算。在数学上等价于$log(softmax(x))$ 1class torch.nn.NLLLoss(weight = None) 作用：这是neg log likelihood loss（NLLLoss），即负对数似然函数。 参数： weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。 输入： input(N,C): C是类别个数。为log_probabilities形式，即概率分布再取log。可以在最后一层加log_softmax,这就要用到函数torch.nn.functional.log_softmax() targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为2,而不是[0,0,1,0,0] loss可以表示为：$$loss(x,class) = -x[class]$$如果指定了weight，可以表示为：$$loss(x,class) = - weight[class]*x[class]$$举个例子: 1234567import torch log_m = torch.nn.functional.log_softmax()loss_function = torch.nn.NLLLoss()inputs = torch.randn(3,5) #batch_size * num_classes = 3 * 5target = torch.LongTensor([1,0,4])loss = loss_function(log_m(inputs),target) # inputs要先做log_softmax，再送入loss_functionloss.backward() class torch.nn.CrossEntropyLoss(weight = None) 作用：将函数log_softmax和NLLLoss集成到一起。在多分类问题中非常有用。 参数： weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。 输入： input(N,C): C是类别个数。每个类别的分数，不用过softmax层。 targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为2,而不是[0,0,1,0,0]。 loss可以表示为：$$loss(x,class) = - \\text{log}\\frac{e^{x[class]}}{ \\sum_{j=0}^{C-1}e^{x[j]}} = -x[class] + \\text{log}(\\sum_{j=0}^{C-1}e^{x[j]}) $$当指定了weight时，loss计算公式为： $$ loss(x, class) = weights[class] \\cdot (-x[class] + \\text{log}(\\sum_{j=0}^{C-1}e^{x[j]})) $$参见： PyTorch学习笔记——多分类交叉熵损失函数 pytorch官方手册参考链接 信息熵，相对熵，交叉熵的理解 Tensorflow基础知识—损失函数详解","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"交叉熵","slug":"交叉熵","permalink":"http://yoursite.com/tags/交叉熵/"},{"name":"相对熵","slug":"相对熵","permalink":"http://yoursite.com/tags/相对熵/"}]},{"title":"python的一些函数","slug":"python的一些函数","date":"2019-03-10T08:12:51.000Z","updated":"2019-07-07T07:07:46.000Z","comments":true,"path":"2019/03/10/python的一些函数/","link":"","permalink":"http://yoursite.com/2019/03/10/python的一些函数/","excerpt":"记录python的一些函数，实现某些功能。","text":"记录python的一些函数，实现某些功能。 求最大/小值的索引对于list12345import numpy as npa = range(100)np.random.shuffle(a)index_max = a.index(max(a)) #求最大值的索引index_min = a.index(min(a)) #求最小值的索引 对于numpy的数组ndarray1234567a = np.array(a)index_max = np.argmax(a) #求最大值的索引index_min = np.argmin(a) #求最小值的索引# 对于二维的数组b = np.arange(100).reshape(10,-1)row_max_list = np.argmax(b,axis = 1) #按行计算最大值在行中的索引line_max_list = np.argmin(b,axis = 0) #按列计算最小值在列中的索引 sort与sorted1sorted(iterable,key,reverse) iterable: 可迭代对象 key：用来进行比较的元素。常用函数： lambda x:x[i] reverse：排序规则。reverse=True按降序排列，reverse=False按升序排列（默认） 比较sort与sorted: 作用对象:sort()只能作用于list,sorted()可以作用于所有可迭代对象。 返回值：sort()没有返回值；sorted()返回一个新的list 字典的items()方法1dict.items() 返回可遍历的元素为（键，值）元组的数组。 模块collections–容器数据类型collections模块是python内建的一个集合模块，提供了许多有用的集成类。提供了list,dict,set,tuple的替代选择，相当于这几个数据类型的加强版。 collections.Counter(iterable)Counter是一个计数器，用于计数可哈希对象，统计元素出现的个数。它是一个集合，元素-计数像键-值对一样存储。 123456&gt;&gt; import collections&gt;&gt; a = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;a&quot;]&gt;&gt; counter = collections.Counter(a)&gt;&gt; print(counter)Counter(&#123;&apos;a&apos;: 3, &apos;b&apos;: 2, &apos;c&apos;: 1&#125;)","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"全文搜索引擎-Elasticsearch入门","slug":"全文搜索引擎-Elasticsearch入门","date":"2019-03-10T07:11:17.000Z","updated":"2019-07-22T00:38:38.000Z","comments":true,"path":"2019/03/10/全文搜索引擎-Elasticsearch入门/","link":"","permalink":"http://yoursite.com/2019/03/10/全文搜索引擎-Elasticsearch入门/","excerpt":"Elasticsearch是一个开源的搜索引擎框架。","text":"Elasticsearch是一个开源的搜索引擎框架。 Elasticsearch安装和启动安装前提：Elasticsearch需要Java7或以上的版本。 下载压缩包并解压： 123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.zipunzip elasticsearch-6.6.1.zipcd elasticsearch-6.6.1 进入解压后的文件目录，启动elasticsearch： 1./bin/elasticsearch 如果一切正常，elasticsearch默认在本机9200端口运行。打开另一个命令行窗口，执行以下命令，检查elasticsearch是否运行成功： 1curl localhost:9200 如果输出以下信息，则运行正常。 默认情况下，elasticsearch只允许本机访问。要想其他电脑可以访问，也就是实现远程访问，需要修改文件/config/elasticsearch.yml,取消字段network.host的注释，把该字段的值改为0.0.0.0。这样的话所有的电脑都能访问，实际情况中，最好不要这样。 如果启动遇到错误“Native controller process has stopped - no new native processes can be started”或“max virtual memory areas vm.maxmapcount [65530] is too low”。解决方法是执行以下命令： 1sudo sysctl -w vm.max_map_count=262144 在python中使用elasticsearch要先安装elasticsearch包。在python中使用elasticsearch要先启动elasticsearch。 1pip install elasticsearch 创建index12345from elasticsearch import Elasticsearch es = Elasticsearch() #创建实例，默认localhost:9200# es = Elasticsearch([&#123;&apos;host&apos;:&apos;10.112.1.109&apos;,&apos;port&apos;:&apos;9200&apos;&#125;]) #远程访问result = es.indices.create(index = &apos;news&apos;,ignore = 400)print(result) 如果创建成功，会返回以下信息 1&#123;&apos;acknowledged&apos;: True, &apos;shards_acknowledged&apos;: True, &apos;index&apos;: &apos;test_es&apos;&#125; 如果再次创建，就会返回以下信息： 1&#123;&apos;error&apos;: &#123;&apos;root_cause&apos;: [&#123;&apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;], &apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;, &apos;status&apos;: 400&#125; 表示创建失败，失败的原因是要创建的index已经存在了。status状态码是400。 插入数据1234567891011datas = [ &#123;&apos;title&apos;:&quot;算法导论（原书第2版）&quot;, &apos;url&apos;:&quot;https://book.douban.com/subject/1885170/&quot;, &apos;introduction&apos;:&quot;这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。&quot;&#125;, &#123;&apos;title&apos;:&quot;计算机程序的构造和解释&quot;, &apos;url&apos;:&quot;https://book.douban.com/subject/1148282/&quot;, &apos;introduction&apos;:&quot;《计算机程序的构造和解释(原书第2版)》1984年出版，成型于美国麻省理工学院(MIT)多年使用的一本教材，1996年修订为第2版。在过去的二十多年里，《计算机程序的构造和解释(原书第2版)》对于计算机科学的教育计划产生了深刻的影响。第2版中大部分重要程序设计系统都重新修改并做过测试，包括各种解释器和编译器。作者根据其后十余年的教学实践，还对其他许多细节做了相应的修改。&quot;&#125;, ]for i in range(len(datas)): es.index(index = &apos;book&apos;,doc_type = &apos;computer&apos;,id = i+1,body = datas[i]) index()方法可以完成两个操作，如果数据不存在，那就插入数据；如果数据已经存在，那就更新数据。 get()按ID查询12result= es.get(index=&apos;book&apos;,doc_type=&apos;computer&apos;,id =1)print(result[&apos;_source&apos;]) search()实现全文检索对于中文，我们要安装一个中文分词插件elasticsearch-analysis-ik。可以使用elastic的另一个命令行工具elastisearch-plugin来安装，要确保版本号一致。进入elastic的目录下，执行： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip 注意将6.3.0替换为自己的版本号。安装成功后，重启elasticsearch，就会自动加载这个中文分词插件。 1234567891011es = Elasticsearch()mapping = &#123; &apos;properties&apos;: &#123; &apos;title&apos;: &#123; &apos;type&apos;: &apos;text&apos;, &apos;analyzer&apos;: &apos;ik_max_word&apos;, &apos;search_analyzer&apos;: &apos;ik_max_word&apos; &#125; &#125;&#125;result = es.indices.put_mapping(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=mapping) 指定使用中文分词器，如果不指定默认使用英文分词器。 12345678dsl = &#123; &apos;query&apos;: &#123; &apos;match&apos;: &#123; &apos;introduction&apos;: &apos;计算机&apos; &#125; &#125;&#125;result = es.search(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=dsl) 参考链接 全文搜索引擎 Elasticsearch 入门教程—–阮一峰 Python Elasticsearch文档 Elasticsearch官方文档 Elasticsearch基本介绍及其与Python的对接实现–崔庆才 Elasticsearch搜索中文分词优化","categories":[{"name":"web搜索","slug":"web搜索","permalink":"http://yoursite.com/categories/web搜索/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Ubuntu服务器遇到的一些问题","slug":"Ubuntu服务器遇到的一些问题","date":"2019-03-07T11:53:00.000Z","updated":"2019-03-12T06:13:31.000Z","comments":true,"path":"2019/03/07/Ubuntu服务器遇到的一些问题/","link":"","permalink":"http://yoursite.com/2019/03/07/Ubuntu服务器遇到的一些问题/","excerpt":"记录Ubuntu服务器遇到的一些问题。","text":"记录Ubuntu服务器遇到的一些问题。 linux服务器连不上网 先检查网线是否插好了，若网线口发亮才是插好。检查电脑是否能ping通局域网的其他电脑。 查看其他电脑的ip地址ifconfig | grep inet ping其他电脑的IP地址ping 10.112.0.1如果可以ping通其他电脑，再检查下一步。 可以ping通其他电脑，但ping 10.3.8.211校园网网关失败。这时可能是路由出错，查看服务器的路由是否正确。 查看比较服务器与可以正常联网的电脑的路由。route -n 添加正确的默认路由。route add default gw 10.112.0.1 检查能否连接到校园网。ping 10.3.8.211Ubuntu配置路由参见: ubuntu配置静态路由及重启生效 连接到校园网，但是ping www.baidu.com失败。服务器ping不通域名，但可以ping通百度的ip地址112.34.112.40。这是服务器的DNS配置出错了，无法解析域名。 修改文件/etc/resolv.conf，必须有sudo权限。sudo vim /etc/resolv.conf 添加以下内容nameserver 8.8.8.8 重启网络使修改立即生效。sudo /etc/init.d/networking restart 这时应该能ping通百度了。 重启电源后，以上方法会被清除而失效，导致开机后需要重新配置。有效的方法是卸载开机重写/etc/resolv.conf的resolvconf，执行命令sudo apt-get autoremove resolvconf配置域名解析参见：Ubuntu 无法解析域名 不能通过Xshell或ssh命令连接到服务器 检查是否安装了ssh-server服务。ps -e | grep ssh 若没有安装，使用以下命令安装：sudo apt-get install openssh-server 若安装了ssh-server服务，检查ssh服务是否打开。需要sudo权限 检查ssh服务状态service sshd status或/etc/init.d/ssh status 开启ssh服务service sshd start或/etc/init.d/ssh start","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"},{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/tags/服务器/"}]},{"title":"pytorch学习笔记","slug":"pytorch学习笔记","date":"2019-03-05T01:27:14.000Z","updated":"2019-07-07T07:09:35.000Z","comments":true,"path":"2019/03/05/pytorch学习笔记/","link":"","permalink":"http://yoursite.com/2019/03/05/pytorch学习笔记/","excerpt":"这里是pytorch学习笔记。","text":"这里是pytorch学习笔记。 创建操作torch.randn()1torch.randn(*size,out = None) 输入： size(int)：指定了输出张量的形状 输出：输出结果为张量 作用：返回一个张量，从标准正态分布中抽取一组随机数。形状由*size决定 张量维度操作转置：transpose1torch.transpose(input,dim0,dim1) 参数： input: 输入张量，可以是二维及二维以上的张量 dim0,dim1: 要转置的两个维度。 作用： 返回输入矩阵的转置。一次只能转置张量的两个维度。输出张量与输入张量共享内存，同步改变。 1torch.t(tensor) 输入一个二维张量（矩阵），并转置0,1维。可以看做函数torch.transpose(input,0,1)的简写函数。比较下transpose与view这两个函数： 1234567891011121314151617181920212223242526272829303132333435&gt;&gt; a = torch.randn(2,3,5)&gt;&gt; b = torch.transpose(a,1,2)&gt;&gt; c = a.view(2,5,3)&gt;&gt; print(a)tensor([[[ 0.9926, -0.1669, -1.6571, -0.2730, -0.1313], [ 0.9811, -1.9854, 1.5519, 0.1383, 1.4571], [ 0.8221, -1.1283, -0.7675, -2.0497, 0.4748]], [[ 0.1594, 0.7166, -0.2603, 1.1027, 1.5283], [-0.7652, -1.4711, 0.5077, 0.6639, 0.0374], [ 1.8121, -1.4864, -2.9863, -0.5769, -0.2915]]]) &gt;&gt; print(b)tensor([[[ 0.9926, 0.9811, 0.8221], [-0.1669, -1.9854, -1.1283], [-1.6571, 1.5519, -0.7675], [-0.2730, 0.1383, -2.0497], [-0.1313, 1.4571, 0.4748]], [[ 0.1594, -0.7652, 1.8121], [ 0.7166, -1.4711, -1.4864], [-0.2603, 0.5077, -2.9863], [ 1.1027, 0.6639, -0.5769], [ 1.5283, 0.0374, -0.2915]]]) &gt;&gt; print(c)tensor([[[ 0.9926, -0.1669, -1.6571], [-0.2730, -0.1313, 0.9811], [-1.9854, 1.5519, 0.1383], [ 1.4571, 0.8221, -1.1283], [-0.7675, -2.0497, 0.4748]], [[ 0.1594, 0.7166, -0.2603], [ 1.1027, 1.5283, -0.7652], [-1.4711, 0.5077, 0.6639], [ 0.0374, 1.8121, -1.4864], [-2.9863, -0.5769, -0.2915]]]) 可以看到:二者得到的结果并不相同。transpose是进行转置操作。view对张量整形时，张量中元素的顺序保持不变。相当于将这个三维张量按顺序 torch.Tensortorch.manual_seed()1torch.manual_seed(seed) 输入： seed(int or long)：设定种子，为int类型或long类型 作用：设定生成随机数的种子。种子相同，生成的随机数就是相同的，实验结果就可以复现。 参见：利用随机数种子来使pytorch中的结果可以复现 .view() 整形1tensor.view(*size) 输入： *size(int)：指定了输出张量的形状 输出：输出结果为张量 作用：整形，只改变原张量的形状，形状由*size指定。 例子： 123456789&gt;&gt; x = torch.randn(5,4)&gt;&gt; x.size()torch.Size([5,4]&gt;&gt; x.view(30)&gt;&gt; x.size()torch.Size([20])&gt;&gt; x.view(1,1,-1) # -1表示该维度由其他的维度推断。&gt;&gt; x.size()torch.Size([1, 1, 20]) torch.cat() 连接1torch.cat(inputs,dimension = 0) 输入： inputs(sequence of Tensors)： 多个Tensor的python序列。 如[tensor1,tensor2…]或(Tensor1，tensor2) dimension(int,optional): 沿着该维连接张量序列。默认为0。 作用：在指定维度上，对输入张量序列进行连接操作。 举个例子： 12345678910&gt;&gt; impotr torch &gt;&gt; x = torch.randn(4,3)&gt;&gt; x.size()torch.Size([4, 3])&gt;&gt; y = torch.cat((x,x,x),0)&gt;&gt; y.size()torch.Size([12, 3])&gt;&gt; z = torch.cat((x,x,x),1)&gt;&gt; z.size() torch.Size([4, 9]) torch.optimclass torch.optim.SGD(params,lr=,momentum=0,weight_decay=0) 参数： params： 待优化参数的iterable lr(float): 学习率 momentum(float,可选)： 动量因子，默认为0 weight_decay(float,可选): 权重衰减，默认为0 作用：实现随机梯度下降算法。 如何使用optimizer? 123456789import torch.optim as optim optimizer = optim.SGD(model.parameters(),lr = 0.01) #构建一个optimizer,model.parameters()给出了所有要优化的参数for input,target in dataset: optimizer.zero_grad() #清空所有被优化过的Variable的梯度 output = model(input) loss = loss_fn(output,target) loss.backward() #反向传播算法，计算好所有要优化Variable的梯度。 optimizer.step() #单步优化，基于计算得到的梯度进行参数更新。","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://yoursite.com/tags/pytorch/"}]},{"title":"Numpy学习笔记","slug":"Numpy学习笔记","date":"2019-02-27T12:33:15.000Z","updated":"2019-07-22T00:37:14.000Z","comments":true,"path":"2019/02/27/Numpy学习笔记/","link":"","permalink":"http://yoursite.com/2019/02/27/Numpy学习笔记/","excerpt":"这里是numpy学习笔记。","text":"这里是numpy学习笔记。 np.copy1np.copy(a) a为ndarray数组。np.copy复制一个与a完全相同的dnarray数组。来看看=与np.copy的区别。 12345678&gt;&gt; x = np.array([1,2,3])&gt;&gt; y = x&gt;&gt; z = x.copy(x)&gt;&gt; x[0] = 10&gt;&gt; x[0] == y[0]True&gt;&gt; x[0] == z[0]False 对于=，x与y共享同一内存，数据同步改变。一个改变另一个跟着改变。对于np.copy,x与z在不同的内存，数据改变互不影响。 np.load与np.savenumpy可以读写磁盘上的二进制数据。为将ndarray数组对象保存到文件，引入了文件格式npy。数组对象ndarray以未压缩的原始二进制格式保存在扩展名为.npy的文件中。 np.save(file,array) 作用： 将数组以二进制格式保存到扩展名为npy的文件中。 np.load(file) 作用： 从.npy文件中读取二进制数据还原为数组。举个例子：1234import numpy as np a = arange(5)np.save(&apos;a.npy&apos;,a)b = np.load(&apos;a.npy&apos;) np.full1np.full(shape,fill_value,dtype) 作用： 返回一个 给定形状为shape，数据类型为dtype，全都由fill_value填充后的ndarray。 np.foat32np.float32(x)作用：变换数据类型为float32 np.pad参数解释 1numpy.pad(array, pad_width, mode, **kwargs) 输入 array 为待填充的数组 pad_width 为((before_1,after_1),(before_2,after_2),….,(before_N,after_N))在每个维度前后填充的个数 mode 常用constant,用常数填充。 返回值： 填充后的ndarray 举个例子 123456import numpy as np a = range(5)# 在一维数组前后分别填充2,3位数字；填充的数字分别为0,2ndarray = np.pad(a,(2,3),&apos;constant&apos;,constant_values=(0,2))print(a)print(ndarray) 执行结果为 12[0, 1, 2, 3, 4][0 0 0 1 2 3 4 2 2 2] np.randomnp.random.uniformnp.random.uniform(low = 0,high = 1,size = None)作用： 生成一个形状为size的随机数矩阵，每个数从均匀分布的半开半闭区间[low,high)中抽样得到。 参考链接 Numpy官方手册","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"http://yoursite.com/categories/学习笔记/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://yoursite.com/tags/Numpy/"}]},{"title":"2018-群星陨落","slug":"2018-群星陨落","date":"2019-01-16T12:56:00.000Z","updated":"2019-07-22T00:35:37.000Z","comments":true,"path":"2019/01/16/2018-群星陨落/","link":"","permalink":"http://yoursite.com/2019/01/16/2018-群星陨落/","excerpt":"游龙当归海，海不迎我自来也！","text":"游龙当归海，海不迎我自来也！ 今天是农历12月十一，明天就是爸爸的生日。阳历的2018年已经过去了，但春节还没来呢！研究生一年级的学期期末，我想着回顾一下这一年，看看自己这一年是怎么度过的。2018年，许多了不起的人去世了，因此选了这个小标题。2018年，我大学毕业，顺利通过了研究生复试。考研，是与高考同等级别的考试，不同的是考研没有老师和家长的督促，只得一个人奋战。备战考研初试已经是2017年的往事了，但还是值得一提的。准备初试的小半年里基本上每天泡在图书馆里，坐在某个固定的座位。虽然有时候也会懈怠懒惰，但大部分时间都是在认真地学习。晚上九点半从图书馆出来，我常要绕着学校跑上五公里；回宿舍经过一楼的镜子，我看着自己嘴里念叨着学校的名字。如愿以偿，2018年年初，我查自己的考研初试成绩409，是比高考还令人满意的了。查到成绩后，我欢欣鼓舞了一阵子。 上半年 - 本科最后的日子年初，我因为考驾照的缘故住在我姨家里，有了跟亲戚一起生活半个月的机会。亲戚走动的机会不多，平时没有机会去了解他们促进感情，全凭着血缘的纽带联系着。但这次难得地坐在亲戚家里有了不一样的感受，感受到日久生情（可能用词不当，但意思到了呢）。有天中午，我姨有事外出，我跟表妹一块做了午饭，手忙脚乱，做出来的饭味道竟还不错！考过了科二，我便回了家。每年过年都得下一场雪，有到脚踝那么厚，到处都白茫茫的一片。我拿着铲子去屋顶铲雪，花了两个半天，把屋顶上的雪收拾到了院子里。只是那时候竟没有兴致堆个雪人，今天冬天北京一丝雪都没有落，真想回到家能看一场雪啊。高中的班主任还在带着高三的学生，过了年没几天高三的学生便开学了。班主任想着我给高三的同学们聊聊天，说“现在的学生都没有你高三下苦”，我勉强算是个刻苦的笨孩子吧。在老班家里吃了午饭，下午便回到曾经的教室里，跟高三的同学们聊天。时间在一天一天地度过，我们在一步一步地向前走呢！ 大四下，已经没课了，只剩下做毕业设计。我在明德楼B座的人才办找了一份勤工俭学的差事，平常的工作是给老师整理文件，打印，拿快递和打扫卫生。这是段闲适又想着充实的日子，想着好好完成毕业设计，想着去做大学里想做还没做的事，想着去看几本书，想着跟朋友们告别。我每天在老师的办公室里做毕设，中午便在沙发上睡一会，算不上特别努力，只是想在大学最后的几个月里把事情做好一些。办公室的窗口有一些灰尘，窗台上有爬山虎的脚，窗外有许多树。阳光照进来，树影婆娑的，像极了大学最后的这段日子。大四下，我去圣昆仑音乐厅听了音乐会，只听得一半便溜了出来；也在圣昆仑音乐厅看了话剧《蜀道难》，虽然有一些瑕疵，但依然十分精彩，震撼人心，让我领略到了现场话剧的魅力；我常去文理馆三楼找文学类的书，在做毕设的闲暇翻看。读了王小波写给李银河的信，知道爱情会让人牵肠挂肚；读了杨绛先生的散文集，我原先以为散文集是无趣的，但读了杨绛先生的散文集，才体会到朴实文字的动人之处。尤其当读到杨绛先生写文革期间女婿被红卫兵打死，父亲去世却奔丧不成，自己被分配到厕所刷马桶这些情节，就好像在讲述一些生活中的小事。也看到杨绛先生去菜园找钱钟书时流露出的不难察觉的爱意和在苦难中的幽默感。 杨绛先生和她的女儿钱瑗、丈夫钱钟书 车协是我大学四年最重要的地方吧，也是我大学归属感的来源。在最后这半年里，在周二周四的晚上参加了许多次的体训，周一周三周五也偶尔跟着大家骑车去怪坡。最后还参加了几次小假期的拉练：“放火烧山，牢底坐穿”，黄巢的篝火没有烧起来，但心里的篝火不曾熄灭，\"聚是一团火，散作满天星\"；“清明拉练”去看了遍野的油菜花，大一第一次参加拉练也是清明拉练呢；五一是药乡选拔，新的远征队又将踩着单车用车轮丈量祖国的大好河山！快要离开的时候，想着要做一些事，比平时更用心些。婷婷提出要办一个车协的\"考研、保研、工作交流会\"，但她因为要工作提前离校了，我接过这个活，跟车协的伙伴一起办了这个交流会。在北京比赛之前，我想起了前两年给京赛队员加油的火腿肠，今年该是我了吧！大学的许多生活都与车协相关，大学许多朋友都是车协的伙伴。我似乎没给留下些什么，但一同经历就是意义啊！ 16年黄巢拉练的篝火 大学最后的尾声是在拍毕业照和送老会中进行的。终于穿上那一身幼时憧憬的毕业服，拍学院里的毕业照，拍班级里的毕业照。也拍协会那一群人的毕业照，小树林，南门外，臧克家和闻一多的雕像旁，曾经朝夕相处，今日便要离之而去。在车协的送老会上，我以为我不会哭的，两年远征都没有哭，只是未曾坦开心扉，在送老会上，平时话不多的我也不知哪来那么多的话和眼泪。嘿，毕业快乐！！！ 暑期 - 目的地在尼泊尔的新藏线大学毕业后，趁着年轻力壮去骑了新藏线。时间跨度是7月21日到8月28日近四十天的时间。先坐火车从北京到叶城，中途在吐鲁番转车，从祖国的东边到最西边。骑车的路段是“喀什-萨嘎县-吉隆口岸-加德满都”。返程是费尽周折的，先坐吉普车从加德满都回国到基隆口岸，再做大巴车到日喀则，再坐火车“日喀则-青海西宁-陕西西安-山西临汾”，从陆地回家的成本虽然只有机票钱的一半，但花了整整五天。之前有过骑车出国门的想法，但不是十分笃定的。旺哥在论坛上发帖征新藏线的队友，我毅然地回了贴。原先计划的新藏线小队有我、旺哥和宇哥三个人，后来宇哥因为入职没能成行。新藏线是一条比川藏线要难的路线，我每天晚上跑步五公里来做体力储备，有时候也偷懒；在美骑网上看别人的路书和骑行游记，做了自己的路书和攻略。特地回老家办了护照和边防证，办边防证的过程费了一些周折，办边防证需要一份小领导写个名字，但他又不来上班，花了几天才终于办好了。又在北京的尼泊尔大使馆办了签证，准备地差不多只等出发了。 7月21日，坐火车从北京出发，坐了38个小时的硬座到了吐鲁番，再坐19个小时的硬座到新疆喀什地区喀什市跟旺哥会合。全国用的都是北京时间，因此到晚上九点，喀什的天还是亮的。略作修整，便到了开始每天的骑行。第一天的骑行就差点要了老命，一是第一天的路程远，又有很大的逆风，晚上到宾馆已经累地快虚脱了。新藏线前几天的路是比较平坦的，海拔上升不太大。骑车的第四天我遇到一个致命的问题，差点导致我的新藏线骑行半路夭折。我自行车后轮的花鼓在一个小村子阿克美其特村里断了，小村子里连去城里的公交车都没有，完全不可能修理。我跟旺哥商议，旺哥继续骑车到下个地点补给更方便的库地达坂等我，我返回叶城去修车。我拆了自行车的后轮，村里的村干部开车送我到了镇上，我在镇上坐了个出租车返回了叶城。为了节省时间，我没去更远更大的喀什市去修理，只在叶城找人修理，这给我后来的不幸埋下了隐患。在叶城找了一个不靠谱的单车修理店给我修理后轮，信誓旦旦地坐车回到阿克美其特村，结果骑了不到二公里，自行车的后轮又坏了，是根本没有修理好。悔不该杀那华佗！我终于下定决心老老实实地去喀什找捷安特专卖店买了轮子，又是艰难的交通，先在路边搭回城里的顺风车，再坐火车到喀什，买好轮子，再坐火车返回叶城，最后搭了许多车（私家车，大卡车）终于回到阿克美其特村。我修理好车，战战兢兢的出发了，小心翼翼地，生怕轮子再出幺蛾子。所幸后面一路上都没再出什么大问题，这一次就够我折腾的了，反反复复弄弄三天。以致于我想要不车子也不要了，买张票回家吧！现在回过头来看，所幸坚持了下来。 骑行在新藏线上 新藏线广袤荒芜，很少有高的树，当你看到高大的杨树和绿色的植物，就说明附近有水源，此处有人烟。新藏线上是一条连接新疆与西藏的纽带，有许多跑新藏线的大卡车司机。搭车的时候跟几位司机聊天，司机师傅大多是山西河南的，汉族人多，藏族少。在新藏线上跑大卡车，就是在拿命挣钱，高原反应和疲劳驾驶，一个不小心就是车毁人亡。新藏线路边会常看得到翻倒在路边的大卡车，有的男人带着妻子来跑车，出了事一家子就没了，剩个孩子跟着爷爷奶奶。新藏线有除了踩单车的，还有骑摩托车的，自驾游的。常看得到自驾游的大妈披着丝巾拍照。封路的时候，自驾游的车夹杂着拉水泥拉货的大卡车长长地排着队。一路上的比较大的客栈旅店，大都是汉族人开的，四川人很多，四川人勤劳敢拼，新疆西藏到处可见到他们的身影。新藏线这条长长的路上，整整齐齐训练有素的军车是一道靓丽的风景，让人感叹“好男儿就要当兵，保家卫国”。新藏线上，有人的地方就有众生相。到了萨嘎县之后，我们四人要分道扬镳了，我一路出吉隆口岸到加德满都，他们三人一路。旺哥先去珠峰大本营再去拉萨，邓翱和贤弟先去珠峰大本营再去加德满都。萨嘎县到吉隆县的路是非常难走的，没有硬实的柏油路，只有下雨后泥泞的泥路。十米宽的水坑，我脱了鞋子挂在车把上，推着车趟过去。泥泞的路，一脚一脚的踩踏着，车轮上滚满了泥巴，像是炸酥肉前给肉裹上厚厚的面。顶着下午刮个不停的巨大的逆风，长长的上坡，我再也没有了绝不推车的坚持。遇到难爬的上坡，就下来推着车走一段再骑车。大风和泥巴路，骑了一整天只走了五十公里，眼看就要露宿野外而帐篷早寄走了，终于看到一个大大的施工营地，像是溺水的人抓住一根救命稻草。营地里七八个西安的爷们收留了我一宿，度过了危机。到了基隆口岸，我换好尼币，经过中国海关，出了国门。尼泊尔的路才是真的烂路，石头路，泥路，山体滑坡封路都是家常便饭，偶尔能看到一段柏油路便开心地不行。尼泊尔的路是不推荐骑自行车的。尼泊尔是山地里的国家，较大的城镇都分布在平坦的河谷地。我买了尼泊尔的电话卡，但是用不了网络，不能看谷歌地图。我只能一边走，一边用蹩脚的英语问路“这条路是到加德满都吗？”；尼泊尔年轻人大多会说英语，偶尔几个会说汉语。在烂路里走了两天，我他娘的终于到了加德满都，在泰米尔区见到了中国人和中国店铺，激动地不行。只要有出发的勇气，就一定会到达！我终于到了加德满都！ 在山坡上俯瞰加德满都 新藏线一路上盖了一些邮戳，但并不多。骑车的间隙，抽空给15和16远征队的队友和一些老友寄去了明信片，现在有些明信片寄到了，有些寄丢了。我寄给自己的丢在路上了，就让他代替我在路上飘荡吧！骑完新藏线是一件了不起的事，是一件有意义的事。 下半年 - 研究生的日子暑假，在去新藏线之前来了读研的学校上了三周的暑期课程，《机器学习》和《凸优化》。9月初开学正式开始了研究生生活。之前我有一个错误的认识，认为研究生生活应该完全舍弃掉自行车，因为之前骑车太多了吧。后来，我认识到生活需要balance，不能只是学习，要让骑车和娱乐成为生活里积极的一部分。来了北京之后，出去玩的次数并不多。训超骑车从济南到北京，来我宿舍住了一晚，我跟他一块去了故宫。国庆节那天，旺哥来北京转车，我和训超早早地去看升旗，给旺哥接风洗尘。后来约着在北京读书工作的高中同学聚了一次。周末骑了两次车，去了卢沟桥和香山公园旁的西山国家森林公园。研一上，还有一些课要修。除了上课，其他时间是待在实验室里，看论文，写代码。前期，感觉有许多需要学习的东西，总能早早地起床。后来感觉不到自己的进步，有些陷入迷茫，不知道该做些什么，就失去了劲头，起床时间晚了许多。我找了一个辅导考研专业课的差事，每周末辅导一个半小时，《通信原理》已经大半年没看过了，但上手简单看一遍就能想起来那些知识点。妈妈生日那天，我买了一只天鹅项链做生日礼物。弟弟生日那天，我给他发了红包，他没领说自己有钱。爸爸今天生日，我鼓动身边的朋友给老爸发了祝福的短信。2019年，希望可以更加勇敢和更加积极地面对生活里的变化。你好，2019！","categories":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/categories/年度总结/"}],"tags":[{"name":"年度总结","slug":"年度总结","permalink":"http://yoursite.com/tags/年度总结/"},{"name":"生活记录","slug":"生活记录","permalink":"http://yoursite.com/tags/生活记录/"}]},{"title":"用Hexo+Next+github page搭建个人博客","slug":"用hexo搭建个人博客","date":"2019-01-13T02:08:31.000Z","updated":"2019-07-22T00:33:50.000Z","comments":true,"path":"2019/01/13/用hexo搭建个人博客/","link":"","permalink":"http://yoursite.com/2019/01/13/用hexo搭建个人博客/","excerpt":"Hexo是一个简洁漂亮的博客框架，用来生成静态网页。本文介绍使用hexo搭建博客。","text":"Hexo是一个简洁漂亮的博客框架，用来生成静态网页。本文介绍使用hexo搭建博客。 准备：安装Hexo安装前提在安装Hexo之前，应先安装以下应用程序： Node.js Git 安装Git对Linux(ubuntu,debian) 1sudo apt-get install git-core 安装Node.js1wget -qO- https://raw.github.com/creationix/nvm/v0.33.11/install.sh | sh 安装完成后，需要重启终端再执行以下命令： 1nvm install stable 安装Hexo安装好Git和Node.js后，使用npm安装Hexo: 1npm install -g hexo-cli hexo常用命令 hexo init #用于新建一个网站 hexo new #在站点根目录下执行，以/scaffolds/post.md为模板，创建一篇新的文章。新建文章存放在/source/_posts下。 hexo generate 简写为 hexo g #生成静态网页 hexo server 简写为 hexo s #启动服务器，默认访问网址为http://localhost:4000/ hexo clean #用于清楚缓存文件和已经生成的静态文件。当对站点的修改无论如何不能生效时，运行该命令。 hexo deploy 简写为 hexo d #将生成的静态文件部署到github等远程服务器。常用的套路是先使用hexo g和hexo s在本地编辑和调试博客，当调试无误后，再用hexo d部署到GitHub。 本地建站选好路径，以下命令会自动创建文件夹来存放博客内容。 1hexo init &lt;folder&gt; #生成hexo模板 上述命令生成了以下文件,生成文件夹的目录如下。 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 上述文件中，只需要重点关注和了解这几个文件： _config.yml 是站点配置文件。用来配置博客网站的各种信息。 /source/_posts 用来存放我们自己之后写的博客。 themes 用来存放博客的主题。再执行以下命令:123cd &lt;folder&gt;npm installhexo server 执行完以上命令，可以访问 http://localhost:4000 看到博客已经顺利搭建。 修改站点配置文件hexo中有两个重要的配置文件，名称都是 _config.yml： 站点配置文件，位于站点根目录下，用于站点本身的配置。 主题配置文件，位于主题目录下，用于主题相关的配置。 通过修改站点配置文件的下述字段，来分别修改网站名称、副标题、个性签名(或网站简介)、作者和语言设置。 12345title: subtitle:description: author: language: zh-Hans #设置语言为中文 将博客关联到GitHub1、在github创建仓库 &lt;your-user-name&gt;.git.io,&lt;your-user-name&gt;必须与你的github用户名相同。2、编辑站点配置文件，找到字段deploy，作如下的修改。 1234deploy: type: git repository: https://github.com/&lt;your-user-name&gt;/&lt;your-user-name&gt;.github.io.git branch: master 3、使用命令npm install --save安装插件。 1npm install hexo-deployer-git --save 4、生成静态网页 1hexo g 5、部署到GitHub 1hexo d 运行上述命令后，访问 http://&lt;your-user-name&gt;.git.io ‘hexo d’部署时不输密码执行hexo d将博客部署到github时，每次都要输入github的账号和密码，非常麻烦。我们利用ssh的公钥登录免去输入密码的步骤。 ssh的公钥登录简介所谓公钥登录。本地主机生成一个公钥和一个秘钥，把公钥告诉给远程主机（github），而秘钥只有自己知道。当本地主机要登录远程主机时，远程主机（github）发送一个消息序列给本地主机，本地主机用秘钥将消息序列加密后再发送给远程主机；远程主机收到后用公钥解密，若与原消息序列相同，则可以登录。通过这种“公钥登录”的方式保证了安全性，不需要再用账户和密码来验证身份。 本机主机生成公钥和秘钥在命令行执行： 1ssh-keygen 生成的文件有id_rsa.pub和id_rsa，分别为公钥和私钥。Linux系统下，生成文件在home/ssh/目录下。windows系统,在C:/user/.ssh下。 把公钥告诉给github在Linux系统下，执行以下命令查看公钥，然后复制。在windows系统下，将id_rsa.pub用文本格式打开来复制。 12cd ~/.sshcat id_rsa.pub 在github的settings&gt;SSH and GPG keys&gt;New SSH keys新建一个ssh key，将复制的公钥粘贴保存即可。 修改站点配置文件用ssh方式来访问仓库，而不是用https方式打开站点配置文件，修改字段deploy下的repository的值。 12345deploy: type: git # repository: https://github.com/spring-quan/spring-quan.github.io repository: git@github.com:spring-quan/spring-quan.github.io.git branch: master 参见SSH远程登录Hexo免输入密码部署到Github 绑定域名购买并注册域名在阿里云的万网购买一个域名。常用的域名后缀有.com/.cn/.me/.top。不同后缀的域名价格不同，可以按照喜好选择一个。购买好域名后，需要完成 域名实名认证，才能正常进行域名解析，该域名才能使用。 注意：注册域名后，需要域名实名认证，不需要备案，域名就可以正常使用。 给GitHub添加域名在博客对应的github仓库下，新建一个文件CNAME，文件名要大写。内容如下，注意要为顶级域名，不能包括www或http。 1example.com 在用hexo d命令将博客部署到GitHub上时，文件CNAME会被覆盖删除掉。为了解决这个问题，还需要在站点目录下/source，新建同样的文件CNAME。 设置域名解析设置域名解析就是将注册到的域名指向一个IP地址，首先我们要查询github.io域名对应的IP地址。在命令行中ping自己的github.io域名，就可以得到IP地址。 1ping &lt;your-user-name&gt;.github.io 接着，在阿里云万网的管理控制台&gt;域名服务&gt;域名列表中设置域名解析即可。这时域名就绑定好了，可以通过buptccq.top 和www.buptccq.top 来访问博客。 注意：1、设置域名解析不是立即生效的，需要等10分钟左右。2、若用自定义的域名访问失败后，经过修改，避免浏览器的DNS缓存，可以使用浏览器的无痕模式。 写作博客+技巧在站点根目录下，执行以下命令创建一篇新文章： 1hexo new &lt;title&gt; 上述命令在站点根目录下的\\source_posts 文件夹生成一个 .md文件。按照markdown规则，编辑该文件。接下来介绍编辑md文件的几个技巧。 块引用1、一种方法。 12&gt; 只是块引用&gt; 是的啊 效果图如下： 只是块引用是的啊 markdown语法 参考链接markdown中文文档Markdown 入门参考MarkDown语法简介Markdown 书写风格指南 绘制表格用|来分隔不同的单元格，用-来分隔表头和其他行。关于对齐： :--- 左对齐，默认情况 :---: 居中 ----: 右对齐 使用方式举例： 1234|主公|刘备|曹操|孙权||----|----|----|----||政权|蜀|魏|吴||相关|关羽、张飞|曹丕、曹植|周瑜、鲁肃| 实现效果： 主公 刘备 曹操 孙权 政权 蜀 魏 吴 相关 关羽、张飞 曹丕、曹植 周瑜、鲁肃 删除线使用方式： 1~~删除线~~ 使用效果为：删除线 markdown其他用法12345678910111213141516171819202122&lt;!-- more --&gt; #在首页显示摘要 content #块引用&lt;www.baidu.com&gt; #链接[百度](www.baidu.com) #内嵌链接![](/path/to/image &apos;image name&apos;)有序列表1. 2.3.无序列表- - - **粗体字***斜体字*``代码``## 二级标题### 三级标题 文字居左，居中，居右居中 1&lt;center&gt;诶嘿&lt;/center&gt; 居左 1&lt;p align=&quot;left&quot;&gt;诶嘿&lt;/p&gt; 居右 1&lt;p align=&quot;right&quot;&gt;诶嘿&lt;/p&gt; 插入表情😀😇👹 给博客选用主题这里要介绍一下“主题”这个概念。主题也就是博客的样式。给博客安装一个好的主题，就好像给人选一件漂亮的衣服穿。hexo有多种不同的主题，可以改hexo选择不同的样式和布局。主题列表有详细的介绍。使用最多的主题是Next,接下来介绍主题的安装。 安装主题Next 先下载主题Next，在站点根目录下执行： 1git clone https://github.com/iissnan/hexo-theme-next themes/next 启用主题，打开站点配置文件，找到theme字段，把它的值修改为 next 1theme: next 验证主题。运行’hexo server’,访问http://localhost:4000。 选择Scheme通过修改主题配置文件完成。打开主题配置文件，找到字段scheme，将不要的主题注释掉，要用的主题不注释。 123#scheme: Muse#scheme: Mistscheme: Pisces 设置语言为中文通过修改站点配置文件完成。打开站点配置文件，找到字段language，把它值修改为 zh-Hans 1language: zh-Hans 主题美化-1设置上部的菜单栏 设置菜单内容。通过修改主题配置文件完成。打开主题配置文件，找到字段menu，不用的菜单栏选项注释掉，要用的不注释。 1234567menu: home: / archives: /archives # 冒号右边为主题目录下/source下的文件夹。 #about: /about #categories: /categories tags: /tags #commonweal: /404.html 设置菜单项的显示文本。编辑主题目录下的/languages/zh-Hans.yml。 12345678menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 commonweal: 公益404 设置菜单项的图标。编辑主题配置文件，找到字段menu_icons,修改为： 12menu_icons: enable: true 设置头像打开主题配置文件，找到字段avatar，把它的值修改为头像照片的路径。将头像照片放在主题目录/source/images(如果不存在就创建)。 1avatar: /images/avatar.png 添加‘标签’页面首先要在菜单栏显示‘标签’链接。在站点目录下，新建一个页面： 1hexo new page tags 编辑刚刚生成的页面,修改字段 title 和 type 123title: 标签date: 2019-01-13 12:22:58type: &quot;tags&quot; 在菜单栏中添加链接。修改主题配置文件，找到字段menu下的字段tags，把它值修改为 /tags 1234menu: home: / archives: /archives tags: /tags 添加‘分类’页面与添加‘标签’页面基本相同。首先要在菜单栏显示‘分类’链接。在站点目录下，新建一个页面： 1hexo new page categories 编辑刚刚生成的页面,修改字段 title 和 type 123title: 分类date: 2019-01-13 12:22:58type: &quot;categories&quot; 在菜单栏中添加链接。修改主题配置文件，找到字段menu下的字段tags，把它值修改为 /tags 12345menu: home: / archives: /archives tags: /tags categories: /categories 新建一个友链页面 在菜单栏中添加链接和链接图标。编辑站点配置文件，找到字段menu，添加以下内容： 12menu: links: /links/ || link # ||左边代表文件夹/source/links,右边代表图标 修改链接的显示文本，编辑文件/themes/next/languages/zh-Hans.yml,找到字段menu。 12menu: links: 友链 在站点目录下执行以下命令,会生成/source/links/index.md文件。通过编辑该文件可以控制友链页面的显示内容。 1hexo new page links 关闭’标签’、’分类’页的评论分别编辑站点目录下的/source/tags/index.md和/source/categories/index.md,添加字段comments,把它的值修改为false。 1comments: false 侧边栏添加社交链接通过修改主题配置文件完成。打开 主题配置文件 ，修改两个部分，链接和链接图标。1、添加链接。找到字段 social ，一行为一个链接。格式为 显示文本：链接 1234# Social linkssocial: GitHub: https://github.com/your-user-name Twitter: https://twitter.com/your-user-name 2、修改链接图标。找到字段 social_icons 。键值对格式为 匹配键：图标名称 。在图标库找到合适的图标，将名字作为图标名称。 123456# Social Iconssocial_icons: enable: true # Icon Mappings GitHub: github Twitter: twitter 侧边栏添加友情链接打开 主题配置文件 ，找到字段 links 。键值对格式为 显示文本：链接 12345678# Blog rollslinks_icon: link #链接的图标links_title: 推荐阅读 #链接的名称links_layout: block #链接的布局，一行一个链接#links_layout: inline #一行多个链接links: # Title: http://example.com/ 廖雪峰: https://www.liaoxuefeng.com/ 开启打赏功能通过修改主题配置文件完成。打开 主题配置文件 ，找到以下字段。将收款二维码放到站点目录下的/source/images/(没有该文件夹，就新建)中。 123reward_comment: 坚持原创技术分享，您的支持将鼓励我继续创作！wechatpay: /path/to/wechat-reward-imagealipay: /path/to/alipay-reward-image 修改打赏字体不闪动修改文件MyBlog/themes/next/source/css/_common/components/post/post-reward.styl，注释文字闪动的函数。 123456789101112/*注释文字闪动的函数#wechat:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;#alipay:hover p&#123; animation: roll 0.1s infinite linear; -webkit-animation: roll 0.1s infinite linear; -moz-animation: roll 0.1s infinite linear;&#125;*/ 订阅微信公众号在每篇文章的末尾显示微信公众号。把微信公众号二维码放在MyBlog/themes/next/source/images/下。编辑 主题配置文件 ，找到字段wechat_subscriber，修改它的值。 12345# Wechat Subscriberwechat_subscriber: enabled: true qcode: /path/to/your/wechatqcode description: 欢迎关注我 设置腾讯公益404界面1、在目录MyBlog/themes/next/source/下新建404.html页面，内容如下： 123456789101112131415161718&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 2、修改 主题配置文件 ,找到字段 menu 下的字段 commonweal，把它值修改为 /404.html 12menu: commonweal: /404.html || heartbeat 设置背景动画编辑 主题配置文件。以下四个字段为不同的背景动画效果，要想启用，将false改为true。 1234567891011# Canvas-nestcanvas_nest: false# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false 添加评论系统‘网易云跟帖’评论已经不提供服务。我们采用’来必力’评论系统时，需要获取uid。1、先在来必力的官方网站注册账号，在&gt;安装下安装city版本，再在&gt;管理页面&gt;代码管理下获取uid。data-uid字段的值就是我们需要的uid。2、编辑 主题配置文件 ，找到字段’livere_uid’,把它的值改为data-uid字段的值。 1livere_uid: #your uid 添加搜索功能通过Local Search搜索服务实现搜索功能。1、安装插件hexo-generator-searchdb，在站点的根目录下执行 1npm install hexo-generator-searchdb --save 2、编辑 站点配置文件,在文末新增以下内容： 12345search: path: search.xml field: post format: html limit: 10000 3、编辑主题配置文件,找到字段”local_search”，修改值为true。 12local_search: enable: true 主题美化-2设置【阅读全文】在首页只显示文章的部分内容，通过按钮【阅读全文】来实现跳转。有三种方式可以实现该功能，这里介绍最方便的一种。在文章中使用&lt;!-- more --&gt;来进行截断，&lt;!-- more --&gt;之前的内容显示在首页，后面的内容不显示。 1&lt;!-- more --&gt; 分页-设置页面文章的篇数为网站首页、归档页和标签页设置不同的文章篇数。1、使用npm install --save安装插件。 123npm install --save hexo-generator-indexnpm install --save hexo-generator-archivenpm install --save hexo-generator-tag 2、编辑站点配置文件,找到以下字段并修改。 12345678910index_generator: per_page: 10 archive_generator: per_page: 20 yearly: true monthly: truetag_generator: per_page: 10 设置网站的图标在EasyIcon中找一张中意的图标。修改图标名称为favicon.ico，将图标放在路径/next/source/images/下。编辑主题配置文件，找到字段favicon，修改它的值。 123favicon: small: /images/favicon.ico medium: /images/favicon.ico 添加顶部的加载条修改主题配置文件,找到字段pace,把值设为true。还可以选择不同风格的加载条。 12345678910111213141516pace: true # Themes list:#pace-theme-big-counter#pace-theme-bounce#pace-theme-barber-shop#pace-theme-center-atom#pace-theme-center-circle#pace-theme-center-radar#pace-theme-center-simple#pace-theme-corner-indicator#pace-theme-fill-left#pace-theme-flash#pace-theme-loading-bar#pace-theme-mac-osx#pace-theme-minimalpace_theme: pace-theme-flash 修改文章底部的标签#为修改文件 /themes/next/layout/_macro/post.swig,搜索rel=&quot;tags&quot;&gt;#，将#换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;。也可以在图标库找自己中意的图标。 修改网页底部的桃心编辑/themes/next/layout/_partials/footer.swig，找到以下代码,将第二个class的值并修改为&quot;fa fa-heart&quot;，在图标库找自己中意的图标。 123&lt;span class=&quot;with-love&quot;&gt; &lt;i class=&quot;fa fa-&#123;&#123; theme.footer.icon &#125;&#125;&quot;&gt;&lt;/i&gt;&lt;/span&gt; 实现统计功能使用npm install --save安装插件 1npm install hexo-wordcount --save 网站底部字数统计编辑/themes/next/layout/_partials/footer.swig，在文末添加以下代码： 1234&lt;div class=&quot;theme-info&quot;&gt; &lt;div class=&quot;powered-by&quot;&gt;&lt;/div&gt; &lt;span class=&quot;post-count&quot;&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 文章字数和阅读时长编辑主题配置文件，找到字段post_wordcount,并修改。 123456post_wordcount: item_text: true wordcount: true #字数统计 min2read: true #阅读时长统计 totalcount: true #站点总字数 separated_meta: true 统计访问量和阅读量我们用busuanzi实现统计功能。1、全局配置。编辑主题配置文件，找到字段busuanzi_count,将enable的值改为true。2、编辑字段site_uv,统计本站访客数。3、编辑字段site_pv,统计本站总访问量。4、编辑字段page_pv,统计每篇文章的阅读量。 123456789101112131415busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;本站访客数 site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt;本站总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt;本文总阅读量 page_pv_footer: 次 问题：busuanzi统计不显示数字（2019.1.15）。原因是buxuanzi的域名变更，由&lt;dn-lbstatics.qbox.me&gt;变为&lt;busuanzi.ibruce.info&gt;,导致统计功能不能实现。解决方案如下： 编辑文件themes\\next\\layout\\_third-party\\analytics\\busuanzi-counter.swig,找到以下代码 1&lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 将其修改为： 1&lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 重新推送后，可以看到统计功能正常。 实现点击出现桃心1、在路径/theme/next/source/js/src/下新建 love.js2、编辑文件/theme/next/layout/_layout.swig文件，在文末添加代码： 12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; 修改链接文本样式修改文件themes\\next\\source\\css\\_common\\components\\post\\post.styl,在文末添加如下代码。关于颜色的代码见详情 1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 置顶某篇文章修改文件MyBlog/node_modules/hexo-generator-index/lib/generator.js,将全部代码替换为 12345678910111213141516171819202122232425262728&apos;use strict&apos;;var pagination = require(&apos;hexo-pagination&apos;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &apos;page&apos;; return pagination(&apos;&apos;, posts, &#123; perPage: config.index_generator.per_page, layout: [&apos;index&apos;, &apos;archive&apos;], format: paginationDir + &apos;/%d/&apos;, data: &#123; __index: true &#125; &#125;);&#125;; 在文章front-matter中添加字段top，该字段的数值越大文章越靠前。 12title: 第三篇top: 10 文章加密访问编辑文件MyBlog/themes/next/layout/_partials/head.swig,在第五行插入以下代码： 1234567891011121314&lt;script&gt; (function () &#123; if (&apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; alert(&apos;密码错误！&apos;); if (history.length === 1) &#123; location.replace(&quot;http://xxxxxxx.xxx&quot;); // 这里替换成你的首页 &#125; else &#123; history.back(); &#125; &#125; &#125; &#125;)();&lt;/script&gt; 在文章front-matter中添加字段password，该字段的值不为空则文章加密，若字段的值为空则文章没有加密。 12title: 第三篇password: 自定义新建文章的md文件修改/scaffolds/post.md文件为： 123456789---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags: # 标签categories: # 分类copyright: # true添加版权信息top: # 数值越大，文章在首页越靠前。用于文章置顶password: # 为空则文章不加密。用于文章加密访问--- 显示浏览进度编辑主题配置文件，修改字段scrollpercent的值为true 12# Scroll percent label in b2t button.scrollpercent: true 使得HEXO的next主题渲染LATEX数学公式编辑主题配置文件，修改字段mathjax的值为true 123# MathJax Supportmathjax: enable: true 主题美化-3修改代码块自定义样式编辑\\themes\\next\\source\\css\\_custom\\custom.styl,在文末添加如下代码。关于颜色的代码见详情 123456789101112131415// Custom styles.code &#123; color: #ff7600; background: #fbf7f8; margin: 2px;&#125;// 大代码块的自定义样式.highlight, pre &#123; margin: 5px 0; padding: 5px; border-radius: 3px;&#125;.highlight, code, pre &#123; border: 1px solid #d6d6d6;&#125; 设置代码高亮主题Next使用 Tomorrow Theme作为代码高亮。有五个主题可供选择。1.编辑 主题配置文件，找到字段highlight_theme,修改它的值。 12345# Code Highlight theme# Available value:# normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: normal 实现代码变成彩色。修改站点配置文件，找到字段highlight下的字段auto_detect的值为true1234highlight: enable: true line_number: true auto_detect: true 添加代码块复制功能通过第三方插件clipboard.js来实现复制功能。1.目录/themes/next/source/js/src下新建两个 .js文件。 clipboard.min.js。下载第三方插件clipboard.min.js clipboard-use.js，文件内容如下： 1234567891011121314151617/*页面载入完成后，创建复制按钮*/!function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = &apos;&apos;; copyHtml += &apos;&lt;button class=&quot;btn-copy&quot; data-clipboard-snippet=&quot;&quot;&gt;&apos;; copyHtml += &apos; &lt;i class=&quot;fa fa-copy&quot;&gt;&lt;/i&gt;&lt;span&gt;复制&lt;/span&gt;&apos;; #复制按钮的图标和显示文本 copyHtml += &apos;&lt;/button&gt;&apos;; $(&quot;.highlight .code pre&quot;).before(copyHtml); new ClipboardJS(&apos;.btn-copy&apos;, &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode();&#125;(window, document); 2.在文件\\themes\\next\\source\\css\\_custom\\custom.styl文末添加代码： 123456789101112131415161718192021222324252627282930313233343536//代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 3.在.\\themes\\next\\layout\\_layout.swig文末添加代码： 123&lt;!-- 代码块复制功能 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clipboard-use.js&quot;&gt;&lt;/script&gt; 参见：Hexo NexT主题代码块添加复制功能Hexo next博客添加折叠块功能添加折叠代码块 文章摘要图片让图片出现在网站首页的文章摘要中，但不出现在文章的正文中。 编辑主题配置文件，修改字段expert_description的值为false 1234excerpt_description: falseauto_excerpt: enable: false 编辑文件themes/next/layout/_macro/post.swig，找到代码： 12&#123;\\% elif post.excerpt %\\&#125; &#123;&#123; post.excerpt &#125;&#125; 在它后面添加如下代码： 12345&#123;\\% if post.image %\\&#125;&lt;div class=&quot;out-img-topic&quot;&gt; &lt;img src=&#123;&#123; post.image &#125;&#125; class=&quot;img-topic&quot; /&gt;&lt;/div&gt;&#123;\\% endif %\\&#125; 编辑文件/themes/next/source/css/_custom/custom.styl,在文末添加如下代码：1234// 自定义的文章摘要图片样式img.img-topic &#123; width: 100%;&#125; 使用方式是，在文章的.md文件的front-matter加上一行image: path/to/image： 12345678910title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;keywords: # 用于SEO搜索引擎优化tags: # 标签categories: # 分类copyright: # true添加版权信息top: # 数值越大，文章在首页越靠前。用于文章置顶password: # 为空则文章不加密。用于文章加密访问description:# 用于设置【阅读全文】image: # 文章摘要图片 设置背景图片方法一通过jquery-backstretch,编辑文件/themes/next/layout/_layout.swig,在最后的&lt;/body&gt;之前添加 123&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;$(&quot;body&quot;).backstretch(&quot;https://背景图.jpg&quot;); 方法二编辑文件/themes/next/source/css/_custom/custom.styl,添加代码： 12345// Custom styles.body &#123; background:url(/images/background.jpg); #图片放在/images文件中 background-attachment: fixed; #固定背景图&#125; 搜索引擎优化（SEO）-提高网站排名我们搭建好博客之后，在搜索引擎（百度、Google）中搜索博客，会发现根本搜索不到。这是因为我们的博客网站没有没搜索引擎收录，也就搜索不到。因此我们需要进行搜索引擎优化。搜索引擎优化是为提高我们的博客在搜索结果中的排名，来增大网站的流量。通俗地说，就是让我们的博客更容易被搜索到。 让搜索引擎收录我们的网站。 提高网站在搜索结果中的排名。 hexo优化准备编辑配置文件站点配置文件站点配置文件中的这四项一定要写 1234title: subtitle: description: url: 打开Next主题自带的SEO配置编辑主题配置文件,找到这四个字段，并设置为true。 1234canonical: trueseo: trueindex_with_subtitle: truebaidu_push: true 设置关键字 设置博客的关键字。编辑站点配置文件，添加字段keywords，值用逗号隔开。 设置文章的关键字。在文章中，添加字段keywords，值用逗号隔开。1keywords: word1,word2,word3 网站首页title优化我们知道读文章时，标题和摘要往往比段落中的大段文字重要。同样在搜索引擎爬取收录网页时，title起到更重要的作用。关键词出现在网站title中，可以提高网站被搜索到的可能。网站的title一般不超过80字符。我们把网站首页的title改为网站名称-关键词-网站描述的形式。编辑文件\\themes\\next\\layout\\index.swig，将以下代码 1&#123;\\% block title %\\&#125;&#123;&#123; config.title &#125;&#125;&#123;\\% if theme.index_with_subtitle and config.subtitle %\\&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;\\% endif %\\&#125;&#123;\\% endblock %\\&#125; 修改为，可以看到通过插入和来添加关键词和网站描述。 1&#123;\\% block title %\\&#125;&#123;&#123; config.title &#125;&#125; - &#123;&#123; config.keywords &#125;&#125; - &#123;&#123; theme.description &#125;&#125;&#123;\\% if theme.index_with_subtitle and config.subtitle %\\&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;\\% endif %\\&#125;&#123;\\% endblock %\\&#125; 网站名称、关键词、网站描述的显示文本分别对应站点配置文件中的字段title，keywords和description的值。 1234567title: spring&apos;Blog #网站标题subtitle: description: #对应网站描述keywords: #关键词author: spring-quanlanguage: zh-Hanstimezone: 给所有外部链接添加nofollow标签 使用npm install --save安装插件hexo-autonofollow 1npm install hexo-autonofollow --save 编辑站点配置文件,在文末添加如下代码。不想添加的链接写在字段exclude后。 12345nofollow: enable: true exclude: - exclude1.com - exclude2.com 提交链接和验证网站这一步是让搜索引擎收录我们的网站。我们需要在搜索引擎提交我们的链接，提交链接时需要验证这个网址是我们的，不是别人的。以下为搜索引擎的网站管理工具 google的网站管理工具Search Console 百度的搜索资源平台 搜索引擎给出了网站验证的多种方式，百度和谷歌进行网站验证的原理和步骤差不多。这里介绍两种验证方式：html文件验证和CNAME验证html文件验证 下载验证文件.html，放到站点目录/source下。 hexo会自动对HTML文件进行渲染，但我们不希望验证文件被渲染，需要做一些配置。修改站点配置文件，编辑字段skip_render的值为 验证文件名.html。该字段后可以有多个值，以逗号分隔。该字段后的文件名会跳过渲染。 1skip_render: google6af533d85b9bfd8c.html,baidu_verify_mreXNGCYRV.html 清除缓存并将修改同步到github. 12hexo cleanhexo g -d 点击完成验证。 CNAME验证 CNAME验证是设置域名解析。需要登录阿里云万网。 选择控制台&gt;产品与服务&gt;域名，管理域名的解析。添加CNAME类型的解析。 站点地图sitemap和爬虫协议robot.txt 站点地图是一个文件。告诉Google和百度应该抓取哪些网页。robot.txt也是一个文件。告诉Google和百度不应该抓取哪些网页。 站点地图sitemap先添加站点地图文件，再提交给Google和百度。告诉Google和百度应该抓取哪些网页。 使用命令安装自动生成站点地图sitemap的插件。 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 编辑站点配置文件，在文末添加以下代码： 1234sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml 执行命令，会在站点目录下的public文件夹生成站点地图文件sitemap.xml和baidusitemap.xml文件，前者提交给Google，后者提交给百度。 1hexo g 把站点地图文件提交给Google和百度。 在Google的旧版Search Console提交站点地图文件https://buptccq.top/sitemap.xml(修改为你自己的域名)。 在百度的网站管理工具提交站点地图文件https://buptccq.top/baidusitemap.xml(修改为你自己的域名)。 爬虫协议robot.txt，又成蜘蛛协议添加爬虫协议文件robot.txt，再提交给Google和百度。告诉Google和百度不要抓取哪些网页。 在站点目录source下新建文件robot.txt。内容如下，注意修改为自己的域名。 12345678910111213141516#hexo robots.txtUser-agent: *Allow: /Allow: /archives/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: http://blog.tangxiaozhu.com/search.xml #注意修改为你自己的域名Sitemap: http://blog.tangxiaozhu.com/sitemap.xmlSitemap: http://blog.tangxiaozhu.com/baidusitemap.xml 执行命令hexo d -g部署到github上。 1hexo d -g 再把爬虫协议文件提交给Google和百度。 Google的旧版Search Console。 百度的网站管理工具。 谷歌收录和百度收录查看网站是否被收录在百度或Google浏览器中，以site:&lt;your url&gt;形式搜索你的博客。如果能搜到，就是被收录了。搜不到就是没有收录。 谷歌收录百度收录我们在前面已经把博客首页的网址收录到百度搜索引擎了，但当我们写出新的博客，产生新的链接后，需要把这些新的链接收录到百度。这里介绍两种实现自动收录的方式：主动推送和自动推送主动推送主动推送将新产生的链接立即推送给百度搜索引擎，从而保证新链接被尽早收录。 使用命令npm install --save安装自动百度推送插件。 1npm install hexo-baidu-url-submit --save 编辑站点配置文件，在文末添加以下代码： 12345baidu_url_submit: count: 3 ## 比如3, 代表提交最新的三个链接 host: &lt;www.henvyluk.com&gt; ## 在百度站长平台中注册的域名 token: &lt;your_token&gt; ## 请注意这是您的秘钥, 请不要发布在公众仓库里! path: baidu_urls.txt ## 文本文档的地址, 新链接会保存在此文本文档里 其中&lt;your_token&gt;可以在百度网址管理工具中找到： 需要注意的是，编辑站点配置文件，确保字段url的值为你的域名。 123# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://buptccq.top #http://yoursite.com 编辑站点配置文件，找到字段deploy,添加一个新的deploy类型-type: baidu_url_submitter。不同的deploy类型用-type表示。 123456deploy:- type: baidu_url_submitter- type: git # repository: https://github.com/spring-quan/spring-quan.github.io repository: git@github.com:spring-quan/spring-quan.github.io.git branch: master 主动推送的原理如下： 产生新链接。hexo g会生成文本文件baidu_urls.txt，里面包含最新的链接。 推送新链接。hexo d会从文本文件baidu_urls.txt中读取新链接，再推送给百度。 自动推送编辑主题配置文件，修改字段baidu_push的值为true。 12# Enable baidu push so that the blog will push the url to baidu automatically which is very helpful for SEObaidu_push: true 参见：基于Hexo搭建个人博客——进阶篇(从入门到入土)Hexo Seo优化让你的博客在google搜索排名第一 ##踩过的坑 “hexo server”本地启动时，报错如下： 1Unhandled rejection Error: ENOENT: no such file or directory, open &apos;/MyBlog/themes/next/layout/_scripts/schemes/.swig&apos; 错误原因处在主题配置文件_config.yml存在不规范的内容。要仔细检查，规范化内容就行。修改过来之后，要先执行hexo clean,再hexo server就可以了。 参考文章 Hexo官方教程 Next官方教程 markdown中文文档 打造个性超赞博客Hexo+NexT+GitHubPages的超深度优化 【推荐，整理很系统】 基于Hexo搭建个人博客——进阶篇(从入门到入土) hexo的next主题个性化教程:打造炫酷网站 Hexo搭建博客教程 手把手教你使用Hexo + Github Pages搭建个人独立博客 从 0 开始搭建 hexo 博客","categories":[{"name":"技术资料","slug":"技术资料","permalink":"http://yoursite.com/categories/技术资料/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"搭建博客","slug":"搭建博客","permalink":"http://yoursite.com/tags/搭建博客/"}]}]}