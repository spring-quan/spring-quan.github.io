<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>spring&#39;s Blog</title>
  
  <subtitle>游龙当归海，海不迎我自来也。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-01-05T03:30:01.152Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>spring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文笔记《》</title>
    <link href="http://yoursite.com/2020/01/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AAssigning-Personality%EF%BC%88Profile%EF%BC%89-to-a-Chatting-Machine-for-Coherent-Conversation-Generation%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/05/论文笔记《Assigning-Personality（Profile）-to-a-Chatting-Machine-for-Coherent-Conversation-Generation》/</id>
    <published>2020-01-05T03:10:26.000Z</published>
    <updated>2020-01-05T03:30:01.152Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】ICJAI2018<br>【链接】<a href="https://arxiv.org/abs/1706.02861" target="_blank" rel="noopener">https://arxiv.org/abs/1706.02861</a><br>【数据集】<a href="http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality" target="_blank" rel="noopener">http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是清华大学<a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈教授组</a>的2017年的工作，2018年发表在IJCAI。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>论文的研究内容是赋予对话系统以个性化信息(personality/profile)来生成具有一致性的回复。具体来说，对话语料中用键值对属性值来描述用户画像。对话系统先使用一个profile detector来检测生成回复时是否使用个性化信息。如果要使用，从所有的键值对属性用户画像中选择一个键值对来生成回复。采用一个bidirectional decoder来生成回复，让键值对出现在生成的回复中。进一步地，为了提高bidirectional decoder的性能，采用了position detector来检测键值对在回复中的位置。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>论文提出的对话模型包括了三个重要模块。profile detector检测是否使用用户画像并选择一个键值对属性。bidirectional decoder根据选中的键值对属性来生成回复。position detector检测键值对属性值在回复中出现的位置。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】ICJAI2018&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/abs/1706.02861&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1706.02861&lt;/a&gt;&lt;br&gt;【数据集】&lt;a href=&quot;http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Bidirectional Decoder" scheme="http://yoursite.com/tags/Bidirectional-Decoder/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Learning Personalized End-to-End Goal-Oriented Dialog》</title>
    <link href="http://yoursite.com/2020/01/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ALearning-Personalized-End-to-End-Goal-Oriented-Dialog%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/03/论文笔记《Learning-Personalized-End-to-End-Goal-Oriented-Dialog》/</id>
    <published>2020-01-03T01:57:32.000Z</published>
    <updated>2020-01-03T04:08:45.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】AAAI2019<br>【链接】<a href="https://arxiv.org/pdf/1811.04604v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.04604v1.pdf</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是北京大学于2019年初发表的。研究内容主要是将用户个性化信息(personalization)结合到端到端的任务型对话模型中，来调整回复的策略和语言风格，并消除歧义。</p><h3 id="个性化对话系统的相关工作"><a href="#个性化对话系统的相关工作" class="headerlink" title="个性化对话系统的相关工作"></a>个性化对话系统的相关工作</h3><p>最早探索个性化对话系统的研究工作是 <a href="https://nlp.stanford.edu/~bdlijiwei/index.html" target="_blank" rel="noopener">Li Jiwei</a>于16年发表的论文<a href="https://arxiv.org/pdf/1603.06155v2.pdf" target="_blank" rel="noopener">《A Persona-Based Neural Conversation Model》</a>。这个工作的具体来说是，给chatbot agent赋予特定的人格来生成一致性的回复。另一个思路，我们更希望chatbot agent可以感知到用户的身份和偏好，来提供个性化的对话。<br>个性化的对话系统也是分为闲聊式对话和任务型对话。</p><ul><li>个性化闲聊式对话的训练语料有<a href="https://arxiv.org/abs/1801.07243" target="_blank" rel="noopener">Persona-Chat</a>和在此基础上扩展得到的<a href="https://arxiv.org/abs/1902.00098" target="_blank" rel="noopener">CONVAI2</a>，另外还有<a href="https://arxiv.org/abs/1809.01984" target="_blank" rel="noopener">Mazare et al.(2018)</a>基于Reddit Corpus构建的个性化对话语料。</li><li>个性化的任务型对话系统的训练语料有<a href="https://arxiv.org/abs/1706.07503" target="_blank" rel="noopener">personalized bAbI dialog corpus</a>。</li></ul><h3 id="缺乏个性化的任务型对话系统的不足"><a href="#缺乏个性化的任务型对话系统的不足" class="headerlink" title="缺乏个性化的任务型对话系统的不足"></a>缺乏个性化的任务型对话系统的不足</h3><p>只基于对话历史而未考虑个性化的任务型对话系统存在以下不足：</p><ol><li>不能根据用户的身份和偏好来动态地调整语言风格。</li><li>缺乏根据用户的信息来动态调整对话策略的能力。</li><li>难以处理用户请求中的歧义项。比如：“contact”可以理解为“电话”，也可以理解为“社交媒体”。个性化模型可以学习到年轻人倾向于社交媒体，而老年人倾向于电话这一事实，从而消除歧义。</li></ol><p>个性化任务型对话系统的研究动机就是解决上述问题。个性化的任务型对话与chatbot有所区别。个性化的chatbot研究倾向于赋予chatbot以特定的用户画像，来生成一致性的回复。而个性化的任务型对话更多的是感知到用户的身份和偏好，从而根据不同的用户身份来调整回复的语言风格和对话策略，从而提高对话效率和用户满意度。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>本文的个性化任务型对话模型是基于Memory Network的，因此对Memory Network做简单的介绍。Memory Network的相关工作有许多，本文中介绍的是发表在ICLR2017的<a href="https://arxiv.org/abs/1605.07683" target="_blank" rel="noopener">《Learning end-to-end goal-oriented dialog》</a>，这是基于检索的任务型对话。Memory Network包括了两个部分：context memory和next sentence prediction。</p><h4 id="Memory-Representation"><a href="#Memory-Representation" class="headerlink" title="Memory Representation"></a>Memory Representation</h4><p>memory中储存的是对话历史。在时间步t，对话历史为$t$句用户的对话$\lbrace{c_1^u,c_2^u,…,c_t^u}\rbrace$以及$t-1$句对话系统的回复$\lbrace{u_1^r,u_2^r,…,u_{t-1}^r}\rbrace$。直接采用了词袋方法，经过embedding层，将对话历史表示为向量。 $$m = (A\Phi(c_1^u),A\Phi(c_1^r),…,A\Phi(c_{t-1}^u),A\Phi(c_{t-1}^r))$$ 其中$\Phi(\cdot)$是one-hot向量表示，$A$是embedding table。<br>用同样的方法，将上一句话$c_t^u$编码为attention机制的的query：$$q = A\Phi(c_t^u)$$ </p><h4 id="Memory-Operation"><a href="#Memory-Operation" class="headerlink" title="Memory Operation"></a>Memory Operation</h4><p>采用attention机制对context memory进行读取。计算方式如下：$$o = R\sum_i\alpha_im_i$$ $$\alpha_i = softmax(q^\top m_i)$$ 其中$R$是线性层的权重矩阵。<br><strong>多跳机制</strong><br>将query按下式进行更新，再采用attention机制对context memory进行读取。$$q_2 = q + o$$</p><h4 id="next-sentence-prediction"><a href="#next-sentence-prediction" class="headerlink" title="next sentence prediction"></a>next sentence prediction</h4><p>设有C个候选句子$y_i$。先将候选句子进行向量化表示: $$r_i = W\Phi(y_i)$$ 其中$W$是另一个embedding table。<br>则最终的预测概率分布为：$$\hat{r} = softmax({q_{N+1}}^\top r_1,…,{q_{N+1}}^\top r_C)$$</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型包括profile model和preference model。profile model将用户的个性化信息结合到模型中。preference model建模用户信息与知识库之间的联系，来消除歧义。</p><div align="center"><img src="/images/personalized_memnet.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="profile-model"><a href="#profile-model" class="headerlink" title="profile model"></a>profile model</h4><p><strong>user embedding的计算</strong><br>对话语料中用$n$个键值对属性$\lbrace(k_i,v_i)\rbrace_{i=1}^n$来描述用户画像。第$i$个属性被表示为one-hot vector $a_i \in R^{d_i}$，其中$d_i$表示第$i$个属性$k_i$可能的取值个数。则总的用户画像的one-hot表示为$$\hat{a} = concat(a_1,a_2,…,a_n)$$ 有$\hat{a}\in R^{d_p}$，其中$d_p = \sum_{i=1}^nd_i$<br>进一步将用户画像表示为分布式向量$$p = P\hat{a}$$ 其中$P$可以看作是embedding table。</p><p><strong>将$p$结合到对话模型</strong><br>将user embedding $p$结合到模型中的两个地方。</p><ol><li>结合到多跳机制的query更新公式中。query对于context memory的读取和预测概率的生成起着重要作用。 $$q_{i+1} = q_i + o_i + p$$</li><li>根据user embedding对候选句子的向量表示进行修改。 $$r_i^* = \sigma(p^\top r_i) r_i$$</li></ol><p><strong>global memory</strong><br>除了context memory外，模型还有一个memory用来储存相似用户的对话历史。本文中将相似用户定义为有相同的属性值的用户。global memory的读取方式与context memory的读取方式相同。最后将两部分的query结合起来：$$q^+ = q + q^g$$</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.luolc.com/" target="_blank" rel="noopener">本文作者的个人主页</a></li><li><a href="https://www.luolc.com/publications/personalized-goal-oriented-dialog/" target="_blank" rel="noopener">作者的英文论文简介</a></li><li><a href="https://mp.weixin.qq.com/s/AqzdRoXthrUFUOqSNwgfqQ" target="_blank" rel="noopener">作者的中文论文简介</a></li><li><a href="https://helicqin.github.io/2018/12/11/Learning%20Personalized%20End-to-End%20Goal-Oriented%20Dialog/" target="_blank" rel="noopener">本篇论文的阅读笔记</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】AAAI2019&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/pdf/1811.04604v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1811.04604v1.pdf&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Goal-Oriented Dialog" scheme="http://yoursite.com/tags/Goal-Oriented-Dialog/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《A Dynamic Speaker Model for Conversational Interactions》</title>
    <link href="http://yoursite.com/2020/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AA-Dynamic-Speaker-Model-for-Conversational-Interactions%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/02/论文笔记《A-Dynamic-Speaker-Model-for-Conversational-Interactions》/</id>
    <published>2020-01-02T12:31:22.000Z</published>
    <updated>2020-01-02T14:08:56.015Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://www.aclweb.org/anthology/N19-1284/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/N19-1284/</a><br>【代码】：<a href="https://github.com/hao-cheng/dynamic_speaker_model" target="_blank" rel="noopener">https://github.com/hao-cheng/dynamic_speaker_model</a></p></blockquote><a id="more"></a><p>这篇论文是由华盛顿大学发表的。</p><h3 id="个性化对话系统的研究现状"><a href="#个性化对话系统的研究现状" class="headerlink" title="个性化对话系统的研究现状"></a>个性化对话系统的研究现状</h3><p>近几年来，基于用户画像（personal information）来生成个性化的回复已经成为对话系统领域的一个研究热点。为什么要将persona结合到对话系统模型中呢？目的是提高生成回复的一致性，来获取用户信任，让用户在对话中更投入。生成回复的一致性具体指什么呢？举个例子，你问对话系统“你的职业是什么？”，它可能回答是出租车司机；当你再次问这个问题时，它又可能回答是老师。给定对话系统一个用户画像（persona），基于这个persona来生成回复，就可以提高生成回复的一致性，避免出现这种问题。基于persona来生成个性化的回复，也是对话系统可以通过图灵测试的必要条件。</p><p>现有的工作中，将persona结合到对话系统模型中的思路有两种。</p><ol><li>学习一个潜在的向量user embedding（或user representation）来潜在地表示用户画像，再基于这个user embedding来生成个性化的回复。这样做的一个原因是现有的对话语料中没有相应的用户画像文本信息，随着个性化对话数据集<a href="https://arxiv.org/abs/1801.07243" target="_blank" rel="noopener">Persona-Chat</a>以及<a href="https://arxiv.org/abs/1902.00098" target="_blank" rel="noopener">CONVAI2</a>的提出，就有了第二种思路。</li><li>直接用键值对属性值信息或者是自由文本来明确地描述用户画像，再生成个性化的回复。</li></ol><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>这篇论文属于第一种思路，主要内容是用神经网络模型从对话历史中学习一个动态更新的user embedding。并且将学习到的user embedding用到了两个下游任务（对话话题分类、dialog acts分类）中来评估user embedding的学习效果。<br>论文中提到，学习一个动态更新的user embedding的动机有两个：</p><ol><li>用户的对话反映了这个用户的对话意图、语言风格等特征。因此，可以从用户的对话中来学习user embedding来建模和表征用户的这些个性化特征。</li><li>随着对话的进行，用户的个人信息得到累积。因此，可以从对话中提炼和动态更新user embedding。</li></ol><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>如下图所示，模型包括三个部分：<strong>Latent Mode Analyzer</strong>，<strong>Speaker State Tracker</strong>，<strong>Speaker Language Predictor</strong>。 </p><div align="center"><img src="/images/Dynamic_user_embedding.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font></div><h4 id="Latent-Mode-Analyzer"><a href="#Latent-Mode-Analyzer" class="headerlink" title="Latent Mode Analyzer"></a>Latent Mode Analyzer</h4><p><strong>Latent Mode Analyzer</strong>的作用是从输入的单轮对话中学到一个 local speaker vector。采用了Bi-LSTM + Attention机制。<br>时间步t，输入为单轮对话$\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}\rbrace}$，经过embedding层后送入到Bi-LSTM层，将前向LSTM和后向LSTM的最后一个隐藏状态连接起来作为句子总的向量表示$s_t$: $$s_t=[e^F_{t,N_t},e^B_{t,1}]$$ 其中，$e^F_{t,N_t},e^B_{t,1}$分别表示前向LSTM和后向LSTM的最后一个隐藏状态。<br>接着，再使用attention机制。将句子的向量表示作为attention机制的query，将 $K$ 个全局的mode vectors $\lbrace{u_1,u_2,…,u_K\rbrace}$作为attention机制的keys和values。这$K$个mode vectors也是模型参数，可以看作是用户在不同方面的个性化特征。那么可以通过attention机制计算得到local speaker vector $\tilde{u_t}$ :$$\tilde{u_t} = \sum_{k=1}^Ka_{t,k}u_k$$ $$a_{t,k} = softmax(\beta_{t,k})$$ $$\beta_{t,k} = &lt;Ps_t,Qu_k&gt;$$ 其中$&lt; , &gt;$表示点乘操作。</p><h4 id="Speaker-State-Tracker"><a href="#Speaker-State-Tracker" class="headerlink" title="Speaker State Tracker"></a>Speaker State Tracker</h4><p><strong>Speaker State Tracker</strong> 的作用是动态更新speaker state vector。实质上是一个单向的LSTM。<br>在时间步t，根据当前的local speaker vector $\tilde{u_t}$ 和 上一个时间步的隐藏状态$h_{t-1}$来更新隐藏状态$h_t$。将$h_t$作为时间步t的speaker state vector。$$h_t = LSTM(\tilde{u_t},h_{t-1})$$</p><h4 id="Speaker-Language-Predictor"><a href="#Speaker-Language-Predictor" class="headerlink" title="Speaker Language Predictor"></a>Speaker Language Predictor</h4><p><strong>Speaker Language Predictor</strong>的作用是促进前两个模块的参数学习，根据speaker state vector来重构句子$\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}}\rbrace$。<br>该模块实质上是一个条件语言模型，预测条件概率 $P(w_{t,n}|w_{t,&lt;n})$。采用了单向的LSTM，LSTM的隐藏状态更新公式为：$$d_{t,n} = LSTM(R^I(w_{t,n-1},h_t),d_{t,n-1})$$ 其中$R^I()$是一个线性层。<br>则语言模型生成下一个词的条件概率为：$$P(w_{t,n}|w_{t,&lt;n}) = softmax(VR^O(h_t,d_{t,n}))$$ 其中$R^O()$是一个线性层。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>模型最小化负对数似然函数来更新模型参数: $$L = -\sum_{t}\sum_{n}log P(w_{t,n}|w_{t,&lt;n})$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://www.aclweb.org/anthology/N19-1284/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aclweb.org/anthology/N19-1284/&lt;/a&gt;&lt;br&gt;【代码】：&lt;a href=&quot;https://github.com/hao-cheng/dynamic_speaker_model&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hao-cheng/dynamic_speaker_model&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Towards Knowledge-Based Personalized Product Description Generation in E-commerce》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ATowards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Towards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce》/</id>
    <published>2019-10-25T10:52:24.000Z</published>
    <updated>2019-10-25T14:49:12.478Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ARXIV 2019<br>【链接】：<a href="https://arxiv.org/abs/1903.12457v3" target="_blank" rel="noopener">https://arxiv.org/abs/1903.12457v3</a><br>【代码、数据集】：<a href="https://github.com/THUDM/KOBE" target="_blank" rel="noopener">https://github.com/THUDM/KOBE</a></p></blockquote><a id="more"></a><p>清华大学和阿里巴巴发表的论文。论文的研究内容是给定商品名称，商品的属性特征和外部知识库，自动生成商品的描述。</p><p><strong>数据集描述</strong><br>论文在淘宝收集了一个真实的商品描述数据集，包含了212,9187个商品名称和描述。数据集是公开的，下载地址为：<a href="https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717" target="_blank" rel="noopener">https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717</a></p><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>给定商品名称$x = \{x_1,…,x_n\}$，要求生成个性化的、富有信息的商品描述$y = \{y_1,…,y_m\}$。引入两个附加信息：商品属性 和 外部知识：</p><ol><li>Attributes<br> 每个商品名称$x$对应$l$个属性$a = \{a_1,…,a_l\}$。论文中包含两种属性，商品的某个方面（如质量、外观等）和用户类型（反映用户的兴趣）。</li><li>Knowledge<br> 论文采用一个大规模的中文知识图谱<code>CN-DBpedia</code>作为外部knowledge。<code>CN-DBpedia</code>包含大量命名实体$V$索引的原始文本条目$W$。每个条目包含一个命名实体$v &ensp; \in V$作为key，对应一个knowledge句子 $w = \{w_1,…,w_u\} \in W$作为value。</li></ol><p>最终的任务定义为：给定商品名称$x$、商品属性$a$和相关的knowledge $w$，要求生成个性化的，信息量丰富的回复$y$。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型采用了基于Transformer的encoder-decoder框架，结合了两个模块：Attribute Fusion和knowledge Incorporation。</p><div align="center"><img src="/images/KOBE.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="encoder-decoder框架"><a href="#encoder-decoder框架" class="headerlink" title="encoder-decoder框架"></a>encoder-decoder框架</h4><p>Transformer是完全基于self-attention机制 和 前馈神经网络（FFN,feed-forward neural network）的。</p><ol><li><strong>encoder</strong><br> 对于输入$x = \{x_1,…,x_n\}$，先经过embedding层得到word embedding representation $e = \{e_1,…,e_n\}$，连同position embedding一起，作为encoder layers的输入，得到context representation $h = \{h_1,…,h_n\}$。<br> 在embedding层之上，encoder layers由完全相同的6层堆叠组成，每层transformer包括<code>multi-head self-attention</code>和<code>FFN</code>两部分。<ol><li><strong>attention机制</strong><br> attention机制根据queries $Q$和keys $K$计算在values $V$上的分布，进而得到attention的输出。$$C = \alpha V$$ $$\alpha = softmax(f(Q,K))$$其中$C$表示attention的输出，$\alpha$表示attention的分布，$f$表示计算attention分数的函数。</li><li><strong>uni-head attention</strong><br> 对于单头attention，queries $Q$，keys $K$和values $V$分别是输入context $e$的线性转换。即$Q = W_Qe$, $K = W_Ke$ 和 $V = W_Ve$。此时，uni-head attention可以表示为 $$C_{self} = softmax(\frac{QK^T}{\sqrt{d_k}})V$$ 其中$d_k$表示输入$e$的维度。</li><li><strong>multi-head attention</strong><br> 对于多头attention，将$C^i_{self}, i\in \{1,2,…,c\}$连接起来，作为FFN的输入。其中$c$表示heads的数量。<br>再经过前馈神经网络（FFN），FFN的函数表示为：$$FFN(z) = W_2(Relu(W_1z + b_1)) + b_2$$</li></ol></li><li><strong>decoder</strong><br> 与encoder类似，decoder也是由完全相同的6层堆叠而成的，每层包含<code>multi-head attention</code>和<code>FFN</code>两部分。不同于encoder的”self-attenion”，decoder的<code>multi-head attention</code>是“context attention”。queries $Q$是decoder state的线性转换，keys $K$和values $V$是context states $h = \{h_1,…,h_n\}$的线性转换。</li><li><strong>Training</strong><br> 模型的目标是最大化似然函数。模型的目标函数是：$$P(y|x) = \prod_{t=1}^m P(y_t|y_{&lt;t},x)\tag{1}$$</li></ol><h4 id="Attribute-Fusion模块"><a href="#Attribute-Fusion模块" class="headerlink" title="Attribute Fusion模块"></a>Attribute Fusion模块</h4><p>商品的属性$a = \{a_1,a_2\}$，包含商品方面 和 用户类型两个属性。先经过embedding层得到attribute representation $\{e_{a_1},e_{a_2}\}$，再做attribute average得到总的attribute representation $e_{attr}$： $$e_{attr} = \frac{1}{2}\sum_{i=1}^2e_{a_i}$$ 如何有效结合$e_{attr}$呢？在基于RNN的模型中，方法比较多，比如：用$e_{attr}$来attend context representation，或着作为decoder隐藏状态更新的输入等。但本文中的模型是基于Transformer的，直接将attribute embedding $e_{attr}$与word embedding $e_i$相加，来结合商品的属性信息。如Fig.2所示。</p><div align="center"><img src="/images/attributeFusion.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.2. Attribute Fusion的示意图</font></div><p>此时，公式（1）中的目标函数变为$$ P(y|x,a) = \prod_{t=1}^m P(y_t|y_{&lt;t},x,a)$$</p><h4 id="Knowledge-Incorporation"><a href="#Knowledge-Incorporation" class="headerlink" title="Knowledge Incorporation"></a>Knowledge Incorporation</h4><p>knowledge incorporation包括三个部分，knowledge检索，knowledge编码，和knowledge结合。</p><ol><li><strong>knowledge retrieval</strong><br> 给定商品名称$x = \{x_1,…,x_n\}$，对于每个word $x_i$匹配对应的命名实体$v_i \in V$。再根据命名实体$v_i$，从$W$中检索对应的knowledge $w_i$。对于每个商品，最多抽取5个匹配的knowledge，再用分隔符 “<sep>”连接起来。</sep></li><li><strong>knowledge encoding</strong><br> 类似于$x = \{x_1,…,x_n\}$通过encoder编码得到context representation $h = \{h_1,….,h_n\}$，将检索到的knowledge经过一个基于Transformer的knowledge encoder得到knowledge representation $u$。</li><li><strong>knowledge combination</strong><br> 用BiDAF(bidirectional attention flow)来结合context representation $h$和knowledge representation $u$。BiDAF计算两个方向的attention：title-to-knowledge attention和knowledge-to-context attention。<ol><li><strong>相似度矩阵S</strong><br> 先计算一个context representation $h \in R^{n \times d}$和knowledge representation $u \in R^{u \times d}$之间的相似度矩阵$S \in R^{n\times u}$。其中$S_{ij}$衡量第i个title word和第j个knowledge word之间的相似度。$$S_{ij} = \alpha(h_i,u_j) \in R$$ 其中$\alpha()$是计算两个向量之间相似度的函数。 $$\alpha(h,u) = W_s^T[h;u;h \cdot u], &emsp; W_s\in R^{3d}$$</li><li><strong>title-to-knowledge attention</strong><br> 表明了对于每个title word，哪个knowledge word是最相关的。<br> $a_i \in R^u$表示对于第i个title word，在所有knowledge words上的attention权重分布。$$a_i = softmax(S_{i:}) &emsp; \in R^u$$ 其中，$S_{i:}$表示相似度矩阵$S \in R^{n\times u}$的第i个行向量。对于所有的$i$，$a_i$满足：$$\sum_ja_{ij} = 1$$ 对于第i个title word，attended knowledge vector为：$$\widetilde{u_i} = \sum_ja_{ij}u_j$$ 则对于所有的title words $\{x_1,…,x_n\}$，有$\widetilde{u} \in R^{n \times d}$</li><li><strong>knowledge-to-title attention</strong><br> 表明了对于每个knowledge word，哪个title word是最相似的。<br> 计算在所有title words上的attention权重分布：$$b = softmax(max(S_{i:}))$$ 则attended title vector为$$\widetilde{h} = \sum_{k}b_kh_k$$ 把$\widetilde{h}$在列上复制n次，得到$\widetilde{h} \in R^{n\times d}$。</li><li><strong>输出融合</strong><br> 做一个简单的连接操作，得到组合representation： $[h;\widetilde{u};h\circ \widetilde{u};h\circ \widetilde{h}] \in R^{4d \times n}$</li></ol></li></ol><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li>BiDAF(bidirectional attention flow)可以参考:<ul><li><a href="https://spring-quan.github.io/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/" target="_blank" rel="noopener">论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》</a></li></ul></li><li>中文知识图谱<code>CN-DBpedia</code>可以参考：<ul><li><a href="https://www.semanticscholar.org/paper/CN-DBpedia%3A-A-Never-Ending-Chinese-Knowledge-System-Xu-Xu/2c69bbb3b7ba3f324276924bab6f41de467c928a" target="_blank" rel="noopener">《CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System》</a></li><li><a href="http://kw.fudan.edu.cn/cndbpedia/download/" target="_blank" rel="noopener">CN-DBpedia Dump数据下载</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ARXIV 2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1903.12457v3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1903.12457v3&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/THUDM/KOBE&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/THUDM/KOBE&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="BiDAF" scheme="http://yoursite.com/tags/BiDAF/"/>
    
      <category term="Knowledge-Based" scheme="http://yoursite.com/tags/Knowledge-Based/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Automatic Generation of Personalized Comment Based on User Profile》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AAutomatic-Generation-of-Personalized-Comment-Based-on-User-Profile%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Automatic-Generation-of-Personalized-Comment-Based-on-User-Profile》/</id>
    <published>2019-10-25T07:15:24.000Z</published>
    <updated>2019-10-25T09:09:29.504Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ACL2019<br>【链接】：<a href="https://arxiv.org/abs/1907.10371" target="_blank" rel="noopener">https://arxiv.org/abs/1907.10371</a><br>【代码、数据集】：<a href="https://github.com/Walleclipse/AGPC" target="_blank" rel="noopener">https://github.com/Walleclipse/AGPC</a></p></blockquote><a id="more"></a><p>北京大学发表在ACL2019的论文。论文的研究内容是基于User profile的评论生成。</p><h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><p>论文从微博收集了一个中文数据集，没有公开，但给出了部分样例数据。这个数据集可以看作基于persona的单轮对话数据集。将微博的博文看作对话历史，将用户的评论看作回复。用户画像包括两个部分，键值对形式的人口统计特征属性（年龄、性别、地区等）和 句子形式的个人描述（微博签名）。</p><div align="center"><img src="/images/PCdata.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 数据样例</font></div><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>给定博文 $X = \lbrace{x_1,…,x_n}\rbrace$和用户画像 $U = \lbrace{F,D}\rbrace$，其中$F = \lbrace{f_1,…,f_k}\rbrace$是用户的数值化属性特征，$D = \lbrace{d_1,…,d_l}\rbrace$是句子形式的个人描述。要求生成与personal profile一致的回复$Y = \lbrace{y_1,…,y_m}\rbrace$。$$Y^* = \underset{Y}{argmax}(Y|X,U)$$</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><div align="center"><img src="/images/PersonalComment.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.2.模型结构的示意图</font></div><h4 id="encoder-decoder框架"><a href="#encoder-decoder框架" class="headerlink" title="encoder-decoder框架"></a>encoder-decoder框架</h4><p>基本框架当然还是<code>seq2seq模型 + attention机制</code>。$X= \{x_1,…,x_n\}$经过encoder转换为向量表示$h^X = \{h^X_1,…,h_n^X\}$，encoder采用Bi-LSTM。$$h_t^X = LSTM_{enc}^X(h_{t-1}^X,x_t)$$ decoder采用单向LSTM，decoder的隐藏状态更新公式为：$$s_t = LSTM_{dec}(s_{t-1},[c_t^X;e(y_{t-1})]) \tag{1}$$其中$c_t^X$表示时间步t，在所有encoder hidden states $h^X = \{h^X_1,…,h_n^X\}$上使用attention得到的context vector。<br>则decoder生成词的概率分布为：$$p(y_t) = softmax(W_os_t)$$ 采用负对数似然函数作为目标函数，模型最大化真实回复 $Y^* = \{y_1,…,y_m\}$ 的似然函数：$$Loss = -\sum_{t=1}^m log\Bigl(p(y_t|y_{&lt;t},X,U)\Bigr)$$</p><h4 id="User-Feature-Embedding-with-Gated-Memory"><a href="#User-Feature-Embedding-with-Gated-Memory" class="headerlink" title="User Feature Embedding with Gated Memory"></a>User Feature Embedding with Gated Memory</h4><p>将用户的数值化特征属性$F = \{f_1,…,f_k\}$经过一个全连接层，得到向量表示$v_u$。$v_u$可以看作是<code>user feature embedding</code>，表明用户的个人特征。如果user feature embedding是静态的，在decode过程中会影响生成回复的语法性。为了解决这个问题，设计了一个<code>gated memory</code>来动态地表达用户的个人特征。<br>在decode过程中，保持一个<code>Internal personal state</code>$M_t$，在decode过程中$M_t$逐渐衰减，decode结束，$M_t$衰减为0，表示用户的个人特征完全表达了。$M_0$的初始值设为$v_u$。 $$g_t^u = sigmoid(W_g^us_t)$$ $$M_0 = v_u$$ $$M_t = g_t^u \cdot M_{t-1}, &emsp; t&gt;0$$ 引入输出门机制$g_t^o$来充值persona信息的流动：$$g_t^o = sigmoid(W_g^o[s_{t-1};e(y_{t-1});c_t^X])$$ 则时间步t，personal information为：$$M_t^o = g_t^o\cdot M_t$$</p><h4 id="Blog-User-Co-Attention"><a href="#Blog-User-Co-Attention" class="headerlink" title="Blog-User Co-Attention"></a>Blog-User Co-Attention</h4><p>实质上是在用户的个人描述$D = \{d_1,…,d_l\}$上使用attention机制。先用另一个persona encoder来编码$D = \{d_1,…,d_l\}$，得到向量表示$\{h_1^D,…,h_l^D\}$。persona encoder采用LSTM: $$h_t^D = LSTM_{enc}^D(h_{t-1}^D,d_t)$$ 在$\{h_1^D,…,h_l^D\}$上使用attention机制，得到总的persona context vector $c_t^D$ $$c_t^D = \sum_{j=1}^k\alpha_{tj}h_j^D$$ $$\alpha_{tj} = softmax(\beta_{tj})$$ $$\beta_{tj} = score(s_{t-1},h_j^D) = s_{t-1}W_ah_j^D$$ 结合$c_t^D$和$c_t^X$作为时间步t总的context vector $c_t$: $$c_t = [c_t^X,c_t^D]$$ 则式（1）中decoder的隐藏状态更新公式变为：$$s_t = LSTM_{dec}(s_{t-1},[c_t;e(y_{t-1});M_t^o])$$</p><h4 id="External-Personal-Expression"><a href="#External-Personal-Expression" class="headerlink" title="External Personal Expression"></a>External Personal Expression</h4><p>通过将<code>internal persona state</code>$M_t$和persona context vector $c_t^D$作为decoder隐藏状态更新的输入，来结合persona信息，进而影响decode过程。这种影响是隐性的，为了更明确地利用用户信息来指导word的生成，将用户信息直接作为输出层的输入。先计算一个<code>user representation</code> $r_t^u$: $$r_t^u = W_r[v_u;c_t^D]$$ 将$r_t^u$作为输出层的输入，则生成词的概率分布为：$$p(y_t) = softmax(W_o[s_t;r_t^u])$$</p><h3 id="相似论文"><a href="#相似论文" class="headerlink" title="相似论文"></a>相似论文</h3><ul><li><a href="https://arxiv.org/abs/1901.09672" target="_blank" rel="noopener">《Personalized Dialogue Generation with Diversified Traits》</a> <ul><li><a href="https://spring-quan.github.io/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8APersonalized-Dialogue-Generation-with-Diversified-Traits%E3%80%8B/" target="_blank" rel="noopener">笔记链接</a></li><li>异同点比较：<ul><li>不同点是：本篇论文用internal persona state $M_t$和personal context vector $c_t^D$来作为decoder隐藏状态$s_t$更新的输入，进而影响word的生成。<br>而相似的这篇论文中，将personal vector $v_p$ 作为attention机制的query，来attend对话历史。含义是用persona vector $v_p$来选择context相关的信息。</li><li>相同点是：两篇论文都把persona information 作为输出层的输入，来明确地影响word的生成。</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1907.10371&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1907.10371&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/Walleclipse/AGPC&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Walleclipse/AGPC&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Personalized Dialogue Generation with Diversified Traits》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8APersonalized-Dialogue-Generation-with-Diversified-Traits%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Personalized-Dialogue-Generation-with-Diversified-Traits》/</id>
    <published>2019-10-25T02:51:12.000Z</published>
    <updated>2019-10-25T06:33:49.351Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【链接】：<a href="https://arxiv.org/abs/1901.09672" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09672</a><br>【代码、数据集】：无</p></blockquote><a id="more"></a><p>三星电子中国、清华大学黄明烈教授提交到<code>ARXIV</code>的论文。论文的研究内容是基于persona的单轮对话系统。<br><strong>数据集描述</strong><br>论文中构建了一个<code>PersonaDialog</code>的数据集，但数据集没有公开。数据是从微博上爬取，把用户的博文作为对话的post，把用户的评论作为回复。persona是用键值对来描述用户的属性（年龄、性别、地址、兴趣标签等）。而不是用几句话的文本来描述用户画像的。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>给定单句的对话历史$X = \lbrace{x_1,…,x_n\rbrace}$，以及回复者的N个属性$T = \lbrace{t_1,…,t_N\rbrace}$，其中$t_i = &ensp; &lt;k_i,v_i&gt;$为键值对。要求生成与用户的画像相一致的回复$Y = \lbrace{y_1,y_2,…,y_m\rbrace}$。$$Y^* = \underset{Y}{arg max} P(Y|X,T)$$</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>基本框架当然是<code>seq2seq模型+attention机制</code>。将对话历史$X = \lbrace{x_1,…,x_n}\rbrace$经过encoder编码为$\lbrace{h_1,…,h_n}\rbrace$。设时间步t，decoder的上一个隐藏状态为$s_{t-1}$，计算attention得到总的context vector $c_t$：$$c_t = \sum_{i=1}^n\alpha_ih_i$$ $$alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i) = V^T\cdot tanh(W^1_{\alpha}s_{t-1} + W^2_{\alpha}h_i) \tag{1}$$decoder RNN的隐藏状态更新公式是：$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 生成$y_t$的概率分布为：$$p(y_t) = softmax(W_os_t + b_{out})$$</p><div align="center"><img src="/images/PersonaTraitFusion.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h3 id="Personality-Trait-Fusion"><a href="#Personality-Trait-Fusion" class="headerlink" title="Personality Trait Fusion"></a>Personality Trait Fusion</h3><p>这个模块用来把整合persona得到persona representation $v_p$，并用$v_p$来影响decode过程。<br>将回复者的属性描述$T = \lbrace{t_1,…,t_N}\rbrace$经过embedding层，得到对应的向量表示$\lbrace{v_{t_1},…,v_{t_N}}\rbrace$。论文提出了三种整合persona Trait的方法。</p><h4 id="Trait-Attention"><a href="#Trait-Attention" class="headerlink" title="Trait Attention"></a>Trait Attention</h4><p>计算decoder的隐藏状态$s_{t-1}$在Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$上的attention，来整合persona Trait，得到persona representation $v_p$。$$v_p = \sum_{i=1}^N\alpha_i^pv_{t_i}$$ $$\alpha_i^p = softmax(\beta_i^p)$$ $$\beta_i^p = score(s_{t-1},v_{t_i}) = V_p^T \cdot tanh(W_p^1s_{t-1} + W_p^2v_{t_i})$$</p><h4 id="Trait-Average"><a href="#Trait-Average" class="headerlink" title="Trait Average"></a>Trait Average</h4><p>直接在Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$上取平均值，来作为persona representation $v_p$：$$v_p = \frac{1}{N}\sum_{i=1}^Nv_{t_i}$$</p><h4 id="Trait-Concatenation"><a href="#Trait-Concatenation" class="headerlink" title="Trait  Concatenation"></a>Trait  Concatenation</h4><p>直接把Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$做连接操作，作为persona representation $v_p$。</p><h3 id="使用persona-representation-v-p-进行decode"><a href="#使用persona-representation-v-p-进行decode" class="headerlink" title="使用persona representation $v_p$进行decode"></a>使用persona representation $v_p$进行decode</h3><h4 id="Persona-Aware-Attention"><a href="#Persona-Aware-Attention" class="headerlink" title="Persona-Aware Attention"></a>Persona-Aware Attention</h4><p>让persona representation $v_p$来影响式（1）中attention权重的计算。这种方法可以获得基于persona representation $v_p$的context vector $c_t$。$$\beta_i = f(s_{t-1},h_i,v_p) = V^T \cdot tanh(W^1_{\alpha}s_{t-1} + W^2_{\alpha}h_i + W^3_{\alpha}v_p)$$</p><h4 id="Persona-Aware-Bias"><a href="#Persona-Aware-Bias" class="headerlink" title="Persona-Aware Bias"></a>Persona-Aware Bias</h4><p>在输出层结合$v_p$，来影响decode过程。$$p(y_t) = softmax(a_t\cdot W_o^1s_t + (1-s_t)\cdot W_o^2v_p + b_{out})$$ $$a_t = \sigma(V_o^T\cdot s_t)$$ 其中$a_t\in [0,1]$作为一个门机制，来控制生成persona相关的词，或者语义相关的词。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1901.09672&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1901.09672&lt;/a&gt;&lt;br&gt;【代码、数据集】：无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》</title>
    <link href="http://yoursite.com/2019/10/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ADEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/24/论文笔记《DEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks》/</id>
    <published>2019-10-24T08:19:48.000Z</published>
    <updated>2019-10-25T03:08:22.933Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【链接】：<a href="https://arxiv.org/abs/1908.10731" target="_blank" rel="noopener">https://arxiv.org/abs/1908.10731</a><br>【代码、数据集】：无</p></blockquote><a id="more"></a><p>论文的研究内容是conditional text generation，基于knowledge facts的单轮对话。基于给定的对话历史和外部知识，生成合适的回复。论文中将K句话的个人描述作为knowledge facts。<br>论文采用的模型是seq2seq模型 + attention机制 + 分级pointer network。<br>论文的亮点是对比模型的思路，可以重点学习一下如何设计对比模型。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>输入分为两部分：对话历史$X = (x_1,…,x_n)$和K个相关的knowledge facts，其中第i个knowledge fact为$f^i = (f^i_1,…,f^i_{n_i}),i\in \lbrack 1,K \rbrack$。要求生成输出$Y = (y_1,…,y_m)$。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="baseline-seq2seq模型-attention机制"><a href="#baseline-seq2seq模型-attention机制" class="headerlink" title="baseline: seq2seq模型 + attention机制"></a>baseline: seq2seq模型 + attention机制</h3><p>seq2seq模型是基于encoder-decoder框架的。encoder包括embedding层和LSTM层，对于输入$X$，经过encoder得到相应的向量表示 $\lbrace{h_1,…,h_n}\rbrace$。decoder采用单向LSTM，设时间步t的隐藏状态为$s_t$，隐藏状态更新公式为：$$s_t = f(s_{t-1},y_t,c_t)$$ 其中$s_{t-1}$是上一个时间步decoder的隐藏状态，$y_{t-1}$是上一个时间步的输出。c_t为用attention机制计算得到的context vector。计算过程如下：$$c_t = \sum_{i=1}^{n}\alpha_ih_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i)$$ 其中$score(s,h)$是计算$s$和$h$之间相似度的函数。<br>在时间步t，decoder的隐藏状态$s_t$和对应的context vector $c_t$，经过线性层和softmax层，得到在固定词汇表上的概率分布。$$p_g(y_t) = softmax(W[h_t,c_t] + b)$$</p><p>可以看出“seq2seq模型 + attention机制”可以用来完成”text-to-text”的text generation的任务。输入是text，没有其他的附加信息（比如knowledge，persona，context等），输出也是text。根据输入的不同，可以得到以下三个模型。</p><ol><li><strong>SEQ2SEQ + NOFACT</strong><br> 只把对话历史$X$作为encoder的输入。</li><li><strong>SEQ2SEQ + BESTFACTCONTEXT</strong><ul><li>先从K个knowledge facts $\lbrace{f^1,…,f^K}\rbrace$中选择与dialog context $X$最相似的fact $f^c$</li><li>再将$f^c$与dialog context $X$连接起来的$[X;f^c]$，作为encoder的输入。</li></ul></li><li><strong>SEQ2SEQ + BESTFACTCONTEXT</strong><ul><li>先从K个knowledge facts $\lbrace{f^1,…,f^K}\rbrace$中选择与truth response $Y$最相似的fact $f^r$</li><li>再将$f^r$与dialog context $X$连接起来的$[X;f^r]$，作为encoder的输入。</li></ul></li></ol><p>从这三个对比试验，可以表明是否添加knowledge fact，以及knowledge fact的不同的选择，对回复生成的影响。</p><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>“seq2seq模型 + attention机制”容易遇到 generic response的问题，需要添加附加信息作为额外的输入，来得到信息更丰富的回复。与直接把knowledge fact $f$与dialog context $X$的连接$[X;f]$作为encoder的输入不同，用Memory Network可以更好地结合knowledge facts这样的附加信息。</p><p>Memory Network的作用是可以更有效地结合knowledge facts，persona description，dialog context这样的附加信息。Memory Network的工作原理可以分成两个部分：Memory representation 和 read Memory。</p><ul><li><strong>Memory representation</strong><br>  实质上是对附加信息通过另一个facts encoder的向量化表示。$\lbrace{f^1,…,f^K}\rbrace$通过另外一个encoder来编码，经过线性变换分别得到key vectors $\lbrace{k_1,…,k_K}\rbrace$和value vectors$\lbrace{m_1,m_2,…,m_K}\rbrace$。</li><li><strong>read Memory</strong><br>  实质上是计算在$\lbrace{f^1,…,f^K}\rbrace$的attention。论文中用context encoder的最后一个隐藏状态$u$作为query，计算得到总的memory representation： $$o = \sum_{i=1}^K\alpha_im_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(u,k^i)$$ 最后把context encoder的最后一个隐藏状态$u$和总的memory representation $o$组合起来，$$\hat{u} = u + o$$ 接着用$\hat{u}$来初始化decoder的隐藏状态。</li></ul><p>根据是否使用attention机制，可以得到以下四个模型：</p><ol start="3"><li><strong>MEMNET</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 实际上相当于没有用attention机制的seq2seq模型。</li><li><strong>MEMNET + CONTEXTATTENTION</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在context encoder的输出context representation $\lbrace{h_1,…,h_n}\rbrace$上的attention，得到总的context vector $c_t^{(c)}$。$$c_t^{(c)} = \sum_{i=1}^{n}\alpha_ih_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i)$$ 将$c_t^{(c)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(c)})$$</li><li><strong>MEMNET + FACTATTENTION</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在facts encoder的输出facts representation $\lbrace{m_1,…,m_K}\rbrace$上的attention，得到总的facts vector $c_t^{(f)}$。 $$c_t^{(f)} = \sum_{i=1}^K\alpha_im_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},k_i)$$ 将$c_t^{(f)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(f)})$$</li><li><strong>MEMNET + FULLATTENTION</strong><br> 同时在context representation $\lbrace{h_1,…,h_n}\rbrace$和facts representation $\lbrace{m_1,…,m_K}\rbrace$上用attention，得到context vector $c_t^{(c)}$和facts vector $c_t^{(f)}$。把二者连接起来，作为decoder隐藏状态更新的输入。$$s_t = f(s_{t-1},y_{t-1},[c_t^{(c)},c_t^{(f)}])$$</li></ol><h3 id="seq2seq模型-copy机制"><a href="#seq2seq模型-copy机制" class="headerlink" title="seq2seq模型 + copy机制"></a>seq2seq模型 + copy机制</h3><p>seq2seq模型只能从固定的词汇表中生成word。Pointer Network的作用是可以从source input中来复制word。用Pointer Network来实现copy机制也可以分为两个部分。</p><ul><li>计算在所有input tokens $\lbrace{x_1,…,x_n}\rbrace$上的attention权重分布，作为在 $\lbrace{x_1,…,x_n}\rbrace$上的概率分布。</li><li>使用一个“soft switch”机制，在copy模式时，从source input $\lbrace{x_1,…,x_n}\rbrace$中复制word；当在generation模式时，从固定的词汇表中生成word。</li></ul><p>单纯的“seq2seq模型 + attention机制”，再结合copy机制可以得到以下三种模型：</p><ol start="7"><li><strong>SEQ2SEQ + NOFACT + COPY</strong></li><li><strong>SEQ2SEQ + BESTFACTCONTEXT + COPY</strong></li><li><strong>SEQ2SEQ + BESTFACTRESPONSE + COPY</strong></li></ol><h3 id="分级Pointer-Network"><a href="#分级Pointer-Network" class="headerlink" title="分级Pointer Network"></a>分级Pointer Network</h3><p>单纯的Pointer Network可以从单句话中复制word，使用分级Pointer Network可以从K句话中复制word。</p><div align="center"><img src="/images/overall_HPN.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 系统的总体框架图</font></div><h4 id="从dialog-context中复制word"><a href="#从dialog-context中复制word" class="headerlink" title="从dialog context中复制word"></a>从dialog context中复制word</h4><p>dialog context $X = \lbrace{x_1,…,x_n}\rbrace$经过context encoder后的context representation为$\lbrace{h_1,…,h_n}\rbrace$，设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用attention机制：$$c_t^{(x)} = \sum_{i=1}^n\alpha_i^{c}h_i$$ $$\alpha_i^{(x)} = softmax(\beta_i^{(x)})$$ $$\beta_i^{(x)} = score(s_{t-1},h_i)$$ 则从dialog context $X = \lbrace{x_1,…,x_n}\rbrace$中复制word的概率分布为$$p_{copy}^{(x)}(y_t) = \alpha_i^{(x)}  &emsp; i \in [1,n]$$</p><h4 id="从knowledge-facts中复制word"><a href="#从knowledge-facts中复制word" class="headerlink" title="从knowledge facts中复制word"></a>从knowledge facts中复制word</h4><div align="center"><img src="/images/HierPointerNetwork.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. 分级Pointer Network的示意图</font></div><p>knowledge facts包含K个句子$\lbrace{f^1,…,f^K}\rbrace$，其中$f^i = \lbrace{f^i_1,…,f^i_{n_i}}\rbrace$。经过facts encoder后，再经过线性转换，得到词级别的keys vector和values vector分别为$\lbrace{k_1^{f^i},…,k_{n_i}^{f^i}}\rbrace$和$\lbrace{m_1^{f^i},…,m_{n_i}^{f^i}}\rbrace$。设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用词级别的attention机制：$$c_t^{f^i} = \sum_{j=1}^{n_i}\alpha_j^{f^i}m_j^{f^i} &emsp; i\in \lbrack {1,K}\rbrack $$ $$\alpha_j^{f^i} = softmax(score(s_{t-1},k_j^{f^i})) &emsp; i\in [1,K],j\in [1,n_i]$$ 其中$c_t^{f^i}, i\in [1,K]$是K个句子$\lbrace{f^1,…,f^K}\rbrace$句子级别的向量表示。使用句子级别的attention机制：$$c_t^{(f)} = \sum_{i=1}^{K} = \beta_i^fc_t^{f^i}$$ $$\beta_i^f = softmax(score(s_{t-1},c_t^{f^i})) &emsp; i\in [1,K]$$ 其中从$c_t^{(f)}$是knowledge facts总的向量表示。<br>则从knowledge facts中复制word的概率分布为：$$p_{copy}^{(f)}(y_t) = \beta_i^f\alpha_j^{f^i} &emsp; i\in [1,K],j\in [1,n_i]$$</p><h4 id="总的复制word的概率分布"><a href="#总的复制word的概率分布" class="headerlink" title="总的复制word的概率分布"></a>总的复制word的概率分布</h4><p>为了把复制word的两个概率分布$p_{copy}^{(x)}$和$p_{copy}^{(f)}$结合起来，使用decoder的隐藏状态$s_t$在context representation $c_t^{(x)}$和总的facts representation $c_t^{(f)}$上用attention机制。得到attention权重分布为$\lbrack{\gamma,1- \gamma}\rbrack$。$$c_t = \gamma c_t^{(x)} + (1 - \gamma) c_t^{(f)}$$ $$\lbrack{\gamma,1-\gamma}\rbrack = softmax(\lbrack{score(s_{t-1},c_t^{(x)}),score(s_{t-1},c_t^{(f)})\rbrack})$$其中$c_t$既包含了dialog context的信息，也包含了knowledge facts的信息，可以用来更新decoder的隐藏状态。$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 则总的复制word的概率分布为$$p_{copy}(y_t) = \gamma \cdot p_{copy}^{(x)} + (1 - \gamma) \cdot p_{copy}^{(f)}$$</p><h4 id="soft-switch"><a href="#soft-switch" class="headerlink" title="soft switch"></a>soft switch</h4><p>soft switch可以把copy模式和generation这两种模式结合起来。用一个门机制$p_{gen}$来控制是从固定的词汇表中生成词，或者从dialog context和knowledge facts中复制词。$$p_{gen} = sigmoid(W\lbrack{s_t,c_t}\rbrack)$$ $$p(y_t) = p_{gen} \cdot p_g(y_t) + (1 - p_{gen}) \cdot p_{copy}(y_t)$$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>采用负对数似然函数作为优化的目标函数，对于单个word的loss函数为：$$loss(\Theta) = -log(p(y_t|y_{&lt;t},X,\lbrace{f^i}\rbrace_{i=1}^K))$$其中$\Theta$表示模型所有的可训练参数，$y_{&lt;t}$表示$y_t$之前所有的word。则对于一个训练样本的loss函数为：$$J_{loss}(\Theta) = -\frac{1}{|Y|}\sum_{t=1}^{|Y|}log(p(y_t|y_{&lt;t},X,\lbrace{f^i}\rbrace_{i=1}^K))$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1908.10731&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1908.10731&lt;/a&gt;&lt;br&gt;【代码、数据集】：无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Copy Mechanism" scheme="http://yoursite.com/tags/Copy-Mechanism/"/>
    
      <category term="Pointer Network" scheme="http://yoursite.com/tags/Pointer-Network/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》</title>
    <link href="http://yoursite.com/2019/10/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/</id>
    <published>2019-10-23T01:48:41.000Z</published>
    <updated>2019-10-25T14:42:34.397Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ICLR2017<br>【链接】：<a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener">https://arxiv.org/abs/1611.01603</a><br>【代码、数据集】： <a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a></p></blockquote><a id="more"></a><p>这是由华盛顿大学和艾伦人工智能研究所发表的论文。艾伦人工智能研究所是大名鼎鼎的微软联合创始人保罗·艾伦创建的。<br>这是一篇经典的论文，截至目前被引次数高达678次。论文最大的贡献是在阅读理解任务中提出了双向attention机制（BiDirectional attention flow, BiDAF），BiDAF也可以用在其他任务中。</p><h2 id="阅读理解任务定义"><a href="#阅读理解任务定义" class="headerlink" title="阅读理解任务定义"></a>阅读理解任务定义</h2><p>给定文章context $\lbrace{x_1,x_2,…,x_T}\rbrace$及query $\lbrace{q_1,q_2,…,q_J}\rbrace$，在文章context中找到某个段span作为query的答案。输出其实是这个span的起始坐标和结束坐标。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>设encoder包含embedding层和Bi-LSTM层，经过encoder后的context representation为$H = \lbrace{h_1,h_2,…,h_T}\rbrace$，query representation为$U = \lbrace{u_1,u_2,…,u_J}\rbrace$。其中$H \in R^{2d\times  T}, U \in R^{2d\times  J}$。</p><h3 id="传统attention机制的几个特征"><a href="#传统attention机制的几个特征" class="headerlink" title="传统attention机制的几个特征"></a>传统attention机制的几个特征</h3><p>先介绍传统attention的计算方式。在时间步t计算传统attention时，需要用到上个时间步t-1的decoder RNN的隐藏状态$s_{t-1}$。decoder RNN的隐藏状态更新公式为：$$s_t = f(s_{t-1},y_{t-1},c_t)$$其中c_t为context vector，计算方式为：$$c_t = \sum_{i=1}^{T}\alpha_{t,i}h_i$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$\beta_{t,i} = score(s_{t-1},h_t)$$其中score(s,h)函数计算s与t之间的相似度。</p><p>从传统attention的计算方式可以看出，传统attention有以下几个特征：</p><ul><li>attention权重用来将所有的context representation $\lbrace{h_1,h_2,…,h_T}\rbrace$总结为一个固定维度的向量$c_t$。<em>这个过程不可避免地会带来信息丢失。</em></li><li>时间步t的attention权重$\alpha_{t_i}$计算 依赖于上一个时间步的向量$s_{t-1}$。<em>这里可以看出，attention权重的计算是有记忆的。</em></li><li>attention的计算是单向的。</li></ul><div align="center"><img src="/images/BiDAF.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font></div><h3 id="双向attention机制"><a href="#双向attention机制" class="headerlink" title="双向attention机制"></a>双向attention机制</h3><p>论文中模型分为了6层，这里只介绍最关键的一层：attention flow layer。该层的输入是context representation $H$和query representation $U$，输出是意识到query的context words representation $G$,以及前一层的context representation。</p><ol><li><strong>相似度矩阵S</strong><br>先定义一个 $H\in R^{2d\times T}$和 $U\in R^{2d\times J}$之间的共享相似度矩阵$S\in R^{T\times J}$。其中$S_{tj}$衡量了第t个context word与第j个query word之间的相似度。$$S_{tj} = \alpha(H_{:t},U_{:j}) \in R$$ 其中$H_{:t}\in R^{2d}$是H的第t个列向量，$U_{:j}\in R^{2d}$是U的第j个列向量。$\alpha()$是一个计算相似度的函数：$$\alpha(h,u) = w_{(S)}[h;u;h·u]$$</li><li><strong>context-to-query attention</strong><br>表示对于每个context word，哪个query word是最相关的。<br>对于第t个context word，在所有query words $\{q_1,q_2,…,q_J\}$上的attention权重为$a_t\in R^{J}$，有$$\sum_{j}a_{tj} = 1$$<br>attention权重$a_t$的计算方式为：$$a_t = softmax(S_{t:}) \in R^{J}$$ 对于第t个context word的attended query vector为$$\widetilde{U_{:t}} = \sum_{j}a_{tj}U_{:j} \in R^{2d}$$ 对于所有的context words $\{x_1,x_2,…,x_T\}$,则有$\widetilde{U} \in R^{2d\times T}$</li><li><strong>query-to-context attention</strong><br>表示对于每个query words，哪个context word是最相似的，对于回答query最重要。<br>计算在所有context words ${x_1,…,x_T}$上的attention权重为 $$b = softmax(max_{col}(S)) \in R^{T}$$其中$max_{col}(S) \in R^{T}$函数表示在矩阵$S \in R^{T\times J}$的列上取最大值。<br>则attended context vector为$$\widetilde{h} = \sum_{t}b_{t}H_{:t} \in R^{2d}$$ 这个向量的含义是对于query所有重要的context words的加权和。<br>把$\widetilde{h}$在列上复制T次，得到了$\widetilde{H} \in R^{2d\times T}$</li><li><strong>输出融合</strong><br>把上一层的context representation $H$和attended vector $\widetilde{H}$和$\widetilde{U}$总结组合起来得到<em>意识到query的context words representation</em> $G$，计算方式为：$$G_{:t} = \beta(H_{:t},\widetilde{U_{:t}},\widetilde{H_{:t}}) \in R^{d_{G}}$$ 其中$\beta()$函数可以是任意神经网络，比如MLP多层感知机。论文中采用了简单的连接操作，将$\beta()$函数定义为：$$\beta(h,\widetilde{h},\widetilde{u}) = [h;\widetilde{u};h \circ \widetilde{u};h \circ \widetilde{h}] \in R^{8d\times T}$$</li></ol><p>从双向attention机制的计算可以看出，双向attention机制有以下几个特征：</p><ul><li>与传统attention将所有context representation $\lbrace{h_1,h_2,…,h_T}\rbrace$总结为一个固定维度的向量$c_t$不同。双向attention机制为每个时间步都计算attention，并将attended vector $\widetilde{H}$、$\widetilde{U}$和前一层的context representation $H$流动到下一层。这样减少了提前总结为固定维度的向量带来的信息损失。</li><li>这是无记忆的attention机制。当前时间步的attention计算只取决于当前的context representation $H$和query representation $U$，而不依赖于上一个时间步的attention。 这种无记忆的attention机制将<em>attention layer</em>和<em>model layer</em>分隔开，迫使<em>attention layer</em>专注于学习context与query之间的attention，而<em>model layer</em>专注于学习attention layer输出内部之间的联系。</li><li>双向attention机制是双向的，包含query-to-context attention和context-to-query attention，可以彼此之间相互补充。</li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://allenai.github.io/bi-att-flow/" target="_blank" rel="noopener">BiDAF</a></li><li><a href="https://zhuanlan.zhihu.com/p/53626872" target="_blank" rel="noopener">机器阅读理解之双向注意力流||Bidirectional Attention Flow for Machine Comprehension</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ICLR2017&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1611.01603&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1611.01603&lt;/a&gt;&lt;br&gt;【代码、数据集】： &lt;a href=&quot;https://github.com/allenai/bi-att-flow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/allenai/bi-att-flow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="BiDAF" scheme="http://yoursite.com/tags/BiDAF/"/>
    
      <category term="Machine Comprehension" scheme="http://yoursite.com/tags/Machine-Comprehension/"/>
    
  </entry>
  
  <entry>
    <title>对话系统的数据集</title>
    <link href="http://yoursite.com/2019/09/06/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://yoursite.com/2019/09/06/对话系统的数据集/</id>
    <published>2019-09-06T02:07:32.000Z</published>
    <updated>2019-09-06T02:11:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="对话系统" scheme="http://yoursite.com/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="对话系统" scheme="http://yoursite.com/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="数据集" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Bridging the Gap between Training and Inference for Neural Machine Translation》</title>
    <link href="http://yoursite.com/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/</id>
    <published>2019-08-02T06:46:00.000Z</published>
    <updated>2019-08-05T05:14:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ACL2019<br>【链接】：<a href="https://arxiv.org/abs/1906.02448" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02448</a><br>【代码、数据集】： 无</p></blockquote><a id="more"></a><p>这篇论文由中科院发表，获得了ACL2019的 “best long paper”。</p><h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>在Neural Machine Translation(NMT)任务中，模型通常采用encoder-decoder框架，基于RNN 或 CNN 或attention。假设输入为$X = \lbrace{x_1,x_2,…,x_m}\rbrace$，真实输出为$Y = \lbrace{y_1^*,y_2^*,…,y_n^*}\rbrace$，预测输出为$Y’ = \lbrace {y_1’,y_2’,…,y_m’}\rbrace$。</p><p>第一个问题是：decoder会一个词一个词地生成整个回复。在train阶段，在时间步t生成$y_t’$时，decoder会根据之前真实的词$\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\rbrace$来预测$y_t’$。在infer阶段，由于不可能知道真实输出，在时间步生成$y_t’$时，decoder会根据之前预测的词$\lbrace{y_1’,y_2’,…,y_{t-1}’}\rbrace$来预测$y_t’$。<br>可以看到train阶段与infer阶段所依据的词是不同的，train阶段和infer阶段预测的词$y_t’$来自两个不同的概率分布，分别是数据分布(data distribution)和模型的分布(model distribution)，这种差别称为“爆炸偏差(exposure bias)”。随着预测序列的长度增加，错误会逐渐累积。<br>为了解决第一个问题，消除train阶段和infer阶段的这种差别，一个可能的解决方法是：在train阶段，decoder同时根据真实的词$\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\rbrace$和预测的词$\lbrace{y_1’,y_2’,…,y_{t-1}’}\rbrace$来生成$y_t’$。</p><p>第二个问题是: NMT模型通常最优化$Y与Y’$之间的交叉熵目标函数来更新模型参数，但交叉熵函数会严格匹配预测输出$Y’$与真实的输出$Y$。但在NMT任务中，一句话可以有多个不同但合理的翻译。一旦预测输出$Y’$的某个词与$Y$不同，尽管它是合理的，也会被交叉熵函数纠正。这种情况称为“过度纠正的现象”。</p><p>为了消除train阶段与infer阶段的差别，论文提出了一种在train阶段做改进的解决方案。首先，从预测的词中选择oracle word $y_{j-1}^{oracle}$，设真实输出中上一个词为$y_{j-1}^{*}$。接着从$\lbrace{y_{j-1}^{oracle},y_{j-1}^{*}}\rbrace$中抽样一个词，抽中$y_{j-1}^{*}$的概率为$p$，抽中$y_{j-1}^{oracle}$的概率为$1-p$。最后，decoder根据抽样的这个词来预测$y_j$。</p><p>在train阶段刚开始时，抽中真实的词$y_{j-1}^{*}$的概率比较大，随着模型逐渐收敛，抽中预测的词$y_{j-1}^{oracle}$的概率变大，让模型有能力处理”过度纠正的问题”。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/oracle.png" alt="Fig.1 论文提出的方法的结构图" title>                </div>                <div class="image-caption">Fig.1 论文提出的方法的结构图</div>            </figure><h3 id="RNN-based-NMT-Model"><a href="#RNN-based-NMT-Model" class="headerlink" title="RNN-based NMT Model"></a>RNN-based NMT Model</h3><p>NMT任务常采用encoder-decoder框架，可以基于RNN或CNN或纯attention。论文提出的消除train阶段和infer阶段差别的方法，可以用于任何NMT模型。论文以基于RNN的NMT模型为例，来介绍这种方法。这一节先介绍RNN-based NMT模型。下一节介绍NMT模型如何结合这种方法。</p><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>记输入为$X = \lbrace{x_1,x_2,…,x_m}\rbrace$，真实输出为$Y = \lbrace{y_1,y_2,…,y_n}\rbrace$。encoder采用bi-GRU分别获取正向和反向的隐藏状态$\overrightarrow{h_i},\overleftarrow{h_i}$。$x_i$的embedding向量为$e_{x_i}$。$$\overrightarrow{h_i} = GRU(e_{x_i},h_{i-1})$$ $$\overleftarrow{h_i} = GRU(e_{x_i},h_{i+1})$$ 将$\overrightarrow{h_i},\overleftarrow{h_i}$连接起来，作为$x_i$对应的隐藏状态：$$h_i = [\overrightarrow{h_i},\overleftarrow{h_i}] \tag{1}$$</p><h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><p>attention机制用来联系encoder和decoder，更好地捕捉source sequence的信息。也就是在时间步t,通过encoder所有的隐藏状态$\lbrace h_1,h_2,…,h_m \rbrace$来计算context vector $c_t$。记decoder上一时间步的隐藏状态为$s_{t-1}$。 $c_t$是encoder所有隐藏状态$\lbrace h_1,h_2,…,h_m \rbrace$的加权和：$$c_t = \sum_{i=1}^{m}\alpha_{ti}h_i \tag{2}$$ 其中$\alpha_{ti}$是attention权重，计算方式为:$$\beta_{ti} = v_a^\top tanh(W_as_{t-1} + U_ah_i) \tag{3}$$ $$\alpha_{ti} = softmax(\beta_{ti}) = \frac{exp(\beta_{ti})}{\sum_jexp(\beta_{tj})}$$</p><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>decoder采用单向GRU的变体，隐藏状态更新公式为:$$s_t = GRU(s_{t-1},e_{y_{t-1}^*},c_t) \tag{4}$$ 最后根据e_{y_{t-1}^*}，decoder的隐藏状态$s_t$，对应的context vector $c_t$来预测$y_t$。 $$o_t = W_og(e_{y_{t-1}^*},s_t,c_t) \tag{5}$$ 在词汇表上的概率分布为：$$P_t(y_t = w) = softmax(o_t) \tag{6}$$</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>为了消除或减轻train阶段和infer阶段的差别，论文提出 从真实的词$y_{t-1}*$和$y_{t-1}^{oracle}$预测的词中抽样，decoder根据抽样的词来预测下一个词$y_t$。使用论文提出的方法，在时间步t预测$y_t$分为三步：</p><ol><li>先从预测的词中选择$y_{t-1}^{oracle}$。 论文提出了两种方法来选择oracle word，分别是词级别的方法和句子级别的方法。</li><li>从$\lbrace{y_{t-1}^{oracle},y_{t-1}*}\rbrace$中抽样得到$y_{t-1}$，抽中$y_{t-1}*$的概率为$p$，抽中$y_{t-1}^{oracle}$的概率为$1-p$。</li><li>用抽样的词$y_{t-1}$来替换公式$(4)(5)$中的$y_{t-1}^*$来预测下一个词。</li></ol><h4 id="oracle-word的选择"><a href="#oracle-word的选择" class="headerlink" title="oracle word的选择"></a>oracle word的选择</h4><p>传统的方法中，decoder会根据上一个时间步真实的$y_{t-1}^*$来预测$y_t$。为了消除train阶段的infer阶段的差别，可以从预测的词中选择oracle word $y_{t-1}^{oracle}$来代替$y_{t-1}^*$。一种方法是每个时间步采用词级别的greedy search来生成oracle word，称为word-level oracle(WO)，另一种方法是采用beam-search，扩大搜索空间，用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO).</p><h5 id="word-level-oracle"><a href="#word-level-oracle" class="headerlink" title="word-level oracle"></a>word-level oracle</h5><p>选择$y_{t-1}^{oracle}$最简单直观的方法是，在时间步t-1，选择公式$P_{t-1}$中概率最高的词作为$y_{t-1}^{oracle}$，如Fig.2所示。 为了获得更健壮的$y_{t-1}^{oracle}$，更好地选择是使用<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">gumbel max技术</a>来冲离散分布中进行抽样，如Fig.3所示。<br>具体地讲，将gumbel noise $\eta$作为正则化项加到公式(5)中的$o_{t-1}$，再进行softmax操作得到$y_{t-1}$的概率分布。$$\eta = -log(-log(u)) $$ $$\tilde{o_{t-1}} = \frac{o_{t-1} + \eta}{\tau} \tag{7}$$ $$\tilde{P_{t-1}} = softmax(\tilde{o_{t-1}}) \tag{8}$$ 其中变量$u \sim U(0,1)$服从均匀分布。$\tau$为温度系数，当$\tau \to 0$时，公式(8)的softmax()逐渐相当于argmax()函数；当$\tau \to \infty$时，softmax()函数逐渐相当于均匀分布。<br>则$y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{WO} =argmax(\tilde{P_{t-1}}) \tag{9}$$需要注意的是gumbel noise $\eta$只用来选择oracle word，而不会影响train阶段的目标函数。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/word_level_oracle_without_gumbel_noise.png" alt="Fig.2. word level oracle without gumbel noise" title>                </div>                <div class="image-caption">Fig.2. word level oracle without gumbel noise</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/word_level_oracle_with_gumbel_noise.png" alt="Fig.3. word level oracle with gumbel noise" title>                </div>                <div class="image-caption">Fig.3. word level oracle with gumbel noise</div>            </figure><h5 id="sentence-level-oracle"><a href="#sentence-level-oracle" class="headerlink" title="sentence-level oracle"></a>sentence-level oracle</h5><p>为了选择sentence-level oracle word，首先要进行beam-search解码，设beam size为k，得到k个candidate句子。在beam-search解码的过程中，生成每个词时也应用gumbel max技术。<br>接着，得到k个candidate句子后，用句子级衡量指标BLEU来给这k个句子打分，得分最高的句子为oracle sentence $Y^S = \lbrace{y_1^S,y_2^S,..,y_{|y^S|}^S}\rbrace$。<br>则时间步t解码对应的oracle word $y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{SO} = y_{t-1}^{S} \tag{10}$$ 当模型从真实输出$Y$和sentence oracle $Y^S$抽样，这有一个前提是，这两个序列的长度需要是一致的。但beam-search decode不能保证解码序列的长度。为了保证这两个序列长度一致，论文提出了<em>force decoding</em>的解决方法。</p><p><strong>force decoding</strong><br>设真实输出$Y = \lbrace{y_1,y_2,…,y_n}\rbrace$的序列长度为n。<em>force decoding</em>需要解码得到长度同样为n的序列，以特殊字符”EOS”结束。设beam search decode时，时间步t对应的概率分布为$P_t$。</p><ul><li>当$t&lt; n$时，对于概率分布$P_t$，即使字符”EOS”是概率最高的词，那么生成概率次高的词。</li><li>当$t = n+1$时，对于概率分布$P_{n+1}$，即使字符”EOS”不是概率最高的词，也要生成”EOS”。</li></ul><p>果真是强制生成长度为n的序列。这样beam-search decode得到的序列与真实输出序列的长度就是一致的，都为n。</p><h4 id="递减抽样"><a href="#递减抽样" class="headerlink" title="递减抽样"></a>递减抽样</h4><p>根据公式(9)或(10)得到$y_{t-1}^{oracle}$后，下一步是从$\lbrace{y_{t-1}^{oracel},y_{t-1}^*}\rbrace$中抽样，抽中$y_{t-1}^*$的概率是p，抽中$y_{t-1}^{oracle}$的概率是1-p。在训练的初始阶段，如果过多地选择$y_{t-1}^{oracle}$，会导致模型收敛速度慢；在训练的后期阶段，如果过多地选择$y_{t-1}^*$，会导致模型在train阶段没有学习到如何处理infer阶段的差别。<br>因此，好的选择是：在训练的初始阶段，更大概率地选择$y_{t-1}^*$来加快模型收敛，当模型逐渐收敛后，以更大概率选择$y_{t-1}^{oracle}$，来让模型学习到如何处理infer阶段的差别。从数学表示上，概率$p$先大后逐渐衰减，$p$随着训练轮数$e$的增大而逐渐变小。$$p = \frac{\mu}{\mu + exp(\frac{e}{\mu})} \tag{11}$$其中，$\mu$是超参数。$p$是轮数$e$的单调递减函数。$e$从0开始，此时，$p=1$。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>将采样得到的$y_{t-1}$代替公式(4)-(6)中的$y_{t-1}^*$来预测$y_t$在词汇表上的概率分布。采用最大似然估计，相当于最小化以下目标函数：$$L(\theta) = -\sum_{n=1}^{N}\sum_{j=1}^{|y_n|}logP_j^n[y_j^n]$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1906.02448&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1906.02448&lt;/a&gt;&lt;br&gt;【代码、数据集】： 无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="ACL2019" scheme="http://yoursite.com/tags/ACL2019/"/>
    
      <category term="Neural Machine Translation" scheme="http://yoursite.com/tags/Neural-Machine-Translation/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Multi-Level Memory for Task Oriented Dialogs》</title>
    <link href="http://yoursite.com/2019/08/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AMulti-Level-Memory-for-Task-Oriented-Dialogs%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/</id>
    <published>2019-08-01T06:14:37.000Z</published>
    <updated>2019-10-25T07:26:40.134Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.10647.pdf</a><br>【代码、数据集】：<a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></p></blockquote><a id="more"></a><p>已有工作中，端到端的任务型对话系统采用memory network来结合外部的知识库(knowledgt base) 和 对话历史(context)。为了使用从跑一趟 network，通常将二者放在同一个memory中。这样带来的问题是：memory变得太大，模型在读取memory时需要区分外部知识库和对话历史，并且在memory上的推理变得很难。为了解决这个问题，论文将外部知识库和对话历史区分开，另外，将外部知识库保存为分层的memory。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型主要包括三个部分。 </p><ul><li>分级encoder：<br>  分别编码对话历史中的句子。</li><li>milti-level memory<br>  保存了目前为止所有的query以及对应的知识库查询结果，是以分级的方式保存在memory中的。</li><li>copy机制增强的decoder：<br>  从词汇表中生成词，或者从知识库multi-level memory中复制词，或者从对话历史(context)中复制词。</li></ul><div align="center"><img src="/images/multi-memory-model.jpg" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div><h4 id="分级encoder"><a href="#分级encoder" class="headerlink" title="分级encoder"></a>分级encoder</h4><p>在第t轮，对话历史共有2t-1个句子$\lbrace{c_1,c_2,…,c_{2t-1}}\rbrace$，其中用户对话为t轮，回复对话为t-1轮。 每个句子$c_i$都是词序列$\lbrace{w_{i1},w_{i2},…,w_{im}}\rbrace$。<br>每个句子$c_i$先经过embedding layer得到词向量表示，再经过单层bi-GRU得到句子的向量表示$\varphi(c_i)$。$h_{ij}^e$表示词$w_{ij}$对应的隐藏状态。<br>再将$\varphi{c_i}$经过另一个单词GRU来得到context的向量表示$c$。</p><h4 id="multi-level-memory"><a href="#multi-level-memory" class="headerlink" title="multi-level memory"></a>multi-level memory</h4><p>memory的关键是分级的分为三级：query $\to$ result $\to$ result key和result value。见Fig.2。<br>记本轮对话之前所有的知识库query为$q_1,…,q_k$。每个query $q_i$是一个(key,value)对，$q_i = \lbrace{k_a^{q_i}:v_a^{q_i},0&lt; a&lt; n_{q_i}}\rbrace $。其中key和value分别对应query的槽(slots)和槽值，$n_{q_i}$是query $q_i$的槽值个数。<br>第j轮对话，用query $q_i$查询知识库的返回结果为result $r_{ij}$。$r_{ij}$也是一个key-value对，$r_{ij} = \lbrace{k_a^{r_{ij}}:v_a^{r_{ij}},0&lt; a &lt; n_{r_{ij}}}\rbrace$。其中$n_{r_{ij}}$是key-value对的个数。</p><div align="center"><img src="/images/multi-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. multi memory</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div>第一级memory是query的向量表示。 $q_i$的向量表示为$q_i^v$，$q_i^v$为所有values $v_a^{q_i}$的词袋(bag of words)向量表示。第二级memory是result的向量表示。同样地，$r_{ij}$的向量表示为$r_{ij}^v$，$r_{ij}^v$为所有values $v_a^{r_{ij}}$的词袋(BOW)向量表示。第三级memory是result的key-value对，$(k_a^{r_{ij}}:v_a^{r_{ij}})$，其中value $v_a^{r_{ij}}$可能会被复制到回复中。<h4 id="copy机制增强的decoder"><a href="#copy机制增强的decoder" class="headerlink" title="copy机制增强的decoder"></a>copy机制增强的decoder</h4><p>decoder一个词一个词地生成回复。在时间步t生成词$y_t$时，可能从词汇表中生成，也能从两个分开的memory上复制。用门$g_1$来选择是从词汇表上生成，还是从memory中复制。如果是后者，用另一个门$g_2$来选择是从context中复制，还是从知识库复制。</p><ol><li><strong>从词汇表生成词</strong><br> 时间步t，decoder的隐藏状态$h_t$为$$h_t = GRU(y_{t-1},s_{t-1})$$用$h_t$计算在encoder的所有隐藏状态上的attention权重，采用”concat attention”机制：$$a_{ij} = softmax(w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))) = \frac{w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}{\sum_{ij}w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}$$则context vector为$$d_t = \sum_{ij}a_{ij}h^e_{ij}$$ $h_t$和$d_t$连接后经过线性层和softmax层得到在词汇表上的概率分布：$$P_g(y_t) = softmax(W_1[h_t,d_t] + b_1)$$</li><li><strong>从context memory中复制词</strong><br> 直接将计算context vector时的attention权重，作为在context所有词$w_{ij}$上的概率分布：$$P_{con}(y_t = w) = \sum_{ij:w_{ij}=w}a_{ij}$$</li><li><strong>从KB memory中复制实体</strong><br> 时间步t的隐藏状态$h_t$和context vector $d_t$用来计算在所有query上的attention权重。第一级在所有query $q_1,q_2,…,q_k$的attention权重为$$\alpha_i = softmax(w_2^\top tanh(W_4[h_t,d_t,q_i^v])) = \frac{w_2^\top tanh(W_4[h_t,d_t,q_i^v])}{\sum_{i}w_2^\top tanh(W_4[h_t,d_t,q_i^v])}$$<br> 第二级$\beta_i$在$q_i$对应的$r_i$上的attention权重为$$\beta_{ij} = softmax(w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])) = \frac{w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}{\sum_{j}w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}$$<br> 第一级attention和第二级attention的乘积是在所有result上的attention权重分布。则memory总的向量表示为$$m_t = \sum_{i}\sum_j\alpha_i\beta_{ij}r_{ij}^v$$<br> 第三级memory为result的key-value对$(k_a^{r_{ij}}:v_a^{r_{ij}})$，类似于<a href="https://arxiv.org/abs/1705.05414" target="_blank" rel="noopener">(Eric and Manning, 2017)</a>，用key $k_a^{r_{ij}}$来计算attention权重，将对应的value $v_a^{r_{ij}}$复制到回复中。在$r_{ij}$所有keys上的attention权重为$$\gamma_{ijl} = softmax(w_4^\top tanh(W_6[h_t,d_t,m_t,k_l^{r_{ij}}]))$$则在所有values $v_a^{r_{ij}}$的概率分布为:$$P_{kb}(y_t = w) = \sum_{ijl:v_l^{r_{ij}}=w}\alpha_i\beta_{ij}\gamma_{ijl}$$</li><li><strong>decoding</strong><br> 我们用门机制$g_2$来来结合$P_{con}(y_t)$和$P_{kb}(y_t)$，得到memory上的copy概率分布$P_c(y_t)$。$$g_2 = sigmoid(W_7[h_t,d_t,m_t]+b_2)$$ $$P_c(y_t) = g_2P_{kb}(y_t) + (1-g_2)P_{con}(y_t)$$ 用门机制$g_1$来结合$P_{c}(y_t)$和$P_{g}(y_t)$来得到总的概率分布$P(y_t)$：$$g_1 = sigmoid(W_8[h_t,d_t,m_t]+b_3)$$ $$P(y_t) = g_1P_g(y_t) + (1-g_1)P_c(y_t)$$</li></ol><h3 id="相似论文"><a href="#相似论文" class="headerlink" title="相似论文"></a>相似论文</h3><ul><li><a href="https://arxiv.org/abs/1810.10647" target="_blank" rel="noopener">《Multi-level Memory for Task Oriented Dialogs》</a><ul><li>发表在NAACL2019</li><li>github: <a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></li></ul></li><li><a href="https://arxiv.org/abs/1804.08217" target="_blank" rel="noopener">《Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems》</a><ul><li>发表在ACL2018</li><li>github: <a href="https://github.com/HLTCHKUST/Mem2Seq" target="_blank" rel="noopener">https://github.com/HLTCHKUST/Mem2Seq</a></li></ul></li><li><a href="https://www.ijcai.org/proceedings/2018/643" target="_blank" rel="noopener">《Commonsense Knowledge Aware Conversation Generation with Graph Attention》</a><ul><li>发表在IJCAI2018</li><li>github： <a href="https://github.com/tuxchow/ccm" target="_blank" rel="noopener">https://github.com/tuxchow/ccm</a></li></ul></li><li><a href="https://arxiv.org/abs/1908.10731" target="_blank" rel="noopener">《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》</a><ul><li>发表于2019年</li><li>代码：无</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/pdf/1810.10647.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1810.10647.pdf&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/DineshRaghu/multi-level-memory-network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/DineshRaghu/multi-level-memory-network&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>Neural Turing Machines与Memory Network</title>
    <link href="http://yoursite.com/2019/07/26/Neural-Turing-Machines%E4%B8%8EMemory-Network/"/>
    <id>http://yoursite.com/2019/07/26/Neural-Turing-Machines与Memory-Network/</id>
    <published>2019-07-26T03:03:15.000Z</published>
    <updated>2019-08-01T02:30:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Memory Networks。</p><a id="more"></a><h3 id="Neural-Turing-Machines-神经图灵机"><a href="#Neural-Turing-Machines-神经图灵机" class="headerlink" title="Neural Turing Machines-神经图灵机"></a>Neural Turing Machines-神经图灵机</h3><p>Google DeepMind团队在<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>提出Neural Turing Machines，第一次提出用external memory来提高神经网络的记忆能力。这之后又出现了多篇关于Memory Networks的论文。我们先看看Turing Machines的概念。</p><h4 id="Turing-Machines-图灵机"><a href="#Turing-Machines-图灵机" class="headerlink" title="Turing Machines-图灵机"></a>Turing Machines-图灵机</h4><p>计算机先驱<a href="https://baike.baidu.com/item/%E8%89%BE%E4%BC%A6%C2%B7%E9%BA%A6%E5%B8%AD%E6%A3%AE%C2%B7%E5%9B%BE%E7%81%B5/3940576?fromtitle=%E5%9B%BE%E7%81%B5&fromid=121208" target="_blank" rel="noopener">turing</a>在1936年提出了Turing Machines这样一个计算模型。它由三个基本的组件：</p><ul><li>tape: 一个无限长的纸带作为memory，包含无数个symbols，每个symbol的值为0、1或”$\space$”。</li><li>head: 读写头，对tape上的symbols进行读操作和写操作。</li><li>controller： 根据当前状态来控制head的操作。</li></ul><p>理论上Turing Machines可以模拟任何一个计算算法，不管这个算法多么复杂。但现实中，计算机不可能有无限大的memory space，因此Turing Machines只是数学意义上的计算模型。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/turing-machine.jpg" alt="Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)" title>                </div>                <div class="image-caption">Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)</div>            </figure><h4 id="Neural-Turing-Machines"><a href="#Neural-Turing-Machines" class="headerlink" title="Neural Turing Machines"></a>Neural Turing Machines</h4><p>Neural Turing Machines(NTM,<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>)用external memory来提高神经网络的记忆能力。<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM(Long and short memory)</a>通过门机制有效缓解了RNN的’梯度消失和梯度爆炸问题’，可以通过internal memory实现长期记忆。当LSTM的internal memory的记忆能力有限，需要用external memory来提高神经网络的记忆能力。</p><p>Neural Turing Machines包含两个基本组件：<em>a neural network controller</em>和<em>memory bank</em>。<em>memory</em>是一个 $N\cdot M$阶的矩阵，包含N个向量，每个向量的维度是M。我们把每个memory vector称为memory location。<em>controller</em>控制<em>heads</em>对<em>memory</em>进行读写操作。</p><p>如何对<em>memory matrix</em>进行读写操作呢？关键问题是如何让读写操作是可微的，这样才能用梯度下降法来更新模型参数。具体来说，问题是让模型关于memory location是可微的，但memory locations是离散的。Neural Turing Machines用了一个很聪明的方法来解决这个问题：不是对单独某个memory location进行读写操作，而是对所有的memory locations进行不同程度的读写操作，这个程度是通过attention的权重分布来控制的。</p><div align="center"><img src="/images/NTM.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig. 2. Neural Turing Machine Architecture</font></div><h5 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h5><p>记时间步t <em>memory matrix</em>为$N\cdot M$阶矩阵$M_t$，$w_t$是在N个memory向量上的权重分布，是一个N维向量。则时间步t的read vector $r_t$为$$r_t = \sum_{i=1}^{N}w_t(i)\cdot M_t(i)$$ $$where: \sum_{i=1}^{N}w_t(i) = 1; 0 \le w_t(i) \le 1,\forall i $$其中，$w_t(i)$是$w_t$的第i个元素，$M_t(i)$是$M_t$的第i个行向量。</p><h5 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h5><p>受LSTM门机制的启发，将写操作分成两步：先<em>erase</em>，再<em>add</em>。先根据<em>erase vector $e_t$</em>擦去旧的内容，再根据<em>add vector $a_t$</em>添加新的内容。</p><ol><li>先erase：<br> 在时间步t，attention权重分布为$w_t$，<em>erase vector $e_t$</em>是一个M维向量，每个元素取值[0,1]，上一个时间步的<em>memory vector</em>为$M_{t-1}$。则erase操作为$$\tilde{M_{t}}(i) = M_{t-1}(i)[\vec{1}-w_t(i)e_t]$$ $\vec{1}$是一个M维的全1向量。对memory vector的erase操作是逐点进行的。当$e_t$的元素和memory location对应权重$w_t(i)$的元素值都是1时，memory vector $M_t(i)$的元素值才会置为0。如果$e_t$或$w_t(i)$的元素值为0时，memory vector $M_t(i)$的元素值保持不变。</li><li>再add:<br> 每个<em>write head</em>会产生一个M维的<em>add vector a_t</em>，则：$$M_t(i) = \tilde{M_{t}}(i) + w_t(i)a_t$$至此，就完成了写操作。</li></ol><h5 id="寻址机制"><a href="#寻址机制" class="headerlink" title="寻址机制"></a>寻址机制</h5><p>进行读写操作前，要搞清楚对哪个memory location进行读写呢？这就是寻址。为了让模型关于memory locatios可微，Neural Turing Machines不是对某个单独的memory location进行读写操作，而是对所有memory locations进行不同程度的读写操作，这个程度就是由权重分布$w_t$来控制的。模型结合并同时使用了content-based和location-based两种寻址方式来计算这个权重分布$w_t$。具体地，权重计算分为以下几步：</p><ol><li>content-based addressing<br> 时间步t，每个head产出一个M维的<em>key vector $k_t$</em>，通过$k_t$与memory vectors $M_t(i)$之间的相似性来计算content-based attention权重分布$w_{t}^{c}$。相似性是通过余弦相似度来衡量的。$$w_{t}^{c} = softmax(\beta_tK(k_t,M_t(i))) = \frac{\beta_tK(k_t,M_t(i))}{\sum_{j}K(k_t,M_t(j))}$$ $$K(u,v) = \frac{u\cdot v}{|u|\cdot |v|}$$<br> $\beta_t$可以放大或缩小权重的精度。</li><li>内插法<br> 每个head产生一个<em>interpolation gate $g_t$</em>，取值[0,1]。content-based attention权重分布为$w_t^{c}$，上一个时间步的attention权重分布为$w_{t-1}$。则门控制的权重分布$w_t^g$为：$$w_t^g = g_tw_t^c + (1-g_t)w_{t-1}$$当$g_t$为0时，采用上一个时间步的权重分布$w_{t-1}$，当$g_t$为1时，采用content-based attention权重分布$w_t^c$。</li><li>循环卷积<br> 对经过插值后的权重分布$w_t^g$进行循环卷积，主要功能是对权重进行旋转位移。比如当权重分布关注某个memory location时，经过循环卷积就会扩展到附近的memory locations，也会对附近的memory locations进行少量的读写操作。每个head产生的转移权重为$s_t$,循环卷积的操作为:$$\tilde{w_t(i)} = \sum_{j=0}^{N-1}w_t^g(i)s_t(i-j)$$<br> 关于$s_t$的详细介绍可以见<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a>;<br> 循环卷积的详细介绍可以见<a href="https://blog.csdn.net/rtygbwwwerr/article/details/50548311" target="_blank" rel="noopener">Neural Turing Machines-NTM系列（一）简述</a></li><li>锐化<br> 循环卷积往往会造成权重泄漏和分散，为了解决这个问题，需要最后进行锐化操作。$$w_t(i) = \frac{\tilde{w_t(i)^{\gamma_t}}}{\sum_j\tilde{w_t(j)^{\gamma_t}}}$$其中$\gamma_t &gt;1$。至此，就得到了时间步t的权重分布$w_t$。可以根据这个权重分布$w_t$对memory matrix进行读写操作。</li></ol><p>总结以下这4步操作。第一步content-based addressing根据输入得到关于memory locations的相似度；后三步实现了location-based addressing。第二步插值操作引入了上一个时间步的权重分布，对content-based 权重进行修正；第三步循环卷积将每个位置的权重向两边分散；第四步锐化操作将权重突出化，大的更大，小的更小。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/50.jpg" alt="Fig.3. 寻址机制的4步操作" title>                </div>                <div class="image-caption">Fig.3. 寻址机制的4步操作</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/NTM-flow-addressing.png" alt="Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)" title>                </div>                <div class="image-caption">Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)</div>            </figure><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a><br>  用动图直观地表现Neural Turing Machines的计算过程。推荐！👍</li><li><a href="https://zhuanlan.zhihu.com/p/30383994" target="_blank" rel="noopener">记忆网络之Neural Turing Machines</a>，中文</li><li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a></li></ul><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>在Neural Turing Machines提出仅仅五天后，Facebook研究员<a href="http://www.thespermwhale.com/jaseweston/" target="_blank" rel="noopener">Jason Weston</a>发表了<a href="http://arxiv.org/abs/1410.3916" target="_blank" rel="noopener">MEMORY NETWORKS</a>。在QA系统的领域，应用memory network。虽然RNN或LSTM可以通过hidden state和weights来进行短期记忆，但它们的记忆能力是有限的。要实现长期记忆，需要memory network。</p><h4 id="Memory-Network的一般框架"><a href="#Memory-Network的一般框架" class="headerlink" title="Memory Network的一般框架"></a>Memory Network的一般框架</h4><p>memory network包括一个记忆单元memory，和四个基本组件：</p><ul><li>I(input feature map):<br>  将input <em>x</em>进行向量化表示，编码为feature representation <em>I(x)</em>。</li><li>G(generalization):<br>  对memory进行写操作。根据input 来更新memory <em>$m_i$</em>。$m_i = G(m_i,I(x),m)$</li><li>O(output feature map):<br>  对memory进行读操作。根据input和memory生成output feature。$o = O(I(x),m)$</li><li>R(response):<br>  根据output feature o来生成response。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/memn.jpg" alt="Fig.5. memory network的框架图" title>                </div>                <div class="image-caption">Fig.5. memory network的框架图</div>            </figure><h4 id="memory-network框架的实现–MemNNs"><a href="#memory-network框架的实现–MemNNs" class="headerlink" title="memory network框架的实现–MemNNs"></a>memory network框架的实现–MemNNs</h4><p>在I模块将input $x_i$编码为$I(x_i)$后，G模块之间将$I(x_i)$保存到下一个空的memory slot中，而不更新旧的memory slots。真正实现inference的核心模块是O和R。</p><p>O模块在给定x的条件下，依次找到与x最相关的k个memory slots。论文中采用k = 2。先找到第一个最相关的memory slot：$$m_{o1} = \mathop{argmax}\limits_{i = 1,…,N} s_{o1}(x,m_i)$$其中$s_o()$是一个匹配函数，计算x与$m_i$之间的相关程度。接着，根据x和第一个memory找到下一个memory：$$m_{o2} = \mathop{argmax}\limits_{i = 1,…,N} s_{o2}([x,m_{o1}],m_i)$$将output feature o = $[x,m_{o1},m_{o2}]$作为R模块的输入。</p><p>R模块将词汇表中所有词与output feature进行匹配，选择匹配度最高的词作为response。这样生成的response只有一个词。$$r = \mathop{argmax}\limits_{w \in W}s_R([x,m_{o1},m_{o2}],w)$$其中$s_R()$是一个匹配函数。</p><p>匹配函数$s_O$和$s_R$都采用以下函数：$$s(x,y) = \Phi_x(x)^\top U^\top U \Phi_y(y)$$其中$\Phi_x(x),\Phi_y(y)$分别将x/y编码为向量。<br><strong>目标函数</strong><br>在训练阶段采用最大边缘目标函数，设对于question x，真实的label为r，对应的memory为$m_{o1},m_{o2}$。则最大边缘目标函数为：$$\sum_{m_i\ne m_{o1}}max(0,\gamma - s_{O1}(x,m_{o1}) + s_{O1}(x,m_i)) + $$ $$\sum_{m_j\ne m_{o2}}max(0,\gamma - s_{O2}([x,m_{o1}],m_{o2}) + s_{O2}([x,m_{o1}],m_j)) + $$ $$\sum_{r’ \ne r}max(0,\gamma - s_{R}([x,m_{o1},m_{o2}],r) + s_{R}([x,m_{o1},m_{o2}],r’))$$</p><p>由于argmax()函数的存在，这个模型是不可微的。而且中间过程找到相关memory需要监督，这个模型不是端到端的。<br>总的来说，这个memory network是一种普适性的架构，是很初级很简单的，很多部分还不完善，不足以应用具体的任务上。不过，通过多跳方式找到相关memory的思路是很值得学习的。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>Jason Weston作为三作的<a href="http://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">Sainbayar Sukhbaatar2015</a>对Memory network工作的改进，主要改进是实现了端到端，减少了监督。End-to-End Memory Network采用soft attention而不是hard attention来read memory，因此是端到端的。另外不需要对相关memory进行监督。提高memory network的可用性。<br>假设多个句子input $x_1,…,x_n$作为memory，对于query q，输出对应的answer a。给定query q，经过多跳找到相关的memory，并生成对应的answer a。</p><h4 id="single-layer"><a href="#single-layer" class="headerlink" title="single layer"></a>single layer</h4><p>给定input $x_1,x_2,…,x_n$，采用两个不同的embedding matrix A和C分别编码为向量$\lbrace{m_1,…,m_n}\rbrace$，$\lbrace{c_1,…,c_n}\rbrace$,分别对应attention机制的keys和values。将query q经过embedding matrix B编码为向量表示u。</p><p>采用dot-product attention计算权重：$$p_i = softmax(u^\top m_i) = \frac{exp(u^\top m_i)}{\sum_{j}exp(u^\top m_j)}$$<br>则memory representation为：$$o = \sum_{i}p_i m_i$$<br>根据u和o来进行预测：$$\hat{a} = softmax(W (o + u))$$<br>通过最小化a与$\hat{a}$之间的交叉熵来训练模型参数A,B,C,W。这个single layer end-to-end Memory network是简单而直观的。核心是用soft attention来read memory，找到相关的memory，并进行inference。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/mem.png" alt="Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)" title>                </div>                <div class="image-caption">Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)</div>            </figure></p><h4 id="multi-layers"><a href="#multi-layers" class="headerlink" title="multi layers"></a>multi layers</h4><p>将K层single layer进行stack得到K层memory network，进行K跳memory查询操作。具体地stack方式为：</p><ul><li>将第k层的输入$u^k$和memory representation $o^k$相加作为第k+1层的输入:$$u^{k+1} = u^k + o^k$$</li><li>每一层都有单独的embedding matrix $A^k$和$C^k$</li><li>最后一层的预测输出为：$$\hat{a} = softmax(W u^{K+1}) = softmax(W(u^K + o^K))$$</li></ul><p>为了减少参数量，有两种方法：</p><ul><li>adjacent:<br>  让相邻层的embedding matrix A=C，共享参数。即：$C^k = A^{k+1}$，对第一层有$A^1 = B$，最后一层有：$C^K = W$。这样就减少了一半的参数量。</li><li>RNN-like:<br>  跟RNN一样，采用完全参数共享的方法，$A^1 = A^2 = … = A^K$;$C^1 = C^2 = … = C^K$。参数数量大大减少导致模型效果变差，在层与层之间添加一个线性映射：$u^{k+1} = Hu^k + o^k$</li></ul><h3 id="key-value-Memory-Networks"><a href="#key-value-Memory-Networks" class="headerlink" title="key-value Memory Networks"></a>key-value Memory Networks</h3><p>Jason Weston作为作者之一的<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">Alexander Miller2016</a>在End-to-End Memory networks的基础上继续推进，可以更好的通过memory来编码和利用先验知识，并且具体地应用到了QA系统中。</p><p>作为memory的先验知识可以是结构化的三元组知识库，也可以是非结构化的文本。</p><ul><li>三元组知识库。三元组的形式是”实体-关系-实体”，或”主语-谓语-宾语”。三元组知识库的优点是结构化的，便于机器处理。但缺点是与一句完整的话比较，三元组缺少了一些信息。由于三元组知识库是人工构建的，难免会有覆盖不到的知识，对于某个问题可能知识库中根本就没有对应的知识。另外，三元组中的实体可以有多种不同的表达，比如知识库中有三元组”中国-首都-北京”。当问题是“中华人民共和国的首都是？”时，可能就不能很好地回答。</li><li>像“维基百科”这样的非结构化文本。优点时覆盖面广，几乎包含所有问题的知识。缺点是非结构化的，有歧义，需要经过复杂的推理才能找到答案。</li></ul><p>作为先验知识的memory是(key,value)形式的。</p><ul><li>key memory用于寻址(addressing/lookup)阶段，通过计算query与key memory的相关程度来计算attention权重，因此在设计key memory时，key memory的特征应该更好地匹配query。</li><li>value memory用于read阶段，将value memory的加权和作为memory总的向量表示，因此在涉及value memory时，value memory的特征应该更好地匹配response。</li></ul><p>比较一下end-to-end memory network与key-value memory network的区别：</p><ul><li>前者是将相同的输入经过两个不同的embedding matrix编码分为作为key memory和value memory。而后者可以将不同的知识(key,value)分别编码为key memory和value memory，可以更灵活地利用先验知识。</li><li>后者的每个hop之间添加了用$R_j$来进行线性映射。</li></ul><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>在问答系统中，记memory slots为$(k_1,v_1),…,(k_M,v_M)$，问题query为x，真实回复为a，预测回复为$\hat{a}$。$\Phi_{X},\Phi_{Y},\Phi_{K},\Phi_{V}$分别是x,a,key,value的embedding matrix，将文本编码为向量表示。</p><p>则单次memory的寻址和读取可以分为三步：</p><ul><li>key hashing:<br>  当知识库很大时，这一步是非常必要的。根据query从知识库中检索筛选出相关的facts $ (k_{h_1},v_{h_1}),(k_{h_2},v_{h_2}),…,(k_{h_N},v_{h_N})$，筛选条件可以是key中至少包含query中一个相同的词（去除停用词）。这一步可以在数据预处理时进行，直接将query和相关的facts作为模型的输入。</li><li>key addressing(寻址阶段)<br>  计算query与memory的相关程度来分配在memory上的概率分布：$$p_{h_i} = softmax(A\Phi_{X}(x) \cdot A\Phi_{K}(k_{h_i}))$$其中$\Phi$将文本编码为D维向量，A是一个$d\times D$的可训练矩阵。</li><li>value reading：<br>  将value的加权求和作为memory总的向量表示。$$o = \sum_{i}p_{h_i}A\Phi_{V}(v_{h_i})$$</li></ul><p>memory的读取过程是由controller神经网络通过query $q = A\Phi_{X}(x)$来控制的。模型会利用query $q$与上一跳(hop)的$o$来更新query，进而迭代地寻址和读取memory，这个迭代的过程称为多跳(hops)。<br>用多跳方式来迭代地寻址和读取memory，可以这样来理解：浅层神经网络可以学习到低级的特征，随着神经网络层数增多就可以学习到更高级的特征。类比CNN处理人脸图片时，第一层可以学习到一些边缘特征，第二层可以学习到眼睛、鼻子、嘴巴这样的特征，最后一层得到整个人脸的特征。同样地，用多跳方式来寻址和读取memory，可以得到更相关更突出的memory，同时可以起到推理的作用。</p><p>query的更新公式为:$$q_2 = R_1(q + o)$$其中R是一个$d\times d$的可训练矩阵。每一跳使用不同的矩阵$R_j$。<br>则第j跳更新query后，寻址阶段的计算公式为$$p_{h_i} = softmax(q_{j+1}^\top \cdot A\Phi_{K}(k_{h_i}))$$<br>在经过H跳之后，用controller神经网络的最终状态进行预测:$$\hat{a} = argmax_{i=1,…,C}softmax(q_{H+1}B\Phi_{y}(y_i))$$其中B是一个$d\times D$的可训练矩阵，形状跟A一样。$y_i$可以是知识库中的实体，或者候选句子。</p><p>模型的目标函数为预测回复$\hat{a}$与真实回复$a$之间的交叉熵，用梯度下降的方法来更新模型参数：$A,B,R_1,…,R_H$</p><div align="center"><img src="/images/key-value-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.7. 问答系统key-value memory networks的模型框架</font><br><a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener"><font color="grey" size="2">来源:Alexander Miller2016</font></a></div><h4 id="key-value的选择与编码方式"><a href="#key-value的选择与编码方式" class="headerlink" title="key-value的选择与编码方式"></a>key-value的选择与编码方式</h4><p>论文根据不同形式的先验知识，提出了key-value不同的编码方式：</p><ul><li>知识库三元组。三元组形式为”subject-relation-object”，将”subject-relation”作为寻址的key，将”object”作为记忆的value。</li><li>sentence level。直接将句子的词袋向量表示作为key和value，key和value是一样的。每个memory slot存一个句子。</li><li>window level。以大小为W的窗口对文档进行分割（只保留中心词为实体的窗口），将单个窗口内的词作为寻址的key，将窗口的中心词作为value。</li></ul><h3 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://zhuanlan.zhihu.com/c_129532277" target="_blank" rel="noopener">记忆网络-Memory Network</a></li><li><a href="https://jhui.github.io/2017/03/15/Memory-network/" target="_blank" rel="noopener">Memory network (MemNN) &amp; End to end memory network (MemN2N), Dynamic memory network</a></li><li><a href="http://thespermwhale.com/jaseweston/icml2016/" target="_blank" rel="noopener">Memory Networks for Language Understanding, ICML Tutorial 2016</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍Memory Networks。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Neural Turing Machines" scheme="http://yoursite.com/tags/Neural-Turing-Machines/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>attention? attention!</title>
    <link href="http://yoursite.com/2019/07/23/attention-attention/"/>
    <id>http://yoursite.com/2019/07/23/attention-attention/</id>
    <published>2019-07-23T08:20:18.000Z</published>
    <updated>2019-08-28T09:38:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>读了博主<a href="https://lilianweng.github.io/lil-log/contact.html" target="_blank" rel="noopener">Weng, Lilian</a>的文章<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention? attention!</a>，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。<br>attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。</p><a id="more"></a><h3 id="why-we-need-attention-从seq2seq模型谈起"><a href="#why-we-need-attention-从seq2seq模型谈起" class="headerlink" title="why we need attention?从seq2seq模型谈起"></a>why we need attention?从seq2seq模型谈起</h3><p><strong>seq2seq模型</strong>与14年提出(<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever, et al. 2014</a>)，实现输入序列(source sequence)到输出序列(target sequence)的映射，这两个序列的长度都是可变的。序列到序列映射的任务包括机器翻译、问答系统、对话系统、摘要生成等。</p><p>用数学语言来定义序列到序列的任务，给定输入序列(source sequence) $X = \lbrace{ x_1,x_2,…,x_n \rbrace}$，需要生成输出序列(target sequence) $Y = \lbrace{ y_1,y_2,…,y_m \rbrace}$，其中source sequence长度为$n$,target sequence长度为$m$。</p><p><strong>seq2seq模型</strong>基于encoder-decoder框架，包括2个部分：</p><ol><li><p><strong>encoder</strong>将source sequence编码（映射）为一个固定维度的向量表示(context vector,或称为sentence embedding)，我们希望这个向量表示可以很好的表示source sequence的意思。<br> encoder可以采用卷积神经网络CNN，也可以采用循环神经网络RNN，但用的更多的效果也更好的还是RNN。通常使用<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM 或 GRU</a>。<br> encoder RNN的隐藏状态更新公式为：$$\begin{gather}h_t = f(h_{t-1},x_t)\end{gather}$$其中$h_t$为RNN在时间步t的隐藏状态，f为LSTM 或GRU.<br> 对于长度为n的source sequence，一个词接一个词地输入RNN后，可以得到n个隐藏状态$(h_1,h_2,…,h_n)$，通常将最后一个时间步最后一个词对应的隐藏状态$h_t$作为source sequence的向量表示，也就是context vector，记为$c$。</p></li><li><p><strong>decoder</strong>根据source sequence的向量表示context vector，来一个词一个词的生成target sequence。<br> decoder采用单向RNN，decoder RNN隐藏状态的更新公式为:$$\begin{gather}s_t = f(s_{t-1},y_{t-1},c)\end{gather}$$其中$s_t$为decoder在时间步t的隐藏状态，$y_{t-1}$为target sequence中的上一个词，在train阶段，$y_{n-1}$为真实target sequence中的上一个词，在infer阶段，$y_{t-1}$为预测输出的上一个词；c为context vector。<br> 时间步t，隐藏状态$s_t$再经过线性层和softmax得到在词表上的概率分布，将概率最大的词作为prediction word $y_t$。迭代循环直到输出整个target sequence。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/43.png" alt="Fig.1. seq2seq模型的框架图" title>                </div>                <div class="image-caption">Fig.1. seq2seq模型的框架图</div>            </figure><p>我们可以看到当生成不同的$y_t$时，所依据的context vector都是固定不变的。固定的context vector有一个缺点是：当encoder编码完整个source sequence时，会偏向于最近的词，而遗忘了距离更远的最开始的一些词。<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">(Bahdanau et al., 2015)</a>提出了attention机制来解决这个问题。</p><h3 id="attention机制-born-for-Translation"><a href="#attention机制-born-for-Translation" class="headerlink" title="attention机制:born for Translation"></a>attention机制:born for Translation</h3><p>attention机制最先在机器翻译(neural machine translation,NMT)任务上提出。从解决长期依赖问题的角度，attention可以实现长距离的记忆；从注意力的角度，attention机制可以实现对齐(alignment)，用更多的注意力关注到相关的部分，而忽略或低注意力关注到不相关的部分。</p><p>上文中提到，在生成不同的$y_t$时，直接将encoder最后一个时间步的隐藏状态$h_n$作为固定context vector。不同于这种方法，attention机制将所有encoder隐藏状态$\lbrace{ h_1,h_2,…,h_n }\rbrace$的加权和作为context vector，这样在每个时间步t生成$y_t$时，所依据的context vector都是专门针对于$y_t$的。<br>一方面，context vector可以获取到所有隐藏状态，也就是整个source sequence的信息，这样就可以实现长距离的记忆。另一方面，source sequence 与target sequence之间的语义对齐(aligenment)是也是通过context vector实现的。在计算时间步t生成$y_t$对应的context vector $c_t$的计算需要三个部分的信息：</p><ul><li>所有的encoder隐藏状态： $\lbrace{ h_1,h_2,…,h_n }\rbrace$</li><li>上个时间步t-1的decoder 隐藏状态： $s_{t-1}$</li><li>source与target之间的alignment.<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/44.png" alt="Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)" title>                </div>                <div class="image-caption">Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)</div>            </figure></li></ul><h4 id="attention的数学定义"><a href="#attention的数学定义" class="headerlink" title="attention的数学定义"></a>attention的数学定义</h4><p>在计算时间步t生成$y_t$对应的context vector $c_t$时，encoder的所有隐藏状态为 $\lbrace{ h_1,h_2,…,h_n }\rbrace$，时间步t-1的decoder隐藏状态为 $s_{t-1}$，decoder RNN的隐藏状态更新公式变为：$$\begin{gather}s_t = f(s_{t-1},y_{t-1},c_t)\end{gather}$$ context vector $c_t$为encoder hidden state的加权和：<br>$$c_t = \sum_{i=1}^{n}\alpha_{t,i}h_i$$ $$\alpha_{t,i} = softmax(\beta_{t,i}) = \frac{exp(\beta_{t,i})}{\sum_{j = 1}^{n}exp(\beta_{t,j})}$$ $$\beta_{t,i} = score(s_{t-1},h_i)$$<br>其中权重$\alpha_{t,i}$是时间步t生成$y_t$与隐藏状态$h_i$之间的score，从某种意义上说，$h_i$可以看作是$x_i$的表示，也可以看作是$\lbrace{x_1,x_2,…,x_{i}}\rbrace$的表示。因此，$\alpha_{t,i}$可以看作是$y_t$与$x_i$之间联系（相关性）的score。所有权重$\lbrace{\alpha_{t,1},\alpha_{t,2},…,\alpha_{t,n}}\rbrace$衡量了生成$y_t$时应该如何关注到所有的encoder hidden state。</p><p>score()为打分函数，有多种计算方法，下文会详细介绍。在<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015.</a>中，score()采用前馈神经网络，采用非线性激活函数$tanh()$,score()的数学形式为：$$score(s_{t},h_i) = v_a^\top tanh(W_a[s_t;h_i])$$<br>其中$v_a,W_a$是可训练参数。<br>attention权重可视化矩阵很直观地表明了source words与target words之间的关联关系:</p><div align="center"><img src="/images/45.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.3.来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)</font></div><h3 id="各种attention机制"><a href="#各种attention机制" class="headerlink" title="各种attention机制"></a>各种attention机制</h3><h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><p>下表总结了使用比较广泛的attention机制，及其对应的alignment score function。</p><table class="table table-bordered table-striped table-condensed">   <tr>      <th width="25">名字</th>      <th>alignment score funtion</th>      <th width="25">来源</th>   </tr>   <tr>      <td>content-based attention</td>      <td>$score(s_t,h_i) = cosine(s_t,h_i)$</td>      <td><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves2014</a></td>   </tr>   <tr>      <td>concat/additive</td>      <td>$score(s_{t},h_i) = v_a^\top tanh(W_a[s_t;h_i])$</td>      <td><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a></td>   </tr>   <tr>      <td>location-based</td>      <td>$\alpha_{t,i} = softmax(W_as_t)$<br><font color="grey" size="2">将alignment简化为只依赖于target position</font></td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>general</td>      <td>$score(s_{t},h_i) = s_t^\top W_ah_i$</td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>dot-product</td>      <td>$score(s_{t},h_i) = s_t^\top h_i$<br><font color="grey" size="2">note:当general attention的$W_a$为单位矩阵时，就退出为dot-product attention</font></td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>scaled <br>dot-product(*)</td>      <td>$score(s_{t},h_i) = \frac{s_t^\top h_i}{\sqrt{n}}$<br><font color="grey" size="2">note:跟dot-product attention很像，n是encoder hidden state $h_i$的维度</font></td>      <td><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani2017</a></td>   </tr></table>(*)scaled dot-product attention机制添加了比例因子$/frac{1}{/sqrt{n}}$，动机是：对于softmax()函数，当输入很大时，对应的梯度很小（梯度逐渐消失），难以进行高效的优化和学习。因此，添加比例因子可以减小$score(s_t,h_i)$。<p>下表列出了更广范畴上的attention机制。</p><table class="table table-bordered table-striped table-condensed">   <tr>      <th width="25">名字</th>      <th>定义</th>      <th width="25">来源</th>   </tr>   <tr>      <td>self attention(&)</td>      <td><font color="grey" size="2">将input sequence的不同部分联系起来，只用到input sequence本身，而不用target sequence。<br>可以使用上表中的所有score function，只要将target sequence替换为input sequence即可。</font></td>      <td><a href="https://arxiv.org/pdf/1601.06733.pdf" target="_blank" rel="noopener">Cheng2016</a></td>   </tr>   <tr>      <td>global/soft attention</td>      <td><font color="grey" size="2">context vector是整个input sequence的加权和，注意到整个input sequence</font></td>      <td><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu2015</a></td>   </tr>   <tr>      <td>local/hard attention</td>      <td><font color="grey" size="2">context vector是局部input sequence的加权和，注意到局部input sequence</font></td>      <td><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu2015</a>，<br><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr></table>(&)self-attention在一些论文中也被称为intra-attention.<h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p>self-attention,最先在<a href="https://arxiv.org/pdf/1601.06733.pdf" target="_blank" rel="noopener">Cheng2016</a>提出称为”intra-attention”，后来在大作<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">attention is all you need</a>中发挥了更大的影响力。self-attention将同一个sequence的不同位置的tokens联系起来，建模tokens之间的关系，计算这个sequence的向量表示。[Cheng2016]提出self-attention的动机是什么呢？</p><p>我们先看以下LSTM的局限。LSTM在编码sequence的向量表示时，隐藏状态更新公式为：$$h_t = f(h_{t-1},x_t)$$从这个更新公式可以看到：在给定$h_t$的条件下，$h_{t+1}$与之前的状态$\lbrace{h_1,h_2,…,h_{t-1}}\rbrace$及之前的tokens $\lbrace{x_1,x_2,…,x_t}\rbrace$是条件独立的。LSTM的潜在假设是当前状态$h_t$包含了之前所有tokens的信息，这相当于假设LSTM有无限大的memory，这个假设实际上是不成立的。实际上LSTM会偏向于更近的tokens，而逐渐遗忘距离更远的tokens。另一方面，LSTM在编码token的隐藏状态时，没有建模tokens之间的关系。而这恰恰就是self-attention要解决的问题，也就是self-attention的核心思想：在计算sequence的向量表示时，引入tokens之间的关系。</p><p>接下来看self-attention的数学表示。对于sequence $\lbrace{x_1,x_2,…,x_n}\rbrace$，每个token $x_t$分别对应一个hidden vector 和memory vector。当前的memory tape $C_{t-1} = \lbrace{c_1,c_2,…,c_{t-1}}\rbrace$，hidden state tape为$H_{t-1} = \lbrace{h_1,h_2,…,h_{t-1}}\rbrace$。self-attention计算$x_t$与$\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$之间的关系：$$\beta_{t,i} = score(x_t,h_i) = v^\top tanh(W_hh_i,W_xx_t,W_{\tilde{h}}\tilde{h_{t-1}})$$ $$\alpha_{t,i} = softmax(\beta_{t,i})  ;i\in[1,t-1]$$</p><p>attention权重$\alpha_{t,i}$是t时间步x_t在之前的tokens $\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$对应的hidden vector上的概率分布。<div align="center"><img src="/images/46.png" width="60%" height="60%"></div></p><div align="center"><font color="grey" size="2">Fig.4.红色表示当前token，蓝色的深浅表示相关程度。<br>来源:[Cheng2016](https://arxiv.org/pdf/1601.06733.pdf)</font></div><p>比较一下self-attention机制与传统attention机制的区别：</p><ul><li>传统的attention机制是将target sequence与source sequence联系起来，attention权重$\lbrace{\alpha_{t,1},\alpha_{t,2},…,\alpha_{t,n}}\rbrace$是在encoder hidden states $\lbrace{h_1,h_2,…,h_n}\rbrace$上的概率分布。而self-attention是将同个sequence不同位置的tokens联系起来，attention权重$\alpha_{t,i}$是t时间步$x_t$在之前的tokens $\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$对应的hidden vector上的概率分布。</li><li>传统的attention机制常与RNN联合使用，在transformer中self-attention可以与RNN解耦开（也就是分开使用），单独用self-attention也可以编码sequence的表示向量。</li></ul><h4 id="soft-vs-hard-attention"><a href="#soft-vs-hard-attention" class="headerlink" title="soft vs hard attention"></a>soft <em>vs</em> hard attention</h4><p><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Show, Attend and Tell,Kelvin Xu2015</a>将attention机制用到了”给图片生成描述”的任务，第一次明确区分了hard attention与soft attention，区分的依据是attention是关注到整张图片，还是图片的局部。</p><ul><li>soft attention：attention关注到整张图片，或者是整个序列。alignment 权重$\alpha_{t,i}$是在整个序列上的概率分布。就像普通的attention一样。<ul><li>好处：模型是可微的。</li><li>坏处：计算量比较大。</li></ul></li><li>hard attention：attention关注到图片的局部，或者是序列的一部分。<ul><li>好处：减少了计算量。</li><li>坏处：模型不可微，需要用更复杂的技术，比如强化学习或者方差缩减来训练模型。<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></li></ul></li></ul><h4 id="global-vs-local-attention"><a href="#global-vs-local-attention" class="headerlink" title="global vs local attention"></a>global <em>vs</em> local attention</h4><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>在NMT任务上提出了global 和local attention的概念。区分的依据是attention是关注到整个序列，还是关注到序列的一部分。</p><ul><li>global attention。 类似于soft attention，关注到整个序列。这里比较下<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>的global attention与<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a>中attention的区别。<ul><li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a>中attention的计算路径是：$s_{t-1} \to \alpha_{t} \to c_t \to s_t$<br>$$\beta_{t,i} = score(s_{t-1},h_i)$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i = 1}^{n}\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = g(y_{t-1},c_t,s_t)$$</li><li><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>的global attention的计算路径是：$s_t \to \alpha_{t} \to c_t \to \tilde{s_t}$<br>$$\beta_{t,i} = score(s_{t},h_i)$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i = 1}^{n}\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$\tilde{s_t} = tanh(W_c[c_t,s_t])$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = softmax(W_s\tilde{s_t})$$</li></ul></li><li>local attention。是soft 与hard attention的结合，关注到序列的一部分。对hard attention进行改进，使得模型可微，训练和计算变得更容易。改进的方法如下：<ol><li>对于时间步t的target token $y_t$先用模型预测，生成一个对齐的位置$p_t$，</li><li>再根据固定窗口大小内$[p_t - D,p_t + D]$的encoder hidden state来计算context vector。D是窗口大小，是按经验定义好的。</li></ol></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/47.png" alt="Fig.5. global and local attenion.<br>来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)" title>                </div>                <div class="image-caption">Fig.5. global and local attenion.<br>来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)</div>            </figure><h3 id="pointer-network"><a href="#pointer-network" class="headerlink" title="pointer network"></a>pointer network</h3><p>对于输出序列的类别数依赖于输入序列的长度的问题，seq2seq模型或神经图灵机不能解决。因为这类问题中，输出的类别数是可变的，而seq2seq模型的decoder只能在固定数目的类别上生成一个概率分布。<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Vinyals2017</a>提出了pointer network（Pr_Net）来解决输出词表可变的问题。pointer network实际上是以attention为基础的。</p><p>我们比较下attention机制与pointer network的区别。<br>记输入序列$X = \lbrace{x_1,…,x_n}\rbrace$,输出序列$Y = {y_1,…,y_m}$，$y_j$是X的位置索引，$y_i \in [1,n] $。<br>encoder的所有hidden state为$\lbrace{h_1,h_2,…,h_n}\rbrace$，decoder在时间步t的隐藏状态为$s_t$，则：</p><ul><li>attention机制用alignment权重来计算context vector：<br> $$\beta_{t,i} = score(s_t,h_i) = v^\top tanh(W_ss_t,W_hh_i); i \in [1,n]$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i=1}^{n}\alpha_{t,i}h_i$$</li><li>pointer network则用alignment权重在作为在输入序列上的概率分布，将输入序列中的token直接复制到输出序列中：<br> $$\beta_{t,i} = score(s_t,h_i) = v^\top tanh(W_ss_t,W_hh_i); i \in [1,n]$$ $$p(y_i|y_{&lt; i},X) = softmax(\beta_{t,i})$$<div align="center"><img src="/images/49.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.6.Pointer Network model<br>来源:[Vinyals2017](https://arxiv.org/abs/1506.03134)</font></div></li></ul><h4 id="pointer-network解决OOV问题"><a href="#pointer-network解决OOV问题" class="headerlink" title="pointer network解决OOV问题"></a>pointer network解决OOV问题</h4><p>什么是OOV（out of vocabulary）问题？在序列（source sequence）到序列（target sequence）的映射问题（对话系统，问答系统）中，会根据训练集语料来构建词表，根据完成$word \to index \to embedding$的向量化表示。而在测试集的source sequence中难免会出现一些词表中没有的词，通常会将这些out of vocabulary的词映射到一个特定的字符”UNK”，而decoder在生成response时也可能生成”UNK”这个特殊字符。这就是OOV问题。</p><p>pointer network是解决OOV问题的有效方法。当source sequence中出现不在词表中的词时，pointer network可以直接将这个生词从输入序列复制到输出序列中。<a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Abigail See2017</a>就用了pointer network来解决OOV问题。</p><p>记时间步t decoder的隐藏状态为$s_t$,对应的context vector为$c_t$,alignment权重为$\alpha_{t,i},i \in [1,n]$</p><ol><li>在词汇表上的概率分布为:$p_{vocab} = softmax(W[s_t,c_t] + b)$</li><li>在输入序列的概率分布为:$p_{ptr} = \alpha_{t,i} = softmax(\beta_{t_i})$</li><li>选择开关为: $p_{gen} = sigmoid(W_ss_t + W_cc_t + W_xx_t + b)$<br>为逻辑回归，取值为[0,1]</li><li>最终在extend vocabulary上的概率分布为:$p(w) = p_{gen}p_{vocab} + (1-p_{gen})p_{ptr}$.<br>当$p_{gen}$为1时，从词汇表中生成word；当$p_{gen}$为0时，将输入序列的词复制到输出序列中。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/48.png" alt="Fig.7.Pointer-generator model.<br>来源：[Abigail See2017](https://arxiv.org/abs/1704.04368)" title>                </div>                <div class="image-caption">Fig.7.Pointer-generator model.<br>来源：[Abigail See2017](https://arxiv.org/abs/1704.04368)</div>            </figure> <p>类似的论文还有：<a href="https://arxiv.org/pdf/1603.06393v3.pdf" target="_blank" rel="noopener">CopyNet,Jiatao Gu2016</a></p><h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">attention is all you need!</a>(Vaswani, et al., 2017)提出了transformer。transformer也是基于encoder-decoder框架的，也可以看作是一个seq2seq模型。但不同于encoder和decoder都采用RNN的seq2seq模型，transformer完全依赖self-attention机制来计算input和output的向量表示，而不使用RNN或CNN。一般来说，attention机制是与RNN联合使用的，transformer把attention和RNN解耦开了，只使用attention机制。</p><p>为什么transformer用self-attention来编码input和output，而不使用RNN呢？<br>一方面，由RNN的更新公式$s_t = f(s_{t-1},x_t)$可以看到，RNN处理序列时是串行计算的，尤其是处理长序列时更费时间。不利于并行化，计算效率低。而transformer采用attention来编码计算向量，可以进行并行化计算，提高计算效率。<br>另一方面，RNN在编码长序列时，随着距离的增大，往往会偏向于最近的部分，而学习不到长期依赖。但self-attention机制不受距离的限制，可以有效地学习到长期依赖。</p><h4 id="scaled-dot-product-attention与key-value-query"><a href="#scaled-dot-product-attention与key-value-query" class="headerlink" title="scaled dot-product attention与key,value,query"></a>scaled dot-product attention与key,value,query</h4><p>transformer的主要组件是<em>multi-heads self-attenion mechanism</em>，这个组件用到了scaled dot-product attention机制。一般地，attention机制将query和(key,value)映射为output，其中query,key,value,output都是vector。output是所有values的加权和，权重是通过计算query与对应key之间的关联度得到。</p><p>具体来说，将什么作为key,value,query？分两种情况，从框图可以直观的看到：</p><ul><li>在encoder-decoder框架中，联系encoder与decoder的attention机制通常将encoder的所有hidden states乘以两个不同的矩阵$W^Q,W^K$分别作为<em>keys</em>和<em>values</em>。将decoder上一个时间步的hidden state作为query。</li><li>在encoder模块，self-attention机制将input词级别的向量表示乘以三个不同的矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算input总的句子级别的向量表示。同样地，在decoder模块中self-attention机制将output词级别的向量表示分别乘以三个不同矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算output总的句子级别的向量表示。</li></ul><p>有了具体的key,value,query后，scaled dot-product attention机制怎么来计算output呢？<br>记query,key,value的矩阵形式分别为Q,K,V。query和key维度为$d_k$,value维度为$d_v$。则output的计算方式为：$$Attention(Q,K,V) = softmax(\frac{QK^\top}{\sqrt{d_k}})V$$</p><p>多种attention机制中，transformer为什么选择采用scale dot-product attention机制呢？<br>最常用的两种attention机制是dot-product和additive attention机制。dot-product attention用点乘来做打分函数，additive attenion将有一层隐藏层的前馈网络作为打分函数。理论上来说，这两种attention的计算复杂度是一样的；但实际上，dot-product attention计算更快，占用内存更小。因为dot-product attention机制可以采用高度优化的矩阵乘法代码。<br>当维度$d_k$较小时，这两种attention机制的效果是差不多的。当维度$d_k$更大时，additive attention的效果要好于dot-product attention。这可能是因为当维度$d_k$变大时，点乘的值变得过大，而softmax()函数在值过大的范围梯度是很小的，类似于梯度消失问题。因此，添加比例因子$\frac{1}{\sqrt{d_k}}$来减小点乘的值。</p><h4 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h4><p>并不是只用一次attention机制，将维度为$d_{model}$的key,value,query映射为output。而是将query,key,value映射到维度为$d_k,d_k,d_v$不同的子向量空间，并行的计算$h$次，分别得到output做concat操作，得到总的output。$h$为head的个数，也就是并行attention layer的层数。有关系$d_{model} = d_v \cdot h$，论文中采用$d_{model} =512,h=8,d_k = d_v = \frac{d_{model}}{h} = 64$ $$MultiHead(Q,K,V) = concat(head_1,head_2,…,head_h)W^O$$ $$where \quad head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$ 其中$W_i^Q \in R^{d_{model} \cdot d_k},W_i^K \in R^{d_{model} \cdot d_k},W_i^V \in R^{d_{model} \cdot d_v},W^O \in R^{hd_{v} \cdot d_{model}}$</p><div align="center"><img src="/images/multi-head-attention.png" width="40%" height="40%"></div><div align="center"><font color="grey" size="2">Fig.7.multi-head scaled dot-product attention</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>encoder将input text编码为基于attention的包含位置信息的向量表示。</p><ul><li>由6个完全相同的层堆叠起来。</li><li>每一层包含两个子层。第一子层是multi-head attention层，第二层是一个简单的全连接层。</li><li>两个子层之间采用<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">残差连接</a>，并进行归一化。这样所有子层的输出都有相同的维度$d_{model} = 512$</li></ul><div align="center"><img src="/images/transformer-encoder.png" width="70%" height="70%"></div><div align="center"><font color="grey" size="2">Fig.9. transformer encoder</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>从encoder output得到总的context vector，并据此生成response。</p><ul><li>与encoder相同，由6个完全相同的层堆叠起来。</li><li>每一层除了encoder中的两个子层外，还插入了一个multi-head layer来在所有encoder output上进行attention操作。</li><li>两个子层之间采用<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">残差连接</a>，并进行归一化。</li><li>第一个multi-head attention sub-layer进行mask操作，mask掉output当前时间步后所有的tokens。防止attention机制看到未来的信息。</li></ul><div align="center"><img src="/images/transformer-encoder.png" width="70%" height="70%"></div><div align="center"><font color="grey" size="2">Fig.10. transformer decoder</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="transformer的总体结构"><a href="#transformer的总体结构" class="headerlink" title="transformer的总体结构"></a>transformer的总体结构</h4><ul><li>input和output都先经过一个embedding layer得到各自的向量表示，维度为$d_{model} = 512$</li><li>由于self-attention不能像RNN一样自动地编码位置信息，因此需要额外地将位置信息添加到输入。</li><li>在最后decoder的输出外接一个线性层和softmax层。</li></ul><div align="center"><img src="/images/transformer.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.11. transformer的整体框架</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a>  by Weng, Lilian</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;读了博主&lt;a href=&quot;https://lilianweng.github.io/lil-log/contact.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Weng, Lilian&lt;/a&gt;的文章&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;attention? attention!&lt;/a&gt;，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。&lt;br&gt;attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="rnn" scheme="http://yoursite.com/tags/rnn/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>pip安装python模块报错</title>
    <link href="http://yoursite.com/2019/07/12/pip%E5%AE%89%E8%A3%85python%E6%A8%A1%E5%9D%97%E6%8A%A5%E9%94%99/"/>
    <id>http://yoursite.com/2019/07/12/pip安装python模块报错/</id>
    <published>2019-07-12T01:51:58.000Z</published>
    <updated>2019-07-12T02:11:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>在使用<code>pip install</code>命令安装python模块时，报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h3><p>报错信息告诉我们：“不能卸载‘pyyaml’模块，因为这个模块是<code>distutils</code>方式安装的，不能确定哪些文件属于这个模块，因此不能完整地卸载这个模块。”</p><p><a href="https://cloud.tencent.com/developer/section/1371690" target="_blank" rel="noopener">distutils</a>是python最初的模块安装和分发系统，distutils不会保留哪些文件属于哪个安装包的信息，甚至不会保留安装包之间的依赖关系。直接使用<code>distutils</code>的方式已经被淘汰，取而代之的是<a href="https://setuptools.readthedocs.io/en/latest/" target="_blank" rel="noopener">setuptools</a>.<br>    所谓模块的分发，就是开发者打包并发布自己的模块，供其他人使用。</p><p>这样我们就知道了，因为<code>pyyaml</code>模块时通过<code>distutils</code>方式安装的，因此不能明确文件与包之间的隶属关系，不能正确卸载。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>使用下面的命令忽略已安装的模块，强制安装和更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install &lt;package-name&gt; --ignore-installed &lt;pyyaml&gt; --upgrade</span><br></pre></td></tr></table></figure><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.jianshu.com/p/94caf01dd9a6" target="_blank" rel="noopener">强制安装和更新</a></li><li><a href="https://cloud.tencent.com/developer/ask/196670" target="_blank" rel="noopener">如何在Windows操作系统中升级/卸载distutils软件包（PyYAML）？</a></li><li><a href="https://docs.python.org/zh-cn/3/installing/index.html" target="_blank" rel="noopener">python官方手册-安装python模块</a></li><li><a href="https://cloud.tencent.com/developer/section/1371690" target="_blank" rel="noopener">setuptools与distutils</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在使用&lt;code&gt;pip install&lt;/code&gt;命令安装python模块时，报错：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Cannot uninstall &amp;apos;PyYAML&amp;apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>NAACL2019-对话系统</title>
    <link href="http://yoursite.com/2019/07/10/NAACL2019-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2019/07/10/NAACL2019-对话系统/</id>
    <published>2019-07-10T12:55:17.000Z</published>
    <updated>2019-07-21T15:03:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。<br>NAACL2019的会议列表链接：<a href="https://naacl2019.org/program/accepted/" target="_blank" rel="noopener">https://naacl2019.org/program/accepted/</a></p><a id="more"></a><h3 id="《Evaluating-Coherence-in-Dialogue-Systems-using-Entailment》"><a href="#《Evaluating-Coherence-in-Dialogue-Systems-using-Entailment》" class="headerlink" title="《Evaluating Coherence in Dialogue Systems using Entailment》"></a>《Evaluating Coherence in Dialogue Systems using Entailment》</h3><p>【链接】<a href="https://arxiv.org/abs/1904.03371" target="_blank" rel="noopener">https://arxiv.org/abs/1904.03371</a><br>【代码】<a href="https://github.com/nouhadziri/DialogEntailment" target="_blank" rel="noopener">https://github.com/nouhadziri/DialogEntailment</a></p><p>加拿大阿尔伯塔大学发表的论文。论文提出了一种评估对话系统生成回复好坏的指标。<br>这篇论文的想法来源于：发表在ACL2019上的论文<a href="https://arxiv.org/abs/1811.00671" target="_blank" rel="noopener">《Dialogue Natural Language Inference》</a>提出利用NLI(natural language inference)任务来提高对话系统生成回复的一致性。<br>本文的作者则想到用NLI任务来评估对话系统生成回复的好坏。具体地，论文用了BERT<a href="https://arxiv.org/abs/1609.06038" target="_blank" rel="noopener">[Devlin et al., 2018]</a>和The Enhanced Sequential Inference Model(ESIM)<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">[Chen et al., 2016]</a> 这两种方法来训练NLI模型。另外论文还公开了一个用于NLI任务的数据集。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;NAACL2019的会议列表链接：&lt;a href=&quot;https://naacl2019.org/program/accepted/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://naacl2019.org/program/accepted/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
  </entry>
  
  <entry>
    <title>ACL2019-对话系统</title>
    <link href="http://yoursite.com/2019/07/07/ACL2019-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2019/07/07/ACL2019-对话系统/</id>
    <published>2019-07-07T11:19:27.000Z</published>
    <updated>2019-07-31T03:40:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。<br>ACL2019的会议列表链接：<a href="http://www.acl2019.org/EN/program.xhtml" target="_blank" rel="noopener">http://www.acl2019.org/EN/program.xhtml</a></p><a id="more"></a><h3 id="《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》"><a href="#《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》" class="headerlink" title="《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》"></a>《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.01788" target="_blank" rel="noopener">https://arxiv.org/abs/1906.01788</a><br>【源码】：无</p><p>中科院自动化所发表的短论文。在多轮对话中，对话历史（context information）对回复（response）的生成有重要作用。任务型对话中的管道模型分为4个模块：NLU、对话状态追踪、对话策略学习 及NLG。对话状态追踪又包含任务：domain classification、intent detection和slot filling。domain classification和intent detection任务当做分类任务来处理，常采用SVM或深度神经网络的方法；slot filling任务被当做序列标注任务来处理，常采用BiLSTM+CRF模型。NLU能否充分利用context information，对这三个下游任务有很大影响。<br>为了更好的利用context information，本文提出了对话逻辑推断任务（DLI,dialog logic inference），任务定义为：将打乱顺序的多轮对话重新排序；输入之前的对话，从剩余的utterance candidates中选中下一句对话。NLU任务采用了所谓的memory network，其实就是采用多个encoder对context information进行编码，再用attention机制或别的方法得到context information总的向量化表示。本文联合训练DLI任务和NLU任务，通过两个任务共享encoder和memory retrieve模块，来让NLU任务更好地利用context information。其实是得到context information更合理的向量化表示，来作为下游domain classification、intent detection和slot filling任务的输入。</p><p>论文提出的将打乱顺序的对话重新排序的DLI任务，可以进一步深入，将句子切分为几段打乱顺序再重新排序；可以应用到闲聊式对话系统中。</p><h3 id="《Dialogue-Natural-Language-Inference》"><a href="#《Dialogue-Natural-Language-Inference》" class="headerlink" title="《Dialogue Natural Language Inference》"></a>《Dialogue Natural Language Inference》</h3><p>【链接】：<a href="https://arxiv.org/abs/1811.00671" target="_blank" rel="noopener">https://arxiv.org/abs/1811.00671</a><br>【代码】：无<br>【数据集】：<a href="https://wellecks.github.io/dialogue_nli/" target="_blank" rel="noopener">https://wellecks.github.io/dialogue_nli/</a></p><p>加利福尼亚大学、Facebook AI Lab发表的论文。核心是提出用NLI(natural language inference)任务来提高persona-based dialog system的一致性。这里就要先搞清楚NLI任务和一致性问题两个概念。</p><ul><li><p>先从问题出发，所谓对话的一致性问题。可以分为两类：</p><ul><li><p>logical contradiction，逻辑矛盾。比如同一个人的两句话:”我有一只狗”，”我没养过狗”。就是逻辑矛盾的。</p></li><li><p>比较模糊的非逻辑矛盾。同一个人不可能说出的两句话：“我从来不运动”，“我去篮球了”。就是这种非逻辑矛盾。真香警告。</p><p>至于persona一致性问题，就是回复的utterance不能与说话人的persona矛盾，也不能与之前的回复有矛盾。</p></li></ul></li><li><p>具体介绍NLI任务。这其实是一个分类问题。论文公开了一个自己标注的NLI数据集。</p><ul><li>训练阶段：训练集形式是 {$（s_1,s_2）$,label }，对应labels $\in$（一致、无关、矛盾）。</li><li>在test阶段，给定一个句子对（句子1，句子2）来判断对应的label。</li></ul></li></ul><p>论文的最终目的是通过NLI任务训练的模型来提高persona dialog system的一致性。这是如何来实现的呢？对于一个dialog system，给定对话历史$（u_1,u_2,…,u_t）$ 及说话人的persona文本描述$（p_1,p_2,…,p_n）$,从response candidates$（y_1,y_2,…,y_m）$中选择一个$u_{t+1}$（如何生成多个responses不是这篇论文要解决的）。<br>用NLI任务的模型来预测$(y_i,u_j),(y_i,p_k)其中：i\in [1,m],j\in [1,t],k \in [1,n]$对应的label，如果句子之间是矛盾的，则添加惩罚项。从而得到一致性最好的utterance作为response。</p><h3 id="《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》"><a href="#《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》" class="headerlink" title="《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》"></a>《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》</h3><p>【链接】：<a href="https://arxiv.org/abs/1907.05339" target="_blank" rel="noopener">https://arxiv.org/abs/1907.05339</a><br>【数据集】：<a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" target="_blank" rel="noopener">English Ubuntu dialogue corpus</a><br>【代码】：<a href="https://github.com/zhanghainan/ReCoSa" target="_blank" rel="noopener">https://github.com/zhanghainan/ReCoSa</a></p><p>中科院发表的论文。<br>在多轮对话中，生成response时，对话历史中最相关的部分起着重要的作用。论文要解决的问题：如何更准确地找到并利用relevant context来生成response。<br>多轮对话中广泛使用的HRED模型,[<a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="noopener">(Serban et al.,2016;</a>,<a href="https://arxiv.org/abs/1507.02221" target="_blank" rel="noopener">Sordoni et al., 2015</a>]无差别地利用context information，忽略了relevant context。虽然有利用relevant context的相关工作，但这些工作都有各自的问题。[<a href="https://www.aclweb.org/anthology/P17-2036" target="_blank" rel="noopener">Tian et al., 2017</a>]提出计算context 与post之间的cosine similarity来衡量context relevance，其假设是context与response之间的relevance等价于post与response之间的relevance，这个假设是站不住脚的。[<a href>Xing et al., 2018</a>]向HRED模型引入了attention机制，但attention机制定位relevant context时会产生偏差，因为基于RNN的attention机制倾向于最靠近的context（close context）。论文提出了自己的解决办法，用self-attention机制来衡量context于response之间的relevance。self-attention机制的优点是可以有效捕捉到长距离的依赖关系。</p><p>模型分为三个部分：<br>context包含N轮对话： ${s_1,s_2,…,s_N}$其中，$s_i = {x_1,x_2,…,x_M}$，M为句子长度。<br>response为$Y = {y_1,y_2,…,y_M}$</p><ol><li><strong>context representation encoder</strong>：<br> 将context encode为vector。<ol><li>word-level encoder：<br> 用LSTM对sentence编码，将LSTM最后一个时间步的hidden state作为sentence representation: $h^{s_i}$；<br> 由于self-attention机制不能区分word位置信息，还需要添加position embedding: $p^{s_i}$,<br> 把两个向量做concatenate操作，得到总得sentence representation:$(h^{s_i},p^{s_i})$。<br> 对于context中的N个句子有${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$</li><li>context self-attention:<br> 采用multi-head self-attention机制，将${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$经过不同的线性变换作为query、keys、values matrix,由N个sentence representation得到总的context representation $O_s$。</li></ol></li><li><strong>response representation encoder</strong><br> 同样用multi-head self-attention机制,将response的word embedding及position embedding ${(w_1,p_1),…,(w_{t-1},p_{t-1})}$经过不同的线性变换作为query、keys、values matrix，得到response representation $O_r$。<ol><li>在train阶段<br> 采用mask操作，在时间步t对于word $y_t$，mask掉${y_t,y_{t+1},…,y_M}$，只保留${y_1,y_2,…,y_{t-1}}$来计算response representation。</li><li>在infer阶段<br> 在生成response的时间步t，将生成的response ${g_1,…,g_{t-1}}$，来作为response representation。</li></ol></li><li><strong>context-response attention decoder</strong><br> 采用multi-head self-attention机制，将context attention representation $O_s$作为keys、values matrix，将response hidden representation $O_r$作为query matrix。得到输出$O_d$. <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/42.png" alt="模型框架图" title>                </div>                <div class="image-caption">模型框架图</div>            </figure></li></ol><h3 id="《Persuasion-for-Good-Towards-a-Personalized-Persuasive-Dialogue-System-for-Social-Good》"><a href="#《Persuasion-for-Good-Towards-a-Personalized-Persuasive-Dialogue-System-for-Social-Good》" class="headerlink" title="《Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good》"></a>《Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.06725" target="_blank" rel="noopener">https://arxiv.org/abs/1906.06725</a><br>【代码、数据集】：<a href="https://gitlab.com/ucdavisnlp/persuasionforgood/tree/master" target="_blank" rel="noopener">https://gitlab.com/ucdavisnlp/persuasionforgood/tree/master</a></p><p>浙江大学、加利福尼亚大学发表。获得ACL2019 best paper提名。<br>论文的主要贡献是公开了一个包含说话人个人信息的劝说数据集，在子集上标注了十种不同的劝说策略。并训练了用于分类不同劝说策略的分类器。</p><h3 id="《Do-Neural-Dialog-Systems-Use-the-Conversation-History-Effectively-An-Empirical-Study》"><a href="#《Do-Neural-Dialog-Systems-Use-the-Conversation-History-Effectively-An-Empirical-Study》" class="headerlink" title="《Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study》"></a>《Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.01603" target="_blank" rel="noopener">https://arxiv.org/abs/1906.01603</a><br>【代码】：<a href="https://github.com/chinnadhurai/ParlAI/" target="_blank" rel="noopener">https://github.com/chinnadhurai/ParlAI/</a></p><p>论文获得ACL2019 best short paper提名。<br>论文的研究点是：生成式对话系统是否有效利用或正确理解了对话历史？论文通过向对话历史中引入不同类型的扰动，来研究生成式对话系统生成回复的困惑度变化。这个方法的一个前提是如果生成式对话系统对某种信息的扰动不敏感，那么它没有有效利用这种信息。</p><p>论文在比较了三种模型。</p><ul><li>基于LSTM的seq2seq模型。</li><li>基于LSTM的seq2seq模型 + attention机制。</li><li>基于transformer的seq2seq模型。</li></ul><p>论文在四个多轮对话数据集上进行实验。</p><ul><li>bAbI dialog。<a href="https://arxiv.org/abs/1605.07683" target="_blank" rel="noopener">(Bordes and Weston, 2016)</a></li><li>Persona Chat。<a href="https://www.aclweb.org/anthology/papers/P/P18/P18-1205/" target="_blank" rel="noopener">(Zhang et al., 2018)</a></li><li>Dailydialog。 <a href="https://arxiv.org/abs/1710.03957" target="_blank" rel="noopener">(Li et al., 2017)</a></li><li>MutualFriends。<a href="https://arxiv.org/abs/1704.07130" target="_blank" rel="noopener">(He et al., 2017)</a></li></ul><p>论文向对话历史引入了不同的扰动。</p><ul><li>句子级别的扰动：<ul><li>随机打乱对话历史中句子的顺序。</li><li>倒序对话历史中句子的顺序。</li><li>随机去掉对话历史中特定的句子。</li><li>对话历史中有n个句子，只保留最近的k个句子$(k \le n)$。</li></ul></li><li>词级别的扰动：<ul><li>随机打乱一个句子中词的顺序。</li><li>倒序一个句子中词的顺序。</li><li>随机去掉对话历史中30%的词。</li><li>去掉所有的名词。</li><li>去掉所有的动词。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;ACL2019的会议列表链接：&lt;a href=&quot;http://www.acl2019.org/EN/program.xhtml&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.acl2019.org/EN/program.xhtml&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="ACL2019" scheme="http://yoursite.com/tags/ACL2019/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理---会议列表</title>
    <link href="http://yoursite.com/2019/04/24/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E4%BC%9A%E8%AE%AE%E5%88%97%E8%A1%A8/"/>
    <id>http://yoursite.com/2019/04/24/自然语言处理-会议列表/</id>
    <published>2019-04-24T06:55:43.000Z</published>
    <updated>2019-07-07T09:00:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录自然语言处理方向的国际会议列表。</p><a id="more"></a><h2 id="A类会议"><a href="#A类会议" class="headerlink" title="A类会议"></a>A类会议</h2><p><a href="http://www.aaai.org/" target="_blank" rel="noopener">AAAI</a>，Association for the Advancement of Artificial Intelligence</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><a href="https://iclr.cc/" target="_blank" rel="noopener">ICLR</a>，The International Conference on Learning Representations，国际学习表征会议<br>2013年才刚刚成立了第一届。这个一年一度的会议已经被学术研究者们广泛认可，被认为「深度学习的顶级会议」。<br>这个会议的来头不小，由位列深度学习三大巨头之二的 Yoshua Bengio 和 Yann LeCun 牵头创办。Yoshua Bengio 是蒙特利尔大学教授，深度学习三巨头之一，他领导蒙特利尔大学的人工智能实验室（MILA）进行 AI 技术的学术研究。MILA 是世界上最大的人工智能研究中心之一，与谷歌也有着密切的合作。而 Yann LeCun 就自不用提，同为深度学习三巨头之一的他现任 Facebook 人工智能研究院（FAIR）院长、纽约大学教授。作为卷积神经网络之父，他为深度学习的发展和创新作出了重要贡献。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://www.ccf.org.cn/xspj/gyml/" target="_blank" rel="noopener">中国计算机学会推荐国际学术会议和期刊目录</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录自然语言处理方向的国际会议列表。&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="会议列表" scheme="http://yoursite.com/tags/%E4%BC%9A%E8%AE%AE%E5%88%97%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>python读写csv文件</title>
    <link href="http://yoursite.com/2019/04/19/python%E8%AF%BB%E5%86%99csv%E6%96%87%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/04/19/python读写csv文件/</id>
    <published>2019-04-19T10:32:22.000Z</published>
    <updated>2019-07-21T15:22:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍csv文件的读写。</p><a id="more"></a><h2 id="csv模块"><a href="#csv模块" class="headerlink" title="csv模块"></a>csv模块</h2><h3 id="csv-writer-csvfile"><a href="#csv-writer-csvfile" class="headerlink" title="csv.writer(csvfile)"></a>csv.writer(csvfile)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import csv </span><br><span class="line">row = [&apos;Symbol&apos;,&apos;Price&apos;,&apos;Date&apos;,&apos;Time&apos;,&apos;Change&apos;,&apos;Volume&apos;]</span><br><span class="line">rows = [(&apos;AA&apos;, 39.48, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.18, 181800),</span><br><span class="line">         (&apos;AIG&apos;, 71.38, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.15, 195500),</span><br><span class="line">         (&apos;AXP&apos;, 62.58, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.46, 935000),</span><br><span class="line">       ]</span><br><span class="line">with open(&apos;name.csv&apos;,&apos;w&apos;) as csvfile:</span><br><span class="line">    writer = csv.writer(csvfile,delimiter = &apos;\t&apos;,lineterminator = &apos;\n&apos;) #delimiter和lineterminator分别是分隔符，行结束符</span><br><span class="line">    writer.writerow(row) #写入单行</span><br><span class="line">    writer.writerows(rows) #写入多行</span><br></pre></td></tr></table></figure><h3 id="csv-reader-csvfile"><a href="#csv-reader-csvfile" class="headerlink" title="csv.reader(csvfile)"></a>csv.reader(csvfile)</h3><p>该函数接收一个可迭代对象，返回对象<code>reader</code>是一个生成器，不能直接用下标访问。可以用for循环和next()函数访问。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile:</span><br><span class="line">    reader = csv.reader(csvfile,delimiter = &apos;\t&apos;) #迭代器</span><br><span class="line">    rows = [row for row in reader] #用for循环访问：</span><br><span class="line">for row in rows:</span><br><span class="line">    print(row)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/36.png" alt title>                </div>                <div class="image-caption"></div>            </figure>如果要读取csv文件的某列，可以看下面的例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile:</span><br><span class="line">    reader = csv.reader(csvfile,delimiter = &apos;\t&apos;) #迭代器</span><br><span class="line">    column = [row[2] for row in reader] #用for循环访问：</span><br><span class="line">print(column)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/37.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h3 id="csv-DictReader-csvfile"><a href="#csv-DictReader-csvfile" class="headerlink" title="csv.DictReader(csvfile)"></a>csv.DictReader(csvfile)</h3><p>与csv.reader()函数相同，接收一个可迭代对象，返回一个生成器。不同之处是，返回的每个单元格放在字典的值中，字典的键就是这个单元格的列头。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./name.csv&apos;,&apos;r&apos;) as f:</span><br><span class="line">    reader = csv.DictReader(f,delimiter = &apos;\t&apos;)</span><br><span class="line">    rows = [row for row in reader]</span><br><span class="line">for row in rows:</span><br><span class="line">    print(rows)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/38.png" alt title>                </div>                <div class="image-caption"></div>            </figure>如果要读取csv文件的某列，可以看下面的例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./name.csv&apos;,&apos;r&apos;) as f:</span><br><span class="line">    reader = csv.DictReader(f,delimiter = &apos;\t&apos;)</span><br><span class="line">    column = [row[&apos;Time&apos;] for row in reader]</span><br><span class="line">print(column)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/39.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h3 id="csv-DictWriter-csvfile"><a href="#csv-DictWriter-csvfile" class="headerlink" title="csv.DictWriter(csvfile)"></a>csv.DictWriter(csvfile)</h3><h2 id="pandas读写csv"><a href="#pandas读写csv" class="headerlink" title="pandas读写csv"></a>pandas读写csv</h2><p>也可以直接用pandas的函数<code>read_csv()</code>来读取csv文件的列。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">f = pd.read_csv(&apos;name.csv&apos;,delimiter = &apos;\t&apos;)</span><br><span class="line">time = f.Time</span><br><span class="line">print(time)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/40.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://docs.python.org/zh-cn/3/library/csv.html" target="_blank" rel="noopener">python官方文档-csv模块</a></li><li><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html" target="_blank" rel="noopener">读写csv数据</a></li><li><a href="https://blog.csdn.net/Allyli0022/article/details/79125672" target="_blank" rel="noopener">使用python获取csv文本的某行或某列数据</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍csv文件的读写。&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="csv" scheme="http://yoursite.com/tags/csv/"/>
    
      <category term="panda" scheme="http://yoursite.com/tags/panda/"/>
    
  </entry>
  
  <entry>
    <title>torch.cuda.is_available()返回False,但nvidia-smi正常</title>
    <link href="http://yoursite.com/2019/04/16/torch-cuda-is-available-%E8%BF%94%E5%9B%9EFalse-%E4%BD%86nvidia-smi%E6%AD%A3%E5%B8%B8/"/>
    <id>http://yoursite.com/2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/</id>
    <published>2019-04-16T09:04:44.000Z</published>
    <updated>2019-07-22T01:27:54.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>torch.cuda.is_available()</code>返回False,但nvidia-smi可以正常运行。</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在pytorch用GPU来加速计算时发现。<code>torch.cuda.is_available()</code>返回False,但nvidia-smi可以正常运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; torch.cuda.is_available()</span><br><span class="line">False</span><br></pre></td></tr></table></figure><p>此时，<code>nvidia-smi</code>可以正常运行。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/34.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h2 id="可能原因"><a href="#可能原因" class="headerlink" title="可能原因"></a>可能原因</h2><p>在<code>nvidia-smi</code>的运行结果中可以看到，driver version是<code>390.xx</code>。可能是driver version版本太低，造成了这个问题，实际上也是如此。<br>driver version的常见版本是<code>384.xx</code>,<code>390.xx</code>,<code>396.xx</code>。接下来，把driver version升级到<code>396.xx</code>看能不能解决问题。</p><h2 id="升级nvidia-driver-version"><a href="#升级nvidia-driver-version" class="headerlink" title="升级nvidia driver version"></a>升级nvidia driver version</h2><ol><li><p>卸载旧版本的NVIDIA driver </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia-\*</span><br></pre></td></tr></table></figure></li><li><p>添加NVIDIA的ppa源.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br></pre></td></tr></table></figure></li><li><p>安装新版本的NVIDIA driver </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install nvidia-driver-396</span><br></pre></td></tr></table></figure></li></ol><p>此时，运行<code>nvidia-smi</code>，会报以下错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to initialize NVML: Driver/library version mismatch</span><br></pre></td></tr></table></figure><p>这是更新NVIDIA driver版本后的常见问题。这个问题出现的原因是kernel mod的NVIDIA driver版本没有更新。<br>执行以下命令可以查看nvidia kernel mod的version。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/driver/nvidia/version</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/32.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>可以看到已经加载的nvidia kernel mod的版本是还是旧版本<code>390.xx</code>。一般情况下，重启服务器就能解决问题。<br>如果由于某些原因不能重启，可以重新加载kernel mod。思路是先unload kernel mod，再reload kernel mod. 详见<a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a><br><code>nvidia-smi</code>可以正常运行后，问题就解决了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; torch.cuda.is_available()</span><br><span class="line">True</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://discuss.pytorch.org/t/torch-cuda-is-available-returns-false-nvidia-smi-is-working/20614" target="_blank" rel="noopener">Torch.cuda.is_available() returns False, nvidia-smi is working</a></li><li><a href="https://askubuntu.com/questions/1063871/how-can-i-update-the-nvidia-drivers-to-version-390-77" target="_blank" rel="noopener">How can I update the NVIDIA drivers to version 390.77?</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;torch.cuda.is_available()&lt;/code&gt;返回False,但nvidia-smi可以正常运行。&lt;/p&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="nvidia-smi" scheme="http://yoursite.com/tags/nvidia-smi/"/>
    
      <category term="GPU" scheme="http://yoursite.com/tags/GPU/"/>
    
      <category term="cuda" scheme="http://yoursite.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>nvidia-smi返回错误信息‘Failed to initialize NVML: Driver/library version mismatch’</title>
    <link href="http://yoursite.com/2019/03/29/nvidia-smi%E8%BF%94%E5%9B%9E%E9%94%99%E8%AF%AF%E4%BF%A1%E6%81%AF%E2%80%98Failed-to-initialize-NVML-Driver-library-version-mismatch%E2%80%99/"/>
    <id>http://yoursite.com/2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/</id>
    <published>2019-03-29T11:47:03.000Z</published>
    <updated>2019-07-21T15:23:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Ubuntu运行命令<code>nvidia-smi</code>出错。</p><a id="more"></a> <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在Ubuntu18.04的命令行中运行命令<code>nvidia-smi</code>，返回错误信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to initialize NVML: Driver/library version mismatch</span><br></pre></td></tr></table></figure><h2 id="方法1：重启解决大部分问题"><a href="#方法1：重启解决大部分问题" class="headerlink" title="方法1：重启解决大部分问题"></a>方法1：重启解决大部分问题</h2><p>博客<a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a>讲述的很清楚，这里就不再赘述。<br>或者参考大型交友网站stack overflow的问题<a href="https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch/45319156" target="_blank" rel="noopener">NVIDIA NVML Driver/library version mismatch</a></p><h2 id="方法2：重装驱动"><a href="#方法2：重装驱动" class="headerlink" title="方法2：重装驱动"></a>方法2：重装驱动</h2><p>看返回的错误信息，这个问题出现的原因是NVIDIA Driver的版本不匹配。如果重启不能解决问题，我们需要卸载重装NVIDIA driver。</p><ol><li><strong>查看驱动程序版本</strong></li></ol><ul><li><code>dpkg -l | grep nvidia</code> <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/31.png" alt title>                </div>                <div class="image-caption"></div>            </figure>可以看到驱动版本是<code>390.116</code></li><li><code>cat /proc/driver/nvidia/version</code> <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/32.png" alt title>                </div>                <div class="image-caption"></div>            </figure>这里NVRM的版本是<code>390.87</code>。错误信息就是这两个版本不匹配造成的。接下来先卸载NVIDIA driver，再重新安装。</li></ul><ol start="2"><li><p><strong>卸载旧版本的NVIDIA driver</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia-\*</span><br></pre></td></tr></table></figure></li><li><p><strong>添加NVIDIA的ppa源</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br></pre></td></tr></table></figure></li><li><p><strong>重新安装NVIDIA的驱动</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install nvidia-390</span><br></pre></td></tr></table></figure><p> 用你自己的版本号替换<code>390</code>。</p></li></ol><p>这时再用<code>cat /proc/driver/nvidia/version</code>查看NVIDIA driver的驱动。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/33.png" alt title>                </div>                <div class="image-caption"></div>            </figure> 可以看到NVRM的版本是<code>390.116</code>，这时版本就匹配了。<br>再次执行<code>nvidia-smi</code>,终于看到<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/34.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><p>在<a href="https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa" target="_blank" rel="noopener">nvidia driver各版本总览</a>可以看到NVIDIA driver的各个版本。</p><h2 id="额外的：update-与-upgrade"><a href="#额外的：update-与-upgrade" class="headerlink" title="额外的：update 与 upgrade"></a>额外的：update 与 upgrade</h2><p>记录下<code>sudo apt-get update</code>与<code>sudo apt-get upgrade</code>的区别。<br>在windows系统中安装软件，只需要有exe文件，双击即可安装了。Linux系统中则不同，Linux会维护一个自己的软件仓库，几乎所有软件都在这个仓库里，而且里面的软件完全安全，绝对可以安装。<br>我们自己的Ubuntu服务器上，维护一个软件源列表文件<code>/etc/apt/sources.list</code>,里面都是一些网址信息，每个网址就是一个软件源，这个地址指向的数据标识着有哪些软件可以安装使用。</p><ol><li><p>查看源列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li><li><p>更新软件列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><p> 这个命令对访问源列表里的每个网址，读取软件列表，保存到本地电脑。</p></li><li><p>更新软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure><p> 这个命令会把本地已安装的软件，与软件列表里对应软件做对比，如果有可更新版本就更新软件。<br>总的来说，<code>sudo apt-get update</code>是更新软件列表，<code>sudo apt-get upgrade</code>是更新软件。</p></li></ol><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a></li><li><a href="https://www.cnblogs.com/yizhichun/p/6397168.html" target="_blank" rel="noopener">Ubuntu配置GPU+CUDA+CAFFE</a></li><li><a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---Tensorflow-gpu%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85.html" target="_blank" rel="noopener">ubuntu下安装安装CUDA、cuDNN和tensotflow-gpu版本流程和问题总结</a></li><li><a href="https://wiki.ubuntu.org.cn/NVIDIA#PPA.E6.BA.90" target="_blank" rel="noopener">NVIDIA的wiki</a></li><li><a href="https://www.cnblogs.com/darkknightzh/p/5638185.html" target="_blank" rel="noopener">（原）Ubuntu16中安装nvidia的显卡驱动</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ubuntu运行命令&lt;code&gt;nvidia-smi&lt;/code&gt;出错。&lt;/p&gt;
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
      <category term="nvidia-smi" scheme="http://yoursite.com/tags/nvidia-smi/"/>
    
  </entry>
  
</feed>
