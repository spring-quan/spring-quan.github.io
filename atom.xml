<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>spring&#39;s Blog</title>
  
  <subtitle>游龙当归海，海不迎我自来也。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-13T10:30:34.529Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>spring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pySpark学习笔记</title>
    <link href="http://yoursite.com/2020/02/13/pySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/02/13/pySpark学习笔记/</id>
    <published>2020-02-13T06:14:50.000Z</published>
    <updated>2020-02-13T10:30:34.529Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="pyspark下载与环境设置"><a href="#pyspark下载与环境设置" class="headerlink" title="pyspark下载与环境设置"></a>pyspark下载与环境设置</h3><p><strong>前提：</strong> 安装pyspark之前，要检查电脑是否安装了JAVA环境，可以用命令<code>java -version</code>来查看。<br>参考链接：<a href="https://blog.csdn.net/ricardo_leite/article/details/76070490" target="_blank" rel="noopener">Centos下JDK的安装与卸载</a></p><ul><li><p>Centos下JDK的安装</p><ol><li>查看yum库中有哪些可用的JDK版本：<code>yum search java | grep jdk</code>。</li><li>选择版本安装JDK，可以用<code>yum install java-1.8.0-openjdk-devel.x86_64</code>命令来安装JAVA环境。</li></ol></li><li><p>Centos下JDK的卸载</p><ol><li>先查看系统中安装了哪些rpm软件包,查看相关Java包的信息：<br> <code>rpm -qa | grep java</code></li><li>卸载已安装的JDK: <code>yum -y remove java &lt;包名&gt;</code>，比如<code>yum -y remove java java-1.6.0-openjdk-1.6.0.38-1.13.10.0.el7_2.x86_64</code>。</li></ol></li></ul><p>在清华大学镜像源<a href="http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz</a>下载pyspark安装包。</p><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><ol><li>在<code>./bin/pyspark</code>启动pyspark时，报以下错误：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></figure></li></ol><p>出错原因是：pyspark 2.1需要Java 1.7以上的版本，而安装的Java版本是1.6的。</p><ol start="2"><li>在python代码中调用pyspark报错：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &apos;py4j&apos;</span><br></pre></td></tr></table></figure></li></ol><p>这是因为<code>~/.bashrc</code>中py4j的版本与实际的版本不同。修改<code>~/.bashrc</code>中py4j的版本为实际的版本即可。<br>参考链接：<a href="https://blog.csdn.net/skyejy/article/details/90690742" target="_blank" rel="noopener">https://blog.csdn.net/skyejy/article/details/90690742</a></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="http://codingdict.com/article/8881" target="_blank" rel="noopener">编程字典-pyspark教程</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;pyspark下载与环境设置&quot;&gt;&lt;a href=&quot;#pyspark下载与环境设置&quot; class=&quot;headerlink&quot; title=&quot;pyspark下载与环境设置&quot;&gt;&lt;/a&gt;pyspark下载与环境设置&lt;/h3&gt;&lt;p&gt;&lt;str
      
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="pySpark" scheme="http://yoursite.com/tags/pySpark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记</title>
    <link href="http://yoursite.com/2020/02/11/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/02/11/Hadoop学习笔记/</id>
    <published>2020-02-11T07:09:31.000Z</published>
    <updated>2020-02-11T09:36:22.143Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。</p><a id="more"></a><h3 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h3><p>Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。随着技术发展，人类每天都会产生海量数据，用单一的机器来存储和处理这些数据已经不能满足需求。而Hadoop允许在从单一的机器扩展到上千台机器，从而在跨计算机的分布式环境中来存储和处理大数据。</p><p>Hadoop的架构如下图所示：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/hadoop_architecture.png" alt="Hadoop的架构图" title>                </div>                <div class="image-caption">Hadoop的架构图</div>            </figure></p><ul><li>HDFS: 分布式文件存储系统</li><li>YARN: 分布式资源管理</li><li>MapReduce: 分布式计算</li><li>Others: 利用YARN的资源管理来实现其他的数据处理方式</li></ul><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>HDFS(Hadoop Distributed File System)是Hadoop应用主要的分布式文件系统。HDFS是基于“Master-Worker”架构的，一个HDFS集群由一个NameNode和多个DataNode组成。NameNode是一个中心服务器，管理文件系统的命名空间(NameSpace)，管理文件系统的元数据(MataData)，相当于是一个目录。DataNode负责存储实际的数据。</p><p>   HDFS暴露了文件系统的命名空间，用户可以以文件的形式来存储数据。具体来看，一个文件被分为一个或者多个数据块(Block)，这些数据块存储在一组DataNode上；每个数据库对应NameNode上的一条记录。NameNode执行文件系统的命名空间操作，比如打开、关闭、重命名文件或目录；它也负责数据库与DataNode之间的映射。DataNode负责处理文件系统客户端的读写请求，在NameNode的统一管理下来进行数据块的创建、删除和复制。</p><div align="center"><img src="/images/hdfs-architecture.png" width="85%" height="85%"></div><div align="center"><font color="grey" size="2">HDFS的架构</font></div><ul><li><strong>Block</strong> <ol><li>数据块block是基本存储单位，一般大小为64MB。</li><li>一个文件会被分成一个或多个数据块来存储。如果文件大小小于一个Block的大小，那么实际占用的空间为文件大小。</li><li>Block是基本的读写单位，相当于磁盘的页，每次都会读写一个Block。</li><li>每个Block会被存储到多个机器，默认是3个。防止机器故障造成数据丢失。</li></ol></li><li><strong>NameNode</strong><ol><li>存储文件的元数据，管理文件系统的命名空间。整个HDFS可存储文件的大小受限于NameNode的内存大小。</li><li>一个Block对应NameNode中的一条记录。</li><li>NameNode失效后，整个HDFS就都失效了。要保证NameNode的可用性。</li></ol></li><li><strong>DataNode</strong><ol><li>存储具体的block数据。</li><li>负责数据的读写操作和复制操作。</li><li>DataNode启动时会向NameNode汇报当前存储的block信息，随后也会定时向NameNode汇报修改信息。</li><li>DataNode之间会进行通信，复制block，保证数据的冗余性。</li></ol></li></ul><h4 id="HDFS-shell命令"><a href="#HDFS-shell命令" class="headerlink" title="HDFS shell命令"></a>HDFS shell命令</h4><p>Hadoop包含了一系列类shell命令，可以直接和HDFS或hadoop支持的其他文件系统进行直接的交互。<code>hadoop fs -help</code>可以列出所有的shell命令。这些shell命令支持大部分普通文件系统的操作，比如复制、删除文件、更改文件权限等。</p><p>调用文件系统shell命令应该采用<code>hadoop fs &lt;arg&gt;</code>的形式，所有的FS shell命令都使用URI路径作为参数。URI路径的格式是’scheme://authority/path’，对于HDFS文件系统，scheme是<code>hdfs</code>；对于本地文件系统，scheme是<code>file</code>。其中scheme参数和authority参数是可选的，省略的话会使用默认的参数scheme。</p><ul><li><p><strong>ls</strong><br>  使用方式： <code>hadoop fs -ls &lt;args&gt;</code><br>  如果是文件，会显示文件信息；如果是目录，会显示目录下所有的文件。</p></li><li><p><strong>test</strong><br>  使用方式： <code>hadoop fs -test [edz] URI</code><br>  选项：<br>  -e 检查文件是否存在，如果存在返回0；<br>  -z 检查文件是否是0字节，如果是返回0；<br>  -d 如果路径是个目录，返回1，否则返回0</p></li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_design.html" target="_blank" rel="noopener">官方手册-Hadoop分布式文件系统：架构和设计</a></li><li><a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" target="_blank" rel="noopener">官方手册-Hadoop Shell命令</a></li><li><a href="https://pennywong.gitbooks.io/hadoop-notebook/content/introduction.html" target="_blank" rel="noopener">Hadoop简介</a></li><li><a href="https://www.w3cschool.cn/hadoop/i1la1jyc.html" target="_blank" rel="noopener">Hadoop教程-W3Cschool</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。&lt;/p&gt;
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>2019-蓟门烟树</title>
    <link href="http://yoursite.com/2020/01/13/2019-%E8%93%9F%E9%97%A8%E7%83%9F%E6%A0%91/"/>
    <id>http://yoursite.com/2020/01/13/2019-蓟门烟树/</id>
    <published>2020-01-13T15:01:16.000Z</published>
    <updated>2020-01-13T16:01:36.719Z</updated>
    
    <content type="html"><![CDATA[<p>这是在明光桥北度过的第二个冬天，已经下了两三场大雪，雪后的天空格外晴朗。又到了一年的末尾，没有经常写日记，只能从朋友圈、论坛发的骑行贴、日记本上不多的几篇日记，印象笔记上的记录来尽力回忆这一年是如何度过的。回首这一年，平凡普通，不惊心动魄，有些许遗憾，也有一些小的闪光和美好。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/snow_2020.png" alt="北京的雪" title>                </div>                <div class="image-caption">北京的雪</div>            </figure><a id="more"></a><h2 id="平淡的科研生活"><a href="#平淡的科研生活" class="headerlink" title="平淡的科研生活"></a>平淡的科研生活</h2><p>这一年大部分的时光是在实验室的座位上度过的，虽然有时会懈怠偷懒，但大部分的时间还是在学习与研究方向相关的内容。在一位北邮博士师兄的毕业论文中看到这样一段话：“不需要多么玩命，只要你能每天规律地作息且每天到实验室，学会控制情绪，在实验室的时间全部用来做与科研相关的事情就可以了。”研一的时候，我纠结着是否要读博士？慢慢地我打消了读博士的想法。虽然这一年待在实验室很长时间，但并没有取得太大的进展。虽然最近完成了一篇论文，但我逐渐明白我还不具备独立做科研的能力，也缺乏投身科研的热情，并且缺乏老师的指导。想要读博的动机是功利性的，想要更高的学位，进而毕业后可以获取到更好的工作机会和社会地位。我畏惧读博路上的艰难孤独，缺乏读博的足够动力，学位并不应该是目的，两种选择都不是错误，做好了抉择坚定自己的内心就可以了。</p><p>研究生的生活日常是这样的。给老师做一些项目，标注数据，给老师写一些琐碎无聊的项目书和PPT报告。当然老师的项目中，并不都是琐碎无聊，也有一些值得学习深入研究的内容。辩证法的角度来看，任何事物都是如此的，正面与反面是并存的。在找实习时投递的个人简历上，做项目是一个重要的经历。除了做项目之外，大部分的时间都在阅读论文，这一年来读了上百篇对话系统相关的论文。在量变积累的过程中，自然地对对话系统这个研究方向有了大致的了解，进而可以进一步地进行深入地研究和探索。阅读论文不是目的，阅读论文的数量也不应该成为追求，应该要做的事情是在阅读论文的过程中去发现问题并提出自己的解决方案。吸收知识，然后去应用知识和创造。创造的过程是充满的乐趣的。解决问题的过程就是自己产出的过程，我慢慢地才明白了这一点。</p><p>与导师、同学的相处也是研究生生活的一个重要议题。G老师不在学校在外面谋求仕途晋升之道，做了甩手掌柜，虽然一年中抽出时间开了几次电话会议和两次线下会议。每周一次的小组会是L老师负责的，研三的师兄师姐已经出去实习了，剩下博士学姐、研一研零的学弟学妹和研二的我们开组会。研一时的每次组会，我都会被L老师批评，那时我以为是自己的问题，并对自己产生了怀疑。每次跟着L老师开完组会，学习的积极性都会被打击到，需要花一两天时间来调整心态。如今研一研零的学弟学妹也被老师这样批评着，我渐渐明白了这是L老师的个人风格，并不完全是自己的原因。L老师和G老师不算是很坑的导师，但不同的导师性格不同，教导学生的方式也不同。要学会与不同性格的导师去相处，像水一样，去适应和配合来最大程度地让自己取得进步。跟我一届的几个女生都说”不想开小组会“，这是内心的写照了。大组会的氛围要好一些，实验室有时会请博士和一些已经毕业工作的师兄师姐来做展示交流。实验室的大boss有时也会到场，大boss有次说”要内心强大，要lold住。“，这句话我印象深刻，生活中的许多场景，内心强大，能hold住是非常重要的。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/cat.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>这个学期做了勤工俭学的工作，每周打扫实验室一次，一个月三百块的工资。我常常会压抑自己的欲望和需要，12个小时回家的火车票大多会买硬座票，很少买卧铺。这样的心理，有些凄惨。最近我慢慢地转变了这种心理和想法，想要穿羊毛绒的袜子，想坐卧铺车或者动车回家出行，想要烫一个帅气的发型，想要穿帅气的衣服。今年的收入除了实验室工资、奖学金和勤工俭学的工资之外，我还做了一份线上考研辅导的工作。这是第二次考研辅导，辅导对象是成都某大学的一个女生，辅导她通信原理专业课，薪水是三千块。通信原理是我考研时的专业课，研究生的专业与通信已没有关系了，离我考研已经过去了整整两年，但当我看一遍通信原理的教材，我还是可以理解书上的知识点和难点，我考研时应当是真的付出了很大的努力。我还记得蒋震图书馆前的那排高大的杨树，还记得考研复习时的心理活动：当树叶全部落完时，我就要考研了；当树叶再次长出来郁郁葱葱时，我就要在清风的吹拂中毕业了。如今，校园的主干道的两旁是梧桐树，每当夕阳落下，暮色昏沉，一大群乌鸦总会停在梧桐树枝头，喧闹地叫着，树下是呈正态分布的鸟屎。</p><p>研一下学期结束后的那个暑假，我找了一份中科院自动化所的实习。实习地点在知春路附近的自动化大厦，离学校三四公里远。这是我的第一份实习，实习工资并不高，做了一些文本纠错和文本摘要的任务。原本打算暑期在实习中度过，但还是抽出一周时间回了趟家。今年年初的时候，有个想法是带着爷爷奶奶来北京。爷爷奶奶应该从没有去过省外，也没有来过北京。但奶奶身体并不好，恐怕受不了舟车劳顿，这个想法只能作罢。</p><h2 id="饭局少不了"><a href="#饭局少不了" class="headerlink" title="饭局少不了"></a>饭局少不了</h2><p>北京城，天子脚下，有不少同学会来，因此接待了不少朋友。有一块长大的发小。C已经毕业，在北京工作，在县城里买了房子，也与女方定了婚。年初我们一起去游了圆明园，圆明园离北大不远，历史课本中那副经典照片中的断垣残壁依旧挺立着。后来R来北京探亲，姐姐姐夫在北京工作，姐姐生下了孩子。在杏坛路上的一家店，吃了鱼。我们俩从小就一块打桌球，现在我已经不是他的对手了。</p><p>也有高中的同窗。高中同学有好几个在北京，有的读研，有的工作。年中组织过一次聚餐，亲切中带着一丝生疏，逛着北航的校园，吃了饭，看了部电影《无双》。S是高中班里的才子了，他在西安读研，暑期后不久来参加头条组织的一次夏令营活动。我们在杏坛路的一家店吃了串串香，在漫咖啡用玩了线上桌球。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/entailment.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>更多的是大学的同窗。年初W和Y来北京参加考研复试，我们在小龙坎为他们接风洗尘，但后来Y虽然初试成绩不错但遗憾地没能通过复试。Y小侄女毕业前来北京玩，我们一起游了颐和园，颐和园真是风景宜人，值得一去。Z在准备考研的过程中，忙里偷闲来北京看了话剧。Z律师跟着领导来上北大的培训班。J从日本飞回来后，一块去爬了长城。真的是记成流水账了。</p><p>回头看这一年，还是有不少社交活动的。</p><h2 id="骑车、跑步与越野"><a href="#骑车、跑步与越野" class="headerlink" title="骑车、跑步与越野"></a>骑车、跑步与越野</h2><p>这部分是我平淡生活中的一抹色彩。 在刚读研究生时，原本已经打算与骑行告别了，骑行的热情也消散了。但在实验室同桌X同学巨大骑行热情的影响下，我又重新燃起了骑行的热情，正确地认识了骑行在我生活中的位置。我并不应该将骑行从我生活中完全地剥离出去，而应该让骑行成为我生活中的一部分。我们组成了一个四人的骑行小队，这是一个慢慢培养了骑行默契的小队了。小队的第一次骑行是在五一假期，从北京到张北草原天路。途中经过了雄伟壮观的八达岭长城，蜿蜒的长城在青山的山脊上蔓延展开；也经过了巨大的官厅水库，流经卢沟桥的永定河就是从官厅水库流出的；张北草原天路只走了五公里，并且由于海拔较高五月份草还没长出来，最佳的观赏时节是七八月份，但从张北草原天路到张家口市的那段一路下坡的废弃国道的风景农田也是极美的。小队的第二次骑行是在十月份的一个周末，由北邮出发前往百里山水画廊，再返回北邮。里程有二百八十公里，天黑得又早，两天的行程是比较仓促的。第一天的骑行爬了很多上坡，天黑了才到达落脚地点千家店镇，难度确实太大了，两个女生骑得有些崩溃。第二天的路程虽然较长但是缓下平坦路面，轻松愉快地返回了学校。后来，我们四个人一块去学校附近的一家日料店吃了日料，顺便喝了一点青梅酒，彼此开诚布公袒露心扉，约定有机会的话一块去环海南岛。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/riding_bike.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>2019年的跑量有1120公里，平均配速5分15，并且刷新了自己的十公里、半马和全马的配速。年初过完春节回来有些吃胖了，学期开始后晚上会去操场跑步。后来报名了11月3日举行的北京马拉松比赛，原本对中签没有报太大的期望，因此还报名了北马的志愿者，幸运的是竟然中签了。中签后，特定买了新的跑步鞋和运动裤，大部分时间都能保持较好的跑步训练，有时候也会偷懒几天。11月2号坐地铁去国际会展中心领取了参赛包，晚上早早地就上床睡了。3日凌晨不到5点就起床了，出了宿舍楼门，漆黑的夜里下着小雨，我骑车去了积水潭地铁站。北马赛事有三万五千人的规模，鸣枪开跑后十分钟才通过起跑线。刚开跑时，温度稍微有点低，但一公里后身体就发热了，太阳升起，温度也慢慢上来了。总的来说，11月的北京天气凉爽，并且北马补给充足，我用了3小时35分完赛，大大超出了自己的预期。这是我参加的第二个全马，相比于跑完东马的双腿疼痛，这次北马的准备要充足很多，双腿的酸痛感轻很多。年初我的体重在73公斤左右，托跑步的福，体重下降到了67公斤，这跟我远征完的体重差不多。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/marathon.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>这一年还解锁了越野赛。G学长报名了香山21公里越野赛，但因工作原因不能参加。于是，我就得到了第一次参加山地越野赛赛的机会。不久前下的雪还没有融化，香山越野的21公里因此缩短为了18公里。虽然有段时间没有跑步了，但参加北马的体力基础还在，以七十多名的成绩完赛。山地越野比单纯的马拉松比赛要有趣很多，有柏油路，也有山间的小路，还有下坡的石头路。由于没有训练过如何快速地下坡，15公里时左腿的膝盖已经非常疼了，坚持了完成了比赛。除了山地越野赛之外，还接触到了定向越野赛。先是实验室组队参加了5公里西山定向越野，赢取了一份零食大礼包，在越野赛上遇到了一位女生S。后来，加了她的微信，慢慢有了接触，一起去看了电影，一起约了早饭，但最后并没有一个好的结果。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/xiangshan.png" alt="香山越野赛" title>                </div>                <div class="image-caption">香山越野赛</div>            </figure><p>身边有许多学长去登了雪山，希望以后有机会可以登上一座雪山。</p><p>2020年的主要基调是实习和找工作。希望这个寒假不要虚度，多刷几道算法题。<br>2020的愿望是有更大信心去接纳自己,拥抱生活。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是在明光桥北度过的第二个冬天，已经下了两三场大雪，雪后的天空格外晴朗。又到了一年的末尾，没有经常写日记，只能从朋友圈、论坛发的骑行贴、日记本上不多的几篇日记，印象笔记上的记录来尽力回忆这一年是如何度过的。回首这一年，平凡普通，不惊心动魄，有些许遗憾，也有一些小的闪光和美好。&lt;/p&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;/images/snow_2020.png&quot; alt=&quot;北京的雪&quot; title&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;北京的雪&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="年度总结" scheme="http://yoursite.com/categories/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="年度总结" scheme="http://yoursite.com/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
      <category term="生活记录" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Large-Scale Transfer Learning for Natural Language Generation》</title>
    <link href="http://yoursite.com/2020/01/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ALarge-Scale-Transfer-Learning-for-Natural-Language-Generation%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/07/论文笔记《Large-Scale-Transfer-Learning-for-Natural-Language-Generation》/</id>
    <published>2020-01-07T01:22:38.000Z</published>
    <updated>2020-01-07T02:29:27.650Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】ACL2019<br>【链接】<a href="https://www.aclweb.org/anthology/P19-1608.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P19-1608.pdf</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是<a href="https://huggingface.co/" target="_blank" rel="noopener">huggingface</a>发表在ACL2019的短文。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>传统的有监督学习方法是：在特定的任务上，在有标签的数据集上，有监督地训练一个模型。有监督学习的局限在于许多NLP任务缺乏有标签的数据集，或者有标签数据集的规模比较小。迁移学习就派上用场了，迁移学习在许多NLP任务上取得了好的效果。迁移学习的思路是：先在大规模的未标注文本语料上无监督地预训练一个语言模型，再把预训练好的语言模型迁移到特定的任务上，对模型参数进行微调。<br>目前迁移学习的大部分研究集中在文本分类和NLU(natural language understanding)任务上，迁移学习应用在NLG(natural language generation)任务上的研究比较少。论文认为NLG任务可以分为两类：</p><ul><li>high entropy任务。例如story generation，chit-chat dialog。输入文本包含的信息有限，可能不包含生成输出文本所需要的信息。</li><li>low entropy任务。例如文本摘要、机器翻译。特点是输入文本的信息量比较多，生成输出文本需要的信息包含在输入文本中。</li></ul><p>论文主要研究了迁移学习在对话系统上的应用。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>对话系统主要有三种输入：dialogue history,facts以及previous decoded tokens。论文研究单输入模型和多输入模型。单输入模型应用场景有限，主要关注下输入的部分。多输入模型基于encoder-decoder框架，关注下decoder部分的调整。</p><div align="center"><img src="/images/single_multi_input_model.png" width="50%" height="50%"></div><div align="center"><font color="grey" size="2">Fig.1. 单输入模型与多输入模型的结构图</font></div><h4 id="单输入模型"><a href="#单输入模型" class="headerlink" title="单输入模型"></a>单输入模型</h4><p>单输入模型把三种输入连接起来作为模型的输入。连接方式有三种：</p><ul><li>用自然分隔符连接输入。论文中给每句对话添加双引号。</li><li>用空间分隔符连接。比如用’_SEP’把每个句子连接起来。</li><li>直接把句子连接起来，再用context-type embedding(CTE)来表示输入的类型。例如：$w_{CTE}^{info}$表示用户画像信息，$w_{CTE}^{p^1}$表示对话人1说的话，$w_{CTE}^{p^2}$表示对话人2说的话。<div align="center"><img src="/images/single_multi_input.png" width="40%" height="40%"></div><div align="center"><font color="grey" size="2">Fig.2. (a)单输入模型使用CTE方式的输入(b)多输入模型用起始分隔符连接的输入</font></div></li></ul><h4 id="多输入模型"><a href="#多输入模型" class="headerlink" title="多输入模型"></a>多输入模型</h4><p>多输入模型基于encoder-decoder框架。用预训练的语言模型参数来初始化encoder和decoder。多输入模型的输入同样可以采用单输入模型的处理方式。将persona information和dialogue history分别送入encoder进行编码得到两个向量表示。重点在于decoder部分的调整。decoder的multi-head attention模块处理三种特征输入(personal information,dialogue history,previous decoded tokens)，再把三者的结果取平均值即可。</p><div align="center"><img src="/images/single_multi_input_decoder.png" width="30%" height="30%"></div><div align="center"><font color="grey" size="2">Fig.3. 基于transformer模型的多输入模型</font></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】ACL2019&lt;br&gt;【链接】&lt;a href=&quot;https://www.aclweb.org/anthology/P19-1608.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aclweb.org/anthology/P19-1608.pdf&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Transfer Learning" scheme="http://yoursite.com/tags/Transfer-Learning/"/>
    
      <category term="NLG" scheme="http://yoursite.com/tags/NLG/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《A Pre-training Based Personalized Dialogue Generation Model with Persona-sparse Data》</title>
    <link href="http://yoursite.com/2020/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AA-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/06/论文笔记《A-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data》/</id>
    <published>2020-01-06T03:30:13.000Z</published>
    <updated>2020-01-06T07:23:14.566Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】AAAI2020<br>【链接】<a href="https://arxiv.org/abs/1911.04700" target="_blank" rel="noopener">https://arxiv.org/abs/1911.04700</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是清华大学<a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈</a>教授组发表在AAAI2020的论文。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>论文主要研究个性化的对话系统。论文中提出了“persona-dense”和“persona-sparse”的概念。</p><ul><li>persona-dense: 像<code>Persona-Chat</code>数据集中，在收集语料的过程中，对话人要求在有限的轮数内交流彼此的个性化信息，对话内容是与persona是密切相关的。</li><li>persona-sparse：而在现实的对话中，只有少数的对话与persona是相关的，大多数的对话往往是与persona不相关的。直接在真实对话的语料上训练和微调模型，可能会让模型学习到大多数与persona无关的对话，而把少数与persona相关的对话当作是语料中的噪声。</li></ul><p>为了解决这个问题，对话模型应该学习到哪些对话是与persona相关的，哪些对话是与persona不相关的。<br>论文采用了基于encoder-decoder框架的transformer模型。预训练的方法在许多NLP任务上取得了好的效果，论文提出用预训练的语言模型参数来初始化encoder和decoder。将attribute embedding添加到了encoder的输入；用了attention route机制来建模dialogue history，target persona和previous tokens这三种不同的特征；另外，用了dynamic weight predictor来控制这三种不同的特征在解码生成回复时起到不同程度的作用。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>任务可以描述为：给定对话历史$C$和回复者的target persona $T$，要求生成流畅的回复$Y$。 $$Y = argmax_{Y^{‘}}P(Y^{‘}|C,T)$$ 其中persona $T$由一系列属性来描述，$T = {t_1,t_2,…,t_N}$；每个属性用键值对来表示：$t_i = &lt;k_i,v_i&gt;$。对话历史扩展了说话人的信息，$C=\lbrace{(U_1,T_1),(U_2,T_2),…,(U_M,T_M)}\rbrace$</p><div align="center"><img src="/images/pretrain_personalized_dialogue.png" width="120%" height="120%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="Encoding-with-Persona"><a href="#Encoding-with-Persona" class="headerlink" title="Encoding with Persona"></a>Encoding with Persona</h4><p>模型的输入包括两个部分对话历史$C$和回复者的target persona $T$。</p><ol><li>先说对话历史$C=\lbrace{(U_1,T_1),(U_2,T_2),…,(U_M,T_M)}\rbrace$的的编码，将所有的句子用特定的字符’_SPE’连接起来，并且将对话人$T_i$属性$t_i$映射为embedding表示。具体地，论文中的用户属性包括性别、地址、爱好三个。前两个的属性只有一个值，直接经过embedding层就可以了。爱好可能有多个值，经过embedding层后再取平均值即可。最终将word embedding + positional embedding + attribute embedding作为transformer encoder的输入。 <div align="center"><img src="/images/input_representation.png" width="60%" height="60%"></div> <div align="center"><font color="grey" size="2">Fig.2. input representation of dialogue context</font></div></li><li>至于回复者的target persona $T$的编码，将所有的键值对属性连接起来作为一个序列，经过embedding层，直接作为transformer encoder的输入。</li></ol><h4 id="Attention-Routing"><a href="#Attention-Routing" class="headerlink" title="Attention Routing"></a>Attention Routing</h4><p>对于<code>persona-sparse</code>的对话语料，与persona无关的训练样例，在生成回复时不使用persona信息；与persona相关的训练样例，在生成回复时使用persona信息。论文为此设计了<code>Attention routing</code>模块来控制target persona $T$在生成回复时起到的作用。<br>具体地，将previous decoded tokens的表示$E_{prev}$作为attention的query，在对话历史$E_C$、回复者的target persona $E_T$和previous decoded tokens $E_{prev}$这三种特征上使用multi-head attention机制： $$O_T = MultiHead(E_{prev},E_T,E_T)$$ $$O_C = MultiHead(E_{prev},E_C,E_C)$$ $$O_{prev} = MultiHead(E_{prev},E_{prev},E_{prev})$$ 计算$O_T$和$O_C$时使用unmasked self-attention机制，计算$O_{prev}$时使用masked self-attention机制为避免decoder看到未来时刻的信息。<br>用一个persona weight $\alpha$把三个attention route $O_T,O_C,O_{prev}$结合起来：$$O_{merge} = \alpha O_T + (1-\alpha)O_C + O_C + O_{prev}$$ persona weight $\alpha$应该基于对话是否与persona有关，论文设计了<code>dynamic weight predictor</code>来预测$\alpha$。具体地，这个predictor是一个以对话历史$E_C$为输入的二元分类器$P_{\theta}(r|E_C)$：$$\alpha = P_{\theta}(r = 1|E_C)$$ 这个<code>dynamic weight predictor</code>的训练损失采用交叉熵函数：$$L_W(\theta) = -\sum_i r_i log P_{\theta}(r_i|E_C) + (1-r_i)log(1-P_{\theta}(r_i|E_C))$$</p><h4 id="Pre-training-and-Fine-tuning"><a href="#Pre-training-and-Fine-tuning" class="headerlink" title="Pre-training and Fine-tuning"></a>Pre-training and Fine-tuning</h4><p>我们先在大规模的文本语料上预训练一个语言模型，最小化负对数似然函数：$$L_{LM}(\phi) = -\sum_ilog P_\phi(u_i|u_{i-k},…,u_{i-1})$$其中$\phi$是语言模型的参数，$k$是窗口大小。<br>接着用预训练好的语言模型的参数来初始化transformer模型的encoder和decoder，对于回复生成任务，最优化以下的目标函数：$$L_D(\phi) = -\sum_ilogP_{\phi}(u_i|u_{i-k},…,u_{i-1},E_C,E_T)$$<br>为了把<code>预训练阶段</code>和<code>微调阶段</code>联系起来，在微调阶段，也会在对话语料训练集上最小化语言模型的损失函数。也就是把语言模型的损失函数作为微调阶段的辅助损失函数。则微调阶段，模型总的损失函数为：$$L(\phi,\theta) = L_D(\phi) + \lambda_1L_{LM}(\phi) + \lambda_2L_W(\theta)$$</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://github.com/silverriver/PersonalDilaog" target="_blank" rel="noopener">https://github.com/silverriver/PersonalDilaog</a></li><li><a href="https://github.com/SpiderClub/weibospider" target="_blank" rel="noopener">https://github.com/SpiderClub/weibospider</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】AAAI2020&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/abs/1911.04700&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1911.04700&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Transformer" scheme="http://yoursite.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Assigning Personality（Profile） to a Chatting Machine for Coherent Conversation Generation》</title>
    <link href="http://yoursite.com/2020/01/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AAssigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/05/论文笔记《Assigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation》/</id>
    <published>2020-01-05T03:10:26.000Z</published>
    <updated>2020-01-05T04:51:42.806Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】ICJAI2018<br>【链接】<a href="https://arxiv.org/abs/1706.02861" target="_blank" rel="noopener">https://arxiv.org/abs/1706.02861</a><br>【数据集】<a href="http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality" target="_blank" rel="noopener">http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是清华大学<a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈教授组</a>的2017年的工作，2018年发表在IJCAI。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>论文的研究内容是赋予对话系统以个性化信息(personality/profile)来生成具有一致性的回复。具体来说，对话语料中用键值对属性值来描述用户画像。对话系统先使用一个profile detector来检测生成回复时是否使用个性化信息。如果要使用，从所有的键值对属性用户画像中选择一个键值对来生成回复。采用一个bidirectional decoder来生成回复，让键值对出现在生成的回复中。进一步地，为了提高bidirectional decoder的性能，采用了position detector来检测键值对在回复中的位置。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>论文提出的对话模型包括了三个重要模块。profile detector检测是否使用用户画像并选择一个键值对属性。bidirectional decoder根据选中的键值对属性来生成回复。position detector检测键值对属性值在回复中出现的位置。</p><h4 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h4><p>给定post $X = \lbrace{x_1,x_2,…,x_n}\rbrace$以及描述用户个性化信息的键值对属性$\lbrace{&lt;k_i,v_i&gt;|i=1,2,…,K}\rbrace$，目标是生成有一致性的回复$Y = \lbrace{y_1,y_2,…,y_m}\rbrace$。<br>生成过程可以定义为: $$P(Y|X,\lbrace&lt;k_i,v_i&gt;\rbrace) = P(z=0|X) \cdot P^{fr}(Y|X) + P(z=1|X) \cdot P^{bi}(Y|X,\lbrace&lt;k_i,v_i&gt;\rbrace)$$</p><div align="center"><img src="/images/bidirectional_decoder.png" width="80%" height="80%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>采用GRU将post $X = \lbrace{x_1,x_2,…,x_n}\rbrace$编码为$\lbrace{h_1,h_2,…,h_n}\rbrace$。GRU的更新公式如下：$$h_t = GRU(h_{t-1},x_t)$$</p><h4 id="profile-detector"><a href="#profile-detector" class="headerlink" title="profile detector"></a>profile detector</h4><p>作用有两个。一是检测是否使用profile，二是如果要使用，选择一个特定的键值对属性$&lt;key,value&gt;$来生成回复。<br>第一个作用，是否使用profile是在有标签数据上训练的二元分类器$P(z|X)$，计算方式如下：$$P(z|X) = P(z|\tilde{h}) = \sigma(W_p\tilde{h})$$ $$\tilde{h} = \sum_{j}h_j$$ 第二个作用，选择一个特定的键值对属性。生成一个在所有键值对上的概率分布：$$\beta_i=MLP([\tilde{h},k_i,v_i])=softmax(W\cdot [\tilde{h},k_i,v_i])$$其中$W$是可训练的模型参数，$\tilde{h}$是post的表示。则取概率最大的键值对作为选中的键值对：$$\tilde{v} = argmax_i(\beta_i)$$ 进一步地，bidirectional decoder的解码过程可以定义为：$$P^{bi}(Y|X,&lt;\lbrace{k_i,v_i}\rbrace&gt;) = P^{bi}(Y|X,\tilde{v})$$</p><h4 id="bidirectional-decoder"><a href="#bidirectional-decoder" class="headerlink" title="bidirectional decoder"></a>bidirectional decoder</h4><p>让选中的键值对属性值$\tilde{v}$出现在生成的回复中：$Y = (Y^b,\tilde{v},Y^f) = (y_1^b,…,y_{t-1}^b,\tilde{v},y_{t+1}^f,…,y_m^f)$。backward deocder生成$Y^b$，forward decoder生成$Y^f$。 解码过程可以定义为$$P^{bi}(Y|X,\tilde{v}) = P^{b}(Y^b|X,\tilde{v}) * P^f(Y^f|Y^b,X,\tilde{v})$$ $$P^{b}(Y^b|X,\tilde{v}) = \prod_{j=t-1}^{1} P^b(y^b_j|y^b_{&lt;j},X,\tilde{v})$$ $$P^{f}(Y^f|Y^b,X,\tilde{v}) = \prod_{j=t+1}^{m}P^f(y_j^f|y_{&lt;j}^f,Y^b,X,\tilde{v})$$ 具体地，生成一个word的概率分布为：$$P^b(y^b_j|y^b_{&lt;j},X,\tilde{v}) \propto MLP([s_j^b;y^b_{j+1};c_j^b])$$ $$P^f(y_j^f|y_{&lt;j}^f,Y^b,X,\tilde{v}) \propto MLP([s_j^f;y_{j-1}^f;c_j^f])$$ 注意区分backWard decoder与forward decoder的差别，分别是逆序和顺序的，一个输入的是$y^b_{j+1}$，而另一个是$y^f_{j-1}$。$s_j^{*},{*} \in \lbrace{f,b}\rbrace$ 是相应decoder的隐藏状态，$c_j^{*}$为相应的context vector。更新方式如下：$$s_j^{*} = GRU(s_{j+l}^{*}, [y_{j+l}^{*},c_j^{*}])$$ $$c_j^* = \sum_{t=1}^n\alpha_{j,t}^{*}h_t$$ $$\alpha_{j,t} \propto MLP([s_{j+l}^{*},h_t])$$ 对于backWard decoder，有$* = b,l = 1$，而对于forward decoder，有$* = f,l = -1$。</p><h4 id="position-detector"><a href="#position-detector" class="headerlink" title="position detector"></a>position detector</h4><p>为了检测属性值$v$在回复中出现的位置，在训练阶段，检测哪个词可以被profile中的属性值代替。也就是估计概率：$P(j|y_1y_2…y_m,&lt;k,v&gt;),j\in[1,m]$，这个概率分布的意义是词$y_j$可以被属性值$v$替换的可能性大小。这里采用最大化$y_j$与属性值$v$之间的余弦相似度：$$P(j|Y,&lt;k,v&gt;) \propto cos({y_j},{v})$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】ICJAI2018&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/abs/1706.02861&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1706.02861&lt;/a&gt;&lt;br&gt;【数据集】&lt;a href=&quot;http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Bidirectional Decoder" scheme="http://yoursite.com/tags/Bidirectional-Decoder/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Learning Personalized End-to-End Goal-Oriented Dialog》</title>
    <link href="http://yoursite.com/2020/01/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ALearning-Personalized-End-to-End-Goal-Oriented-Dialog%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/03/论文笔记《Learning-Personalized-End-to-End-Goal-Oriented-Dialog》/</id>
    <published>2020-01-03T01:57:32.000Z</published>
    <updated>2020-01-03T04:08:45.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】AAAI2019<br>【链接】<a href="https://arxiv.org/pdf/1811.04604v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.04604v1.pdf</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是北京大学于2019年初发表的。研究内容主要是将用户个性化信息(personalization)结合到端到端的任务型对话模型中，来调整回复的策略和语言风格，并消除歧义。</p><h3 id="个性化对话系统的相关工作"><a href="#个性化对话系统的相关工作" class="headerlink" title="个性化对话系统的相关工作"></a>个性化对话系统的相关工作</h3><p>最早探索个性化对话系统的研究工作是 <a href="https://nlp.stanford.edu/~bdlijiwei/index.html" target="_blank" rel="noopener">Li Jiwei</a>于16年发表的论文<a href="https://arxiv.org/pdf/1603.06155v2.pdf" target="_blank" rel="noopener">《A Persona-Based Neural Conversation Model》</a>。这个工作的具体来说是，给chatbot agent赋予特定的人格来生成一致性的回复。另一个思路，我们更希望chatbot agent可以感知到用户的身份和偏好，来提供个性化的对话。<br>个性化的对话系统也是分为闲聊式对话和任务型对话。</p><ul><li>个性化闲聊式对话的训练语料有<a href="https://arxiv.org/abs/1801.07243" target="_blank" rel="noopener">Persona-Chat</a>和在此基础上扩展得到的<a href="https://arxiv.org/abs/1902.00098" target="_blank" rel="noopener">CONVAI2</a>，另外还有<a href="https://arxiv.org/abs/1809.01984" target="_blank" rel="noopener">Mazare et al.(2018)</a>基于Reddit Corpus构建的个性化对话语料。</li><li>个性化的任务型对话系统的训练语料有<a href="https://arxiv.org/abs/1706.07503" target="_blank" rel="noopener">personalized bAbI dialog corpus</a>。</li></ul><h3 id="缺乏个性化的任务型对话系统的不足"><a href="#缺乏个性化的任务型对话系统的不足" class="headerlink" title="缺乏个性化的任务型对话系统的不足"></a>缺乏个性化的任务型对话系统的不足</h3><p>只基于对话历史而未考虑个性化的任务型对话系统存在以下不足：</p><ol><li>不能根据用户的身份和偏好来动态地调整语言风格。</li><li>缺乏根据用户的信息来动态调整对话策略的能力。</li><li>难以处理用户请求中的歧义项。比如：“contact”可以理解为“电话”，也可以理解为“社交媒体”。个性化模型可以学习到年轻人倾向于社交媒体，而老年人倾向于电话这一事实，从而消除歧义。</li></ol><p>个性化任务型对话系统的研究动机就是解决上述问题。个性化的任务型对话与chatbot有所区别。个性化的chatbot研究倾向于赋予chatbot以特定的用户画像，来生成一致性的回复。而个性化的任务型对话更多的是感知到用户的身份和偏好，从而根据不同的用户身份来调整回复的语言风格和对话策略，从而提高对话效率和用户满意度。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>本文的个性化任务型对话模型是基于Memory Network的，因此对Memory Network做简单的介绍。Memory Network的相关工作有许多，本文中介绍的是发表在ICLR2017的<a href="https://arxiv.org/abs/1605.07683" target="_blank" rel="noopener">《Learning end-to-end goal-oriented dialog》</a>，这是基于检索的任务型对话。Memory Network包括了两个部分：context memory和next sentence prediction。</p><h4 id="Memory-Representation"><a href="#Memory-Representation" class="headerlink" title="Memory Representation"></a>Memory Representation</h4><p>memory中储存的是对话历史。在时间步t，对话历史为$t$句用户的对话$\lbrace{c_1^u,c_2^u,…,c_t^u}\rbrace$以及$t-1$句对话系统的回复$\lbrace{u_1^r,u_2^r,…,u_{t-1}^r}\rbrace$。直接采用了词袋方法，经过embedding层，将对话历史表示为向量。 $$m = (A\Phi(c_1^u),A\Phi(c_1^r),…,A\Phi(c_{t-1}^u),A\Phi(c_{t-1}^r))$$ 其中$\Phi(\cdot)$是one-hot向量表示，$A$是embedding table。<br>用同样的方法，将上一句话$c_t^u$编码为attention机制的的query：$$q = A\Phi(c_t^u)$$ </p><h4 id="Memory-Operation"><a href="#Memory-Operation" class="headerlink" title="Memory Operation"></a>Memory Operation</h4><p>采用attention机制对context memory进行读取。计算方式如下：$$o = R\sum_i\alpha_im_i$$ $$\alpha_i = softmax(q^\top m_i)$$ 其中$R$是线性层的权重矩阵。<br><strong>多跳机制</strong><br>将query按下式进行更新，再采用attention机制对context memory进行读取。$$q_2 = q + o$$</p><h4 id="next-sentence-prediction"><a href="#next-sentence-prediction" class="headerlink" title="next sentence prediction"></a>next sentence prediction</h4><p>设有C个候选句子$y_i$。先将候选句子进行向量化表示: $$r_i = W\Phi(y_i)$$ 其中$W$是另一个embedding table。<br>则最终的预测概率分布为：$$\hat{r} = softmax({q_{N+1}}^\top r_1,…,{q_{N+1}}^\top r_C)$$</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型包括profile model和preference model。profile model将用户的个性化信息结合到模型中。preference model建模用户信息与知识库之间的联系，来消除歧义。</p><div align="center"><img src="/images/personalized_memnet.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="profile-model"><a href="#profile-model" class="headerlink" title="profile model"></a>profile model</h4><p><strong>user embedding的计算</strong><br>对话语料中用$n$个键值对属性$\lbrace(k_i,v_i)\rbrace_{i=1}^n$来描述用户画像。第$i$个属性被表示为one-hot vector $a_i \in R^{d_i}$，其中$d_i$表示第$i$个属性$k_i$可能的取值个数。则总的用户画像的one-hot表示为$$\hat{a} = concat(a_1,a_2,…,a_n)$$ 有$\hat{a}\in R^{d_p}$，其中$d_p = \sum_{i=1}^nd_i$<br>进一步将用户画像表示为分布式向量$$p = P\hat{a}$$ 其中$P$可以看作是embedding table。</p><p><strong>将$p$结合到对话模型</strong><br>将user embedding $p$结合到模型中的两个地方。</p><ol><li>结合到多跳机制的query更新公式中。query对于context memory的读取和预测概率的生成起着重要作用。 $$q_{i+1} = q_i + o_i + p$$</li><li>根据user embedding对候选句子的向量表示进行修改。 $$r_i^* = \sigma(p^\top r_i) r_i$$</li></ol><p><strong>global memory</strong><br>除了context memory外，模型还有一个memory用来储存相似用户的对话历史。本文中将相似用户定义为有相同的属性值的用户。global memory的读取方式与context memory的读取方式相同。最后将两部分的query结合起来：$$q^+ = q + q^g$$</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.luolc.com/" target="_blank" rel="noopener">本文作者的个人主页</a></li><li><a href="https://www.luolc.com/publications/personalized-goal-oriented-dialog/" target="_blank" rel="noopener">作者的英文论文简介</a></li><li><a href="https://mp.weixin.qq.com/s/AqzdRoXthrUFUOqSNwgfqQ" target="_blank" rel="noopener">作者的中文论文简介</a></li><li><a href="https://helicqin.github.io/2018/12/11/Learning%20Personalized%20End-to-End%20Goal-Oriented%20Dialog/" target="_blank" rel="noopener">本篇论文的阅读笔记</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】AAAI2019&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/pdf/1811.04604v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1811.04604v1.pdf&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Goal-Oriented Dialog" scheme="http://yoursite.com/tags/Goal-Oriented-Dialog/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《A Dynamic Speaker Model for Conversational Interactions》</title>
    <link href="http://yoursite.com/2020/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AA-Dynamic-Speaker-Model-for-Conversational-Interactions%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/02/论文笔记《A-Dynamic-Speaker-Model-for-Conversational-Interactions》/</id>
    <published>2020-01-02T12:31:22.000Z</published>
    <updated>2020-01-02T14:08:56.015Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://www.aclweb.org/anthology/N19-1284/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/N19-1284/</a><br>【代码】：<a href="https://github.com/hao-cheng/dynamic_speaker_model" target="_blank" rel="noopener">https://github.com/hao-cheng/dynamic_speaker_model</a></p></blockquote><a id="more"></a><p>这篇论文是由华盛顿大学发表的。</p><h3 id="个性化对话系统的研究现状"><a href="#个性化对话系统的研究现状" class="headerlink" title="个性化对话系统的研究现状"></a>个性化对话系统的研究现状</h3><p>近几年来，基于用户画像（personal information）来生成个性化的回复已经成为对话系统领域的一个研究热点。为什么要将persona结合到对话系统模型中呢？目的是提高生成回复的一致性，来获取用户信任，让用户在对话中更投入。生成回复的一致性具体指什么呢？举个例子，你问对话系统“你的职业是什么？”，它可能回答是出租车司机；当你再次问这个问题时，它又可能回答是老师。给定对话系统一个用户画像（persona），基于这个persona来生成回复，就可以提高生成回复的一致性，避免出现这种问题。基于persona来生成个性化的回复，也是对话系统可以通过图灵测试的必要条件。</p><p>现有的工作中，将persona结合到对话系统模型中的思路有两种。</p><ol><li>学习一个潜在的向量user embedding（或user representation）来潜在地表示用户画像，再基于这个user embedding来生成个性化的回复。这样做的一个原因是现有的对话语料中没有相应的用户画像文本信息，随着个性化对话数据集<a href="https://arxiv.org/abs/1801.07243" target="_blank" rel="noopener">Persona-Chat</a>以及<a href="https://arxiv.org/abs/1902.00098" target="_blank" rel="noopener">CONVAI2</a>的提出，就有了第二种思路。</li><li>直接用键值对属性值信息或者是自由文本来明确地描述用户画像，再生成个性化的回复。</li></ol><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>这篇论文属于第一种思路，主要内容是用神经网络模型从对话历史中学习一个动态更新的user embedding。并且将学习到的user embedding用到了两个下游任务（对话话题分类、dialog acts分类）中来评估user embedding的学习效果。<br>论文中提到，学习一个动态更新的user embedding的动机有两个：</p><ol><li>用户的对话反映了这个用户的对话意图、语言风格等特征。因此，可以从用户的对话中来学习user embedding来建模和表征用户的这些个性化特征。</li><li>随着对话的进行，用户的个人信息得到累积。因此，可以从对话中提炼和动态更新user embedding。</li></ol><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>如下图所示，模型包括三个部分：<strong>Latent Mode Analyzer</strong>，<strong>Speaker State Tracker</strong>，<strong>Speaker Language Predictor</strong>。 </p><div align="center"><img src="/images/Dynamic_user_embedding.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font></div><h4 id="Latent-Mode-Analyzer"><a href="#Latent-Mode-Analyzer" class="headerlink" title="Latent Mode Analyzer"></a>Latent Mode Analyzer</h4><p><strong>Latent Mode Analyzer</strong>的作用是从输入的单轮对话中学到一个 local speaker vector。采用了Bi-LSTM + Attention机制。<br>时间步t，输入为单轮对话$\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}\rbrace}$，经过embedding层后送入到Bi-LSTM层，将前向LSTM和后向LSTM的最后一个隐藏状态连接起来作为句子总的向量表示$s_t$: $$s_t=[e^F_{t,N_t},e^B_{t,1}]$$ 其中，$e^F_{t,N_t},e^B_{t,1}$分别表示前向LSTM和后向LSTM的最后一个隐藏状态。<br>接着，再使用attention机制。将句子的向量表示作为attention机制的query，将 $K$ 个全局的mode vectors $\lbrace{u_1,u_2,…,u_K\rbrace}$作为attention机制的keys和values。这$K$个mode vectors也是模型参数，可以看作是用户在不同方面的个性化特征。那么可以通过attention机制计算得到local speaker vector $\tilde{u_t}$ :$$\tilde{u_t} = \sum_{k=1}^Ka_{t,k}u_k$$ $$a_{t,k} = softmax(\beta_{t,k})$$ $$\beta_{t,k} = &lt;Ps_t,Qu_k&gt;$$ 其中$&lt; , &gt;$表示点乘操作。</p><h4 id="Speaker-State-Tracker"><a href="#Speaker-State-Tracker" class="headerlink" title="Speaker State Tracker"></a>Speaker State Tracker</h4><p><strong>Speaker State Tracker</strong> 的作用是动态更新speaker state vector。实质上是一个单向的LSTM。<br>在时间步t，根据当前的local speaker vector $\tilde{u_t}$ 和 上一个时间步的隐藏状态$h_{t-1}$来更新隐藏状态$h_t$。将$h_t$作为时间步t的speaker state vector。$$h_t = LSTM(\tilde{u_t},h_{t-1})$$</p><h4 id="Speaker-Language-Predictor"><a href="#Speaker-Language-Predictor" class="headerlink" title="Speaker Language Predictor"></a>Speaker Language Predictor</h4><p><strong>Speaker Language Predictor</strong>的作用是促进前两个模块的参数学习，根据speaker state vector来重构句子$\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}}\rbrace$。<br>该模块实质上是一个条件语言模型，预测条件概率 $P(w_{t,n}|w_{t,&lt;n})$。采用了单向的LSTM，LSTM的隐藏状态更新公式为：$$d_{t,n} = LSTM(R^I(w_{t,n-1},h_t),d_{t,n-1})$$ 其中$R^I()$是一个线性层。<br>则语言模型生成下一个词的条件概率为：$$P(w_{t,n}|w_{t,&lt;n}) = softmax(VR^O(h_t,d_{t,n}))$$ 其中$R^O()$是一个线性层。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>模型最小化负对数似然函数来更新模型参数: $$L = -\sum_{t}\sum_{n}log P(w_{t,n}|w_{t,&lt;n})$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://www.aclweb.org/anthology/N19-1284/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aclweb.org/anthology/N19-1284/&lt;/a&gt;&lt;br&gt;【代码】：&lt;a href=&quot;https://github.com/hao-cheng/dynamic_speaker_model&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hao-cheng/dynamic_speaker_model&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Towards Knowledge-Based Personalized Product Description Generation in E-commerce》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ATowards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Towards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce》/</id>
    <published>2019-10-25T10:52:24.000Z</published>
    <updated>2019-10-25T14:49:12.478Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ARXIV 2019<br>【链接】：<a href="https://arxiv.org/abs/1903.12457v3" target="_blank" rel="noopener">https://arxiv.org/abs/1903.12457v3</a><br>【代码、数据集】：<a href="https://github.com/THUDM/KOBE" target="_blank" rel="noopener">https://github.com/THUDM/KOBE</a></p></blockquote><a id="more"></a><p>清华大学和阿里巴巴发表的论文。论文的研究内容是给定商品名称，商品的属性特征和外部知识库，自动生成商品的描述。</p><p><strong>数据集描述</strong><br>论文在淘宝收集了一个真实的商品描述数据集，包含了212,9187个商品名称和描述。数据集是公开的，下载地址为：<a href="https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717" target="_blank" rel="noopener">https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717</a></p><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>给定商品名称$x = \{x_1,…,x_n\}$，要求生成个性化的、富有信息的商品描述$y = \{y_1,…,y_m\}$。引入两个附加信息：商品属性 和 外部知识：</p><ol><li>Attributes<br> 每个商品名称$x$对应$l$个属性$a = \{a_1,…,a_l\}$。论文中包含两种属性，商品的某个方面（如质量、外观等）和用户类型（反映用户的兴趣）。</li><li>Knowledge<br> 论文采用一个大规模的中文知识图谱<code>CN-DBpedia</code>作为外部knowledge。<code>CN-DBpedia</code>包含大量命名实体$V$索引的原始文本条目$W$。每个条目包含一个命名实体$v &ensp; \in V$作为key，对应一个knowledge句子 $w = \{w_1,…,w_u\} \in W$作为value。</li></ol><p>最终的任务定义为：给定商品名称$x$、商品属性$a$和相关的knowledge $w$，要求生成个性化的，信息量丰富的回复$y$。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型采用了基于Transformer的encoder-decoder框架，结合了两个模块：Attribute Fusion和knowledge Incorporation。</p><div align="center"><img src="/images/KOBE.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="encoder-decoder框架"><a href="#encoder-decoder框架" class="headerlink" title="encoder-decoder框架"></a>encoder-decoder框架</h4><p>Transformer是完全基于self-attention机制 和 前馈神经网络（FFN,feed-forward neural network）的。</p><ol><li><strong>encoder</strong><br> 对于输入$x = \{x_1,…,x_n\}$，先经过embedding层得到word embedding representation $e = \{e_1,…,e_n\}$，连同position embedding一起，作为encoder layers的输入，得到context representation $h = \{h_1,…,h_n\}$。<br> 在embedding层之上，encoder layers由完全相同的6层堆叠组成，每层transformer包括<code>multi-head self-attention</code>和<code>FFN</code>两部分。<ol><li><strong>attention机制</strong><br> attention机制根据queries $Q$和keys $K$计算在values $V$上的分布，进而得到attention的输出。$$C = \alpha V$$ $$\alpha = softmax(f(Q,K))$$其中$C$表示attention的输出，$\alpha$表示attention的分布，$f$表示计算attention分数的函数。</li><li><strong>uni-head attention</strong><br> 对于单头attention，queries $Q$，keys $K$和values $V$分别是输入context $e$的线性转换。即$Q = W_Qe$, $K = W_Ke$ 和 $V = W_Ve$。此时，uni-head attention可以表示为 $$C_{self} = softmax(\frac{QK^T}{\sqrt{d_k}})V$$ 其中$d_k$表示输入$e$的维度。</li><li><strong>multi-head attention</strong><br> 对于多头attention，将$C^i_{self}, i\in \{1,2,…,c\}$连接起来，作为FFN的输入。其中$c$表示heads的数量。<br>再经过前馈神经网络（FFN），FFN的函数表示为：$$FFN(z) = W_2(Relu(W_1z + b_1)) + b_2$$</li></ol></li><li><strong>decoder</strong><br> 与encoder类似，decoder也是由完全相同的6层堆叠而成的，每层包含<code>multi-head attention</code>和<code>FFN</code>两部分。不同于encoder的”self-attenion”，decoder的<code>multi-head attention</code>是“context attention”。queries $Q$是decoder state的线性转换，keys $K$和values $V$是context states $h = \{h_1,…,h_n\}$的线性转换。</li><li><strong>Training</strong><br> 模型的目标是最大化似然函数。模型的目标函数是：$$P(y|x) = \prod_{t=1}^m P(y_t|y_{&lt;t},x)\tag{1}$$</li></ol><h4 id="Attribute-Fusion模块"><a href="#Attribute-Fusion模块" class="headerlink" title="Attribute Fusion模块"></a>Attribute Fusion模块</h4><p>商品的属性$a = \{a_1,a_2\}$，包含商品方面 和 用户类型两个属性。先经过embedding层得到attribute representation $\{e_{a_1},e_{a_2}\}$，再做attribute average得到总的attribute representation $e_{attr}$： $$e_{attr} = \frac{1}{2}\sum_{i=1}^2e_{a_i}$$ 如何有效结合$e_{attr}$呢？在基于RNN的模型中，方法比较多，比如：用$e_{attr}$来attend context representation，或着作为decoder隐藏状态更新的输入等。但本文中的模型是基于Transformer的，直接将attribute embedding $e_{attr}$与word embedding $e_i$相加，来结合商品的属性信息。如Fig.2所示。</p><div align="center"><img src="/images/attributeFusion.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.2. Attribute Fusion的示意图</font></div><p>此时，公式（1）中的目标函数变为$$ P(y|x,a) = \prod_{t=1}^m P(y_t|y_{&lt;t},x,a)$$</p><h4 id="Knowledge-Incorporation"><a href="#Knowledge-Incorporation" class="headerlink" title="Knowledge Incorporation"></a>Knowledge Incorporation</h4><p>knowledge incorporation包括三个部分，knowledge检索，knowledge编码，和knowledge结合。</p><ol><li><strong>knowledge retrieval</strong><br> 给定商品名称$x = \{x_1,…,x_n\}$，对于每个word $x_i$匹配对应的命名实体$v_i \in V$。再根据命名实体$v_i$，从$W$中检索对应的knowledge $w_i$。对于每个商品，最多抽取5个匹配的knowledge，再用分隔符 “<sep>”连接起来。</sep></li><li><strong>knowledge encoding</strong><br> 类似于$x = \{x_1,…,x_n\}$通过encoder编码得到context representation $h = \{h_1,….,h_n\}$，将检索到的knowledge经过一个基于Transformer的knowledge encoder得到knowledge representation $u$。</li><li><strong>knowledge combination</strong><br> 用BiDAF(bidirectional attention flow)来结合context representation $h$和knowledge representation $u$。BiDAF计算两个方向的attention：title-to-knowledge attention和knowledge-to-context attention。<ol><li><strong>相似度矩阵S</strong><br> 先计算一个context representation $h \in R^{n \times d}$和knowledge representation $u \in R^{u \times d}$之间的相似度矩阵$S \in R^{n\times u}$。其中$S_{ij}$衡量第i个title word和第j个knowledge word之间的相似度。$$S_{ij} = \alpha(h_i,u_j) \in R$$ 其中$\alpha()$是计算两个向量之间相似度的函数。 $$\alpha(h,u) = W_s^T[h;u;h \cdot u], &emsp; W_s\in R^{3d}$$</li><li><strong>title-to-knowledge attention</strong><br> 表明了对于每个title word，哪个knowledge word是最相关的。<br> $a_i \in R^u$表示对于第i个title word，在所有knowledge words上的attention权重分布。$$a_i = softmax(S_{i:}) &emsp; \in R^u$$ 其中，$S_{i:}$表示相似度矩阵$S \in R^{n\times u}$的第i个行向量。对于所有的$i$，$a_i$满足：$$\sum_ja_{ij} = 1$$ 对于第i个title word，attended knowledge vector为：$$\widetilde{u_i} = \sum_ja_{ij}u_j$$ 则对于所有的title words $\{x_1,…,x_n\}$，有$\widetilde{u} \in R^{n \times d}$</li><li><strong>knowledge-to-title attention</strong><br> 表明了对于每个knowledge word，哪个title word是最相似的。<br> 计算在所有title words上的attention权重分布：$$b = softmax(max(S_{i:}))$$ 则attended title vector为$$\widetilde{h} = \sum_{k}b_kh_k$$ 把$\widetilde{h}$在列上复制n次，得到$\widetilde{h} \in R^{n\times d}$。</li><li><strong>输出融合</strong><br> 做一个简单的连接操作，得到组合representation： $[h;\widetilde{u};h\circ \widetilde{u};h\circ \widetilde{h}] \in R^{4d \times n}$</li></ol></li></ol><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li>BiDAF(bidirectional attention flow)可以参考:<ul><li><a href="https://spring-quan.github.io/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/" target="_blank" rel="noopener">论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》</a></li></ul></li><li>中文知识图谱<code>CN-DBpedia</code>可以参考：<ul><li><a href="https://www.semanticscholar.org/paper/CN-DBpedia%3A-A-Never-Ending-Chinese-Knowledge-System-Xu-Xu/2c69bbb3b7ba3f324276924bab6f41de467c928a" target="_blank" rel="noopener">《CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System》</a></li><li><a href="http://kw.fudan.edu.cn/cndbpedia/download/" target="_blank" rel="noopener">CN-DBpedia Dump数据下载</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ARXIV 2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1903.12457v3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1903.12457v3&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/THUDM/KOBE&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/THUDM/KOBE&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="BiDAF" scheme="http://yoursite.com/tags/BiDAF/"/>
    
      <category term="Knowledge-Based" scheme="http://yoursite.com/tags/Knowledge-Based/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Automatic Generation of Personalized Comment Based on User Profile》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AAutomatic-Generation-of-Personalized-Comment-Based-on-User-Profile%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Automatic-Generation-of-Personalized-Comment-Based-on-User-Profile》/</id>
    <published>2019-10-25T07:15:24.000Z</published>
    <updated>2019-10-25T09:09:29.504Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ACL2019<br>【链接】：<a href="https://arxiv.org/abs/1907.10371" target="_blank" rel="noopener">https://arxiv.org/abs/1907.10371</a><br>【代码、数据集】：<a href="https://github.com/Walleclipse/AGPC" target="_blank" rel="noopener">https://github.com/Walleclipse/AGPC</a></p></blockquote><a id="more"></a><p>北京大学发表在ACL2019的论文。论文的研究内容是基于User profile的评论生成。</p><h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><p>论文从微博收集了一个中文数据集，没有公开，但给出了部分样例数据。这个数据集可以看作基于persona的单轮对话数据集。将微博的博文看作对话历史，将用户的评论看作回复。用户画像包括两个部分，键值对形式的人口统计特征属性（年龄、性别、地区等）和 句子形式的个人描述（微博签名）。</p><div align="center"><img src="/images/PCdata.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 数据样例</font></div><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>给定博文 $X = \lbrace{x_1,…,x_n}\rbrace$和用户画像 $U = \lbrace{F,D}\rbrace$，其中$F = \lbrace{f_1,…,f_k}\rbrace$是用户的数值化属性特征，$D = \lbrace{d_1,…,d_l}\rbrace$是句子形式的个人描述。要求生成与personal profile一致的回复$Y = \lbrace{y_1,…,y_m}\rbrace$。$$Y^* = \underset{Y}{argmax}(Y|X,U)$$</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><div align="center"><img src="/images/PersonalComment.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.2.模型结构的示意图</font></div><h4 id="encoder-decoder框架"><a href="#encoder-decoder框架" class="headerlink" title="encoder-decoder框架"></a>encoder-decoder框架</h4><p>基本框架当然还是<code>seq2seq模型 + attention机制</code>。$X= \{x_1,…,x_n\}$经过encoder转换为向量表示$h^X = \{h^X_1,…,h_n^X\}$，encoder采用Bi-LSTM。$$h_t^X = LSTM_{enc}^X(h_{t-1}^X,x_t)$$ decoder采用单向LSTM，decoder的隐藏状态更新公式为：$$s_t = LSTM_{dec}(s_{t-1},[c_t^X;e(y_{t-1})]) \tag{1}$$其中$c_t^X$表示时间步t，在所有encoder hidden states $h^X = \{h^X_1,…,h_n^X\}$上使用attention得到的context vector。<br>则decoder生成词的概率分布为：$$p(y_t) = softmax(W_os_t)$$ 采用负对数似然函数作为目标函数，模型最大化真实回复 $Y^* = \{y_1,…,y_m\}$ 的似然函数：$$Loss = -\sum_{t=1}^m log\Bigl(p(y_t|y_{&lt;t},X,U)\Bigr)$$</p><h4 id="User-Feature-Embedding-with-Gated-Memory"><a href="#User-Feature-Embedding-with-Gated-Memory" class="headerlink" title="User Feature Embedding with Gated Memory"></a>User Feature Embedding with Gated Memory</h4><p>将用户的数值化特征属性$F = \{f_1,…,f_k\}$经过一个全连接层，得到向量表示$v_u$。$v_u$可以看作是<code>user feature embedding</code>，表明用户的个人特征。如果user feature embedding是静态的，在decode过程中会影响生成回复的语法性。为了解决这个问题，设计了一个<code>gated memory</code>来动态地表达用户的个人特征。<br>在decode过程中，保持一个<code>Internal personal state</code>$M_t$，在decode过程中$M_t$逐渐衰减，decode结束，$M_t$衰减为0，表示用户的个人特征完全表达了。$M_0$的初始值设为$v_u$。 $$g_t^u = sigmoid(W_g^us_t)$$ $$M_0 = v_u$$ $$M_t = g_t^u \cdot M_{t-1}, &emsp; t&gt;0$$ 引入输出门机制$g_t^o$来充值persona信息的流动：$$g_t^o = sigmoid(W_g^o[s_{t-1};e(y_{t-1});c_t^X])$$ 则时间步t，personal information为：$$M_t^o = g_t^o\cdot M_t$$</p><h4 id="Blog-User-Co-Attention"><a href="#Blog-User-Co-Attention" class="headerlink" title="Blog-User Co-Attention"></a>Blog-User Co-Attention</h4><p>实质上是在用户的个人描述$D = \{d_1,…,d_l\}$上使用attention机制。先用另一个persona encoder来编码$D = \{d_1,…,d_l\}$，得到向量表示$\{h_1^D,…,h_l^D\}$。persona encoder采用LSTM: $$h_t^D = LSTM_{enc}^D(h_{t-1}^D,d_t)$$ 在$\{h_1^D,…,h_l^D\}$上使用attention机制，得到总的persona context vector $c_t^D$ $$c_t^D = \sum_{j=1}^k\alpha_{tj}h_j^D$$ $$\alpha_{tj} = softmax(\beta_{tj})$$ $$\beta_{tj} = score(s_{t-1},h_j^D) = s_{t-1}W_ah_j^D$$ 结合$c_t^D$和$c_t^X$作为时间步t总的context vector $c_t$: $$c_t = [c_t^X,c_t^D]$$ 则式（1）中decoder的隐藏状态更新公式变为：$$s_t = LSTM_{dec}(s_{t-1},[c_t;e(y_{t-1});M_t^o])$$</p><h4 id="External-Personal-Expression"><a href="#External-Personal-Expression" class="headerlink" title="External Personal Expression"></a>External Personal Expression</h4><p>通过将<code>internal persona state</code>$M_t$和persona context vector $c_t^D$作为decoder隐藏状态更新的输入，来结合persona信息，进而影响decode过程。这种影响是隐性的，为了更明确地利用用户信息来指导word的生成，将用户信息直接作为输出层的输入。先计算一个<code>user representation</code> $r_t^u$: $$r_t^u = W_r[v_u;c_t^D]$$ 将$r_t^u$作为输出层的输入，则生成词的概率分布为：$$p(y_t) = softmax(W_o[s_t;r_t^u])$$</p><h3 id="相似论文"><a href="#相似论文" class="headerlink" title="相似论文"></a>相似论文</h3><ul><li><a href="https://arxiv.org/abs/1901.09672" target="_blank" rel="noopener">《Personalized Dialogue Generation with Diversified Traits》</a> <ul><li><a href="https://spring-quan.github.io/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8APersonalized-Dialogue-Generation-with-Diversified-Traits%E3%80%8B/" target="_blank" rel="noopener">笔记链接</a></li><li>异同点比较：<ul><li>不同点是：本篇论文用internal persona state $M_t$和personal context vector $c_t^D$来作为decoder隐藏状态$s_t$更新的输入，进而影响word的生成。<br>而相似的这篇论文中，将personal vector $v_p$ 作为attention机制的query，来attend对话历史。含义是用persona vector $v_p$来选择context相关的信息。</li><li>相同点是：两篇论文都把persona information 作为输出层的输入，来明确地影响word的生成。</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1907.10371&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1907.10371&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/Walleclipse/AGPC&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Walleclipse/AGPC&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Personalized Dialogue Generation with Diversified Traits》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8APersonalized-Dialogue-Generation-with-Diversified-Traits%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Personalized-Dialogue-Generation-with-Diversified-Traits》/</id>
    <published>2019-10-25T02:51:12.000Z</published>
    <updated>2019-10-25T06:33:49.351Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【链接】：<a href="https://arxiv.org/abs/1901.09672" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09672</a><br>【代码、数据集】：无</p></blockquote><a id="more"></a><p>三星电子中国、清华大学黄明烈教授提交到<code>ARXIV</code>的论文。论文的研究内容是基于persona的单轮对话系统。<br><strong>数据集描述</strong><br>论文中构建了一个<code>PersonaDialog</code>的数据集，但数据集没有公开。数据是从微博上爬取，把用户的博文作为对话的post，把用户的评论作为回复。persona是用键值对来描述用户的属性（年龄、性别、地址、兴趣标签等）。而不是用几句话的文本来描述用户画像的。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>给定单句的对话历史$X = \lbrace{x_1,…,x_n\rbrace}$，以及回复者的N个属性$T = \lbrace{t_1,…,t_N\rbrace}$，其中$t_i = &ensp; &lt;k_i,v_i&gt;$为键值对。要求生成与用户的画像相一致的回复$Y = \lbrace{y_1,y_2,…,y_m\rbrace}$。$$Y^* = \underset{Y}{arg max} P(Y|X,T)$$</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>基本框架当然是<code>seq2seq模型+attention机制</code>。将对话历史$X = \lbrace{x_1,…,x_n}\rbrace$经过encoder编码为$\lbrace{h_1,…,h_n}\rbrace$。设时间步t，decoder的上一个隐藏状态为$s_{t-1}$，计算attention得到总的context vector $c_t$：$$c_t = \sum_{i=1}^n\alpha_ih_i$$ $$alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i) = V^T\cdot tanh(W^1_{\alpha}s_{t-1} + W^2_{\alpha}h_i) \tag{1}$$decoder RNN的隐藏状态更新公式是：$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 生成$y_t$的概率分布为：$$p(y_t) = softmax(W_os_t + b_{out})$$</p><div align="center"><img src="/images/PersonaTraitFusion.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h3 id="Personality-Trait-Fusion"><a href="#Personality-Trait-Fusion" class="headerlink" title="Personality Trait Fusion"></a>Personality Trait Fusion</h3><p>这个模块用来把整合persona得到persona representation $v_p$，并用$v_p$来影响decode过程。<br>将回复者的属性描述$T = \lbrace{t_1,…,t_N}\rbrace$经过embedding层，得到对应的向量表示$\lbrace{v_{t_1},…,v_{t_N}}\rbrace$。论文提出了三种整合persona Trait的方法。</p><h4 id="Trait-Attention"><a href="#Trait-Attention" class="headerlink" title="Trait Attention"></a>Trait Attention</h4><p>计算decoder的隐藏状态$s_{t-1}$在Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$上的attention，来整合persona Trait，得到persona representation $v_p$。$$v_p = \sum_{i=1}^N\alpha_i^pv_{t_i}$$ $$\alpha_i^p = softmax(\beta_i^p)$$ $$\beta_i^p = score(s_{t-1},v_{t_i}) = V_p^T \cdot tanh(W_p^1s_{t-1} + W_p^2v_{t_i})$$</p><h4 id="Trait-Average"><a href="#Trait-Average" class="headerlink" title="Trait Average"></a>Trait Average</h4><p>直接在Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$上取平均值，来作为persona representation $v_p$：$$v_p = \frac{1}{N}\sum_{i=1}^Nv_{t_i}$$</p><h4 id="Trait-Concatenation"><a href="#Trait-Concatenation" class="headerlink" title="Trait  Concatenation"></a>Trait  Concatenation</h4><p>直接把Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$做连接操作，作为persona representation $v_p$。</p><h3 id="使用persona-representation-v-p-进行decode"><a href="#使用persona-representation-v-p-进行decode" class="headerlink" title="使用persona representation $v_p$进行decode"></a>使用persona representation $v_p$进行decode</h3><h4 id="Persona-Aware-Attention"><a href="#Persona-Aware-Attention" class="headerlink" title="Persona-Aware Attention"></a>Persona-Aware Attention</h4><p>让persona representation $v_p$来影响式（1）中attention权重的计算。这种方法可以获得基于persona representation $v_p$的context vector $c_t$。$$\beta_i = f(s_{t-1},h_i,v_p) = V^T \cdot tanh(W^1_{\alpha}s_{t-1} + W^2_{\alpha}h_i + W^3_{\alpha}v_p)$$</p><h4 id="Persona-Aware-Bias"><a href="#Persona-Aware-Bias" class="headerlink" title="Persona-Aware Bias"></a>Persona-Aware Bias</h4><p>在输出层结合$v_p$，来影响decode过程。$$p(y_t) = softmax(a_t\cdot W_o^1s_t + (1-s_t)\cdot W_o^2v_p + b_{out})$$ $$a_t = \sigma(V_o^T\cdot s_t)$$ 其中$a_t\in [0,1]$作为一个门机制，来控制生成persona相关的词，或者语义相关的词。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1901.09672&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1901.09672&lt;/a&gt;&lt;br&gt;【代码、数据集】：无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》</title>
    <link href="http://yoursite.com/2019/10/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ADEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/24/论文笔记《DEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks》/</id>
    <published>2019-10-24T08:19:48.000Z</published>
    <updated>2019-10-25T03:08:22.933Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【链接】：<a href="https://arxiv.org/abs/1908.10731" target="_blank" rel="noopener">https://arxiv.org/abs/1908.10731</a><br>【代码、数据集】：无</p></blockquote><a id="more"></a><p>论文的研究内容是conditional text generation，基于knowledge facts的单轮对话。基于给定的对话历史和外部知识，生成合适的回复。论文中将K句话的个人描述作为knowledge facts。<br>论文采用的模型是seq2seq模型 + attention机制 + 分级pointer network。<br>论文的亮点是对比模型的思路，可以重点学习一下如何设计对比模型。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>输入分为两部分：对话历史$X = (x_1,…,x_n)$和K个相关的knowledge facts，其中第i个knowledge fact为$f^i = (f^i_1,…,f^i_{n_i}),i\in \lbrack 1,K \rbrack$。要求生成输出$Y = (y_1,…,y_m)$。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="baseline-seq2seq模型-attention机制"><a href="#baseline-seq2seq模型-attention机制" class="headerlink" title="baseline: seq2seq模型 + attention机制"></a>baseline: seq2seq模型 + attention机制</h3><p>seq2seq模型是基于encoder-decoder框架的。encoder包括embedding层和LSTM层，对于输入$X$，经过encoder得到相应的向量表示 $\lbrace{h_1,…,h_n}\rbrace$。decoder采用单向LSTM，设时间步t的隐藏状态为$s_t$，隐藏状态更新公式为：$$s_t = f(s_{t-1},y_t,c_t)$$ 其中$s_{t-1}$是上一个时间步decoder的隐藏状态，$y_{t-1}$是上一个时间步的输出。c_t为用attention机制计算得到的context vector。计算过程如下：$$c_t = \sum_{i=1}^{n}\alpha_ih_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i)$$ 其中$score(s,h)$是计算$s$和$h$之间相似度的函数。<br>在时间步t，decoder的隐藏状态$s_t$和对应的context vector $c_t$，经过线性层和softmax层，得到在固定词汇表上的概率分布。$$p_g(y_t) = softmax(W[h_t,c_t] + b)$$</p><p>可以看出“seq2seq模型 + attention机制”可以用来完成”text-to-text”的text generation的任务。输入是text，没有其他的附加信息（比如knowledge，persona，context等），输出也是text。根据输入的不同，可以得到以下三个模型。</p><ol><li><strong>SEQ2SEQ + NOFACT</strong><br> 只把对话历史$X$作为encoder的输入。</li><li><strong>SEQ2SEQ + BESTFACTCONTEXT</strong><ul><li>先从K个knowledge facts $\lbrace{f^1,…,f^K}\rbrace$中选择与dialog context $X$最相似的fact $f^c$</li><li>再将$f^c$与dialog context $X$连接起来的$[X;f^c]$，作为encoder的输入。</li></ul></li><li><strong>SEQ2SEQ + BESTFACTCONTEXT</strong><ul><li>先从K个knowledge facts $\lbrace{f^1,…,f^K}\rbrace$中选择与truth response $Y$最相似的fact $f^r$</li><li>再将$f^r$与dialog context $X$连接起来的$[X;f^r]$，作为encoder的输入。</li></ul></li></ol><p>从这三个对比试验，可以表明是否添加knowledge fact，以及knowledge fact的不同的选择，对回复生成的影响。</p><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>“seq2seq模型 + attention机制”容易遇到 generic response的问题，需要添加附加信息作为额外的输入，来得到信息更丰富的回复。与直接把knowledge fact $f$与dialog context $X$的连接$[X;f]$作为encoder的输入不同，用Memory Network可以更好地结合knowledge facts这样的附加信息。</p><p>Memory Network的作用是可以更有效地结合knowledge facts，persona description，dialog context这样的附加信息。Memory Network的工作原理可以分成两个部分：Memory representation 和 read Memory。</p><ul><li><strong>Memory representation</strong><br>  实质上是对附加信息通过另一个facts encoder的向量化表示。$\lbrace{f^1,…,f^K}\rbrace$通过另外一个encoder来编码，经过线性变换分别得到key vectors $\lbrace{k_1,…,k_K}\rbrace$和value vectors$\lbrace{m_1,m_2,…,m_K}\rbrace$。</li><li><strong>read Memory</strong><br>  实质上是计算在$\lbrace{f^1,…,f^K}\rbrace$的attention。论文中用context encoder的最后一个隐藏状态$u$作为query，计算得到总的memory representation： $$o = \sum_{i=1}^K\alpha_im_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(u,k^i)$$ 最后把context encoder的最后一个隐藏状态$u$和总的memory representation $o$组合起来，$$\hat{u} = u + o$$ 接着用$\hat{u}$来初始化decoder的隐藏状态。</li></ul><p>根据是否使用attention机制，可以得到以下四个模型：</p><ol start="3"><li><strong>MEMNET</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 实际上相当于没有用attention机制的seq2seq模型。</li><li><strong>MEMNET + CONTEXTATTENTION</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在context encoder的输出context representation $\lbrace{h_1,…,h_n}\rbrace$上的attention，得到总的context vector $c_t^{(c)}$。$$c_t^{(c)} = \sum_{i=1}^{n}\alpha_ih_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i)$$ 将$c_t^{(c)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(c)})$$</li><li><strong>MEMNET + FACTATTENTION</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在facts encoder的输出facts representation $\lbrace{m_1,…,m_K}\rbrace$上的attention，得到总的facts vector $c_t^{(f)}$。 $$c_t^{(f)} = \sum_{i=1}^K\alpha_im_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},k_i)$$ 将$c_t^{(f)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(f)})$$</li><li><strong>MEMNET + FULLATTENTION</strong><br> 同时在context representation $\lbrace{h_1,…,h_n}\rbrace$和facts representation $\lbrace{m_1,…,m_K}\rbrace$上用attention，得到context vector $c_t^{(c)}$和facts vector $c_t^{(f)}$。把二者连接起来，作为decoder隐藏状态更新的输入。$$s_t = f(s_{t-1},y_{t-1},[c_t^{(c)},c_t^{(f)}])$$</li></ol><h3 id="seq2seq模型-copy机制"><a href="#seq2seq模型-copy机制" class="headerlink" title="seq2seq模型 + copy机制"></a>seq2seq模型 + copy机制</h3><p>seq2seq模型只能从固定的词汇表中生成word。Pointer Network的作用是可以从source input中来复制word。用Pointer Network来实现copy机制也可以分为两个部分。</p><ul><li>计算在所有input tokens $\lbrace{x_1,…,x_n}\rbrace$上的attention权重分布，作为在 $\lbrace{x_1,…,x_n}\rbrace$上的概率分布。</li><li>使用一个“soft switch”机制，在copy模式时，从source input $\lbrace{x_1,…,x_n}\rbrace$中复制word；当在generation模式时，从固定的词汇表中生成word。</li></ul><p>单纯的“seq2seq模型 + attention机制”，再结合copy机制可以得到以下三种模型：</p><ol start="7"><li><strong>SEQ2SEQ + NOFACT + COPY</strong></li><li><strong>SEQ2SEQ + BESTFACTCONTEXT + COPY</strong></li><li><strong>SEQ2SEQ + BESTFACTRESPONSE + COPY</strong></li></ol><h3 id="分级Pointer-Network"><a href="#分级Pointer-Network" class="headerlink" title="分级Pointer Network"></a>分级Pointer Network</h3><p>单纯的Pointer Network可以从单句话中复制word，使用分级Pointer Network可以从K句话中复制word。</p><div align="center"><img src="/images/overall_HPN.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 系统的总体框架图</font></div><h4 id="从dialog-context中复制word"><a href="#从dialog-context中复制word" class="headerlink" title="从dialog context中复制word"></a>从dialog context中复制word</h4><p>dialog context $X = \lbrace{x_1,…,x_n}\rbrace$经过context encoder后的context representation为$\lbrace{h_1,…,h_n}\rbrace$，设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用attention机制：$$c_t^{(x)} = \sum_{i=1}^n\alpha_i^{c}h_i$$ $$\alpha_i^{(x)} = softmax(\beta_i^{(x)})$$ $$\beta_i^{(x)} = score(s_{t-1},h_i)$$ 则从dialog context $X = \lbrace{x_1,…,x_n}\rbrace$中复制word的概率分布为$$p_{copy}^{(x)}(y_t) = \alpha_i^{(x)}  &emsp; i \in [1,n]$$</p><h4 id="从knowledge-facts中复制word"><a href="#从knowledge-facts中复制word" class="headerlink" title="从knowledge facts中复制word"></a>从knowledge facts中复制word</h4><div align="center"><img src="/images/HierPointerNetwork.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. 分级Pointer Network的示意图</font></div><p>knowledge facts包含K个句子$\lbrace{f^1,…,f^K}\rbrace$，其中$f^i = \lbrace{f^i_1,…,f^i_{n_i}}\rbrace$。经过facts encoder后，再经过线性转换，得到词级别的keys vector和values vector分别为$\lbrace{k_1^{f^i},…,k_{n_i}^{f^i}}\rbrace$和$\lbrace{m_1^{f^i},…,m_{n_i}^{f^i}}\rbrace$。设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用词级别的attention机制：$$c_t^{f^i} = \sum_{j=1}^{n_i}\alpha_j^{f^i}m_j^{f^i} &emsp; i\in \lbrack {1,K}\rbrack $$ $$\alpha_j^{f^i} = softmax(score(s_{t-1},k_j^{f^i})) &emsp; i\in [1,K],j\in [1,n_i]$$ 其中$c_t^{f^i}, i\in [1,K]$是K个句子$\lbrace{f^1,…,f^K}\rbrace$句子级别的向量表示。使用句子级别的attention机制：$$c_t^{(f)} = \sum_{i=1}^{K} = \beta_i^fc_t^{f^i}$$ $$\beta_i^f = softmax(score(s_{t-1},c_t^{f^i})) &emsp; i\in [1,K]$$ 其中从$c_t^{(f)}$是knowledge facts总的向量表示。<br>则从knowledge facts中复制word的概率分布为：$$p_{copy}^{(f)}(y_t) = \beta_i^f\alpha_j^{f^i} &emsp; i\in [1,K],j\in [1,n_i]$$</p><h4 id="总的复制word的概率分布"><a href="#总的复制word的概率分布" class="headerlink" title="总的复制word的概率分布"></a>总的复制word的概率分布</h4><p>为了把复制word的两个概率分布$p_{copy}^{(x)}$和$p_{copy}^{(f)}$结合起来，使用decoder的隐藏状态$s_t$在context representation $c_t^{(x)}$和总的facts representation $c_t^{(f)}$上用attention机制。得到attention权重分布为$\lbrack{\gamma,1- \gamma}\rbrack$。$$c_t = \gamma c_t^{(x)} + (1 - \gamma) c_t^{(f)}$$ $$\lbrack{\gamma,1-\gamma}\rbrack = softmax(\lbrack{score(s_{t-1},c_t^{(x)}),score(s_{t-1},c_t^{(f)})\rbrack})$$其中$c_t$既包含了dialog context的信息，也包含了knowledge facts的信息，可以用来更新decoder的隐藏状态。$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 则总的复制word的概率分布为$$p_{copy}(y_t) = \gamma \cdot p_{copy}^{(x)} + (1 - \gamma) \cdot p_{copy}^{(f)}$$</p><h4 id="soft-switch"><a href="#soft-switch" class="headerlink" title="soft switch"></a>soft switch</h4><p>soft switch可以把copy模式和generation这两种模式结合起来。用一个门机制$p_{gen}$来控制是从固定的词汇表中生成词，或者从dialog context和knowledge facts中复制词。$$p_{gen} = sigmoid(W\lbrack{s_t,c_t}\rbrack)$$ $$p(y_t) = p_{gen} \cdot p_g(y_t) + (1 - p_{gen}) \cdot p_{copy}(y_t)$$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>采用负对数似然函数作为优化的目标函数，对于单个word的loss函数为：$$loss(\Theta) = -log(p(y_t|y_{&lt;t},X,\lbrace{f^i}\rbrace_{i=1}^K))$$其中$\Theta$表示模型所有的可训练参数，$y_{&lt;t}$表示$y_t$之前所有的word。则对于一个训练样本的loss函数为：$$J_{loss}(\Theta) = -\frac{1}{|Y|}\sum_{t=1}^{|Y|}log(p(y_t|y_{&lt;t},X,\lbrace{f^i}\rbrace_{i=1}^K))$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1908.10731&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1908.10731&lt;/a&gt;&lt;br&gt;【代码、数据集】：无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Copy Mechanism" scheme="http://yoursite.com/tags/Copy-Mechanism/"/>
    
      <category term="Pointer Network" scheme="http://yoursite.com/tags/Pointer-Network/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》</title>
    <link href="http://yoursite.com/2019/10/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/</id>
    <published>2019-10-23T01:48:41.000Z</published>
    <updated>2019-10-25T14:42:34.397Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ICLR2017<br>【链接】：<a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener">https://arxiv.org/abs/1611.01603</a><br>【代码、数据集】： <a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a></p></blockquote><a id="more"></a><p>这是由华盛顿大学和艾伦人工智能研究所发表的论文。艾伦人工智能研究所是大名鼎鼎的微软联合创始人保罗·艾伦创建的。<br>这是一篇经典的论文，截至目前被引次数高达678次。论文最大的贡献是在阅读理解任务中提出了双向attention机制（BiDirectional attention flow, BiDAF），BiDAF也可以用在其他任务中。</p><h2 id="阅读理解任务定义"><a href="#阅读理解任务定义" class="headerlink" title="阅读理解任务定义"></a>阅读理解任务定义</h2><p>给定文章context $\lbrace{x_1,x_2,…,x_T}\rbrace$及query $\lbrace{q_1,q_2,…,q_J}\rbrace$，在文章context中找到某个段span作为query的答案。输出其实是这个span的起始坐标和结束坐标。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>设encoder包含embedding层和Bi-LSTM层，经过encoder后的context representation为$H = \lbrace{h_1,h_2,…,h_T}\rbrace$，query representation为$U = \lbrace{u_1,u_2,…,u_J}\rbrace$。其中$H \in R^{2d\times  T}, U \in R^{2d\times  J}$。</p><h3 id="传统attention机制的几个特征"><a href="#传统attention机制的几个特征" class="headerlink" title="传统attention机制的几个特征"></a>传统attention机制的几个特征</h3><p>先介绍传统attention的计算方式。在时间步t计算传统attention时，需要用到上个时间步t-1的decoder RNN的隐藏状态$s_{t-1}$。decoder RNN的隐藏状态更新公式为：$$s_t = f(s_{t-1},y_{t-1},c_t)$$其中c_t为context vector，计算方式为：$$c_t = \sum_{i=1}^{T}\alpha_{t,i}h_i$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$\beta_{t,i} = score(s_{t-1},h_t)$$其中score(s,h)函数计算s与t之间的相似度。</p><p>从传统attention的计算方式可以看出，传统attention有以下几个特征：</p><ul><li>attention权重用来将所有的context representation $\lbrace{h_1,h_2,…,h_T}\rbrace$总结为一个固定维度的向量$c_t$。<em>这个过程不可避免地会带来信息丢失。</em></li><li>时间步t的attention权重$\alpha_{t_i}$计算 依赖于上一个时间步的向量$s_{t-1}$。<em>这里可以看出，attention权重的计算是有记忆的。</em></li><li>attention的计算是单向的。</li></ul><div align="center"><img src="/images/BiDAF.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font></div><h3 id="双向attention机制"><a href="#双向attention机制" class="headerlink" title="双向attention机制"></a>双向attention机制</h3><p>论文中模型分为了6层，这里只介绍最关键的一层：attention flow layer。该层的输入是context representation $H$和query representation $U$，输出是意识到query的context words representation $G$,以及前一层的context representation。</p><ol><li><strong>相似度矩阵S</strong><br>先定义一个 $H\in R^{2d\times T}$和 $U\in R^{2d\times J}$之间的共享相似度矩阵$S\in R^{T\times J}$。其中$S_{tj}$衡量了第t个context word与第j个query word之间的相似度。$$S_{tj} = \alpha(H_{:t},U_{:j}) \in R$$ 其中$H_{:t}\in R^{2d}$是H的第t个列向量，$U_{:j}\in R^{2d}$是U的第j个列向量。$\alpha()$是一个计算相似度的函数：$$\alpha(h,u) = w_{(S)}[h;u;h·u]$$</li><li><strong>context-to-query attention</strong><br>表示对于每个context word，哪个query word是最相关的。<br>对于第t个context word，在所有query words $\{q_1,q_2,…,q_J\}$上的attention权重为$a_t\in R^{J}$，有$$\sum_{j}a_{tj} = 1$$<br>attention权重$a_t$的计算方式为：$$a_t = softmax(S_{t:}) \in R^{J}$$ 对于第t个context word的attended query vector为$$\widetilde{U_{:t}} = \sum_{j}a_{tj}U_{:j} \in R^{2d}$$ 对于所有的context words $\{x_1,x_2,…,x_T\}$,则有$\widetilde{U} \in R^{2d\times T}$</li><li><strong>query-to-context attention</strong><br>表示对于每个query words，哪个context word是最相似的，对于回答query最重要。<br>计算在所有context words ${x_1,…,x_T}$上的attention权重为 $$b = softmax(max_{col}(S)) \in R^{T}$$其中$max_{col}(S) \in R^{T}$函数表示在矩阵$S \in R^{T\times J}$的列上取最大值。<br>则attended context vector为$$\widetilde{h} = \sum_{t}b_{t}H_{:t} \in R^{2d}$$ 这个向量的含义是对于query所有重要的context words的加权和。<br>把$\widetilde{h}$在列上复制T次，得到了$\widetilde{H} \in R^{2d\times T}$</li><li><strong>输出融合</strong><br>把上一层的context representation $H$和attended vector $\widetilde{H}$和$\widetilde{U}$总结组合起来得到<em>意识到query的context words representation</em> $G$，计算方式为：$$G_{:t} = \beta(H_{:t},\widetilde{U_{:t}},\widetilde{H_{:t}}) \in R^{d_{G}}$$ 其中$\beta()$函数可以是任意神经网络，比如MLP多层感知机。论文中采用了简单的连接操作，将$\beta()$函数定义为：$$\beta(h,\widetilde{h},\widetilde{u}) = [h;\widetilde{u};h \circ \widetilde{u};h \circ \widetilde{h}] \in R^{8d\times T}$$</li></ol><p>从双向attention机制的计算可以看出，双向attention机制有以下几个特征：</p><ul><li>与传统attention将所有context representation $\lbrace{h_1,h_2,…,h_T}\rbrace$总结为一个固定维度的向量$c_t$不同。双向attention机制为每个时间步都计算attention，并将attended vector $\widetilde{H}$、$\widetilde{U}$和前一层的context representation $H$流动到下一层。这样减少了提前总结为固定维度的向量带来的信息损失。</li><li>这是无记忆的attention机制。当前时间步的attention计算只取决于当前的context representation $H$和query representation $U$，而不依赖于上一个时间步的attention。 这种无记忆的attention机制将<em>attention layer</em>和<em>model layer</em>分隔开，迫使<em>attention layer</em>专注于学习context与query之间的attention，而<em>model layer</em>专注于学习attention layer输出内部之间的联系。</li><li>双向attention机制是双向的，包含query-to-context attention和context-to-query attention，可以彼此之间相互补充。</li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://allenai.github.io/bi-att-flow/" target="_blank" rel="noopener">BiDAF</a></li><li><a href="https://zhuanlan.zhihu.com/p/53626872" target="_blank" rel="noopener">机器阅读理解之双向注意力流||Bidirectional Attention Flow for Machine Comprehension</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ICLR2017&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1611.01603&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1611.01603&lt;/a&gt;&lt;br&gt;【代码、数据集】： &lt;a href=&quot;https://github.com/allenai/bi-att-flow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/allenai/bi-att-flow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="BiDAF" scheme="http://yoursite.com/tags/BiDAF/"/>
    
      <category term="Machine Comprehension" scheme="http://yoursite.com/tags/Machine-Comprehension/"/>
    
  </entry>
  
  <entry>
    <title>对话系统的数据集</title>
    <link href="http://yoursite.com/2019/09/06/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://yoursite.com/2019/09/06/对话系统的数据集/</id>
    <published>2019-09-06T02:07:32.000Z</published>
    <updated>2019-09-06T02:11:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="对话系统" scheme="http://yoursite.com/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="对话系统" scheme="http://yoursite.com/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="数据集" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Bridging the Gap between Training and Inference for Neural Machine Translation》</title>
    <link href="http://yoursite.com/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/</id>
    <published>2019-08-02T06:46:00.000Z</published>
    <updated>2019-08-05T05:14:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ACL2019<br>【链接】：<a href="https://arxiv.org/abs/1906.02448" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02448</a><br>【代码、数据集】： 无</p></blockquote><a id="more"></a><p>这篇论文由中科院发表，获得了ACL2019的 “best long paper”。</p><h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>在Neural Machine Translation(NMT)任务中，模型通常采用encoder-decoder框架，基于RNN 或 CNN 或attention。假设输入为$X = \lbrace{x_1,x_2,…,x_m}\rbrace$，真实输出为$Y = \lbrace{y_1^*,y_2^*,…,y_n^*}\rbrace$，预测输出为$Y’ = \lbrace {y_1’,y_2’,…,y_m’}\rbrace$。</p><p>第一个问题是：decoder会一个词一个词地生成整个回复。在train阶段，在时间步t生成$y_t’$时，decoder会根据之前真实的词$\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\rbrace$来预测$y_t’$。在infer阶段，由于不可能知道真实输出，在时间步生成$y_t’$时，decoder会根据之前预测的词$\lbrace{y_1’,y_2’,…,y_{t-1}’}\rbrace$来预测$y_t’$。<br>可以看到train阶段与infer阶段所依据的词是不同的，train阶段和infer阶段预测的词$y_t’$来自两个不同的概率分布，分别是数据分布(data distribution)和模型的分布(model distribution)，这种差别称为“爆炸偏差(exposure bias)”。随着预测序列的长度增加，错误会逐渐累积。<br>为了解决第一个问题，消除train阶段和infer阶段的这种差别，一个可能的解决方法是：在train阶段，decoder同时根据真实的词$\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\rbrace$和预测的词$\lbrace{y_1’,y_2’,…,y_{t-1}’}\rbrace$来生成$y_t’$。</p><p>第二个问题是: NMT模型通常最优化$Y与Y’$之间的交叉熵目标函数来更新模型参数，但交叉熵函数会严格匹配预测输出$Y’$与真实的输出$Y$。但在NMT任务中，一句话可以有多个不同但合理的翻译。一旦预测输出$Y’$的某个词与$Y$不同，尽管它是合理的，也会被交叉熵函数纠正。这种情况称为“过度纠正的现象”。</p><p>为了消除train阶段与infer阶段的差别，论文提出了一种在train阶段做改进的解决方案。首先，从预测的词中选择oracle word $y_{j-1}^{oracle}$，设真实输出中上一个词为$y_{j-1}^{*}$。接着从$\lbrace{y_{j-1}^{oracle},y_{j-1}^{*}}\rbrace$中抽样一个词，抽中$y_{j-1}^{*}$的概率为$p$，抽中$y_{j-1}^{oracle}$的概率为$1-p$。最后，decoder根据抽样的这个词来预测$y_j$。</p><p>在train阶段刚开始时，抽中真实的词$y_{j-1}^{*}$的概率比较大，随着模型逐渐收敛，抽中预测的词$y_{j-1}^{oracle}$的概率变大，让模型有能力处理”过度纠正的问题”。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/oracle.png" alt="Fig.1 论文提出的方法的结构图" title>                </div>                <div class="image-caption">Fig.1 论文提出的方法的结构图</div>            </figure><h3 id="RNN-based-NMT-Model"><a href="#RNN-based-NMT-Model" class="headerlink" title="RNN-based NMT Model"></a>RNN-based NMT Model</h3><p>NMT任务常采用encoder-decoder框架，可以基于RNN或CNN或纯attention。论文提出的消除train阶段和infer阶段差别的方法，可以用于任何NMT模型。论文以基于RNN的NMT模型为例，来介绍这种方法。这一节先介绍RNN-based NMT模型。下一节介绍NMT模型如何结合这种方法。</p><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>记输入为$X = \lbrace{x_1,x_2,…,x_m}\rbrace$，真实输出为$Y = \lbrace{y_1,y_2,…,y_n}\rbrace$。encoder采用bi-GRU分别获取正向和反向的隐藏状态$\overrightarrow{h_i},\overleftarrow{h_i}$。$x_i$的embedding向量为$e_{x_i}$。$$\overrightarrow{h_i} = GRU(e_{x_i},h_{i-1})$$ $$\overleftarrow{h_i} = GRU(e_{x_i},h_{i+1})$$ 将$\overrightarrow{h_i},\overleftarrow{h_i}$连接起来，作为$x_i$对应的隐藏状态：$$h_i = [\overrightarrow{h_i},\overleftarrow{h_i}] \tag{1}$$</p><h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><p>attention机制用来联系encoder和decoder，更好地捕捉source sequence的信息。也就是在时间步t,通过encoder所有的隐藏状态$\lbrace h_1,h_2,…,h_m \rbrace$来计算context vector $c_t$。记decoder上一时间步的隐藏状态为$s_{t-1}$。 $c_t$是encoder所有隐藏状态$\lbrace h_1,h_2,…,h_m \rbrace$的加权和：$$c_t = \sum_{i=1}^{m}\alpha_{ti}h_i \tag{2}$$ 其中$\alpha_{ti}$是attention权重，计算方式为:$$\beta_{ti} = v_a^\top tanh(W_as_{t-1} + U_ah_i) \tag{3}$$ $$\alpha_{ti} = softmax(\beta_{ti}) = \frac{exp(\beta_{ti})}{\sum_jexp(\beta_{tj})}$$</p><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>decoder采用单向GRU的变体，隐藏状态更新公式为:$$s_t = GRU(s_{t-1},e_{y_{t-1}^*},c_t) \tag{4}$$ 最后根据e_{y_{t-1}^*}，decoder的隐藏状态$s_t$，对应的context vector $c_t$来预测$y_t$。 $$o_t = W_og(e_{y_{t-1}^*},s_t,c_t) \tag{5}$$ 在词汇表上的概率分布为：$$P_t(y_t = w) = softmax(o_t) \tag{6}$$</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>为了消除或减轻train阶段和infer阶段的差别，论文提出 从真实的词$y_{t-1}*$和$y_{t-1}^{oracle}$预测的词中抽样，decoder根据抽样的词来预测下一个词$y_t$。使用论文提出的方法，在时间步t预测$y_t$分为三步：</p><ol><li>先从预测的词中选择$y_{t-1}^{oracle}$。 论文提出了两种方法来选择oracle word，分别是词级别的方法和句子级别的方法。</li><li>从$\lbrace{y_{t-1}^{oracle},y_{t-1}*}\rbrace$中抽样得到$y_{t-1}$，抽中$y_{t-1}*$的概率为$p$，抽中$y_{t-1}^{oracle}$的概率为$1-p$。</li><li>用抽样的词$y_{t-1}$来替换公式$(4)(5)$中的$y_{t-1}^*$来预测下一个词。</li></ol><h4 id="oracle-word的选择"><a href="#oracle-word的选择" class="headerlink" title="oracle word的选择"></a>oracle word的选择</h4><p>传统的方法中，decoder会根据上一个时间步真实的$y_{t-1}^*$来预测$y_t$。为了消除train阶段的infer阶段的差别，可以从预测的词中选择oracle word $y_{t-1}^{oracle}$来代替$y_{t-1}^*$。一种方法是每个时间步采用词级别的greedy search来生成oracle word，称为word-level oracle(WO)，另一种方法是采用beam-search，扩大搜索空间，用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO).</p><h5 id="word-level-oracle"><a href="#word-level-oracle" class="headerlink" title="word-level oracle"></a>word-level oracle</h5><p>选择$y_{t-1}^{oracle}$最简单直观的方法是，在时间步t-1，选择公式$P_{t-1}$中概率最高的词作为$y_{t-1}^{oracle}$，如Fig.2所示。 为了获得更健壮的$y_{t-1}^{oracle}$，更好地选择是使用<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">gumbel max技术</a>来冲离散分布中进行抽样，如Fig.3所示。<br>具体地讲，将gumbel noise $\eta$作为正则化项加到公式(5)中的$o_{t-1}$，再进行softmax操作得到$y_{t-1}$的概率分布。$$\eta = -log(-log(u)) $$ $$\tilde{o_{t-1}} = \frac{o_{t-1} + \eta}{\tau} \tag{7}$$ $$\tilde{P_{t-1}} = softmax(\tilde{o_{t-1}}) \tag{8}$$ 其中变量$u \sim U(0,1)$服从均匀分布。$\tau$为温度系数，当$\tau \to 0$时，公式(8)的softmax()逐渐相当于argmax()函数；当$\tau \to \infty$时，softmax()函数逐渐相当于均匀分布。<br>则$y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{WO} =argmax(\tilde{P_{t-1}}) \tag{9}$$需要注意的是gumbel noise $\eta$只用来选择oracle word，而不会影响train阶段的目标函数。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/word_level_oracle_without_gumbel_noise.png" alt="Fig.2. word level oracle without gumbel noise" title>                </div>                <div class="image-caption">Fig.2. word level oracle without gumbel noise</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/word_level_oracle_with_gumbel_noise.png" alt="Fig.3. word level oracle with gumbel noise" title>                </div>                <div class="image-caption">Fig.3. word level oracle with gumbel noise</div>            </figure><h5 id="sentence-level-oracle"><a href="#sentence-level-oracle" class="headerlink" title="sentence-level oracle"></a>sentence-level oracle</h5><p>为了选择sentence-level oracle word，首先要进行beam-search解码，设beam size为k，得到k个candidate句子。在beam-search解码的过程中，生成每个词时也应用gumbel max技术。<br>接着，得到k个candidate句子后，用句子级衡量指标BLEU来给这k个句子打分，得分最高的句子为oracle sentence $Y^S = \lbrace{y_1^S,y_2^S,..,y_{|y^S|}^S}\rbrace$。<br>则时间步t解码对应的oracle word $y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{SO} = y_{t-1}^{S} \tag{10}$$ 当模型从真实输出$Y$和sentence oracle $Y^S$抽样，这有一个前提是，这两个序列的长度需要是一致的。但beam-search decode不能保证解码序列的长度。为了保证这两个序列长度一致，论文提出了<em>force decoding</em>的解决方法。</p><p><strong>force decoding</strong><br>设真实输出$Y = \lbrace{y_1,y_2,…,y_n}\rbrace$的序列长度为n。<em>force decoding</em>需要解码得到长度同样为n的序列，以特殊字符”EOS”结束。设beam search decode时，时间步t对应的概率分布为$P_t$。</p><ul><li>当$t&lt; n$时，对于概率分布$P_t$，即使字符”EOS”是概率最高的词，那么生成概率次高的词。</li><li>当$t = n+1$时，对于概率分布$P_{n+1}$，即使字符”EOS”不是概率最高的词，也要生成”EOS”。</li></ul><p>果真是强制生成长度为n的序列。这样beam-search decode得到的序列与真实输出序列的长度就是一致的，都为n。</p><h4 id="递减抽样"><a href="#递减抽样" class="headerlink" title="递减抽样"></a>递减抽样</h4><p>根据公式(9)或(10)得到$y_{t-1}^{oracle}$后，下一步是从$\lbrace{y_{t-1}^{oracel},y_{t-1}^*}\rbrace$中抽样，抽中$y_{t-1}^*$的概率是p，抽中$y_{t-1}^{oracle}$的概率是1-p。在训练的初始阶段，如果过多地选择$y_{t-1}^{oracle}$，会导致模型收敛速度慢；在训练的后期阶段，如果过多地选择$y_{t-1}^*$，会导致模型在train阶段没有学习到如何处理infer阶段的差别。<br>因此，好的选择是：在训练的初始阶段，更大概率地选择$y_{t-1}^*$来加快模型收敛，当模型逐渐收敛后，以更大概率选择$y_{t-1}^{oracle}$，来让模型学习到如何处理infer阶段的差别。从数学表示上，概率$p$先大后逐渐衰减，$p$随着训练轮数$e$的增大而逐渐变小。$$p = \frac{\mu}{\mu + exp(\frac{e}{\mu})} \tag{11}$$其中，$\mu$是超参数。$p$是轮数$e$的单调递减函数。$e$从0开始，此时，$p=1$。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>将采样得到的$y_{t-1}$代替公式(4)-(6)中的$y_{t-1}^*$来预测$y_t$在词汇表上的概率分布。采用最大似然估计，相当于最小化以下目标函数：$$L(\theta) = -\sum_{n=1}^{N}\sum_{j=1}^{|y_n|}logP_j^n[y_j^n]$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1906.02448&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1906.02448&lt;/a&gt;&lt;br&gt;【代码、数据集】： 无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="ACL2019" scheme="http://yoursite.com/tags/ACL2019/"/>
    
      <category term="Neural Machine Translation" scheme="http://yoursite.com/tags/Neural-Machine-Translation/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Multi-Level Memory for Task Oriented Dialogs》</title>
    <link href="http://yoursite.com/2019/08/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AMulti-Level-Memory-for-Task-Oriented-Dialogs%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/</id>
    <published>2019-08-01T06:14:37.000Z</published>
    <updated>2019-10-25T07:26:40.134Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.10647.pdf</a><br>【代码、数据集】：<a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></p></blockquote><a id="more"></a><p>已有工作中，端到端的任务型对话系统采用memory network来结合外部的知识库(knowledgt base) 和 对话历史(context)。为了使用从跑一趟 network，通常将二者放在同一个memory中。这样带来的问题是：memory变得太大，模型在读取memory时需要区分外部知识库和对话历史，并且在memory上的推理变得很难。为了解决这个问题，论文将外部知识库和对话历史区分开，另外，将外部知识库保存为分层的memory。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型主要包括三个部分。 </p><ul><li>分级encoder：<br>  分别编码对话历史中的句子。</li><li>milti-level memory<br>  保存了目前为止所有的query以及对应的知识库查询结果，是以分级的方式保存在memory中的。</li><li>copy机制增强的decoder：<br>  从词汇表中生成词，或者从知识库multi-level memory中复制词，或者从对话历史(context)中复制词。</li></ul><div align="center"><img src="/images/multi-memory-model.jpg" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div><h4 id="分级encoder"><a href="#分级encoder" class="headerlink" title="分级encoder"></a>分级encoder</h4><p>在第t轮，对话历史共有2t-1个句子$\lbrace{c_1,c_2,…,c_{2t-1}}\rbrace$，其中用户对话为t轮，回复对话为t-1轮。 每个句子$c_i$都是词序列$\lbrace{w_{i1},w_{i2},…,w_{im}}\rbrace$。<br>每个句子$c_i$先经过embedding layer得到词向量表示，再经过单层bi-GRU得到句子的向量表示$\varphi(c_i)$。$h_{ij}^e$表示词$w_{ij}$对应的隐藏状态。<br>再将$\varphi{c_i}$经过另一个单词GRU来得到context的向量表示$c$。</p><h4 id="multi-level-memory"><a href="#multi-level-memory" class="headerlink" title="multi-level memory"></a>multi-level memory</h4><p>memory的关键是分级的分为三级：query $\to$ result $\to$ result key和result value。见Fig.2。<br>记本轮对话之前所有的知识库query为$q_1,…,q_k$。每个query $q_i$是一个(key,value)对，$q_i = \lbrace{k_a^{q_i}:v_a^{q_i},0&lt; a&lt; n_{q_i}}\rbrace $。其中key和value分别对应query的槽(slots)和槽值，$n_{q_i}$是query $q_i$的槽值个数。<br>第j轮对话，用query $q_i$查询知识库的返回结果为result $r_{ij}$。$r_{ij}$也是一个key-value对，$r_{ij} = \lbrace{k_a^{r_{ij}}:v_a^{r_{ij}},0&lt; a &lt; n_{r_{ij}}}\rbrace$。其中$n_{r_{ij}}$是key-value对的个数。</p><div align="center"><img src="/images/multi-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. multi memory</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div>第一级memory是query的向量表示。 $q_i$的向量表示为$q_i^v$，$q_i^v$为所有values $v_a^{q_i}$的词袋(bag of words)向量表示。第二级memory是result的向量表示。同样地，$r_{ij}$的向量表示为$r_{ij}^v$，$r_{ij}^v$为所有values $v_a^{r_{ij}}$的词袋(BOW)向量表示。第三级memory是result的key-value对，$(k_a^{r_{ij}}:v_a^{r_{ij}})$，其中value $v_a^{r_{ij}}$可能会被复制到回复中。<h4 id="copy机制增强的decoder"><a href="#copy机制增强的decoder" class="headerlink" title="copy机制增强的decoder"></a>copy机制增强的decoder</h4><p>decoder一个词一个词地生成回复。在时间步t生成词$y_t$时，可能从词汇表中生成，也能从两个分开的memory上复制。用门$g_1$来选择是从词汇表上生成，还是从memory中复制。如果是后者，用另一个门$g_2$来选择是从context中复制，还是从知识库复制。</p><ol><li><strong>从词汇表生成词</strong><br> 时间步t，decoder的隐藏状态$h_t$为$$h_t = GRU(y_{t-1},s_{t-1})$$用$h_t$计算在encoder的所有隐藏状态上的attention权重，采用”concat attention”机制：$$a_{ij} = softmax(w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))) = \frac{w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}{\sum_{ij}w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}$$则context vector为$$d_t = \sum_{ij}a_{ij}h^e_{ij}$$ $h_t$和$d_t$连接后经过线性层和softmax层得到在词汇表上的概率分布：$$P_g(y_t) = softmax(W_1[h_t,d_t] + b_1)$$</li><li><strong>从context memory中复制词</strong><br> 直接将计算context vector时的attention权重，作为在context所有词$w_{ij}$上的概率分布：$$P_{con}(y_t = w) = \sum_{ij:w_{ij}=w}a_{ij}$$</li><li><strong>从KB memory中复制实体</strong><br> 时间步t的隐藏状态$h_t$和context vector $d_t$用来计算在所有query上的attention权重。第一级在所有query $q_1,q_2,…,q_k$的attention权重为$$\alpha_i = softmax(w_2^\top tanh(W_4[h_t,d_t,q_i^v])) = \frac{w_2^\top tanh(W_4[h_t,d_t,q_i^v])}{\sum_{i}w_2^\top tanh(W_4[h_t,d_t,q_i^v])}$$<br> 第二级$\beta_i$在$q_i$对应的$r_i$上的attention权重为$$\beta_{ij} = softmax(w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])) = \frac{w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}{\sum_{j}w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}$$<br> 第一级attention和第二级attention的乘积是在所有result上的attention权重分布。则memory总的向量表示为$$m_t = \sum_{i}\sum_j\alpha_i\beta_{ij}r_{ij}^v$$<br> 第三级memory为result的key-value对$(k_a^{r_{ij}}:v_a^{r_{ij}})$，类似于<a href="https://arxiv.org/abs/1705.05414" target="_blank" rel="noopener">(Eric and Manning, 2017)</a>，用key $k_a^{r_{ij}}$来计算attention权重，将对应的value $v_a^{r_{ij}}$复制到回复中。在$r_{ij}$所有keys上的attention权重为$$\gamma_{ijl} = softmax(w_4^\top tanh(W_6[h_t,d_t,m_t,k_l^{r_{ij}}]))$$则在所有values $v_a^{r_{ij}}$的概率分布为:$$P_{kb}(y_t = w) = \sum_{ijl:v_l^{r_{ij}}=w}\alpha_i\beta_{ij}\gamma_{ijl}$$</li><li><strong>decoding</strong><br> 我们用门机制$g_2$来来结合$P_{con}(y_t)$和$P_{kb}(y_t)$，得到memory上的copy概率分布$P_c(y_t)$。$$g_2 = sigmoid(W_7[h_t,d_t,m_t]+b_2)$$ $$P_c(y_t) = g_2P_{kb}(y_t) + (1-g_2)P_{con}(y_t)$$ 用门机制$g_1$来结合$P_{c}(y_t)$和$P_{g}(y_t)$来得到总的概率分布$P(y_t)$：$$g_1 = sigmoid(W_8[h_t,d_t,m_t]+b_3)$$ $$P(y_t) = g_1P_g(y_t) + (1-g_1)P_c(y_t)$$</li></ol><h3 id="相似论文"><a href="#相似论文" class="headerlink" title="相似论文"></a>相似论文</h3><ul><li><a href="https://arxiv.org/abs/1810.10647" target="_blank" rel="noopener">《Multi-level Memory for Task Oriented Dialogs》</a><ul><li>发表在NAACL2019</li><li>github: <a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></li></ul></li><li><a href="https://arxiv.org/abs/1804.08217" target="_blank" rel="noopener">《Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems》</a><ul><li>发表在ACL2018</li><li>github: <a href="https://github.com/HLTCHKUST/Mem2Seq" target="_blank" rel="noopener">https://github.com/HLTCHKUST/Mem2Seq</a></li></ul></li><li><a href="https://www.ijcai.org/proceedings/2018/643" target="_blank" rel="noopener">《Commonsense Knowledge Aware Conversation Generation with Graph Attention》</a><ul><li>发表在IJCAI2018</li><li>github： <a href="https://github.com/tuxchow/ccm" target="_blank" rel="noopener">https://github.com/tuxchow/ccm</a></li></ul></li><li><a href="https://arxiv.org/abs/1908.10731" target="_blank" rel="noopener">《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》</a><ul><li>发表于2019年</li><li>代码：无</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/pdf/1810.10647.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1810.10647.pdf&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/DineshRaghu/multi-level-memory-network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/DineshRaghu/multi-level-memory-network&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
  </entry>
  
  <entry>
    <title>Neural Turing Machines与Memory Network</title>
    <link href="http://yoursite.com/2019/07/26/Neural-Turing-Machines%E4%B8%8EMemory-Network/"/>
    <id>http://yoursite.com/2019/07/26/Neural-Turing-Machines与Memory-Network/</id>
    <published>2019-07-26T03:03:15.000Z</published>
    <updated>2019-08-01T02:30:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Memory Networks。</p><a id="more"></a><h3 id="Neural-Turing-Machines-神经图灵机"><a href="#Neural-Turing-Machines-神经图灵机" class="headerlink" title="Neural Turing Machines-神经图灵机"></a>Neural Turing Machines-神经图灵机</h3><p>Google DeepMind团队在<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>提出Neural Turing Machines，第一次提出用external memory来提高神经网络的记忆能力。这之后又出现了多篇关于Memory Networks的论文。我们先看看Turing Machines的概念。</p><h4 id="Turing-Machines-图灵机"><a href="#Turing-Machines-图灵机" class="headerlink" title="Turing Machines-图灵机"></a>Turing Machines-图灵机</h4><p>计算机先驱<a href="https://baike.baidu.com/item/%E8%89%BE%E4%BC%A6%C2%B7%E9%BA%A6%E5%B8%AD%E6%A3%AE%C2%B7%E5%9B%BE%E7%81%B5/3940576?fromtitle=%E5%9B%BE%E7%81%B5&fromid=121208" target="_blank" rel="noopener">turing</a>在1936年提出了Turing Machines这样一个计算模型。它由三个基本的组件：</p><ul><li>tape: 一个无限长的纸带作为memory，包含无数个symbols，每个symbol的值为0、1或”$\space$”。</li><li>head: 读写头，对tape上的symbols进行读操作和写操作。</li><li>controller： 根据当前状态来控制head的操作。</li></ul><p>理论上Turing Machines可以模拟任何一个计算算法，不管这个算法多么复杂。但现实中，计算机不可能有无限大的memory space，因此Turing Machines只是数学意义上的计算模型。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/turing-machine.jpg" alt="Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)" title>                </div>                <div class="image-caption">Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)</div>            </figure><h4 id="Neural-Turing-Machines"><a href="#Neural-Turing-Machines" class="headerlink" title="Neural Turing Machines"></a>Neural Turing Machines</h4><p>Neural Turing Machines(NTM,<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>)用external memory来提高神经网络的记忆能力。<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM(Long and short memory)</a>通过门机制有效缓解了RNN的’梯度消失和梯度爆炸问题’，可以通过internal memory实现长期记忆。当LSTM的internal memory的记忆能力有限，需要用external memory来提高神经网络的记忆能力。</p><p>Neural Turing Machines包含两个基本组件：<em>a neural network controller</em>和<em>memory bank</em>。<em>memory</em>是一个 $N\cdot M$阶的矩阵，包含N个向量，每个向量的维度是M。我们把每个memory vector称为memory location。<em>controller</em>控制<em>heads</em>对<em>memory</em>进行读写操作。</p><p>如何对<em>memory matrix</em>进行读写操作呢？关键问题是如何让读写操作是可微的，这样才能用梯度下降法来更新模型参数。具体来说，问题是让模型关于memory location是可微的，但memory locations是离散的。Neural Turing Machines用了一个很聪明的方法来解决这个问题：不是对单独某个memory location进行读写操作，而是对所有的memory locations进行不同程度的读写操作，这个程度是通过attention的权重分布来控制的。</p><div align="center"><img src="/images/NTM.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig. 2. Neural Turing Machine Architecture</font></div><h5 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h5><p>记时间步t <em>memory matrix</em>为$N\cdot M$阶矩阵$M_t$，$w_t$是在N个memory向量上的权重分布，是一个N维向量。则时间步t的read vector $r_t$为$$r_t = \sum_{i=1}^{N}w_t(i)\cdot M_t(i)$$ $$where: \sum_{i=1}^{N}w_t(i) = 1; 0 \le w_t(i) \le 1,\forall i $$其中，$w_t(i)$是$w_t$的第i个元素，$M_t(i)$是$M_t$的第i个行向量。</p><h5 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h5><p>受LSTM门机制的启发，将写操作分成两步：先<em>erase</em>，再<em>add</em>。先根据<em>erase vector $e_t$</em>擦去旧的内容，再根据<em>add vector $a_t$</em>添加新的内容。</p><ol><li>先erase：<br> 在时间步t，attention权重分布为$w_t$，<em>erase vector $e_t$</em>是一个M维向量，每个元素取值[0,1]，上一个时间步的<em>memory vector</em>为$M_{t-1}$。则erase操作为$$\tilde{M_{t}}(i) = M_{t-1}(i)[\vec{1}-w_t(i)e_t]$$ $\vec{1}$是一个M维的全1向量。对memory vector的erase操作是逐点进行的。当$e_t$的元素和memory location对应权重$w_t(i)$的元素值都是1时，memory vector $M_t(i)$的元素值才会置为0。如果$e_t$或$w_t(i)$的元素值为0时，memory vector $M_t(i)$的元素值保持不变。</li><li>再add:<br> 每个<em>write head</em>会产生一个M维的<em>add vector a_t</em>，则：$$M_t(i) = \tilde{M_{t}}(i) + w_t(i)a_t$$至此，就完成了写操作。</li></ol><h5 id="寻址机制"><a href="#寻址机制" class="headerlink" title="寻址机制"></a>寻址机制</h5><p>进行读写操作前，要搞清楚对哪个memory location进行读写呢？这就是寻址。为了让模型关于memory locatios可微，Neural Turing Machines不是对某个单独的memory location进行读写操作，而是对所有memory locations进行不同程度的读写操作，这个程度就是由权重分布$w_t$来控制的。模型结合并同时使用了content-based和location-based两种寻址方式来计算这个权重分布$w_t$。具体地，权重计算分为以下几步：</p><ol><li>content-based addressing<br> 时间步t，每个head产出一个M维的<em>key vector $k_t$</em>，通过$k_t$与memory vectors $M_t(i)$之间的相似性来计算content-based attention权重分布$w_{t}^{c}$。相似性是通过余弦相似度来衡量的。$$w_{t}^{c} = softmax(\beta_tK(k_t,M_t(i))) = \frac{\beta_tK(k_t,M_t(i))}{\sum_{j}K(k_t,M_t(j))}$$ $$K(u,v) = \frac{u\cdot v}{|u|\cdot |v|}$$<br> $\beta_t$可以放大或缩小权重的精度。</li><li>内插法<br> 每个head产生一个<em>interpolation gate $g_t$</em>，取值[0,1]。content-based attention权重分布为$w_t^{c}$，上一个时间步的attention权重分布为$w_{t-1}$。则门控制的权重分布$w_t^g$为：$$w_t^g = g_tw_t^c + (1-g_t)w_{t-1}$$当$g_t$为0时，采用上一个时间步的权重分布$w_{t-1}$，当$g_t$为1时，采用content-based attention权重分布$w_t^c$。</li><li>循环卷积<br> 对经过插值后的权重分布$w_t^g$进行循环卷积，主要功能是对权重进行旋转位移。比如当权重分布关注某个memory location时，经过循环卷积就会扩展到附近的memory locations，也会对附近的memory locations进行少量的读写操作。每个head产生的转移权重为$s_t$,循环卷积的操作为:$$\tilde{w_t(i)} = \sum_{j=0}^{N-1}w_t^g(i)s_t(i-j)$$<br> 关于$s_t$的详细介绍可以见<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a>;<br> 循环卷积的详细介绍可以见<a href="https://blog.csdn.net/rtygbwwwerr/article/details/50548311" target="_blank" rel="noopener">Neural Turing Machines-NTM系列（一）简述</a></li><li>锐化<br> 循环卷积往往会造成权重泄漏和分散，为了解决这个问题，需要最后进行锐化操作。$$w_t(i) = \frac{\tilde{w_t(i)^{\gamma_t}}}{\sum_j\tilde{w_t(j)^{\gamma_t}}}$$其中$\gamma_t &gt;1$。至此，就得到了时间步t的权重分布$w_t$。可以根据这个权重分布$w_t$对memory matrix进行读写操作。</li></ol><p>总结以下这4步操作。第一步content-based addressing根据输入得到关于memory locations的相似度；后三步实现了location-based addressing。第二步插值操作引入了上一个时间步的权重分布，对content-based 权重进行修正；第三步循环卷积将每个位置的权重向两边分散；第四步锐化操作将权重突出化，大的更大，小的更小。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/50.jpg" alt="Fig.3. 寻址机制的4步操作" title>                </div>                <div class="image-caption">Fig.3. 寻址机制的4步操作</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/NTM-flow-addressing.png" alt="Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)" title>                </div>                <div class="image-caption">Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)</div>            </figure><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a><br>  用动图直观地表现Neural Turing Machines的计算过程。推荐！👍</li><li><a href="https://zhuanlan.zhihu.com/p/30383994" target="_blank" rel="noopener">记忆网络之Neural Turing Machines</a>，中文</li><li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a></li></ul><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>在Neural Turing Machines提出仅仅五天后，Facebook研究员<a href="http://www.thespermwhale.com/jaseweston/" target="_blank" rel="noopener">Jason Weston</a>发表了<a href="http://arxiv.org/abs/1410.3916" target="_blank" rel="noopener">MEMORY NETWORKS</a>。在QA系统的领域，应用memory network。虽然RNN或LSTM可以通过hidden state和weights来进行短期记忆，但它们的记忆能力是有限的。要实现长期记忆，需要memory network。</p><h4 id="Memory-Network的一般框架"><a href="#Memory-Network的一般框架" class="headerlink" title="Memory Network的一般框架"></a>Memory Network的一般框架</h4><p>memory network包括一个记忆单元memory，和四个基本组件：</p><ul><li>I(input feature map):<br>  将input <em>x</em>进行向量化表示，编码为feature representation <em>I(x)</em>。</li><li>G(generalization):<br>  对memory进行写操作。根据input 来更新memory <em>$m_i$</em>。$m_i = G(m_i,I(x),m)$</li><li>O(output feature map):<br>  对memory进行读操作。根据input和memory生成output feature。$o = O(I(x),m)$</li><li>R(response):<br>  根据output feature o来生成response。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/memn.jpg" alt="Fig.5. memory network的框架图" title>                </div>                <div class="image-caption">Fig.5. memory network的框架图</div>            </figure><h4 id="memory-network框架的实现–MemNNs"><a href="#memory-network框架的实现–MemNNs" class="headerlink" title="memory network框架的实现–MemNNs"></a>memory network框架的实现–MemNNs</h4><p>在I模块将input $x_i$编码为$I(x_i)$后，G模块之间将$I(x_i)$保存到下一个空的memory slot中，而不更新旧的memory slots。真正实现inference的核心模块是O和R。</p><p>O模块在给定x的条件下，依次找到与x最相关的k个memory slots。论文中采用k = 2。先找到第一个最相关的memory slot：$$m_{o1} = \mathop{argmax}\limits_{i = 1,…,N} s_{o1}(x,m_i)$$其中$s_o()$是一个匹配函数，计算x与$m_i$之间的相关程度。接着，根据x和第一个memory找到下一个memory：$$m_{o2} = \mathop{argmax}\limits_{i = 1,…,N} s_{o2}([x,m_{o1}],m_i)$$将output feature o = $[x,m_{o1},m_{o2}]$作为R模块的输入。</p><p>R模块将词汇表中所有词与output feature进行匹配，选择匹配度最高的词作为response。这样生成的response只有一个词。$$r = \mathop{argmax}\limits_{w \in W}s_R([x,m_{o1},m_{o2}],w)$$其中$s_R()$是一个匹配函数。</p><p>匹配函数$s_O$和$s_R$都采用以下函数：$$s(x,y) = \Phi_x(x)^\top U^\top U \Phi_y(y)$$其中$\Phi_x(x),\Phi_y(y)$分别将x/y编码为向量。<br><strong>目标函数</strong><br>在训练阶段采用最大边缘目标函数，设对于question x，真实的label为r，对应的memory为$m_{o1},m_{o2}$。则最大边缘目标函数为：$$\sum_{m_i\ne m_{o1}}max(0,\gamma - s_{O1}(x,m_{o1}) + s_{O1}(x,m_i)) + $$ $$\sum_{m_j\ne m_{o2}}max(0,\gamma - s_{O2}([x,m_{o1}],m_{o2}) + s_{O2}([x,m_{o1}],m_j)) + $$ $$\sum_{r’ \ne r}max(0,\gamma - s_{R}([x,m_{o1},m_{o2}],r) + s_{R}([x,m_{o1},m_{o2}],r’))$$</p><p>由于argmax()函数的存在，这个模型是不可微的。而且中间过程找到相关memory需要监督，这个模型不是端到端的。<br>总的来说，这个memory network是一种普适性的架构，是很初级很简单的，很多部分还不完善，不足以应用具体的任务上。不过，通过多跳方式找到相关memory的思路是很值得学习的。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>Jason Weston作为三作的<a href="http://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">Sainbayar Sukhbaatar2015</a>对Memory network工作的改进，主要改进是实现了端到端，减少了监督。End-to-End Memory Network采用soft attention而不是hard attention来read memory，因此是端到端的。另外不需要对相关memory进行监督。提高memory network的可用性。<br>假设多个句子input $x_1,…,x_n$作为memory，对于query q，输出对应的answer a。给定query q，经过多跳找到相关的memory，并生成对应的answer a。</p><h4 id="single-layer"><a href="#single-layer" class="headerlink" title="single layer"></a>single layer</h4><p>给定input $x_1,x_2,…,x_n$，采用两个不同的embedding matrix A和C分别编码为向量$\lbrace{m_1,…,m_n}\rbrace$，$\lbrace{c_1,…,c_n}\rbrace$,分别对应attention机制的keys和values。将query q经过embedding matrix B编码为向量表示u。</p><p>采用dot-product attention计算权重：$$p_i = softmax(u^\top m_i) = \frac{exp(u^\top m_i)}{\sum_{j}exp(u^\top m_j)}$$<br>则memory representation为：$$o = \sum_{i}p_i m_i$$<br>根据u和o来进行预测：$$\hat{a} = softmax(W (o + u))$$<br>通过最小化a与$\hat{a}$之间的交叉熵来训练模型参数A,B,C,W。这个single layer end-to-end Memory network是简单而直观的。核心是用soft attention来read memory，找到相关的memory，并进行inference。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/mem.png" alt="Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)" title>                </div>                <div class="image-caption">Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)</div>            </figure></p><h4 id="multi-layers"><a href="#multi-layers" class="headerlink" title="multi layers"></a>multi layers</h4><p>将K层single layer进行stack得到K层memory network，进行K跳memory查询操作。具体地stack方式为：</p><ul><li>将第k层的输入$u^k$和memory representation $o^k$相加作为第k+1层的输入:$$u^{k+1} = u^k + o^k$$</li><li>每一层都有单独的embedding matrix $A^k$和$C^k$</li><li>最后一层的预测输出为：$$\hat{a} = softmax(W u^{K+1}) = softmax(W(u^K + o^K))$$</li></ul><p>为了减少参数量，有两种方法：</p><ul><li>adjacent:<br>  让相邻层的embedding matrix A=C，共享参数。即：$C^k = A^{k+1}$，对第一层有$A^1 = B$，最后一层有：$C^K = W$。这样就减少了一半的参数量。</li><li>RNN-like:<br>  跟RNN一样，采用完全参数共享的方法，$A^1 = A^2 = … = A^K$;$C^1 = C^2 = … = C^K$。参数数量大大减少导致模型效果变差，在层与层之间添加一个线性映射：$u^{k+1} = Hu^k + o^k$</li></ul><h3 id="key-value-Memory-Networks"><a href="#key-value-Memory-Networks" class="headerlink" title="key-value Memory Networks"></a>key-value Memory Networks</h3><p>Jason Weston作为作者之一的<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">Alexander Miller2016</a>在End-to-End Memory networks的基础上继续推进，可以更好的通过memory来编码和利用先验知识，并且具体地应用到了QA系统中。</p><p>作为memory的先验知识可以是结构化的三元组知识库，也可以是非结构化的文本。</p><ul><li>三元组知识库。三元组的形式是”实体-关系-实体”，或”主语-谓语-宾语”。三元组知识库的优点是结构化的，便于机器处理。但缺点是与一句完整的话比较，三元组缺少了一些信息。由于三元组知识库是人工构建的，难免会有覆盖不到的知识，对于某个问题可能知识库中根本就没有对应的知识。另外，三元组中的实体可以有多种不同的表达，比如知识库中有三元组”中国-首都-北京”。当问题是“中华人民共和国的首都是？”时，可能就不能很好地回答。</li><li>像“维基百科”这样的非结构化文本。优点时覆盖面广，几乎包含所有问题的知识。缺点是非结构化的，有歧义，需要经过复杂的推理才能找到答案。</li></ul><p>作为先验知识的memory是(key,value)形式的。</p><ul><li>key memory用于寻址(addressing/lookup)阶段，通过计算query与key memory的相关程度来计算attention权重，因此在设计key memory时，key memory的特征应该更好地匹配query。</li><li>value memory用于read阶段，将value memory的加权和作为memory总的向量表示，因此在涉及value memory时，value memory的特征应该更好地匹配response。</li></ul><p>比较一下end-to-end memory network与key-value memory network的区别：</p><ul><li>前者是将相同的输入经过两个不同的embedding matrix编码分为作为key memory和value memory。而后者可以将不同的知识(key,value)分别编码为key memory和value memory，可以更灵活地利用先验知识。</li><li>后者的每个hop之间添加了用$R_j$来进行线性映射。</li></ul><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>在问答系统中，记memory slots为$(k_1,v_1),…,(k_M,v_M)$，问题query为x，真实回复为a，预测回复为$\hat{a}$。$\Phi_{X},\Phi_{Y},\Phi_{K},\Phi_{V}$分别是x,a,key,value的embedding matrix，将文本编码为向量表示。</p><p>则单次memory的寻址和读取可以分为三步：</p><ul><li>key hashing:<br>  当知识库很大时，这一步是非常必要的。根据query从知识库中检索筛选出相关的facts $ (k_{h_1},v_{h_1}),(k_{h_2},v_{h_2}),…,(k_{h_N},v_{h_N})$，筛选条件可以是key中至少包含query中一个相同的词（去除停用词）。这一步可以在数据预处理时进行，直接将query和相关的facts作为模型的输入。</li><li>key addressing(寻址阶段)<br>  计算query与memory的相关程度来分配在memory上的概率分布：$$p_{h_i} = softmax(A\Phi_{X}(x) \cdot A\Phi_{K}(k_{h_i}))$$其中$\Phi$将文本编码为D维向量，A是一个$d\times D$的可训练矩阵。</li><li>value reading：<br>  将value的加权求和作为memory总的向量表示。$$o = \sum_{i}p_{h_i}A\Phi_{V}(v_{h_i})$$</li></ul><p>memory的读取过程是由controller神经网络通过query $q = A\Phi_{X}(x)$来控制的。模型会利用query $q$与上一跳(hop)的$o$来更新query，进而迭代地寻址和读取memory，这个迭代的过程称为多跳(hops)。<br>用多跳方式来迭代地寻址和读取memory，可以这样来理解：浅层神经网络可以学习到低级的特征，随着神经网络层数增多就可以学习到更高级的特征。类比CNN处理人脸图片时，第一层可以学习到一些边缘特征，第二层可以学习到眼睛、鼻子、嘴巴这样的特征，最后一层得到整个人脸的特征。同样地，用多跳方式来寻址和读取memory，可以得到更相关更突出的memory，同时可以起到推理的作用。</p><p>query的更新公式为:$$q_2 = R_1(q + o)$$其中R是一个$d\times d$的可训练矩阵。每一跳使用不同的矩阵$R_j$。<br>则第j跳更新query后，寻址阶段的计算公式为$$p_{h_i} = softmax(q_{j+1}^\top \cdot A\Phi_{K}(k_{h_i}))$$<br>在经过H跳之后，用controller神经网络的最终状态进行预测:$$\hat{a} = argmax_{i=1,…,C}softmax(q_{H+1}B\Phi_{y}(y_i))$$其中B是一个$d\times D$的可训练矩阵，形状跟A一样。$y_i$可以是知识库中的实体，或者候选句子。</p><p>模型的目标函数为预测回复$\hat{a}$与真实回复$a$之间的交叉熵，用梯度下降的方法来更新模型参数：$A,B,R_1,…,R_H$</p><div align="center"><img src="/images/key-value-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.7. 问答系统key-value memory networks的模型框架</font><br><a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener"><font color="grey" size="2">来源:Alexander Miller2016</font></a></div><h4 id="key-value的选择与编码方式"><a href="#key-value的选择与编码方式" class="headerlink" title="key-value的选择与编码方式"></a>key-value的选择与编码方式</h4><p>论文根据不同形式的先验知识，提出了key-value不同的编码方式：</p><ul><li>知识库三元组。三元组形式为”subject-relation-object”，将”subject-relation”作为寻址的key，将”object”作为记忆的value。</li><li>sentence level。直接将句子的词袋向量表示作为key和value，key和value是一样的。每个memory slot存一个句子。</li><li>window level。以大小为W的窗口对文档进行分割（只保留中心词为实体的窗口），将单个窗口内的词作为寻址的key，将窗口的中心词作为value。</li></ul><h3 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://zhuanlan.zhihu.com/c_129532277" target="_blank" rel="noopener">记忆网络-Memory Network</a></li><li><a href="https://jhui.github.io/2017/03/15/Memory-network/" target="_blank" rel="noopener">Memory network (MemNN) &amp; End to end memory network (MemN2N), Dynamic memory network</a></li><li><a href="http://thespermwhale.com/jaseweston/icml2016/" target="_blank" rel="noopener">Memory Networks for Language Understanding, ICML Tutorial 2016</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍Memory Networks。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Neural Turing Machines" scheme="http://yoursite.com/tags/Neural-Turing-Machines/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>attention? attention!</title>
    <link href="http://yoursite.com/2019/07/23/attention-attention/"/>
    <id>http://yoursite.com/2019/07/23/attention-attention/</id>
    <published>2019-07-23T08:20:18.000Z</published>
    <updated>2019-08-28T09:38:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>读了博主<a href="https://lilianweng.github.io/lil-log/contact.html" target="_blank" rel="noopener">Weng, Lilian</a>的文章<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention? attention!</a>，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。<br>attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。</p><a id="more"></a><h3 id="why-we-need-attention-从seq2seq模型谈起"><a href="#why-we-need-attention-从seq2seq模型谈起" class="headerlink" title="why we need attention?从seq2seq模型谈起"></a>why we need attention?从seq2seq模型谈起</h3><p><strong>seq2seq模型</strong>与14年提出(<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever, et al. 2014</a>)，实现输入序列(source sequence)到输出序列(target sequence)的映射，这两个序列的长度都是可变的。序列到序列映射的任务包括机器翻译、问答系统、对话系统、摘要生成等。</p><p>用数学语言来定义序列到序列的任务，给定输入序列(source sequence) $X = \lbrace{ x_1,x_2,…,x_n \rbrace}$，需要生成输出序列(target sequence) $Y = \lbrace{ y_1,y_2,…,y_m \rbrace}$，其中source sequence长度为$n$,target sequence长度为$m$。</p><p><strong>seq2seq模型</strong>基于encoder-decoder框架，包括2个部分：</p><ol><li><p><strong>encoder</strong>将source sequence编码（映射）为一个固定维度的向量表示(context vector,或称为sentence embedding)，我们希望这个向量表示可以很好的表示source sequence的意思。<br> encoder可以采用卷积神经网络CNN，也可以采用循环神经网络RNN，但用的更多的效果也更好的还是RNN。通常使用<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM 或 GRU</a>。<br> encoder RNN的隐藏状态更新公式为：$$\begin{gather}h_t = f(h_{t-1},x_t)\end{gather}$$其中$h_t$为RNN在时间步t的隐藏状态，f为LSTM 或GRU.<br> 对于长度为n的source sequence，一个词接一个词地输入RNN后，可以得到n个隐藏状态$(h_1,h_2,…,h_n)$，通常将最后一个时间步最后一个词对应的隐藏状态$h_t$作为source sequence的向量表示，也就是context vector，记为$c$。</p></li><li><p><strong>decoder</strong>根据source sequence的向量表示context vector，来一个词一个词的生成target sequence。<br> decoder采用单向RNN，decoder RNN隐藏状态的更新公式为:$$\begin{gather}s_t = f(s_{t-1},y_{t-1},c)\end{gather}$$其中$s_t$为decoder在时间步t的隐藏状态，$y_{t-1}$为target sequence中的上一个词，在train阶段，$y_{n-1}$为真实target sequence中的上一个词，在infer阶段，$y_{t-1}$为预测输出的上一个词；c为context vector。<br> 时间步t，隐藏状态$s_t$再经过线性层和softmax得到在词表上的概率分布，将概率最大的词作为prediction word $y_t$。迭代循环直到输出整个target sequence。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/43.png" alt="Fig.1. seq2seq模型的框架图" title>                </div>                <div class="image-caption">Fig.1. seq2seq模型的框架图</div>            </figure><p>我们可以看到当生成不同的$y_t$时，所依据的context vector都是固定不变的。固定的context vector有一个缺点是：当encoder编码完整个source sequence时，会偏向于最近的词，而遗忘了距离更远的最开始的一些词。<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">(Bahdanau et al., 2015)</a>提出了attention机制来解决这个问题。</p><h3 id="attention机制-born-for-Translation"><a href="#attention机制-born-for-Translation" class="headerlink" title="attention机制:born for Translation"></a>attention机制:born for Translation</h3><p>attention机制最先在机器翻译(neural machine translation,NMT)任务上提出。从解决长期依赖问题的角度，attention可以实现长距离的记忆；从注意力的角度，attention机制可以实现对齐(alignment)，用更多的注意力关注到相关的部分，而忽略或低注意力关注到不相关的部分。</p><p>上文中提到，在生成不同的$y_t$时，直接将encoder最后一个时间步的隐藏状态$h_n$作为固定context vector。不同于这种方法，attention机制将所有encoder隐藏状态$\lbrace{ h_1,h_2,…,h_n }\rbrace$的加权和作为context vector，这样在每个时间步t生成$y_t$时，所依据的context vector都是专门针对于$y_t$的。<br>一方面，context vector可以获取到所有隐藏状态，也就是整个source sequence的信息，这样就可以实现长距离的记忆。另一方面，source sequence 与target sequence之间的语义对齐(aligenment)是也是通过context vector实现的。在计算时间步t生成$y_t$对应的context vector $c_t$的计算需要三个部分的信息：</p><ul><li>所有的encoder隐藏状态： $\lbrace{ h_1,h_2,…,h_n }\rbrace$</li><li>上个时间步t-1的decoder 隐藏状态： $s_{t-1}$</li><li>source与target之间的alignment.<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/44.png" alt="Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)" title>                </div>                <div class="image-caption">Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)</div>            </figure></li></ul><h4 id="attention的数学定义"><a href="#attention的数学定义" class="headerlink" title="attention的数学定义"></a>attention的数学定义</h4><p>在计算时间步t生成$y_t$对应的context vector $c_t$时，encoder的所有隐藏状态为 $\lbrace{ h_1,h_2,…,h_n }\rbrace$，时间步t-1的decoder隐藏状态为 $s_{t-1}$，decoder RNN的隐藏状态更新公式变为：$$\begin{gather}s_t = f(s_{t-1},y_{t-1},c_t)\end{gather}$$ context vector $c_t$为encoder hidden state的加权和：<br>$$c_t = \sum_{i=1}^{n}\alpha_{t,i}h_i$$ $$\alpha_{t,i} = softmax(\beta_{t,i}) = \frac{exp(\beta_{t,i})}{\sum_{j = 1}^{n}exp(\beta_{t,j})}$$ $$\beta_{t,i} = score(s_{t-1},h_i)$$<br>其中权重$\alpha_{t,i}$是时间步t生成$y_t$与隐藏状态$h_i$之间的score，从某种意义上说，$h_i$可以看作是$x_i$的表示，也可以看作是$\lbrace{x_1,x_2,…,x_{i}}\rbrace$的表示。因此，$\alpha_{t,i}$可以看作是$y_t$与$x_i$之间联系（相关性）的score。所有权重$\lbrace{\alpha_{t,1},\alpha_{t,2},…,\alpha_{t,n}}\rbrace$衡量了生成$y_t$时应该如何关注到所有的encoder hidden state。</p><p>score()为打分函数，有多种计算方法，下文会详细介绍。在<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015.</a>中，score()采用前馈神经网络，采用非线性激活函数$tanh()$,score()的数学形式为：$$score(s_{t},h_i) = v_a^\top tanh(W_a[s_t;h_i])$$<br>其中$v_a,W_a$是可训练参数。<br>attention权重可视化矩阵很直观地表明了source words与target words之间的关联关系:</p><div align="center"><img src="/images/45.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.3.来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)</font></div><h3 id="各种attention机制"><a href="#各种attention机制" class="headerlink" title="各种attention机制"></a>各种attention机制</h3><h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><p>下表总结了使用比较广泛的attention机制，及其对应的alignment score function。</p><table class="table table-bordered table-striped table-condensed">   <tr>      <th width="25">名字</th>      <th>alignment score funtion</th>      <th width="25">来源</th>   </tr>   <tr>      <td>content-based attention</td>      <td>$score(s_t,h_i) = cosine(s_t,h_i)$</td>      <td><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves2014</a></td>   </tr>   <tr>      <td>concat/additive</td>      <td>$score(s_{t},h_i) = v_a^\top tanh(W_a[s_t;h_i])$</td>      <td><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a></td>   </tr>   <tr>      <td>location-based</td>      <td>$\alpha_{t,i} = softmax(W_as_t)$<br><font color="grey" size="2">将alignment简化为只依赖于target position</font></td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>general</td>      <td>$score(s_{t},h_i) = s_t^\top W_ah_i$</td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>dot-product</td>      <td>$score(s_{t},h_i) = s_t^\top h_i$<br><font color="grey" size="2">note:当general attention的$W_a$为单位矩阵时，就退出为dot-product attention</font></td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>scaled <br>dot-product(*)</td>      <td>$score(s_{t},h_i) = \frac{s_t^\top h_i}{\sqrt{n}}$<br><font color="grey" size="2">note:跟dot-product attention很像，n是encoder hidden state $h_i$的维度</font></td>      <td><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani2017</a></td>   </tr></table>(*)scaled dot-product attention机制添加了比例因子$/frac{1}{/sqrt{n}}$，动机是：对于softmax()函数，当输入很大时，对应的梯度很小（梯度逐渐消失），难以进行高效的优化和学习。因此，添加比例因子可以减小$score(s_t,h_i)$。<p>下表列出了更广范畴上的attention机制。</p><table class="table table-bordered table-striped table-condensed">   <tr>      <th width="25">名字</th>      <th>定义</th>      <th width="25">来源</th>   </tr>   <tr>      <td>self attention(&)</td>      <td><font color="grey" size="2">将input sequence的不同部分联系起来，只用到input sequence本身，而不用target sequence。<br>可以使用上表中的所有score function，只要将target sequence替换为input sequence即可。</font></td>      <td><a href="https://arxiv.org/pdf/1601.06733.pdf" target="_blank" rel="noopener">Cheng2016</a></td>   </tr>   <tr>      <td>global/soft attention</td>      <td><font color="grey" size="2">context vector是整个input sequence的加权和，注意到整个input sequence</font></td>      <td><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu2015</a></td>   </tr>   <tr>      <td>local/hard attention</td>      <td><font color="grey" size="2">context vector是局部input sequence的加权和，注意到局部input sequence</font></td>      <td><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu2015</a>，<br><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr></table>(&)self-attention在一些论文中也被称为intra-attention.<h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p>self-attention,最先在<a href="https://arxiv.org/pdf/1601.06733.pdf" target="_blank" rel="noopener">Cheng2016</a>提出称为”intra-attention”，后来在大作<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">attention is all you need</a>中发挥了更大的影响力。self-attention将同一个sequence的不同位置的tokens联系起来，建模tokens之间的关系，计算这个sequence的向量表示。[Cheng2016]提出self-attention的动机是什么呢？</p><p>我们先看以下LSTM的局限。LSTM在编码sequence的向量表示时，隐藏状态更新公式为：$$h_t = f(h_{t-1},x_t)$$从这个更新公式可以看到：在给定$h_t$的条件下，$h_{t+1}$与之前的状态$\lbrace{h_1,h_2,…,h_{t-1}}\rbrace$及之前的tokens $\lbrace{x_1,x_2,…,x_t}\rbrace$是条件独立的。LSTM的潜在假设是当前状态$h_t$包含了之前所有tokens的信息，这相当于假设LSTM有无限大的memory，这个假设实际上是不成立的。实际上LSTM会偏向于更近的tokens，而逐渐遗忘距离更远的tokens。另一方面，LSTM在编码token的隐藏状态时，没有建模tokens之间的关系。而这恰恰就是self-attention要解决的问题，也就是self-attention的核心思想：在计算sequence的向量表示时，引入tokens之间的关系。</p><p>接下来看self-attention的数学表示。对于sequence $\lbrace{x_1,x_2,…,x_n}\rbrace$，每个token $x_t$分别对应一个hidden vector 和memory vector。当前的memory tape $C_{t-1} = \lbrace{c_1,c_2,…,c_{t-1}}\rbrace$，hidden state tape为$H_{t-1} = \lbrace{h_1,h_2,…,h_{t-1}}\rbrace$。self-attention计算$x_t$与$\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$之间的关系：$$\beta_{t,i} = score(x_t,h_i) = v^\top tanh(W_hh_i,W_xx_t,W_{\tilde{h}}\tilde{h_{t-1}})$$ $$\alpha_{t,i} = softmax(\beta_{t,i})  ;i\in[1,t-1]$$</p><p>attention权重$\alpha_{t,i}$是t时间步x_t在之前的tokens $\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$对应的hidden vector上的概率分布。<div align="center"><img src="/images/46.png" width="60%" height="60%"></div></p><div align="center"><font color="grey" size="2">Fig.4.红色表示当前token，蓝色的深浅表示相关程度。<br>来源:[Cheng2016](https://arxiv.org/pdf/1601.06733.pdf)</font></div><p>比较一下self-attention机制与传统attention机制的区别：</p><ul><li>传统的attention机制是将target sequence与source sequence联系起来，attention权重$\lbrace{\alpha_{t,1},\alpha_{t,2},…,\alpha_{t,n}}\rbrace$是在encoder hidden states $\lbrace{h_1,h_2,…,h_n}\rbrace$上的概率分布。而self-attention是将同个sequence不同位置的tokens联系起来，attention权重$\alpha_{t,i}$是t时间步$x_t$在之前的tokens $\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$对应的hidden vector上的概率分布。</li><li>传统的attention机制常与RNN联合使用，在transformer中self-attention可以与RNN解耦开（也就是分开使用），单独用self-attention也可以编码sequence的表示向量。</li></ul><h4 id="soft-vs-hard-attention"><a href="#soft-vs-hard-attention" class="headerlink" title="soft vs hard attention"></a>soft <em>vs</em> hard attention</h4><p><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Show, Attend and Tell,Kelvin Xu2015</a>将attention机制用到了”给图片生成描述”的任务，第一次明确区分了hard attention与soft attention，区分的依据是attention是关注到整张图片，还是图片的局部。</p><ul><li>soft attention：attention关注到整张图片，或者是整个序列。alignment 权重$\alpha_{t,i}$是在整个序列上的概率分布。就像普通的attention一样。<ul><li>好处：模型是可微的。</li><li>坏处：计算量比较大。</li></ul></li><li>hard attention：attention关注到图片的局部，或者是序列的一部分。<ul><li>好处：减少了计算量。</li><li>坏处：模型不可微，需要用更复杂的技术，比如强化学习或者方差缩减来训练模型。<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></li></ul></li></ul><h4 id="global-vs-local-attention"><a href="#global-vs-local-attention" class="headerlink" title="global vs local attention"></a>global <em>vs</em> local attention</h4><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>在NMT任务上提出了global 和local attention的概念。区分的依据是attention是关注到整个序列，还是关注到序列的一部分。</p><ul><li>global attention。 类似于soft attention，关注到整个序列。这里比较下<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>的global attention与<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a>中attention的区别。<ul><li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a>中attention的计算路径是：$s_{t-1} \to \alpha_{t} \to c_t \to s_t$<br>$$\beta_{t,i} = score(s_{t-1},h_i)$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i = 1}^{n}\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = g(y_{t-1},c_t,s_t)$$</li><li><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>的global attention的计算路径是：$s_t \to \alpha_{t} \to c_t \to \tilde{s_t}$<br>$$\beta_{t,i} = score(s_{t},h_i)$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i = 1}^{n}\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$\tilde{s_t} = tanh(W_c[c_t,s_t])$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = softmax(W_s\tilde{s_t})$$</li></ul></li><li>local attention。是soft 与hard attention的结合，关注到序列的一部分。对hard attention进行改进，使得模型可微，训练和计算变得更容易。改进的方法如下：<ol><li>对于时间步t的target token $y_t$先用模型预测，生成一个对齐的位置$p_t$，</li><li>再根据固定窗口大小内$[p_t - D,p_t + D]$的encoder hidden state来计算context vector。D是窗口大小，是按经验定义好的。</li></ol></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/47.png" alt="Fig.5. global and local attenion.<br>来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)" title>                </div>                <div class="image-caption">Fig.5. global and local attenion.<br>来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)</div>            </figure><h3 id="pointer-network"><a href="#pointer-network" class="headerlink" title="pointer network"></a>pointer network</h3><p>对于输出序列的类别数依赖于输入序列的长度的问题，seq2seq模型或神经图灵机不能解决。因为这类问题中，输出的类别数是可变的，而seq2seq模型的decoder只能在固定数目的类别上生成一个概率分布。<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Vinyals2017</a>提出了pointer network（Pr_Net）来解决输出词表可变的问题。pointer network实际上是以attention为基础的。</p><p>我们比较下attention机制与pointer network的区别。<br>记输入序列$X = \lbrace{x_1,…,x_n}\rbrace$,输出序列$Y = {y_1,…,y_m}$，$y_j$是X的位置索引，$y_i \in [1,n] $。<br>encoder的所有hidden state为$\lbrace{h_1,h_2,…,h_n}\rbrace$，decoder在时间步t的隐藏状态为$s_t$，则：</p><ul><li>attention机制用alignment权重来计算context vector：<br> $$\beta_{t,i} = score(s_t,h_i) = v^\top tanh(W_ss_t,W_hh_i); i \in [1,n]$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i=1}^{n}\alpha_{t,i}h_i$$</li><li>pointer network则用alignment权重在作为在输入序列上的概率分布，将输入序列中的token直接复制到输出序列中：<br> $$\beta_{t,i} = score(s_t,h_i) = v^\top tanh(W_ss_t,W_hh_i); i \in [1,n]$$ $$p(y_i|y_{&lt; i},X) = softmax(\beta_{t,i})$$<div align="center"><img src="/images/49.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.6.Pointer Network model<br>来源:[Vinyals2017](https://arxiv.org/abs/1506.03134)</font></div></li></ul><h4 id="pointer-network解决OOV问题"><a href="#pointer-network解决OOV问题" class="headerlink" title="pointer network解决OOV问题"></a>pointer network解决OOV问题</h4><p>什么是OOV（out of vocabulary）问题？在序列（source sequence）到序列（target sequence）的映射问题（对话系统，问答系统）中，会根据训练集语料来构建词表，根据完成$word \to index \to embedding$的向量化表示。而在测试集的source sequence中难免会出现一些词表中没有的词，通常会将这些out of vocabulary的词映射到一个特定的字符”UNK”，而decoder在生成response时也可能生成”UNK”这个特殊字符。这就是OOV问题。</p><p>pointer network是解决OOV问题的有效方法。当source sequence中出现不在词表中的词时，pointer network可以直接将这个生词从输入序列复制到输出序列中。<a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Abigail See2017</a>就用了pointer network来解决OOV问题。</p><p>记时间步t decoder的隐藏状态为$s_t$,对应的context vector为$c_t$,alignment权重为$\alpha_{t,i},i \in [1,n]$</p><ol><li>在词汇表上的概率分布为:$p_{vocab} = softmax(W[s_t,c_t] + b)$</li><li>在输入序列的概率分布为:$p_{ptr} = \alpha_{t,i} = softmax(\beta_{t_i})$</li><li>选择开关为: $p_{gen} = sigmoid(W_ss_t + W_cc_t + W_xx_t + b)$<br>为逻辑回归，取值为[0,1]</li><li>最终在extend vocabulary上的概率分布为:$p(w) = p_{gen}p_{vocab} + (1-p_{gen})p_{ptr}$.<br>当$p_{gen}$为1时，从词汇表中生成word；当$p_{gen}$为0时，将输入序列的词复制到输出序列中。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/48.png" alt="Fig.7.Pointer-generator model.<br>来源：[Abigail See2017](https://arxiv.org/abs/1704.04368)" title>                </div>                <div class="image-caption">Fig.7.Pointer-generator model.<br>来源：[Abigail See2017](https://arxiv.org/abs/1704.04368)</div>            </figure> <p>类似的论文还有：<a href="https://arxiv.org/pdf/1603.06393v3.pdf" target="_blank" rel="noopener">CopyNet,Jiatao Gu2016</a></p><h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">attention is all you need!</a>(Vaswani, et al., 2017)提出了transformer。transformer也是基于encoder-decoder框架的，也可以看作是一个seq2seq模型。但不同于encoder和decoder都采用RNN的seq2seq模型，transformer完全依赖self-attention机制来计算input和output的向量表示，而不使用RNN或CNN。一般来说，attention机制是与RNN联合使用的，transformer把attention和RNN解耦开了，只使用attention机制。</p><p>为什么transformer用self-attention来编码input和output，而不使用RNN呢？<br>一方面，由RNN的更新公式$s_t = f(s_{t-1},x_t)$可以看到，RNN处理序列时是串行计算的，尤其是处理长序列时更费时间。不利于并行化，计算效率低。而transformer采用attention来编码计算向量，可以进行并行化计算，提高计算效率。<br>另一方面，RNN在编码长序列时，随着距离的增大，往往会偏向于最近的部分，而学习不到长期依赖。但self-attention机制不受距离的限制，可以有效地学习到长期依赖。</p><h4 id="scaled-dot-product-attention与key-value-query"><a href="#scaled-dot-product-attention与key-value-query" class="headerlink" title="scaled dot-product attention与key,value,query"></a>scaled dot-product attention与key,value,query</h4><p>transformer的主要组件是<em>multi-heads self-attenion mechanism</em>，这个组件用到了scaled dot-product attention机制。一般地，attention机制将query和(key,value)映射为output，其中query,key,value,output都是vector。output是所有values的加权和，权重是通过计算query与对应key之间的关联度得到。</p><p>具体来说，将什么作为key,value,query？分两种情况，从框图可以直观的看到：</p><ul><li>在encoder-decoder框架中，联系encoder与decoder的attention机制通常将encoder的所有hidden states乘以两个不同的矩阵$W^Q,W^K$分别作为<em>keys</em>和<em>values</em>。将decoder上一个时间步的hidden state作为query。</li><li>在encoder模块，self-attention机制将input词级别的向量表示乘以三个不同的矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算input总的句子级别的向量表示。同样地，在decoder模块中self-attention机制将output词级别的向量表示分别乘以三个不同矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算output总的句子级别的向量表示。</li></ul><p>有了具体的key,value,query后，scaled dot-product attention机制怎么来计算output呢？<br>记query,key,value的矩阵形式分别为Q,K,V。query和key维度为$d_k$,value维度为$d_v$。则output的计算方式为：$$Attention(Q,K,V) = softmax(\frac{QK^\top}{\sqrt{d_k}})V$$</p><p>多种attention机制中，transformer为什么选择采用scale dot-product attention机制呢？<br>最常用的两种attention机制是dot-product和additive attention机制。dot-product attention用点乘来做打分函数，additive attenion将有一层隐藏层的前馈网络作为打分函数。理论上来说，这两种attention的计算复杂度是一样的；但实际上，dot-product attention计算更快，占用内存更小。因为dot-product attention机制可以采用高度优化的矩阵乘法代码。<br>当维度$d_k$较小时，这两种attention机制的效果是差不多的。当维度$d_k$更大时，additive attention的效果要好于dot-product attention。这可能是因为当维度$d_k$变大时，点乘的值变得过大，而softmax()函数在值过大的范围梯度是很小的，类似于梯度消失问题。因此，添加比例因子$\frac{1}{\sqrt{d_k}}$来减小点乘的值。</p><h4 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h4><p>并不是只用一次attention机制，将维度为$d_{model}$的key,value,query映射为output。而是将query,key,value映射到维度为$d_k,d_k,d_v$不同的子向量空间，并行的计算$h$次，分别得到output做concat操作，得到总的output。$h$为head的个数，也就是并行attention layer的层数。有关系$d_{model} = d_v \cdot h$，论文中采用$d_{model} =512,h=8,d_k = d_v = \frac{d_{model}}{h} = 64$ $$MultiHead(Q,K,V) = concat(head_1,head_2,…,head_h)W^O$$ $$where \quad head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$ 其中$W_i^Q \in R^{d_{model} \cdot d_k},W_i^K \in R^{d_{model} \cdot d_k},W_i^V \in R^{d_{model} \cdot d_v},W^O \in R^{hd_{v} \cdot d_{model}}$</p><div align="center"><img src="/images/multi-head-attention.png" width="40%" height="40%"></div><div align="center"><font color="grey" size="2">Fig.7.multi-head scaled dot-product attention</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>encoder将input text编码为基于attention的包含位置信息的向量表示。</p><ul><li>由6个完全相同的层堆叠起来。</li><li>每一层包含两个子层。第一子层是multi-head attention层，第二层是一个简单的全连接层。</li><li>两个子层之间采用<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">残差连接</a>，并进行归一化。这样所有子层的输出都有相同的维度$d_{model} = 512$</li></ul><div align="center"><img src="/images/transformer-encoder.png" width="70%" height="70%"></div><div align="center"><font color="grey" size="2">Fig.9. transformer encoder</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>从encoder output得到总的context vector，并据此生成response。</p><ul><li>与encoder相同，由6个完全相同的层堆叠起来。</li><li>每一层除了encoder中的两个子层外，还插入了一个multi-head layer来在所有encoder output上进行attention操作。</li><li>两个子层之间采用<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">残差连接</a>，并进行归一化。</li><li>第一个multi-head attention sub-layer进行mask操作，mask掉output当前时间步后所有的tokens。防止attention机制看到未来的信息。</li></ul><div align="center"><img src="/images/transformer-encoder.png" width="70%" height="70%"></div><div align="center"><font color="grey" size="2">Fig.10. transformer decoder</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="transformer的总体结构"><a href="#transformer的总体结构" class="headerlink" title="transformer的总体结构"></a>transformer的总体结构</h4><ul><li>input和output都先经过一个embedding layer得到各自的向量表示，维度为$d_{model} = 512$</li><li>由于self-attention不能像RNN一样自动地编码位置信息，因此需要额外地将位置信息添加到输入。</li><li>在最后decoder的输出外接一个线性层和softmax层。</li></ul><div align="center"><img src="/images/transformer.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.11. transformer的整体框架</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a>  by Weng, Lilian</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;读了博主&lt;a href=&quot;https://lilianweng.github.io/lil-log/contact.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Weng, Lilian&lt;/a&gt;的文章&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;attention? attention!&lt;/a&gt;，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。&lt;br&gt;attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="rnn" scheme="http://yoursite.com/tags/rnn/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>pip安装python模块报错</title>
    <link href="http://yoursite.com/2019/07/12/pip%E5%AE%89%E8%A3%85python%E6%A8%A1%E5%9D%97%E6%8A%A5%E9%94%99/"/>
    <id>http://yoursite.com/2019/07/12/pip安装python模块报错/</id>
    <published>2019-07-12T01:51:58.000Z</published>
    <updated>2019-07-12T02:11:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>在使用<code>pip install</code>命令安装python模块时，报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h3><p>报错信息告诉我们：“不能卸载‘pyyaml’模块，因为这个模块是<code>distutils</code>方式安装的，不能确定哪些文件属于这个模块，因此不能完整地卸载这个模块。”</p><p><a href="https://cloud.tencent.com/developer/section/1371690" target="_blank" rel="noopener">distutils</a>是python最初的模块安装和分发系统，distutils不会保留哪些文件属于哪个安装包的信息，甚至不会保留安装包之间的依赖关系。直接使用<code>distutils</code>的方式已经被淘汰，取而代之的是<a href="https://setuptools.readthedocs.io/en/latest/" target="_blank" rel="noopener">setuptools</a>.<br>    所谓模块的分发，就是开发者打包并发布自己的模块，供其他人使用。</p><p>这样我们就知道了，因为<code>pyyaml</code>模块时通过<code>distutils</code>方式安装的，因此不能明确文件与包之间的隶属关系，不能正确卸载。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>使用下面的命令忽略已安装的模块，强制安装和更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install &lt;package-name&gt; --ignore-installed &lt;pyyaml&gt; --upgrade</span><br></pre></td></tr></table></figure><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.jianshu.com/p/94caf01dd9a6" target="_blank" rel="noopener">强制安装和更新</a></li><li><a href="https://cloud.tencent.com/developer/ask/196670" target="_blank" rel="noopener">如何在Windows操作系统中升级/卸载distutils软件包（PyYAML）？</a></li><li><a href="https://docs.python.org/zh-cn/3/installing/index.html" target="_blank" rel="noopener">python官方手册-安装python模块</a></li><li><a href="https://cloud.tencent.com/developer/section/1371690" target="_blank" rel="noopener">setuptools与distutils</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在使用&lt;code&gt;pip install&lt;/code&gt;命令安装python模块时，报错：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Cannot uninstall &amp;apos;PyYAML&amp;apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>NAACL2019-对话系统</title>
    <link href="http://yoursite.com/2019/07/10/NAACL2019-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2019/07/10/NAACL2019-对话系统/</id>
    <published>2019-07-10T12:55:17.000Z</published>
    <updated>2019-07-21T15:03:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。<br>NAACL2019的会议列表链接：<a href="https://naacl2019.org/program/accepted/" target="_blank" rel="noopener">https://naacl2019.org/program/accepted/</a></p><a id="more"></a><h3 id="《Evaluating-Coherence-in-Dialogue-Systems-using-Entailment》"><a href="#《Evaluating-Coherence-in-Dialogue-Systems-using-Entailment》" class="headerlink" title="《Evaluating Coherence in Dialogue Systems using Entailment》"></a>《Evaluating Coherence in Dialogue Systems using Entailment》</h3><p>【链接】<a href="https://arxiv.org/abs/1904.03371" target="_blank" rel="noopener">https://arxiv.org/abs/1904.03371</a><br>【代码】<a href="https://github.com/nouhadziri/DialogEntailment" target="_blank" rel="noopener">https://github.com/nouhadziri/DialogEntailment</a></p><p>加拿大阿尔伯塔大学发表的论文。论文提出了一种评估对话系统生成回复好坏的指标。<br>这篇论文的想法来源于：发表在ACL2019上的论文<a href="https://arxiv.org/abs/1811.00671" target="_blank" rel="noopener">《Dialogue Natural Language Inference》</a>提出利用NLI(natural language inference)任务来提高对话系统生成回复的一致性。<br>本文的作者则想到用NLI任务来评估对话系统生成回复的好坏。具体地，论文用了BERT<a href="https://arxiv.org/abs/1609.06038" target="_blank" rel="noopener">[Devlin et al., 2018]</a>和The Enhanced Sequential Inference Model(ESIM)<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">[Chen et al., 2016]</a> 这两种方法来训练NLI模型。另外论文还公开了一个用于NLI任务的数据集。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;NAACL2019的会议列表链接：&lt;a href=&quot;https://naacl2019.org/program/accepted/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://naacl2019.org/program/accepted/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
  </entry>
  
</feed>
