<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>spring&#39;s Blog</title>
  
  <subtitle>游龙当归海，海不迎我自来也。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-07-14T08:55:34.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>spring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文本相似度的计算方法</title>
    <link href="http://yoursite.com/2020/07/13/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2020/07/13/文本相似度的计算方法/</id>
    <published>2020-07-13T07:23:46.000Z</published>
    <updated>2020-07-14T08:55:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>在nlp任务中，我们常常需要判断两个文本的相似程度，计算这两个文本的相似度。比如，在文本聚类任务中，需要将相似度高的文本聚到同一个簇；在文本预处理过程中，基于文本相似度把重复的文本过滤掉；在检索式对话系统中，通过计算用户的query与数据库中的query的相似度，来选择回复。<br>文本相似度计算 有2个关键组件：文本表示模型 和 相似度度量方法。文本表示模型 负责将文本表示为可计算的数值向量，也就是提供特征； 相似度度量方法负责基于数值向量计算文本之间的相似度。</p><a id="more"></a><h3 id="特征构建方式"><a href="#特征构建方式" class="headerlink" title="特征构建方式"></a>特征构建方式</h3><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>TF-IDF(term frequency-inverse document frequency) 是一种数学统计方法，反映了在一个语料集中，某一个词对一篇文档的重要程度，也就是这个词在多大程度上反映了这篇文档的特征。<br>tf-idf分数常常作为信息检索中的权重因子，tf-idf是使用最广的词语加权方案之一。tf-idf分数与一个词出现在一篇文档中的次数成正比，与整个语料库中包含这个词的文档数成反比。给定一个用户查询(query)，搜索引擎根据相关度对文档进行打分排序，tf-idf加权方案就是搜索引擎实现这个过程的一个重要手段。<br>在NLP任务的预处理过程中，我们经常会使用<code>停用词表</code>的方式过滤掉（的、是、在）这些没有实际意义的功能性词语。tf-idf给最常见的词(是、在、的)给予很小的权重分数，较常见的词分配较小的权重，较少见的词分配较大的权重。某种意义上，这个过程实现了跟<code>停用词表</code>类似的功能。</p><p><strong>词频(term frequence)</strong><br>假设对于一个用户查询”中国的蜜蜂养殖”，要根据与query的相关性，对一个文档集合中的文档进行打分排序。我们把这个query分词为”中国”、”的”、”蜜蜂”、”养殖”。最简单的方法是先找出包含这四个词的文档(至少包含其中一个词)。为了进一步对这些文档进行排序，我们可以计算每个词出现在文档中的次数（词频，term frequency），一个词的权重与这个词的词频成正比。<br>词频TF(term frequency)的计算方式有多种。常见的有两种:</p><ol><li>一个词出现在文档中的次数。$$词频TF = 某个词出现在文档中的次数$$</li><li>考虑到文档的长度不同，为了便于不同文章之间的比较，对‘词频’进行归一化。$$词频TF = \frac{某个词出现在文档中的次数}{文档的总词数}$$ 或者 $$词频TF = \frac{某个词出现在文档中的次数}{文档中出现次数最多的词的出现次数}$$</li></ol><p><strong>逆文档频率(inverse document frequency)</strong><br>由于常见词 ‘的’在文档中的出现频率会比较大，词频(term frequency)会错误地给 使用”的”次数更多的文档更大的权重分数，而没有给 “中国”、”蜜蜂”、”养殖”给予足够的重视。像”的”这样的常用词没有实际的意义，不是好的关键词来区分文档的相关程度，而少见词”中国”、”蜜蜂”、”养殖”可以更好地反映文档的相关程度。因此，引入逆文档频率(inverse document frequency)来给常见词分配更小的权重，给少见词分配更大的权重。<br>逆文档频率的计算方式如下: $$逆文档频率IDF = \frac{语料库中的文档数}{包含该词的文档数 + 1}$$ 一个词越常见，那么分母越大，逆文档频率越小。分母之所以要加1，是为了避免分母为0。</p><p><strong>词频-逆文档频率(TF-IDF)</strong><br>TF-IDF定义为 词频和逆文档频率这两个统计量的乘积。$$TF-IDF = 词频TF \prod 逆文档频率IDF$$ 一个词的tf-idf分数反映了这个词对文档的重要性，这个词在多大程度上反映了这篇文档的特征。<br>回到”中国的蜜蜂养殖”这个检索问题上，对于每个文档，分别计算”中国”、”的”、”蜜蜂”、”养殖”这四个词的tf-idf分数，并求和作为这篇文档总的分数。用同样的方法对所有包含这四个词的文档打分，并排序。这样就得到了query”中国的蜜蜂养殖”的查询结果。 TF-IDF这种词语加权方法 常常与<a href="https://spring-quan.github.io/2020/06/22/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/" target="_blank" rel="noopener">倒排索引</a>方法联合使用，来实现文本检索。</p><blockquote><p>参考链接：<br><a href="https://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词</a><br><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener">TF-IDF_wikipedia</a></p></blockquote><h3 id="距离的度量方式"><a href="#距离的度量方式" class="headerlink" title="距离的度量方式"></a>距离的度量方式</h3><h4 id="jaccard相似度"><a href="#jaccard相似度" class="headerlink" title="jaccard相似度"></a>jaccard相似度</h4><p>jaccard相似度系数(jaccard similarity coefficient)，也称为并交比(intersection over union)，可以用来衡量两个有限样本集合之间的相似度。jaccard相似度定义为 两个集合交集大小 与并集大小的比例: $$J(A,B) = \frac{|A \cap B|}{|A \cup B|} = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}$$<br>jaccard相似度的思想很简单：如果两个集合共有的元素越多，那么这两个集合就越相似。<br>如果集合$A$和集合$B$完全重合，则$J(A,B) = 1$。jaccard相似度的取值范围是: $0 \le J(A,B) \le 1$。<br>jaccard距离(jaccard distance)用于衡量样本集之间的不相似度，定义为1减去jaccard相似度。$$d_{J}(A,B) = 1 - J(A,B) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|}$$<br>与欧式距离、余弦相似度等距离度量方式相比，jaccard相似度的优点在于：不需要把文本表示成数值化的向量表示，就可以计算两个文本之间的相似度。<br>在文本聚类任务中，可以先通过jaccard相似度来计算数据集中文本之间的相似度，再用<a href="https://github.com/networkx/networkx" target="_blank" rel="noopener">networkx</a>来构建无向图；在无向图的基础上可以用<a href="https://github.com/taynaud/python-louvain" target="_blank" rel="noopener">louvain算法</a>或<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html" target="_blank" rel="noopener">DBSCAN算法</a>来对文本进行聚类。 需要注意的是，<code>louvain算法</code>的无向图边的权重应该是 文本之间的jaccard相似度，来计算模块度。而<code>DBSCAN算法</code>的无向图边的权重应该是 文本之间的jaccard距离，来计算$\epsilon$-邻域内样本点的数量。</p><blockquote><p>参考链接：<br><a href="https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8D%A1%E5%B0%94%E6%8C%87%E6%95%B0" target="_blank" rel="noopener">wikipedia-雅卡尔系数</a></p></blockquote><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/88938220" target="_blank" rel="noopener">常见文本相似度计算方法简介–哈工大李鹏宇</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在nlp任务中，我们常常需要判断两个文本的相似程度，计算这两个文本的相似度。比如，在文本聚类任务中，需要将相似度高的文本聚到同一个簇；在文本预处理过程中，基于文本相似度把重复的文本过滤掉；在检索式对话系统中，通过计算用户的query与数据库中的query的相似度，来选择回复。&lt;br&gt;文本相似度计算 有2个关键组件：文本表示模型 和 相似度度量方法。文本表示模型 负责将文本表示为可计算的数值向量，也就是提供特征； 相似度度量方法负责基于数值向量计算文本之间的相似度。&lt;/p&gt;
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="jaccard相似度" scheme="http://yoursite.com/tags/jaccard%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>常见的文本聚类算法</title>
    <link href="http://yoursite.com/2020/07/06/%E5%B8%B8%E8%A7%81%E7%9A%84%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2020/07/06/常见的文本聚类算法/</id>
    <published>2020-07-06T05:17:24.000Z</published>
    <updated>2020-07-13T07:48:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>文本聚类就是把一些没有标签的，但有相同特征的数据聚在一起。聚类模型将样本划分为若干个簇(cluster)，每个簇对应一些潜在的概念或类别。</p><a id="more"></a><h3 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h3><p>K-means聚类是最重要的聚类方法之一。我们假设已经将文档(document)表示为长度归一化的向量表示，也就是将文本数据映射到了欧式空间里。<br>K-means聚类算法的主要思想是：对于每个簇(cluster)，选出一个中心点(cluster center)，使得该cluster内的点到该簇<code>中心点</code>的距离小于到其他簇的<code>中心点</code>的距离。我们将簇$w$的cluster center $\vec{\mu}$ 定义为簇的形心: $$\vec{\mu}(w) = \frac{1}{|w|}\sum_{\vec{x} \in w}\vec{x} \tag{1} \label{1}$$ 其中，$|w|$表示簇$w$内 点 的个数。<br>如何衡量cluster center是否很好地表示了整个cluster内的点呢？我们用cluster内每个点到cluster center的残差平方和(residual sum of squares, RSS)来衡量。对于第$k$个cluster：$$RSS_{k} = \sum_{\vec{x} \in w_{k}}|\vec{x} - \vec{\mu}(w_k)|^{2} \tag{2}$$ 对于整个数据集来说: $$RSS = \sum_{k=1}^{K}RSS_{k} = \sum_{k=1}^{K} \sum_{\vec{x} \in w_{k}} |\vec{x} - \vec{\mu}(w_{k})|^{2} \tag{3} \label{RSS}$$ 上式$\eqref{RSS}$就是K-means聚类算法的目标函数, 我们的目标是最小化该目标函数。<br>接下来介绍K-means算法的优化迭代过程。第一步是随机选择$K$个点作为cluster center，接下来K-means模型迭代执行两步来最小化目标函数，直到满足停止条件：(1) 固定cluster center $\vec{\mu}(w_k)$，把数据集中的每个点重新分配到最近的cluster center所属的簇中；(2)根据重新分配后簇内的点，重新计算cluster center $\vec{\mu}(w_k)$。我们可以用以下停止条件:</p><ol><li>固定的迭代次数$maxstep$。限制最大迭代次数 可以限制模型的迭代时间，但有可能由于模型迭代次数不够，导致聚类效果不好。</li><li>一直迭代到每个cluster内的点不再变化。这样会有很好的聚类效果，但可能导致迭代时间太长。</li><li>一直迭代到每个cluster的cluster center不再变化。</li><li>目标函数减小到某个阈值时停止迭代， 这个阈值保证了聚类模型有了不错的聚类效果。</li><li>目标函数的减少幅度小于某个很小的阈值，这表示聚类模型 接近于收敛了。</li></ol><p>图1展示了K-means聚类算法的伪代码。</p><div align="center"><img src="/images/kmeans_1.png" width="70%" height="70%"></div>  <div align="center"><font color="grey" size="2">Fig.1</font></div>图2是一个K=2的K-means算法的例子。<div align="center"><img src="/images/kmeans_2.png" width="80%" height="80%"></div>  <div align="center"><font color="grey" size="2">Fig.2</font></div><h4 id="目标函数的单调递减性"><a href="#目标函数的单调递减性" class="headerlink" title="目标函数的单调递减性"></a>目标函数的单调递减性</h4><p>接下来，我们证明: 每次迭代中，目标函数$RSS$会逐渐减小，从而K-means模型逐渐收敛。</p><ol><li>在重新分配 点 的过程中，由于把每个点分配到 离它最近的 cluster center对应的簇中，从而目标函数$RSS$是减小的。</li><li>在重新计算cluster center的过程中，当cluster center为cluster的形心时，$RSS_k$达到它的最小值。证明如下: $$RSS_k(\vec{v}) = \sum_{\vec{x} \in w_k}|\vec{x} - \vec{v}|^2 = $\sum_{\vec{x} \in w_k} \sum_{m=1}^{M}(x_m - v_m)^2 \tag{4} \label{4}$$ 其中$\vec{x}$和$\vec{v}$是一个$M$维的向量，$x_m, v_m$分别是其对应向量的第$m$个值，令该式$\eqref{4}$的偏导数等于0：$$\frac{\partial{RSS_k(\vec{v})}}{\partial{v_m}} = \sum_{\vec{x} \in w_k}2(v_m - x_m) = 0$$ $$v_m = \frac{1}{|w_k|}\sum_{\vec{x} \in w_k}x_m \tag{5} \label{5}$$ 式子$\eqref{5}$是形心公式$\eqref{1}$的逐点表示形式。当偏导数=0时，即$\vec{v} = \vec{\mu{w_k}}$时，式子$\eqref{4}$取得最小值。 </li></ol><p>需要注意的是，K-means算法不能保证得到目标函数的最小值，这与初始中心点的选择有关。选择不同的初始中心点，可能会得到不同的聚类结果。如果数据集中包含多个离群点(远离其他的点，从而不适合分到任何一个簇中)，这样的离群点被选为初始中心点，在迭代过程中不会有其他的任何一个点被分配到这个簇中，迭代结束后我们会得到一个只有一个点的簇(singleton cluster)。即使还有其他的聚类方法，可以得到更小的目标函数值。<br>因此，初始中心点的选择是个重要的问题。有以下几种方法：</p><ol><li>把离群点从初始中心点的候选集中排除。</li><li>选择不同的初始中心点进行多次聚类，选择目标函数值最小的聚类结果。</li></ol><h4 id="K-means的时间复杂度"><a href="#K-means的时间复杂度" class="headerlink" title="K-means的时间复杂度"></a>K-means的时间复杂度</h4><p>假设数据集有$N$个点，每个点的向量表示的维度为$M$，聚类的簇的个数为$K$。K-means算法的大多数时间花在计算向量之间的距离，每次计算的时间复杂度是$\Theta(M)$。在重新分配点的步骤中，需要进行$KN$次距离计算。每次迭代的时间复杂度为$\Theta(KNM)$。进行$I$次迭代的时间复杂度为$\Theta(IKNM)$。 可以看出: K-means算法的时间复杂度与 迭代次数、 簇的个数、样本数、向量空间的维度线性相关。</p><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html" target="_blank" rel="noopener">K-means</a></li></ul><h3 id="Louvain社区发现算法"><a href="#Louvain社区发现算法" class="headerlink" title="Louvain社区发现算法"></a>Louvain社区发现算法</h3><p>Louvain算法是基于模块度(modularity)的社区发现算法，能够发现层次性的社区结构，其优化目标是最大化整个社区网络的模块度。</p><h4 id="模块度-Modularity"><a href="#模块度-Modularity" class="headerlink" title="模块度(Modularity)"></a>模块度(Modularity)</h4><p>模块度是评估一个社区网络划分好坏的度量方法。模块度的物理意义是：模块度越大，同一个社区内的节点之间联系更加紧密，不同社区之间的节点联系更加松散。对于没有权重的无向图，模块度的取值范围是[-1/2,1]。如果 社区内 <code>边的数量</code>超过随机情况下边的数量，那么模块度是一个正值。</p><p>模块度的定义是 社区(group, or cluster, or community)内节点之间的实际连边数 与随机情况下连边数 的差值。如何理解随机情况下的连边数呢？对于一个有$n$个节点(nodes)，$m$条边(edges)的网络。其中对于节点$v$有$k_v$条边(我们把节点$v$的边数$k_v$成为节点$v$的度)。接下来我们要在满足网络内的节点数$n$、边数$m$、以及每个节点$v$的边数$k_v$保持不变的情况下，生成一个随机网络。具体地，把每条边分成两半(我们称每半条边为一个 stub)，每半条边 与 网络中其他的 任意半条边(stub)连接起来，这样我们就得到了一个完全随机的网络。<br>考虑两个节点有$k_v$条边的$v$和$k_u$条边的$u$。在$2m$半条边中，一个半条边stub连接到另一个半条边的概率是$\frac{1}{2m-1}$，则有$k_v$条半边的节点$v$和有$k_u$条半边的节点$u$的边数为$$\frac{k_vk_u}{2m-1}\approx\frac{k_vk_u}{2m}$$当$m$很大时，可以取近似值。则两个节点之间的实际 连边数与随机情况下 连边数的差值为:$$A_{ij} - \frac{k_i,k_j}{2m} \tag{6} $$<br>在$2m$个节点对上求和，可以得到整个网络的模块度$Q$： $$Q = \frac{1}{2m}\sum_{i,j}[A_{ij} - \frac{k_ik_j}{2m}]\delta(c_i,c_j) \tag{7} \label{7}$$<br>$$ \delta(x,y) =<br>\begin{cases}<br>1; when x==y  \<br>0; when x!= y<br>\end{cases} $$</p><blockquote><p>$A_{ij}$表示节点$i$和节点$j$之间边的权重，当网络是无权图是，所有边的权重可以看作是1；<br>$k_i = \sum_jA_{ij}$表示节点$i$相连的边的权重之和(度数)；<br>$c_i$表示节点$i$所属的社区；<br>$m = \frac{1}{2}\sum_{ij}A_{ij}$表示所有边的权重之和（边的数目）。</p></blockquote><p>对式子$\eqref{7}$进行简化，$$Q = \frac{1}{2m}\sum_{ij}[A_{ij} - \frac{k_ik_j}{2m}]\delta(c_i,c_j) $$ $$ = \frac{1}{2m}[\sum_{ij}A_{ij} - \frac{\sum_ik_i\sum_jk_j}{2m}]\delta(c_i,c_j) $$ $$= \frac{1}{2m}\sum_{c}[\sum_{in} - \frac{(\sum_{total})^2}{2m}] \tag{8} \label{8}$$ 其中$\sum_{in}$表示社区$c$内部 边的权重之和(数目，对于无权图），$\sum_{total}$表示所有与社区$c$内的节点相连的边的权重之和(数目，对于无权图)。<br>对式子$\eqref{8}$进一步简化: $$Q = \sum_{c}[\frac{\sum_{in}}{2m} - (\frac{\sum_{total}}{2m})^2]$$ $$=\sum_{c}[e_c - (a_c)^2]$$ 这样模块度可以理解为 社区内部边的权重和 减去 所有与社区内节点相连的边的权重和的平方。对于无权图，模块度为 社区内部的度数 减去 社区内节点的 总度数的平方。 </p><h4 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://www.cnblogs.com/fengfenggirl/p/louvain.html" target="_blank" rel="noopener">模块度与Louvain社区发现算法</a></li><li><a href="https://en.wikipedia.org/wiki/Modularity_(networks)" target="_blank" rel="noopener">维基百科-模块度</a></li><li><a href="https://arxiv.org/abs/0803.0476" target="_blank" rel="noopener">《Fast unfolding of communities in large networks》–louvain算法原论文，2008</a></li></ul><h3 id="DBSCAN密度聚类算法"><a href="#DBSCAN密度聚类算法" class="headerlink" title="DBSCAN密度聚类算法"></a>DBSCAN密度聚类算法</h3><p>DBSCAN(Density-Based Spatial Cluster of Applications with Noise，有噪声的基于密度的聚类方法)是非常典型的基于密度的聚类方法。DBSCAN算法是一种基于样本分布密度的聚类算法，密度聚类算法一般假定可以类别通过样本分布的紧密程度来决定。把紧密相连的点划分到同一个簇。</p><h4 id="一些基本概念"><a href="#一些基本概念" class="headerlink" title="一些基本概念"></a>一些基本概念</h4><p>设数据集为$D = \lbrace{x_1, x_2, …, x_m\rbrace}$。</p><ol><li><strong>1个核心思想：基于密度</strong><br> DBSCAN算法可以找到 数据集中所有 样本紧密分布的区域，并分别将这些紧密的区域作为一个一个的簇。</li><li><strong>2个算法参数</strong><br> DBSCAN算法用 $\epsilon$-邻域 的概念来衡量样本分布的紧密程度，也就是样本密度。对于样本点$x_j \in D$，该样本点的$\epsilon$-邻域内包含 与$x_j$距离不大于$\epsilon$的子样本集，即$N_{\epsilon}(x_j) = \lbrace{x_i|distance(x_i,x_j) \leq \epsilon\rbrace}$，这个子样本集中的样本数为$|N_{\epsilon}(x_j)|$。<ul><li>$\epsilon$-邻域的半径$\epsilon$。</li><li>$MinPoints$ 描述了某一样本半径为$\epsilon$的$\epsilon$-邻域中样本个数的阈值。</li></ul></li><li><strong>3种样本点</strong><ul><li>核心对象(core point)：对于一个样本点$x_j \in D$，如果其$\epsilon$-邻域中至少包含$MinPoints$个样本点，即$|N_{\epsilon}(x_j)| \geq MinPoints$，则$x_j$是核心对象。</li><li>边界点(boundary point)：对于一个样本点$x_j$，它不是核心对象，但它在某个核心对象的$\epsilon$-邻域内，则$x_j$是边界点。</li><li>噪声点(noise point): 或称为离群点(outlier point)。对于一个样本点$x_j$，它不是核心对象，也不在任何一个核心对象的$\epsilon$-邻域内，则$x_j$是噪声点。</li></ul></li><li><strong>样本点之间的4种关系</strong><ul><li>密度直达(directly reachable)：如果样本点$x_i$在核心对象$x_i$的$\epsilon$-邻域内，则称$x_i$由$x_j$密度可达。注意密度直达关系不是对称的，此时不能说$x_j$由$x_i$密度直达。</li><li>密度可达(reachable)：对于$x_i$和$x_j$如果存在一个样本序列$p_1,p_2,…,p_T$，其中$p_1=x_i, p_T=x_j$，且$p_t+1$由$p_t$密度直达，则称$x_i$由$x_j$密度可达。密度可达具有传递性，此样本序列中的传递样本$p_1, p_2, …, p_{T-1}$都是核心对象，只有核心对象才能使得其他样本密度直达。 注意密度可达关系也不满足对称性，这可以由密度直达关系的不对称性得到。</li><li>密度相连(connectedness)：对于样本点$x_i$和$x_j$，如果存在核心对象$x_k$，使得$x_i$和$x_j$均可以由$x_k$密度可达，则称$x_i$与$x_j$密度相连。密度相连关系是满足对称性的。</li><li>密度不相连：不满足密度相连关系的两个样本点$x_i$和$x_j$，属于两个不同的簇，或者其中存在离群点。</li></ul></li></ol><p>如下图所示，图中$MinPoints$为5，红色的点都是核心对象，因为其$\epsilon$-邻域内至少有5个样本点(包括核心对象本身)，圆圈内的黑点都是边界点，圆圈以外的黑点都是噪声点。对于某个红点来说，其对应的圈内的其他点都是密度直达的。途中用绿色箭头连起来的核心对象组成了密度可达的样本序列，这些密度可达的样本序列的$\epsilon$-邻域内所有的样本点都是密度相连的。</p><div align="center"><img src="/images/dbscan_1.png" width="90%" height="90%"></div>  <div align="center"><font color="grey" size="2">Fig.3 source: https://www.cnblogs.com/pinard/p/6208966.html</font></div><h4 id="DBSCAN密度聚类思想"><a href="#DBSCAN密度聚类思想" class="headerlink" title="DBSCAN密度聚类思想"></a>DBSCAN密度聚类思想</h4><p>DBSCAN的聚类定义为: 把所有密度相连的样本点划分到同一个簇。<br>DBSCAN密度聚类得到的簇有以下特点:</p><ul><li>簇内至少有一个核心对象，也就是至少有$MinPoints$个样本点。</li><li>簇内所有的点都是密度相连的。并且如果某个样本点与某个簇内的其他点密度相连，那么这个样本点属于这个簇。</li><li>离群点(噪音点)跟所有簇内的点都不是密度相连的，因此，噪音点不属于任何一个簇。</li></ul><p>这里需要考虑一个问题，DBSCAN聚类算法会得到有重叠的簇吗？也就是说会有同一个点属于多个簇的情况吗？答案是不会。对于某个样本到两个核心对象的距离都小于$\epsilon$，但这两个核心对象不是密度直达的，不属于同一个簇，怎么界定这个样本点的类别呢？DBSCAN会采取”先来后到”的方式，先进行聚类的类别簇会把这个样本标记为它的类别。 $\tag{9} \label{two}$</p><h4 id="DBSCAN聚类算法"><a href="#DBSCAN聚类算法" class="headerlink" title="DBSCAN聚类算法"></a>DBSCAN聚类算法</h4><p>输入：样本集$D = \lbrace{x_1, x_2, …, x_m}\rbrace$， 邻域参数($\epsilon, MinPoints$)<br>输出：簇的划分$C$</p><ol><li>遍历所有的样本点，找出所有的核心对象。也就是$\epsilon$-邻域内样本数不小于$MinPoints$的样本点。</li><li>先忽略所有的非核心对象，只考虑核心对象，将所有密度可达的核心对象划分到一个簇。</li><li>再考虑非核心对象，将每个核心对象$\epsilon$-邻域中的边界点划分到所属的簇中。噪音点不属于任何一个簇。</li></ol><h4 id="DBSCAN算法的优缺点"><a href="#DBSCAN算法的优缺点" class="headerlink" title="DBSCAN算法的优缺点"></a>DBSCAN算法的优缺点</h4><p>DBSCAN算法有以下优点:</p><ul><li>与传统的K-means算法相比，DBSCAN算法不需要输入聚类的类别数$K$；</li><li>DBSCAN算法可以发现任意形状的聚类簇。传统的K-means算法一般只适用于 凸样本集的聚类，DBSCAN算法既可以用于凸样本集，也可以用于非凸样本集。</li><li>对于传统的K-means算法，离群点会影响聚类的结果。DBSCAN算法有噪音点的概念，可以在聚类过程中发现离群点，因此对离群点不敏感。</li><li>传统的K-means算法，K个随机初始中心点的选择会对聚类结果有很大的影响。相比之下，算法初始值的选择对DBSCAN的聚类结果影响很小(先来后到的例子)，DBSCAN的聚类结果是没有偏倚的。</li></ul><p>DBSCAN算法的缺点:</p><ul><li>DBSCAN的聚类结果不完全是确定的。对于$\eqref{two}$，聚类结果取决于数据处理的先后顺序。但对于核心对象和噪音点，聚类结果是确定的。对于边界点，聚类结果可能会有所不同。</li><li>如果数据集的密度不均匀的情况下，DBSCAN的聚类效果比较差。因为当簇之间的密度差别较大时，两个模型参数$\epsilon$和$MinPoints$不可能适合于所有的簇。</li><li>需要对两个算法参数$\epsilon$和$MinPoints$联合调参，不同的参数组合对聚类效果会有较大影响。</li></ul><div align="center"><img src="/images/dbscan_2.png" width="100%" height="100%"></div>  <div align="center"><font color="grey" size="2">Fig.4 source: https://towardsdatascience.com/dbscan-clustering-best-practices-38de9cf57610</font></div>在上图中，我们可以看出DBSCAN算法和K-means算法的聚类结果之间的差别。K-means算法倾向于生成球形的簇，这主要是由于K-means算法的目标函数实质上是最小化样本点到簇中心点的欧氏距离平均值。DBSCAN算法倾向将样本之间联系紧密的区域作为一个簇，这是由于DBSCAN算法的思想是将所有密度相连的样本点作为一个簇。总的来说，如果数据集是稠密的，并且数据集不是凸的，那么DBSCAN算法的聚类效果会比K-means算法的效果更好。<h4 id="参考链接-2"><a href="#参考链接-2" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">DBSCAN密度聚类算法–刘建平</a></li><li><a href="https://en.wikipedia.org/wiki/DBSCAN" target="_blank" rel="noopener">DBSCAN-wikipedia</a></li><li><a href="https://towardsdatascience.com/dbscan-clustering-best-practices-38de9cf57610" target="_blank" rel="noopener">dbscan-clustering-best-practices</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文本聚类就是把一些没有标签的，但有相同特征的数据聚在一起。聚类模型将样本划分为若干个簇(cluster)，每个簇对应一些潜在的概念或类别。&lt;/p&gt;
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="文本聚类" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB/"/>
    
      <category term="k-means" scheme="http://yoursite.com/tags/k-means/"/>
    
      <category term="DBSCAN" scheme="http://yoursite.com/tags/DBSCAN/"/>
    
      <category term="louvain" scheme="http://yoursite.com/tags/louvain/"/>
    
  </entry>
  
  <entry>
    <title>布尔检索</title>
    <link href="http://yoursite.com/2020/06/22/%E5%B8%83%E5%B0%94%E6%A3%80%E7%B4%A2/"/>
    <id>http://yoursite.com/2020/06/22/布尔检索/</id>
    <published>2020-06-22T09:33:07.000Z</published>
    <updated>2020-07-06T05:01:33.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>信息检索的含义是非常广泛的，在学术界将其定义为：在海量数据中找到符合信息需要的文档或文本。信息检索可以按照操作规模分为三类：网页搜索、个人信息搜索、企业或特定领域的搜索。</p><ul><li>网页搜索： 提供了给 存放在百万台电脑上的百亿篇文档的搜索服务。网页搜索特有的问题是 需要爬取收集建索引的海量文档，并且在海量文档尺度上建立高效的搜索系统。另外，还需要处理一些网页特有的问题，比如超链接的爆炸性增长。</li><li>个人信息搜索： 消费者客户端提供了信息搜索功能。比如个人邮件系统不仅可以搜索，还可以实现垃圾邮件分类。个人信息搜索特有的问题是 搜索服务启动、运行计算、磁盘占用的轻量级。</li><li>企业或特定领域搜索：这个规模是介于 网页搜索和个人信息搜索之间的。</li></ul><h3 id="词-文档矩阵-term-document-matrix"><a href="#词-文档矩阵-term-document-matrix" class="headerlink" title="词-文档矩阵(term-document matrix)"></a>词-文档矩阵(term-document matrix)</h3><p>有一个需求，在《莎士比亚全集》中，我们要找到包含词”Brutus”、”Caesar”但不包含词”Calpurnia”的书。一种方式是从头到尾读一遍这些书，保留包含词”Brutus”、”Caesar”的书，排除掉包含词”Calpurnia”的书。对于计算机来说，找出包含某些词的文档 对应于linux系统下的<code>grep</code>操作。但实际情况中，我们需要的比<code>grep</code>操作更多：</p><ol><li>在海量的数据上进行检索操作。</li><li>需要进行 更复杂的匹配操作。比如: 查询词”Brutus”<em>接近</em> 词”Caesar”的文档，这里<em>接近</em> 可以定义为在同一个句子内。</li><li>对搜索结果进行排序，我们希望 高质量的匹配文档排序更靠前。</li></ol><p>避免每次query都进行线性检索(也就是从头到尾把文档都读一遍)的方法是：提前为每篇文档建立索引。假设我们为每篇文档 记录是否包含某个词，我们可以得到一个 <code>二元词-文档关联矩阵</code>(binary term-document incidence matrix)，如图1所示。对于 词-文档关联矩阵，它的行向量表示对于某个词，这个词出现在了哪些文档中；它的列向量表示 对于某篇文档，这篇文档包含了哪些词。</p><div align="center"><img src="/images/term_document_matrix.png" width="90%" height="90%"></div>      有了词-文档关联矩阵，就可以实现"Brutus AND Caesar AND NOT Calpurnia"这个查询(query)了。词"Brutus"、"Caesar"、"Calpurnia"对应的行向量分为为"110100","110111"、"010000"。<blockquote><p>110100 AND 110111 AND NOT 010000 = 110100 AND 110111 AND 101111 = 100100</p></blockquote><p>那么查询的结果就是《Antony and Cleopatra》和《Hamlet》。<br>布尔检索模型(boolean retrieval model)是可以把查询(query)表示成词的布尔表达式(词之间用与或非逻辑运算符连接)的一种信息检索模型，布尔检索模型把每篇文档看作是词袋（词的集合）。而 词-文档关联矩阵是进行布尔查询的基础。</p><h3 id="倒排索引-inverted-index"><a href="#倒排索引-inverted-index" class="headerlink" title="倒排索引(inverted index)"></a>倒排索引(inverted index)</h3><p>接下来我们考虑一种更现实的搜索场景。假设我们有 N = 1000,000篇文档(documents)，我们在这个文档集合(collections)上进行检索。假设每篇文档有1000个词，每个词占用内存大约6字节，那么整个文档集合占用的内存约为6GB。另外，整个文档集合大约有 500,000个不同的词。这种情况下，构建一个 $500K \times 1M$ 的词-文档矩阵是不切实际的，这样的词-文档矩阵占用了太多的内存，并且这个矩阵是非常稀疏的(矩阵只有很少一部分元素是非零的)。更好的表示方法是只记录词出现在哪些文档中，这种想法是信息检索的一个重要概念：<strong>倒排索引(inverted index)</strong>。<br>图2展示了一个倒排索引的例子，我们维护一个词表(dictionary)，对于每个词，对应一个包含这个词的文档列表。 每个词对应文档列表被称为”postings list”，所有文档列表的集合被称为”Postings”。</p><div align="center"><img src="/images/inverted_index.png" width="90%" height="90%"></div>   <h4 id="如何建立倒排索引"><a href="#如何建立倒排索引" class="headerlink" title="如何建立倒排索引"></a>如何建立倒排索引</h4><p>为了提高检索效率，缩短检索时间，我们需要提前构建好索引。构建倒排索引需要四步：</p><ol><li>收集需要建立索引的文档；</li><li>对文本进行分词处理，将文档转换为词的列表；</li><li>对词进行语言学预处理，得到标准化的词。比如 $Friends \to friend; was \to is$</li><li>为包含每个词的文档建立倒排索引。倒排索引包含两个部分:dictionary 和postings。</li></ol><p>图3左侧，我们为每篇文档分配一个唯一的文档号(docID),文档中的每个词对应它的文档号; 图3中间，按字母对词进行排序，图3右侧，按照词进行分组，合并文档号。这样就得到了倒排索引的dictionary 和 postings。dictionary不仅保存了每个词，还保存了其他一些统计信息(比如 包含每个词的文档数)，这些统计信息可以提高 加权排序检索模型的搜索效率。每个词的postings list中保存了包含这个词的文档列表，也可以保存一些统计信息(比如这个词在每篇文档中的出现次数，出现的位置)。</p><div align="center"><img src="/images/inverted_index_build.png" width="90%" height="90%"></div>  这里有两次排序和一次分组值得注意。两次排序是 按照字母对词进行了排序，按docID对每个词的postings list进行排序。一次分组是 按照词进行分组，合并了每个词对应的文档号，得到了每个词的postings list。<p>接下来讨论一下dictionary和 postings list的存储。dictionary保存在访问速度更快的内存中，postings list更大，通常保存在磁盘中。用什么数据结构来保存postings list呢？固定长度的数组比较浪费存储空间，因为高频词出现在更多的文档中，低频词则出现在很少的文档中。好的两种数据结构是 单链表 和 可变长度的数组。单链表的优点是方便进行postings list的更新操作，可以很快的插入新的docID。可变长度的数组 的优点是不需要存储单链表的指针，从而节省了磁盘空间；另外可变长度数组存储在连续的内存中从而提高了处理速度。</p><h3 id="基于倒排索引进行boolean-query"><a href="#基于倒排索引进行boolean-query" class="headerlink" title="基于倒排索引进行boolean query"></a>基于倒排索引进行boolean query</h3><p>如何基于倒排索引来进行布尔查询呢？考虑最简单的联合查询<code>Brutus AND Calpurnia</code>。如图4所示，可以分为以下两步:</p><ol><li>从dictionary中取出词<code>Brutus</code>的postings list 和 词<code>Calpurnia</code>的postings list.</li><li>取这两个postings list的交集。这个交集就是包含两个词的文档。</li></ol><div align="center"><img src="/images/boolean_intersect.png" width="90%" height="90%"></div>  <div align="center"><font color="grey" size="2">Fig.4</font></div>可以按图5所示的算法，来取两个postings list的交集。为两个postings list分别维护一个指针，遍历两个数组。每一步，当两个指针指向的docID一致时，将这个docID添加到result list; 当不一致时，向前移动指向较小docID的指针。当两个指针都到达列表末尾时，循环结束。设两个postings list的长度分别为x和y，则取交集的时间负责度为$O(x+y)$，而之前线性检索的时间负责度为$\Theta(N)$，通过建立倒排索引，把线性时间复杂度 降低到了常数时间复杂度。<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://nlp.stanford.edu/IR-book/html/htmledition/boolean-retrieval-1.html#810" target="_blank" rel="noopener">boolean-retrieval</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;信息检索的含义是非常广泛的，在学术界将其定义为：在海量数据中找到符合信息需要的文档或文本。信息检索可以按照操作规模分为三类：网页搜索、个人信息搜索、企业或特定领域的搜索。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网页搜索： 提供了给 存放在百万台电脑上
      
    
    </summary>
    
      <category term="信息检索" scheme="http://yoursite.com/categories/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
    
      <category term="信息检索" scheme="http://yoursite.com/tags/%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"/>
    
      <category term="倒排索引" scheme="http://yoursite.com/tags/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/"/>
    
  </entry>
  
  <entry>
    <title>pySpark学习笔记</title>
    <link href="http://yoursite.com/2020/02/13/pySpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/02/13/pySpark学习笔记/</id>
    <published>2020-02-13T06:14:50.000Z</published>
    <updated>2020-02-13T10:30:34.529Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="pyspark下载与环境设置"><a href="#pyspark下载与环境设置" class="headerlink" title="pyspark下载与环境设置"></a>pyspark下载与环境设置</h3><p><strong>前提：</strong> 安装pyspark之前，要检查电脑是否安装了JAVA环境，可以用命令<code>java -version</code>来查看。<br>参考链接：<a href="https://blog.csdn.net/ricardo_leite/article/details/76070490" target="_blank" rel="noopener">Centos下JDK的安装与卸载</a></p><ul><li><p>Centos下JDK的安装</p><ol><li>查看yum库中有哪些可用的JDK版本：<code>yum search java | grep jdk</code>。</li><li>选择版本安装JDK，可以用<code>yum install java-1.8.0-openjdk-devel.x86_64</code>命令来安装JAVA环境。</li></ol></li><li><p>Centos下JDK的卸载</p><ol><li>先查看系统中安装了哪些rpm软件包,查看相关Java包的信息：<br> <code>rpm -qa | grep java</code></li><li>卸载已安装的JDK: <code>yum -y remove java &lt;包名&gt;</code>，比如<code>yum -y remove java java-1.6.0-openjdk-1.6.0.38-1.13.10.0.el7_2.x86_64</code>。</li></ol></li></ul><p>在清华大学镜像源<a href="http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz</a>下载pyspark安装包。</p><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><ol><li>在<code>./bin/pyspark</code>启动pyspark时，报以下错误：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org/apache/spark/launcher/Main : Unsupported major.minor version 52.0</span><br></pre></td></tr></table></figure></li></ol><p>出错原因是：pyspark 2.1需要Java 1.7以上的版本，而安装的Java版本是1.6的。</p><ol start="2"><li>在python代码中调用pyspark报错：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &apos;py4j&apos;</span><br></pre></td></tr></table></figure></li></ol><p>这是因为<code>~/.bashrc</code>中py4j的版本与实际的版本不同。修改<code>~/.bashrc</code>中py4j的版本为实际的版本即可。<br>参考链接：<a href="https://blog.csdn.net/skyejy/article/details/90690742" target="_blank" rel="noopener">https://blog.csdn.net/skyejy/article/details/90690742</a></p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="http://codingdict.com/article/8881" target="_blank" rel="noopener">编程字典-pyspark教程</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;pyspark下载与环境设置&quot;&gt;&lt;a href=&quot;#pyspark下载与环境设置&quot; class=&quot;headerlink&quot; title=&quot;pyspark下载与环境设置&quot;&gt;&lt;/a&gt;pyspark下载与环境设置&lt;/h3&gt;&lt;p&gt;&lt;str
      
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="pySpark" scheme="http://yoursite.com/tags/pySpark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习笔记</title>
    <link href="http://yoursite.com/2020/02/11/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/02/11/Hadoop学习笔记/</id>
    <published>2020-02-11T07:09:31.000Z</published>
    <updated>2020-02-11T09:36:22.143Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。</p><a id="more"></a><h3 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h3><p>Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。随着技术发展，人类每天都会产生海量数据，用单一的机器来存储和处理这些数据已经不能满足需求。而Hadoop允许在从单一的机器扩展到上千台机器，从而在跨计算机的分布式环境中来存储和处理大数据。</p><p>Hadoop的架构如下图所示：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/hadoop_architecture.png" alt="Hadoop的架构图" title>                </div>                <div class="image-caption">Hadoop的架构图</div>            </figure></p><ul><li>HDFS: 分布式文件存储系统</li><li>YARN: 分布式资源管理</li><li>MapReduce: 分布式计算</li><li>Others: 利用YARN的资源管理来实现其他的数据处理方式</li></ul><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><p>HDFS(Hadoop Distributed File System)是Hadoop应用主要的分布式文件系统。HDFS是基于“Master-Worker”架构的，一个HDFS集群由一个NameNode和多个DataNode组成。NameNode是一个中心服务器，管理文件系统的命名空间(NameSpace)，管理文件系统的元数据(MataData)，相当于是一个目录。DataNode负责存储实际的数据。</p><p>   HDFS暴露了文件系统的命名空间，用户可以以文件的形式来存储数据。具体来看，一个文件被分为一个或者多个数据块(Block)，这些数据块存储在一组DataNode上；每个数据库对应NameNode上的一条记录。NameNode执行文件系统的命名空间操作，比如打开、关闭、重命名文件或目录；它也负责数据库与DataNode之间的映射。DataNode负责处理文件系统客户端的读写请求，在NameNode的统一管理下来进行数据块的创建、删除和复制。</p><div align="center"><img src="/images/hdfs-architecture.png" width="85%" height="85%"></div><div align="center"><font color="grey" size="2">HDFS的架构</font></div><ul><li><strong>Block</strong> <ol><li>数据块block是基本存储单位，一般大小为64MB。</li><li>一个文件会被分成一个或多个数据块来存储。如果文件大小小于一个Block的大小，那么实际占用的空间为文件大小。</li><li>Block是基本的读写单位，相当于磁盘的页，每次都会读写一个Block。</li><li>每个Block会被存储到多个机器，默认是3个。防止机器故障造成数据丢失。</li></ol></li><li><strong>NameNode</strong><ol><li>存储文件的元数据，管理文件系统的命名空间。整个HDFS可存储文件的大小受限于NameNode的内存大小。</li><li>一个Block对应NameNode中的一条记录。</li><li>NameNode失效后，整个HDFS就都失效了。要保证NameNode的可用性。</li></ol></li><li><strong>DataNode</strong><ol><li>存储具体的block数据。</li><li>负责数据的读写操作和复制操作。</li><li>DataNode启动时会向NameNode汇报当前存储的block信息，随后也会定时向NameNode汇报修改信息。</li><li>DataNode之间会进行通信，复制block，保证数据的冗余性。</li></ol></li></ul><h4 id="HDFS-shell命令"><a href="#HDFS-shell命令" class="headerlink" title="HDFS shell命令"></a>HDFS shell命令</h4><p>Hadoop包含了一系列类shell命令，可以直接和HDFS或hadoop支持的其他文件系统进行直接的交互。<code>hadoop fs -help</code>可以列出所有的shell命令。这些shell命令支持大部分普通文件系统的操作，比如复制、删除文件、更改文件权限等。</p><p>调用文件系统shell命令应该采用<code>hadoop fs &lt;arg&gt;</code>的形式，所有的FS shell命令都使用URI路径作为参数。URI路径的格式是’scheme://authority/path’，对于HDFS文件系统，scheme是<code>hdfs</code>；对于本地文件系统，scheme是<code>file</code>。其中scheme参数和authority参数是可选的，省略的话会使用默认的参数scheme。</p><ul><li><p><strong>ls</strong><br>  使用方式： <code>hadoop fs -ls &lt;args&gt;</code><br>  如果是文件，会显示文件信息；如果是目录，会显示目录下所有的文件。</p></li><li><p><strong>test</strong><br>  使用方式： <code>hadoop fs -test [edz] URI</code><br>  选项：<br>  -e 检查文件是否存在，如果存在返回0；<br>  -z 检查文件是否是0字节，如果是返回0；<br>  -d 如果路径是个目录，返回1，否则返回0</p></li></ul><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_design.html" target="_blank" rel="noopener">官方手册-Hadoop分布式文件系统：架构和设计</a></li><li><a href="https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" target="_blank" rel="noopener">官方手册-Hadoop Shell命令</a></li><li><a href="https://pennywong.gitbooks.io/hadoop-notebook/content/introduction.html" target="_blank" rel="noopener">Hadoop简介</a></li><li><a href="https://www.w3cschool.cn/hadoop/i1la1jyc.html" target="_blank" rel="noopener">Hadoop教程-W3Cschool</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop是一个开源框架，允许在跨计算机的分布式环境中来存储和处理数据。&lt;/p&gt;
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>2019-蓟门烟树</title>
    <link href="http://yoursite.com/2020/01/13/2019-%E8%93%9F%E9%97%A8%E7%83%9F%E6%A0%91/"/>
    <id>http://yoursite.com/2020/01/13/2019-蓟门烟树/</id>
    <published>2020-01-13T15:01:16.000Z</published>
    <updated>2020-06-28T08:06:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是在明光桥北度过的第二个冬天，已经下了两三场大雪，雪后的天空格外晴朗。又到了一年的末尾，没有经常写日记，只能从朋友圈、论坛发的骑行贴、日记本上不多的几篇日记，印象笔记上的记录来尽力回忆这一年是如何度过的。回首这一年，平凡普通，不惊心动魄，有些许遗憾，也有一些小的闪光和美好。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/snow_2020.png" alt="北京的雪" title>                </div>                <div class="image-caption">北京的雪</div>            </figure><a id="more"></a><h2 id="平淡的科研生活"><a href="#平淡的科研生活" class="headerlink" title="平淡的科研生活"></a>平淡的科研生活</h2><p>这一年大部分的时光是在实验室的座位上度过的，虽然有时会懈怠偷懒，但大部分的时间还是在学习与研究方向相关的内容。在一位北邮博士师兄的毕业论文中看到这样一段话：“不需要多么玩命，只要你能每天规律地作息且每天到实验室，学会控制情绪，在实验室的时间全部用来做与科研相关的事情就可以了。”研一的时候，我纠结着是否要读博士？慢慢地我打消了读博士的想法。虽然这一年待在实验室很长时间，但并没有取得太大的进展。虽然最近完成了一篇论文，但我逐渐明白我还不具备独立做科研的能力，也缺乏投身科研的热情，并且缺乏老师的指导。想要读博的动机是功利性的，想要更高的学位，进而毕业后可以获取到更好的工作机会和社会地位。我畏惧读博路上的艰难孤独，缺乏读博的足够动力，学位并不应该是目的，两种选择都不是错误，做好了抉择坚定自己的内心就可以了。</p><p>研究生的生活日常是这样的。给老师做一些项目，标注数据，给老师写一些琐碎无聊的项目书和PPT报告。当然老师的项目中，并不都是琐碎无聊，也有一些值得学习深入研究的内容。辩证法的角度来看，任何事物都是如此的，正面与反面是并存的。在找实习时投递的个人简历上，做项目是一个重要的经历。除了做项目之外，大部分的时间都在阅读论文，这一年来读了上百篇对话系统相关的论文。在量变积累的过程中，自然地对对话系统这个研究方向有了大致的了解，进而可以进一步地进行深入地研究和探索。阅读论文不是目的，阅读论文的数量也不应该成为追求，应该要做的事情是在阅读论文的过程中去发现问题并提出自己的解决方案。吸收知识，然后去应用知识和创造。创造的过程是充满的乐趣的。解决问题的过程就是自己产出的过程，我慢慢地才明白了这一点。</p><p>与导师、同学的相处也是研究生生活的一个重要议题。G老师不在学校在外面谋求仕途晋升之道，做了甩手掌柜，虽然一年中抽出时间开了几次电话会议和两次线下会议。每周一次的小组会是L老师负责的，研三的师兄师姐已经出去实习了，剩下博士学姐、研一研零的学弟学妹和研二的我们开组会。研一时的每次组会，我都会被L老师批评，那时我以为是自己的问题，并对自己产生了怀疑。每次跟着L老师开完组会，学习的积极性都会被打击到，需要花一两天时间来调整心态。如今研一研零的学弟学妹也被老师这样批评着，我渐渐明白了这是L老师的个人风格，并不完全是自己的原因。L老师和G老师不算是很坑的导师，但不同的导师性格不同，教导学生的方式也不同。要学会与不同性格的导师去相处，像水一样，去适应和配合来最大程度地让自己取得进步。跟我一届的几个女生都说”不想开小组会“，这是内心的写照了。大组会的氛围要好一些，实验室有时会请博士和一些已经毕业工作的师兄师姐来做展示交流。实验室的大boss有时也会到场，大boss有次说”要内心强大，要lold住。”，这句话我印象深刻，生活中的许多场景，内心强大，能hold住是非常重要的。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/cat.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>这个学期做了勤工俭学的工作，每周打扫实验室一次，一个月三百块的工资。我常常会压抑自己的欲望和需要，12个小时回家的火车票大多会买硬座票，很少买卧铺。这样的心理，有些凄惨。最近我慢慢地转变了这种心理和想法，想要穿羊毛绒的袜子，想坐卧铺车或者动车回家出行，想要烫一个帅气的发型，想要穿帅气的衣服。今年的收入除了实验室工资、奖学金和勤工俭学的工资之外，我还做了一份线上考研辅导的工作。这是第二次考研辅导，辅导对象是成都某大学的一个女生，辅导她通信原理专业课，薪水是三千块。通信原理是我考研时的专业课，研究生的专业与通信已没有关系了，离我考研已经过去了整整两年，但当我看一遍通信原理的教材，我还是可以理解书上的知识点和难点，我考研时应当是真的付出了很大的努力。我还记得蒋震图书馆前的那排高大的杨树，还记得考研复习时的心理活动：当树叶全部落完时，我就要考研了；当树叶再次长出来郁郁葱葱时，我就要在清风的吹拂中毕业了。如今，校园的主干道的两旁是梧桐树，每当夕阳落下，暮色昏沉，一大群乌鸦总会停在梧桐树枝头，喧闹地叫着，树下是呈正态分布的鸟屎。</p><p>研一下学期结束后的那个暑假，我找了一份中科院自动化所的实习。实习地点在知春路附近的自动化大厦，离学校三四公里远。这是我的第一份实习，实习工资并不高，做了一些文本纠错和文本摘要的任务。原本打算暑期在实习中度过，但还是抽出一周时间回了趟家。今年年初的时候，有个想法是带着爷爷奶奶来北京。爷爷奶奶应该从没有去过省外，也没有来过北京。但奶奶身体并不好，恐怕受不了舟车劳顿，这个想法只能作罢。</p><h2 id="饭局少不了"><a href="#饭局少不了" class="headerlink" title="饭局少不了"></a>饭局少不了</h2><p>北京城，天子脚下，有不少同学会来，因此接待了不少朋友。有一块长大的发小。C已经毕业，在北京工作，在县城里买了房子，也与女方定了婚。年初我们一起去游了圆明园，圆明园离北大不远，历史课本中那副经典照片中的断垣残壁依旧挺立着。后来R来北京探亲，姐姐姐夫在北京工作，姐姐生下了孩子。在杏坛路上的一家店，吃了鱼。我们俩从小就一块打桌球，现在我已经不是他的对手了。</p><p>也有高中的同窗。高中同学有好几个在北京，有的读研，有的工作。年中组织过一次聚餐，亲切中带着一丝生疏，逛着北航的校园，吃了饭，看了部电影《无双》。S是高中班里的才子了，他在西安读研，暑期后不久来参加头条组织的一次夏令营活动。我们在杏坛路的一家店吃了串串香，在漫咖啡用玩了线上桌球。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/entailment.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>更多的是大学的同窗。年初W和Y来北京参加考研复试，我们在小龙坎为他们接风洗尘，但后来Y虽然初试成绩不错但遗憾地没能通过复试。Y小侄女毕业前来北京玩，我们一起游了颐和园，颐和园真是风景宜人，值得一去。Z在准备考研的过程中，忙里偷闲来北京看了话剧。Z律师跟着领导来上北大的培训班。J从日本飞回来后，一块去爬了长城。真的是记成流水账了。</p><p>回头看这一年，还是有不少社交活动的。</p><h2 id="骑车、跑步与越野"><a href="#骑车、跑步与越野" class="headerlink" title="骑车、跑步与越野"></a>骑车、跑步与越野</h2><p>这部分是我平淡生活中的一抹色彩。 在刚读研究生时，原本已经打算与骑行告别了，骑行的热情也消散了。但在实验室同桌X同学巨大骑行热情的影响下，我又重新燃起了骑行的热情，正确地认识了骑行在我生活中的位置。我并不应该将骑行从我生活中完全地剥离出去，而应该让骑行成为我生活中的一部分。我们组成了一个四人的骑行小队，这是一个慢慢培养了骑行默契的小队了。小队的第一次骑行是在五一假期，从北京到张北草原天路。途中经过了雄伟壮观的八达岭长城，蜿蜒的长城在青山的山脊上蔓延展开；也经过了巨大的官厅水库，流经卢沟桥的永定河就是从官厅水库流出的；张北草原天路只走了五公里，并且由于海拔较高五月份草还没长出来，最佳的观赏时节是七八月份，但从张北草原天路到张家口市的那段一路下坡的废弃国道的风景农田也是极美的。小队的第二次骑行是在十月份的一个周末，由北邮出发前往百里山水画廊，再返回北邮。里程有二百八十公里，天黑得又早，两天的行程是比较仓促的。第一天的骑行爬了很多上坡，天黑了才到达落脚地点千家店镇，难度确实太大了，两个女生骑得有些崩溃。第二天的路程虽然较长但是缓下平坦路面，轻松愉快地返回了学校。后来，我们四个人一块去学校附近的一家日料店吃了日料，顺便喝了一点青梅酒，彼此开诚布公袒露心扉，约定有机会的话一块去环海南岛。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/riding_bike.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>2019年的跑量有1120公里，平均配速5分15，并且刷新了自己的十公里、半马和全马的配速。年初过完春节回来有些吃胖了，学期开始后晚上会去操场跑步。后来报名了11月3日举行的北京马拉松比赛，原本对中签没有报太大的期望，因此还报名了北马的志愿者，幸运的是竟然中签了。中签后，特定买了新的跑步鞋和运动裤，大部分时间都能保持较好的跑步训练，有时候也会偷懒几天。11月2号坐地铁去国际会展中心领取了参赛包，晚上早早地就上床睡了。3日凌晨不到5点就起床了，出了宿舍楼门，漆黑的夜里下着小雨，我骑车去了积水潭地铁站。北马赛事有三万五千人的规模，鸣枪开跑后十分钟才通过起跑线。刚开跑时，温度稍微有点低，但一公里后身体就发热了，太阳升起，温度也慢慢上来了。总的来说，11月的北京天气凉爽，并且北马补给充足，我用了3小时35分完赛，大大超出了自己的预期。这是我参加的第二个全马，相比于跑完东马的双腿疼痛，这次北马的准备要充足很多，双腿的酸痛感轻很多。年初我的体重在73公斤左右，托跑步的福，体重下降到了67公斤，这跟我远征完的体重差不多。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/marathon.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>这一年还解锁了越野赛。G学长报名了香山21公里越野赛，但因工作原因不能参加。于是，我就得到了第一次参加山地越野赛赛的机会。不久前下的雪还没有融化，香山越野的21公里因此缩短为了18公里。虽然有段时间没有跑步了，但参加北马的体力基础还在，以七十多名的成绩完赛。山地越野比单纯的马拉松比赛要有趣很多，有柏油路，也有山间的小路，还有下坡的石头路。由于没有训练过如何快速地下坡，15公里时左腿的膝盖已经非常疼了，坚持了完成了比赛。除了山地越野赛之外，还接触到了定向越野赛。先是实验室组队参加了5公里西山定向越野，赢取了一份零食大礼包，在越野赛上遇到了一位女生S。后来，加了她的微信，慢慢有了接触，一起去看了电影，一起约了早饭，但最后并没有一个好的结果。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/xiangshan.png" alt="香山越野赛" title>                </div>                <div class="image-caption">香山越野赛</div>            </figure><p>身边有许多学长去登了雪山，希望以后有机会可以登上一座雪山。</p><p>2020年的主要基调是实习和找工作。希望这个寒假不要虚度，多刷几道算法题。<br>2020的愿望是有更大信心去接纳自己,拥抱生活。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是在明光桥北度过的第二个冬天，已经下了两三场大雪，雪后的天空格外晴朗。又到了一年的末尾，没有经常写日记，只能从朋友圈、论坛发的骑行贴、日记本上不多的几篇日记，印象笔记上的记录来尽力回忆这一年是如何度过的。回首这一年，平凡普通，不惊心动魄，有些许遗憾，也有一些小的闪光和美好。&lt;/p&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
                    &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt;
                    &lt;img src=&quot;/images/snow_2020.png&quot; alt=&quot;北京的雪&quot; title&gt;
                &lt;/div&gt;
                &lt;div class=&quot;image-caption&quot;&gt;北京的雪&lt;/div&gt;
            &lt;/figure&gt;
    
    </summary>
    
      <category term="年度总结" scheme="http://yoursite.com/categories/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="年度总结" scheme="http://yoursite.com/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
      <category term="生活记录" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Large-Scale Transfer Learning for Natural Language Generation》</title>
    <link href="http://yoursite.com/2020/01/07/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ALarge-Scale-Transfer-Learning-for-Natural-Language-Generation%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/07/论文笔记《Large-Scale-Transfer-Learning-for-Natural-Language-Generation》/</id>
    <published>2020-01-07T01:22:38.000Z</published>
    <updated>2020-01-07T02:29:27.650Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】ACL2019<br>【链接】<a href="https://www.aclweb.org/anthology/P19-1608.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P19-1608.pdf</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是<a href="https://huggingface.co/" target="_blank" rel="noopener">huggingface</a>发表在ACL2019的短文。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>传统的有监督学习方法是：在特定的任务上，在有标签的数据集上，有监督地训练一个模型。有监督学习的局限在于许多NLP任务缺乏有标签的数据集，或者有标签数据集的规模比较小。迁移学习就派上用场了，迁移学习在许多NLP任务上取得了好的效果。迁移学习的思路是：先在大规模的未标注文本语料上无监督地预训练一个语言模型，再把预训练好的语言模型迁移到特定的任务上，对模型参数进行微调。<br>目前迁移学习的大部分研究集中在文本分类和NLU(natural language understanding)任务上，迁移学习应用在NLG(natural language generation)任务上的研究比较少。论文认为NLG任务可以分为两类：</p><ul><li>high entropy任务。例如story generation，chit-chat dialog。输入文本包含的信息有限，可能不包含生成输出文本所需要的信息。</li><li>low entropy任务。例如文本摘要、机器翻译。特点是输入文本的信息量比较多，生成输出文本需要的信息包含在输入文本中。</li></ul><p>论文主要研究了迁移学习在对话系统上的应用。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>对话系统主要有三种输入：dialogue history,facts以及previous decoded tokens。论文研究单输入模型和多输入模型。单输入模型应用场景有限，主要关注下输入的部分。多输入模型基于encoder-decoder框架，关注下decoder部分的调整。</p><div align="center"><img src="/images/single_multi_input_model.png" width="50%" height="50%"></div><div align="center"><font color="grey" size="2">Fig.1. 单输入模型与多输入模型的结构图</font></div><h4 id="单输入模型"><a href="#单输入模型" class="headerlink" title="单输入模型"></a>单输入模型</h4><p>单输入模型把三种输入连接起来作为模型的输入。连接方式有三种：</p><ul><li>用自然分隔符连接输入。论文中给每句对话添加双引号。</li><li>用空间分隔符连接。比如用’_SEP’把每个句子连接起来。</li><li>直接把句子连接起来，再用context-type embedding(CTE)来表示输入的类型。例如：$w_{CTE}^{info}$表示用户画像信息，$w_{CTE}^{p^1}$表示对话人1说的话，$w_{CTE}^{p^2}$表示对话人2说的话。<div align="center"><img src="/images/single_multi_input.png" width="40%" height="40%"></div><div align="center"><font color="grey" size="2">Fig.2. (a)单输入模型使用CTE方式的输入(b)多输入模型用起始分隔符连接的输入</font></div></li></ul><h4 id="多输入模型"><a href="#多输入模型" class="headerlink" title="多输入模型"></a>多输入模型</h4><p>多输入模型基于encoder-decoder框架。用预训练的语言模型参数来初始化encoder和decoder。多输入模型的输入同样可以采用单输入模型的处理方式。将persona information和dialogue history分别送入encoder进行编码得到两个向量表示。重点在于decoder部分的调整。decoder的multi-head attention模块处理三种特征输入(personal information,dialogue history,previous decoded tokens)，再把三者的结果取平均值即可。</p><div align="center"><img src="/images/single_multi_input_decoder.png" width="30%" height="30%"></div><div align="center"><font color="grey" size="2">Fig.3. 基于transformer模型的多输入模型</font></div>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】ACL2019&lt;br&gt;【链接】&lt;a href=&quot;https://www.aclweb.org/anthology/P19-1608.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aclweb.org/anthology/P19-1608.pdf&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Transfer Learning" scheme="http://yoursite.com/tags/Transfer-Learning/"/>
    
      <category term="NLG" scheme="http://yoursite.com/tags/NLG/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《A Pre-training Based Personalized Dialogue Generation Model with Persona-sparse Data》</title>
    <link href="http://yoursite.com/2020/01/06/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AA-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/06/论文笔记《A-Pre-training-Based-Personalized-Dialogue-Generation-Model-with-Persona-sparse-Data》/</id>
    <published>2020-01-06T03:30:13.000Z</published>
    <updated>2020-01-06T07:23:14.566Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】AAAI2020<br>【链接】<a href="https://arxiv.org/abs/1911.04700" target="_blank" rel="noopener">https://arxiv.org/abs/1911.04700</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是清华大学<a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈</a>教授组发表在AAAI2020的论文。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>论文主要研究个性化的对话系统。论文中提出了“persona-dense”和“persona-sparse”的概念。</p><ul><li>persona-dense: 像<code>Persona-Chat</code>数据集中，在收集语料的过程中，对话人要求在有限的轮数内交流彼此的个性化信息，对话内容是与persona是密切相关的。</li><li>persona-sparse：而在现实的对话中，只有少数的对话与persona是相关的，大多数的对话往往是与persona不相关的。直接在真实对话的语料上训练和微调模型，可能会让模型学习到大多数与persona无关的对话，而把少数与persona相关的对话当作是语料中的噪声。</li></ul><p>为了解决这个问题，对话模型应该学习到哪些对话是与persona相关的，哪些对话是与persona不相关的。<br>论文采用了基于encoder-decoder框架的transformer模型。预训练的方法在许多NLP任务上取得了好的效果，论文提出用预训练的语言模型参数来初始化encoder和decoder。将attribute embedding添加到了encoder的输入；用了attention route机制来建模dialogue history，target persona和previous tokens这三种不同的特征；另外，用了dynamic weight predictor来控制这三种不同的特征在解码生成回复时起到不同程度的作用。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>任务可以描述为：给定对话历史$C$和回复者的target persona $T$，要求生成流畅的回复$Y$。 $$Y = argmax_{Y^{‘}}P(Y^{‘}|C,T)$$ 其中persona $T$由一系列属性来描述，$T = {t_1,t_2,…,t_N}$；每个属性用键值对来表示：$t_i = &lt;k_i,v_i&gt;$。对话历史扩展了说话人的信息，$C=\lbrace{(U_1,T_1),(U_2,T_2),…,(U_M,T_M)}\rbrace$</p><div align="center"><img src="/images/pretrain_personalized_dialogue.png" width="120%" height="120%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="Encoding-with-Persona"><a href="#Encoding-with-Persona" class="headerlink" title="Encoding with Persona"></a>Encoding with Persona</h4><p>模型的输入包括两个部分对话历史$C$和回复者的target persona $T$。</p><ol><li>先说对话历史$C=\lbrace{(U_1,T_1),(U_2,T_2),…,(U_M,T_M)}\rbrace$的的编码，将所有的句子用特定的字符’_SPE’连接起来，并且将对话人$T_i$属性$t_i$映射为embedding表示。具体地，论文中的用户属性包括性别、地址、爱好三个。前两个的属性只有一个值，直接经过embedding层就可以了。爱好可能有多个值，经过embedding层后再取平均值即可。最终将word embedding + positional embedding + attribute embedding作为transformer encoder的输入。 <div align="center"><img src="/images/input_representation.png" width="60%" height="60%"></div> <div align="center"><font color="grey" size="2">Fig.2. input representation of dialogue context</font></div></li><li>至于回复者的target persona $T$的编码，将所有的键值对属性连接起来作为一个序列，经过embedding层，直接作为transformer encoder的输入。</li></ol><h4 id="Attention-Routing"><a href="#Attention-Routing" class="headerlink" title="Attention Routing"></a>Attention Routing</h4><p>对于<code>persona-sparse</code>的对话语料，与persona无关的训练样例，在生成回复时不使用persona信息；与persona相关的训练样例，在生成回复时使用persona信息。论文为此设计了<code>Attention routing</code>模块来控制target persona $T$在生成回复时起到的作用。<br>具体地，将previous decoded tokens的表示$E_{prev}$作为attention的query，在对话历史$E_C$、回复者的target persona $E_T$和previous decoded tokens $E_{prev}$这三种特征上使用multi-head attention机制： $$O_T = MultiHead(E_{prev},E_T,E_T)$$ $$O_C = MultiHead(E_{prev},E_C,E_C)$$ $$O_{prev} = MultiHead(E_{prev},E_{prev},E_{prev})$$ 计算$O_T$和$O_C$时使用unmasked self-attention机制，计算$O_{prev}$时使用masked self-attention机制为避免decoder看到未来时刻的信息。<br>用一个persona weight $\alpha$把三个attention route $O_T,O_C,O_{prev}$结合起来：$$O_{merge} = \alpha O_T + (1-\alpha)O_C + O_C + O_{prev}$$ persona weight $\alpha$应该基于对话是否与persona有关，论文设计了<code>dynamic weight predictor</code>来预测$\alpha$。具体地，这个predictor是一个以对话历史$E_C$为输入的二元分类器$P_{\theta}(r|E_C)$：$$\alpha = P_{\theta}(r = 1|E_C)$$ 这个<code>dynamic weight predictor</code>的训练损失采用交叉熵函数：$$L_W(\theta) = -\sum_i r_i log P_{\theta}(r_i|E_C) + (1-r_i)log(1-P_{\theta}(r_i|E_C))$$</p><h4 id="Pre-training-and-Fine-tuning"><a href="#Pre-training-and-Fine-tuning" class="headerlink" title="Pre-training and Fine-tuning"></a>Pre-training and Fine-tuning</h4><p>我们先在大规模的文本语料上预训练一个语言模型，最小化负对数似然函数：$$L_{LM}(\phi) = -\sum_ilog P_\phi(u_i|u_{i-k},…,u_{i-1})$$其中$\phi$是语言模型的参数，$k$是窗口大小。<br>接着用预训练好的语言模型的参数来初始化transformer模型的encoder和decoder，对于回复生成任务，最优化以下的目标函数：$$L_D(\phi) = -\sum_ilogP_{\phi}(u_i|u_{i-k},…,u_{i-1},E_C,E_T)$$<br>为了把<code>预训练阶段</code>和<code>微调阶段</code>联系起来，在微调阶段，也会在对话语料训练集上最小化语言模型的损失函数。也就是把语言模型的损失函数作为微调阶段的辅助损失函数。则微调阶段，模型总的损失函数为：$$L(\phi,\theta) = L_D(\phi) + \lambda_1L_{LM}(\phi) + \lambda_2L_W(\theta)$$</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://github.com/silverriver/PersonalDilaog" target="_blank" rel="noopener">https://github.com/silverriver/PersonalDilaog</a></li><li><a href="https://github.com/SpiderClub/weibospider" target="_blank" rel="noopener">https://github.com/SpiderClub/weibospider</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】AAAI2020&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/abs/1911.04700&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1911.04700&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Transformer" scheme="http://yoursite.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Assigning Personality（Profile） to a Chatting Machine for Coherent Conversation Generation》</title>
    <link href="http://yoursite.com/2020/01/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AAssigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/05/论文笔记《Assigning-Personality-Profile-to-a-Chatting-Machine-for-Coherent-Conversation-Generation》/</id>
    <published>2020-01-05T03:10:26.000Z</published>
    <updated>2020-01-05T04:51:42.806Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】ICJAI2018<br>【链接】<a href="https://arxiv.org/abs/1706.02861" target="_blank" rel="noopener">https://arxiv.org/abs/1706.02861</a><br>【数据集】<a href="http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality" target="_blank" rel="noopener">http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是清华大学<a href="http://coai.cs.tsinghua.edu.cn/hml/" target="_blank" rel="noopener">黄民烈教授组</a>的2017年的工作，2018年发表在IJCAI。</p><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>论文的研究内容是赋予对话系统以个性化信息(personality/profile)来生成具有一致性的回复。具体来说，对话语料中用键值对属性值来描述用户画像。对话系统先使用一个profile detector来检测生成回复时是否使用个性化信息。如果要使用，从所有的键值对属性用户画像中选择一个键值对来生成回复。采用一个bidirectional decoder来生成回复，让键值对出现在生成的回复中。进一步地，为了提高bidirectional decoder的性能，采用了position detector来检测键值对在回复中的位置。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>论文提出的对话模型包括了三个重要模块。profile detector检测是否使用用户画像并选择一个键值对属性。bidirectional decoder根据选中的键值对属性来生成回复。position detector检测键值对属性值在回复中出现的位置。</p><h4 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h4><p>给定post $X = \lbrace{x_1,x_2,…,x_n}\rbrace$以及描述用户个性化信息的键值对属性$\lbrace{&lt;k_i,v_i&gt;|i=1,2,…,K}\rbrace$，目标是生成有一致性的回复$Y = \lbrace{y_1,y_2,…,y_m}\rbrace$。<br>生成过程可以定义为: $$P(Y|X,\lbrace&lt;k_i,v_i&gt;\rbrace) = P(z=0|X) \cdot P^{fr}(Y|X) + P(z=1|X) \cdot P^{bi}(Y|X,\lbrace&lt;k_i,v_i&gt;\rbrace)$$</p><div align="center"><img src="/images/bidirectional_decoder.png" width="80%" height="80%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>采用GRU将post $X = \lbrace{x_1,x_2,…,x_n}\rbrace$编码为$\lbrace{h_1,h_2,…,h_n}\rbrace$。GRU的更新公式如下：$$h_t = GRU(h_{t-1},x_t)$$</p><h4 id="profile-detector"><a href="#profile-detector" class="headerlink" title="profile detector"></a>profile detector</h4><p>作用有两个。一是检测是否使用profile，二是如果要使用，选择一个特定的键值对属性$&lt;key,value&gt;$来生成回复。<br>第一个作用，是否使用profile是在有标签数据上训练的二元分类器$P(z|X)$，计算方式如下：$$P(z|X) = P(z|\tilde{h}) = \sigma(W_p\tilde{h})$$ $$\tilde{h} = \sum_{j}h_j$$ 第二个作用，选择一个特定的键值对属性。生成一个在所有键值对上的概率分布：$$\beta_i=MLP([\tilde{h},k_i,v_i])=softmax(W\cdot [\tilde{h},k_i,v_i])$$其中$W$是可训练的模型参数，$\tilde{h}$是post的表示。则取概率最大的键值对作为选中的键值对：$$\tilde{v} = argmax_i(\beta_i)$$ 进一步地，bidirectional decoder的解码过程可以定义为：$$P^{bi}(Y|X,&lt;\lbrace{k_i,v_i}\rbrace&gt;) = P^{bi}(Y|X,\tilde{v})$$</p><h4 id="bidirectional-decoder"><a href="#bidirectional-decoder" class="headerlink" title="bidirectional decoder"></a>bidirectional decoder</h4><p>让选中的键值对属性值$\tilde{v}$出现在生成的回复中：$Y = (Y^b,\tilde{v},Y^f) = (y_1^b,…,y_{t-1}^b,\tilde{v},y_{t+1}^f,…,y_m^f)$。backward deocder生成$Y^b$，forward decoder生成$Y^f$。 解码过程可以定义为$$P^{bi}(Y|X,\tilde{v}) = P^{b}(Y^b|X,\tilde{v}) * P^f(Y^f|Y^b,X,\tilde{v})$$ $$P^{b}(Y^b|X,\tilde{v}) = \prod_{j=t-1}^{1} P^b(y^b_j|y^b_{&lt;j},X,\tilde{v})$$ $$P^{f}(Y^f|Y^b,X,\tilde{v}) = \prod_{j=t+1}^{m}P^f(y_j^f|y_{&lt;j}^f,Y^b,X,\tilde{v})$$ 具体地，生成一个word的概率分布为：$$P^b(y^b_j|y^b_{&lt;j},X,\tilde{v}) \propto MLP([s_j^b;y^b_{j+1};c_j^b])$$ $$P^f(y_j^f|y_{&lt;j}^f,Y^b,X,\tilde{v}) \propto MLP([s_j^f;y_{j-1}^f;c_j^f])$$ 注意区分backWard decoder与forward decoder的差别，分别是逆序和顺序的，一个输入的是$y^b_{j+1}$，而另一个是$y^f_{j-1}$。$s_j^{*},{*} \in \lbrace{f,b}\rbrace$ 是相应decoder的隐藏状态，$c_j^{*}$为相应的context vector。更新方式如下：$$s_j^{*} = GRU(s_{j+l}^{*}, [y_{j+l}^{*},c_j^{*}])$$ $$c_j^* = \sum_{t=1}^n\alpha_{j,t}^{*}h_t$$ $$\alpha_{j,t} \propto MLP([s_{j+l}^{*},h_t])$$ 对于backWard decoder，有$* = b,l = 1$，而对于forward decoder，有$* = f,l = -1$。</p><h4 id="position-detector"><a href="#position-detector" class="headerlink" title="position detector"></a>position detector</h4><p>为了检测属性值$v$在回复中出现的位置，在训练阶段，检测哪个词可以被profile中的属性值代替。也就是估计概率：$P(j|y_1y_2…y_m,&lt;k,v&gt;),j\in[1,m]$，这个概率分布的意义是词$y_j$可以被属性值$v$替换的可能性大小。这里采用最大化$y_j$与属性值$v$之间的余弦相似度：$$P(j|Y,&lt;k,v&gt;) \propto cos({y_j},{v})$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】ICJAI2018&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/abs/1706.02861&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1706.02861&lt;/a&gt;&lt;br&gt;【数据集】&lt;a href=&quot;http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://coai.cs.tsinghua.edu.cn/hml/dataset/#AssignPersonality&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Bidirectional Decoder" scheme="http://yoursite.com/tags/Bidirectional-Decoder/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Learning Personalized End-to-End Goal-Oriented Dialog》</title>
    <link href="http://yoursite.com/2020/01/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ALearning-Personalized-End-to-End-Goal-Oriented-Dialog%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/03/论文笔记《Learning-Personalized-End-to-End-Goal-Oriented-Dialog》/</id>
    <published>2020-01-03T01:57:32.000Z</published>
    <updated>2020-01-03T04:08:45.359Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】AAAI2019<br>【链接】<a href="https://arxiv.org/pdf/1811.04604v1.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.04604v1.pdf</a><br>【代码】未公布</p></blockquote><a id="more"></a><p>这篇论文是北京大学于2019年初发表的。研究内容主要是将用户个性化信息(personalization)结合到端到端的任务型对话模型中，来调整回复的策略和语言风格，并消除歧义。</p><h3 id="个性化对话系统的相关工作"><a href="#个性化对话系统的相关工作" class="headerlink" title="个性化对话系统的相关工作"></a>个性化对话系统的相关工作</h3><p>最早探索个性化对话系统的研究工作是 <a href="https://nlp.stanford.edu/~bdlijiwei/index.html" target="_blank" rel="noopener">Li Jiwei</a>于16年发表的论文<a href="https://arxiv.org/pdf/1603.06155v2.pdf" target="_blank" rel="noopener">《A Persona-Based Neural Conversation Model》</a>。这个工作的具体来说是，给chatbot agent赋予特定的人格来生成一致性的回复。另一个思路，我们更希望chatbot agent可以感知到用户的身份和偏好，来提供个性化的对话。<br>个性化的对话系统也是分为闲聊式对话和任务型对话。</p><ul><li>个性化闲聊式对话的训练语料有<a href="https://arxiv.org/abs/1801.07243" target="_blank" rel="noopener">Persona-Chat</a>和在此基础上扩展得到的<a href="https://arxiv.org/abs/1902.00098" target="_blank" rel="noopener">CONVAI2</a>，另外还有<a href="https://arxiv.org/abs/1809.01984" target="_blank" rel="noopener">Mazare et al.(2018)</a>基于Reddit Corpus构建的个性化对话语料。</li><li>个性化的任务型对话系统的训练语料有<a href="https://arxiv.org/abs/1706.07503" target="_blank" rel="noopener">personalized bAbI dialog corpus</a>。</li></ul><h3 id="缺乏个性化的任务型对话系统的不足"><a href="#缺乏个性化的任务型对话系统的不足" class="headerlink" title="缺乏个性化的任务型对话系统的不足"></a>缺乏个性化的任务型对话系统的不足</h3><p>只基于对话历史而未考虑个性化的任务型对话系统存在以下不足：</p><ol><li>不能根据用户的身份和偏好来动态地调整语言风格。</li><li>缺乏根据用户的信息来动态调整对话策略的能力。</li><li>难以处理用户请求中的歧义项。比如：“contact”可以理解为“电话”，也可以理解为“社交媒体”。个性化模型可以学习到年轻人倾向于社交媒体，而老年人倾向于电话这一事实，从而消除歧义。</li></ol><p>个性化任务型对话系统的研究动机就是解决上述问题。个性化的任务型对话与chatbot有所区别。个性化的chatbot研究倾向于赋予chatbot以特定的用户画像，来生成一致性的回复。而个性化的任务型对话更多的是感知到用户的身份和偏好，从而根据不同的用户身份来调整回复的语言风格和对话策略，从而提高对话效率和用户满意度。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>本文的个性化任务型对话模型是基于Memory Network的，因此对Memory Network做简单的介绍。Memory Network的相关工作有许多，本文中介绍的是发表在ICLR2017的<a href="https://arxiv.org/abs/1605.07683" target="_blank" rel="noopener">《Learning end-to-end goal-oriented dialog》</a>，这是基于检索的任务型对话。Memory Network包括了两个部分：context memory和next sentence prediction。</p><h4 id="Memory-Representation"><a href="#Memory-Representation" class="headerlink" title="Memory Representation"></a>Memory Representation</h4><p>memory中储存的是对话历史。在时间步t，对话历史为$t$句用户的对话$\lbrace{c_1^u,c_2^u,…,c_t^u}\rbrace$以及$t-1$句对话系统的回复$\lbrace{u_1^r,u_2^r,…,u_{t-1}^r}\rbrace$。直接采用了词袋方法，经过embedding层，将对话历史表示为向量。 $$m = (A\Phi(c_1^u),A\Phi(c_1^r),…,A\Phi(c_{t-1}^u),A\Phi(c_{t-1}^r))$$ 其中$\Phi(\cdot)$是one-hot向量表示，$A$是embedding table。<br>用同样的方法，将上一句话$c_t^u$编码为attention机制的的query：$$q = A\Phi(c_t^u)$$ </p><h4 id="Memory-Operation"><a href="#Memory-Operation" class="headerlink" title="Memory Operation"></a>Memory Operation</h4><p>采用attention机制对context memory进行读取。计算方式如下：$$o = R\sum_i\alpha_im_i$$ $$\alpha_i = softmax(q^\top m_i)$$ 其中$R$是线性层的权重矩阵。<br><strong>多跳机制</strong><br>将query按下式进行更新，再采用attention机制对context memory进行读取。$$q_2 = q + o$$</p><h4 id="next-sentence-prediction"><a href="#next-sentence-prediction" class="headerlink" title="next sentence prediction"></a>next sentence prediction</h4><p>设有C个候选句子$y_i$。先将候选句子进行向量化表示: $$r_i = W\Phi(y_i)$$ 其中$W$是另一个embedding table。<br>则最终的预测概率分布为：$$\hat{r} = softmax({q_{N+1}}^\top r_1,…,{q_{N+1}}^\top r_C)$$</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型包括profile model和preference model。profile model将用户的个性化信息结合到模型中。preference model建模用户信息与知识库之间的联系，来消除歧义。</p><div align="center"><img src="/images/personalized_memnet.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="profile-model"><a href="#profile-model" class="headerlink" title="profile model"></a>profile model</h4><p><strong>user embedding的计算</strong><br>对话语料中用$n$个键值对属性$\lbrace(k_i,v_i)\rbrace_{i=1}^n$来描述用户画像。第$i$个属性被表示为one-hot vector $a_i \in R^{d_i}$，其中$d_i$表示第$i$个属性$k_i$可能的取值个数。则总的用户画像的one-hot表示为$$\hat{a} = concat(a_1,a_2,…,a_n)$$ 有$\hat{a}\in R^{d_p}$，其中$d_p = \sum_{i=1}^nd_i$<br>进一步将用户画像表示为分布式向量$$p = P\hat{a}$$ 其中$P$可以看作是embedding table。</p><p><strong>将$p$结合到对话模型</strong><br>将user embedding $p$结合到模型中的两个地方。</p><ol><li>结合到多跳机制的query更新公式中。query对于context memory的读取和预测概率的生成起着重要作用。 $$q_{i+1} = q_i + o_i + p$$</li><li>根据user embedding对候选句子的向量表示进行修改。 $$r_i^* = \sigma(p^\top r_i) r_i$$</li></ol><p><strong>global memory</strong><br>除了context memory外，模型还有一个memory用来储存相似用户的对话历史。本文中将相似用户定义为有相同的属性值的用户。global memory的读取方式与context memory的读取方式相同。最后将两部分的query结合起来：$$q^+ = q + q^g$$</p><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.luolc.com/" target="_blank" rel="noopener">本文作者的个人主页</a></li><li><a href="https://www.luolc.com/publications/personalized-goal-oriented-dialog/" target="_blank" rel="noopener">作者的英文论文简介</a></li><li><a href="https://mp.weixin.qq.com/s/AqzdRoXthrUFUOqSNwgfqQ" target="_blank" rel="noopener">作者的中文论文简介</a></li><li><a href="https://helicqin.github.io/2018/12/11/Learning%20Personalized%20End-to-End%20Goal-Oriented%20Dialog/" target="_blank" rel="noopener">本篇论文的阅读笔记</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】AAAI2019&lt;br&gt;【链接】&lt;a href=&quot;https://arxiv.org/pdf/1811.04604v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1811.04604v1.pdf&lt;/a&gt;&lt;br&gt;【代码】未公布&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="Goal-Oriented Dialog" scheme="http://yoursite.com/tags/Goal-Oriented-Dialog/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《A Dynamic Speaker Model for Conversational Interactions》</title>
    <link href="http://yoursite.com/2020/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AA-Dynamic-Speaker-Model-for-Conversational-Interactions%E3%80%8B/"/>
    <id>http://yoursite.com/2020/01/02/论文笔记《A-Dynamic-Speaker-Model-for-Conversational-Interactions》/</id>
    <published>2020-01-02T12:31:22.000Z</published>
    <updated>2020-01-02T14:08:56.015Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://www.aclweb.org/anthology/N19-1284/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/N19-1284/</a><br>【代码】：<a href="https://github.com/hao-cheng/dynamic_speaker_model" target="_blank" rel="noopener">https://github.com/hao-cheng/dynamic_speaker_model</a></p></blockquote><a id="more"></a><p>这篇论文是由华盛顿大学发表的。</p><h3 id="个性化对话系统的研究现状"><a href="#个性化对话系统的研究现状" class="headerlink" title="个性化对话系统的研究现状"></a>个性化对话系统的研究现状</h3><p>近几年来，基于用户画像（personal information）来生成个性化的回复已经成为对话系统领域的一个研究热点。为什么要将persona结合到对话系统模型中呢？目的是提高生成回复的一致性，来获取用户信任，让用户在对话中更投入。生成回复的一致性具体指什么呢？举个例子，你问对话系统“你的职业是什么？”，它可能回答是出租车司机；当你再次问这个问题时，它又可能回答是老师。给定对话系统一个用户画像（persona），基于这个persona来生成回复，就可以提高生成回复的一致性，避免出现这种问题。基于persona来生成个性化的回复，也是对话系统可以通过图灵测试的必要条件。</p><p>现有的工作中，将persona结合到对话系统模型中的思路有两种。</p><ol><li>学习一个潜在的向量user embedding（或user representation）来潜在地表示用户画像，再基于这个user embedding来生成个性化的回复。这样做的一个原因是现有的对话语料中没有相应的用户画像文本信息，随着个性化对话数据集<a href="https://arxiv.org/abs/1801.07243" target="_blank" rel="noopener">Persona-Chat</a>以及<a href="https://arxiv.org/abs/1902.00098" target="_blank" rel="noopener">CONVAI2</a>的提出，就有了第二种思路。</li><li>直接用键值对属性值信息或者是自由文本来明确地描述用户画像，再生成个性化的回复。</li></ol><h3 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h3><p>这篇论文属于第一种思路，主要内容是用神经网络模型从对话历史中学习一个动态更新的user embedding。并且将学习到的user embedding用到了两个下游任务（对话话题分类、dialog acts分类）中来评估user embedding的学习效果。<br>论文中提到，学习一个动态更新的user embedding的动机有两个：</p><ol><li>用户的对话反映了这个用户的对话意图、语言风格等特征。因此，可以从用户的对话中来学习user embedding来建模和表征用户的这些个性化特征。</li><li>随着对话的进行，用户的个人信息得到累积。因此，可以从对话中提炼和动态更新user embedding。</li></ol><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>如下图所示，模型包括三个部分：<strong>Latent Mode Analyzer</strong>，<strong>Speaker State Tracker</strong>，<strong>Speaker Language Predictor</strong>。 </p><div align="center"><img src="/images/Dynamic_user_embedding.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font></div><h4 id="Latent-Mode-Analyzer"><a href="#Latent-Mode-Analyzer" class="headerlink" title="Latent Mode Analyzer"></a>Latent Mode Analyzer</h4><p><strong>Latent Mode Analyzer</strong>的作用是从输入的单轮对话中学到一个 local speaker vector。采用了Bi-LSTM + Attention机制。<br>时间步t，输入为单轮对话$\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}\rbrace}$，经过embedding层后送入到Bi-LSTM层，将前向LSTM和后向LSTM的最后一个隐藏状态连接起来作为句子总的向量表示$s_t$: $$s_t=[e^F_{t,N_t},e^B_{t,1}]$$ 其中，$e^F_{t,N_t},e^B_{t,1}$分别表示前向LSTM和后向LSTM的最后一个隐藏状态。<br>接着，再使用attention机制。将句子的向量表示作为attention机制的query，将 $K$ 个全局的mode vectors $\lbrace{u_1,u_2,…,u_K\rbrace}$作为attention机制的keys和values。这$K$个mode vectors也是模型参数，可以看作是用户在不同方面的个性化特征。那么可以通过attention机制计算得到local speaker vector $\tilde{u_t}$ :$$\tilde{u_t} = \sum_{k=1}^Ka_{t,k}u_k$$ $$a_{t,k} = softmax(\beta_{t,k})$$ $$\beta_{t,k} = &lt;Ps_t,Qu_k&gt;$$ 其中$&lt; , &gt;$表示点乘操作。</p><h4 id="Speaker-State-Tracker"><a href="#Speaker-State-Tracker" class="headerlink" title="Speaker State Tracker"></a>Speaker State Tracker</h4><p><strong>Speaker State Tracker</strong> 的作用是动态更新speaker state vector。实质上是一个单向的LSTM。<br>在时间步t，根据当前的local speaker vector $\tilde{u_t}$ 和 上一个时间步的隐藏状态$h_{t-1}$来更新隐藏状态$h_t$。将$h_t$作为时间步t的speaker state vector。$$h_t = LSTM(\tilde{u_t},h_{t-1})$$</p><h4 id="Speaker-Language-Predictor"><a href="#Speaker-Language-Predictor" class="headerlink" title="Speaker Language Predictor"></a>Speaker Language Predictor</h4><p><strong>Speaker Language Predictor</strong>的作用是促进前两个模块的参数学习，根据speaker state vector来重构句子$\lbrace{w_{t,1},w_{t,2},…,w_{t,{N_t}}}\rbrace$。<br>该模块实质上是一个条件语言模型，预测条件概率 $P(w_{t,n}|w_{t,&lt;n})$。采用了单向的LSTM，LSTM的隐藏状态更新公式为：$$d_{t,n} = LSTM(R^I(w_{t,n-1},h_t),d_{t,n-1})$$ 其中$R^I()$是一个线性层。<br>则语言模型生成下一个词的条件概率为：$$P(w_{t,n}|w_{t,&lt;n}) = softmax(VR^O(h_t,d_{t,n}))$$ 其中$R^O()$是一个线性层。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>模型最小化负对数似然函数来更新模型参数: $$L = -\sum_{t}\sum_{n}log P(w_{t,n}|w_{t,&lt;n})$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://www.aclweb.org/anthology/N19-1284/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aclweb.org/anthology/N19-1284/&lt;/a&gt;&lt;br&gt;【代码】：&lt;a href=&quot;https://github.com/hao-cheng/dynamic_speaker_model&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hao-cheng/dynamic_speaker_model&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Towards Knowledge-Based Personalized Product Description Generation in E-commerce》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ATowards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Towards-Knowledge-Based-Personalized-Product-Description-Generation-in-E-commerce》/</id>
    <published>2019-10-25T10:52:24.000Z</published>
    <updated>2019-10-25T14:49:12.478Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ARXIV 2019<br>【链接】：<a href="https://arxiv.org/abs/1903.12457v3" target="_blank" rel="noopener">https://arxiv.org/abs/1903.12457v3</a><br>【代码、数据集】：<a href="https://github.com/THUDM/KOBE" target="_blank" rel="noopener">https://github.com/THUDM/KOBE</a></p></blockquote><a id="more"></a><p>清华大学和阿里巴巴发表的论文。论文的研究内容是给定商品名称，商品的属性特征和外部知识库，自动生成商品的描述。</p><p><strong>数据集描述</strong><br>论文在淘宝收集了一个真实的商品描述数据集，包含了212,9187个商品名称和描述。数据集是公开的，下载地址为：<a href="https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717" target="_blank" rel="noopener">https://tianchi.aliyun.com/dataset/dataDetail?dataId=9717</a></p><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>给定商品名称$x = \{x_1,…,x_n\}$，要求生成个性化的、富有信息的商品描述$y = \{y_1,…,y_m\}$。引入两个附加信息：商品属性 和 外部知识：</p><ol><li>Attributes<br> 每个商品名称$x$对应$l$个属性$a = \{a_1,…,a_l\}$。论文中包含两种属性，商品的某个方面（如质量、外观等）和用户类型（反映用户的兴趣）。</li><li>Knowledge<br> 论文采用一个大规模的中文知识图谱<code>CN-DBpedia</code>作为外部knowledge。<code>CN-DBpedia</code>包含大量命名实体$V$索引的原始文本条目$W$。每个条目包含一个命名实体$v &ensp; \in V$作为key，对应一个knowledge句子 $w = \{w_1,…,w_u\} \in W$作为value。</li></ol><p>最终的任务定义为：给定商品名称$x$、商品属性$a$和相关的knowledge $w$，要求生成个性化的，信息量丰富的回复$y$。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型采用了基于Transformer的encoder-decoder框架，结合了两个模块：Attribute Fusion和knowledge Incorporation。</p><div align="center"><img src="/images/KOBE.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h4 id="encoder-decoder框架"><a href="#encoder-decoder框架" class="headerlink" title="encoder-decoder框架"></a>encoder-decoder框架</h4><p>Transformer是完全基于self-attention机制 和 前馈神经网络（FFN,feed-forward neural network）的。</p><ol><li><strong>encoder</strong><br> 对于输入$x = \{x_1,…,x_n\}$，先经过embedding层得到word embedding representation $e = \{e_1,…,e_n\}$，连同position embedding一起，作为encoder layers的输入，得到context representation $h = \{h_1,…,h_n\}$。<br> 在embedding层之上，encoder layers由完全相同的6层堆叠组成，每层transformer包括<code>multi-head self-attention</code>和<code>FFN</code>两部分。<ol><li><strong>attention机制</strong><br> attention机制根据queries $Q$和keys $K$计算在values $V$上的分布，进而得到attention的输出。$$C = \alpha V$$ $$\alpha = softmax(f(Q,K))$$其中$C$表示attention的输出，$\alpha$表示attention的分布，$f$表示计算attention分数的函数。</li><li><strong>uni-head attention</strong><br> 对于单头attention，queries $Q$，keys $K$和values $V$分别是输入context $e$的线性转换。即$Q = W_Qe$, $K = W_Ke$ 和 $V = W_Ve$。此时，uni-head attention可以表示为 $$C_{self} = softmax(\frac{QK^T}{\sqrt{d_k}})V$$ 其中$d_k$表示输入$e$的维度。</li><li><strong>multi-head attention</strong><br> 对于多头attention，将$C^i_{self}, i\in \{1,2,…,c\}$连接起来，作为FFN的输入。其中$c$表示heads的数量。<br>再经过前馈神经网络（FFN），FFN的函数表示为：$$FFN(z) = W_2(Relu(W_1z + b_1)) + b_2$$</li></ol></li><li><strong>decoder</strong><br> 与encoder类似，decoder也是由完全相同的6层堆叠而成的，每层包含<code>multi-head attention</code>和<code>FFN</code>两部分。不同于encoder的”self-attenion”，decoder的<code>multi-head attention</code>是“context attention”。queries $Q$是decoder state的线性转换，keys $K$和values $V$是context states $h = \{h_1,…,h_n\}$的线性转换。</li><li><strong>Training</strong><br> 模型的目标是最大化似然函数。模型的目标函数是：$$P(y|x) = \prod_{t=1}^m P(y_t|y_{&lt;t},x)\tag{1}$$</li></ol><h4 id="Attribute-Fusion模块"><a href="#Attribute-Fusion模块" class="headerlink" title="Attribute Fusion模块"></a>Attribute Fusion模块</h4><p>商品的属性$a = \{a_1,a_2\}$，包含商品方面 和 用户类型两个属性。先经过embedding层得到attribute representation $\{e_{a_1},e_{a_2}\}$，再做attribute average得到总的attribute representation $e_{attr}$： $$e_{attr} = \frac{1}{2}\sum_{i=1}^2e_{a_i}$$ 如何有效结合$e_{attr}$呢？在基于RNN的模型中，方法比较多，比如：用$e_{attr}$来attend context representation，或着作为decoder隐藏状态更新的输入等。但本文中的模型是基于Transformer的，直接将attribute embedding $e_{attr}$与word embedding $e_i$相加，来结合商品的属性信息。如Fig.2所示。</p><div align="center"><img src="/images/attributeFusion.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.2. Attribute Fusion的示意图</font></div><p>此时，公式（1）中的目标函数变为$$ P(y|x,a) = \prod_{t=1}^m P(y_t|y_{&lt;t},x,a)$$</p><h4 id="Knowledge-Incorporation"><a href="#Knowledge-Incorporation" class="headerlink" title="Knowledge Incorporation"></a>Knowledge Incorporation</h4><p>knowledge incorporation包括三个部分，knowledge检索，knowledge编码，和knowledge结合。</p><ol><li><strong>knowledge retrieval</strong><br> 给定商品名称$x = \{x_1,…,x_n\}$，对于每个word $x_i$匹配对应的命名实体$v_i \in V$。再根据命名实体$v_i$，从$W$中检索对应的knowledge $w_i$。对于每个商品，最多抽取5个匹配的knowledge，再用分隔符 “<sep>”连接起来。</sep></li><li><strong>knowledge encoding</strong><br> 类似于$x = \{x_1,…,x_n\}$通过encoder编码得到context representation $h = \{h_1,….,h_n\}$，将检索到的knowledge经过一个基于Transformer的knowledge encoder得到knowledge representation $u$。</li><li><strong>knowledge combination</strong><br> 用BiDAF(bidirectional attention flow)来结合context representation $h$和knowledge representation $u$。BiDAF计算两个方向的attention：title-to-knowledge attention和knowledge-to-context attention。<ol><li><strong>相似度矩阵S</strong><br> 先计算一个context representation $h \in R^{n \times d}$和knowledge representation $u \in R^{u \times d}$之间的相似度矩阵$S \in R^{n\times u}$。其中$S_{ij}$衡量第i个title word和第j个knowledge word之间的相似度。$$S_{ij} = \alpha(h_i,u_j) \in R$$ 其中$\alpha()$是计算两个向量之间相似度的函数。 $$\alpha(h,u) = W_s^T[h;u;h \cdot u], &emsp; W_s\in R^{3d}$$</li><li><strong>title-to-knowledge attention</strong><br> 表明了对于每个title word，哪个knowledge word是最相关的。<br> $a_i \in R^u$表示对于第i个title word，在所有knowledge words上的attention权重分布。$$a_i = softmax(S_{i:}) &emsp; \in R^u$$ 其中，$S_{i:}$表示相似度矩阵$S \in R^{n\times u}$的第i个行向量。对于所有的$i$，$a_i$满足：$$\sum_ja_{ij} = 1$$ 对于第i个title word，attended knowledge vector为：$$\widetilde{u_i} = \sum_ja_{ij}u_j$$ 则对于所有的title words $\{x_1,…,x_n\}$，有$\widetilde{u} \in R^{n \times d}$</li><li><strong>knowledge-to-title attention</strong><br> 表明了对于每个knowledge word，哪个title word是最相似的。<br> 计算在所有title words上的attention权重分布：$$b = softmax(max(S_{i:}))$$ 则attended title vector为$$\widetilde{h} = \sum_{k}b_kh_k$$ 把$\widetilde{h}$在列上复制n次，得到$\widetilde{h} \in R^{n\times d}$。</li><li><strong>输出融合</strong><br> 做一个简单的连接操作，得到组合representation： $[h;\widetilde{u};h\circ \widetilde{u};h\circ \widetilde{h}] \in R^{4d \times n}$</li></ol></li></ol><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li>BiDAF(bidirectional attention flow)可以参考:<ul><li><a href="https://spring-quan.github.io/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/" target="_blank" rel="noopener">论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》</a></li></ul></li><li>中文知识图谱<code>CN-DBpedia</code>可以参考：<ul><li><a href="https://www.semanticscholar.org/paper/CN-DBpedia%3A-A-Never-Ending-Chinese-Knowledge-System-Xu-Xu/2c69bbb3b7ba3f324276924bab6f41de467c928a" target="_blank" rel="noopener">《CN-DBpedia: A Never-Ending Chinese Knowledge Extraction System》</a></li><li><a href="http://kw.fudan.edu.cn/cndbpedia/download/" target="_blank" rel="noopener">CN-DBpedia Dump数据下载</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ARXIV 2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1903.12457v3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1903.12457v3&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/THUDM/KOBE&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/THUDM/KOBE&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
      <category term="BiDAF" scheme="http://yoursite.com/tags/BiDAF/"/>
    
      <category term="Knowledge-Based" scheme="http://yoursite.com/tags/Knowledge-Based/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Automatic Generation of Personalized Comment Based on User Profile》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AAutomatic-Generation-of-Personalized-Comment-Based-on-User-Profile%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Automatic-Generation-of-Personalized-Comment-Based-on-User-Profile》/</id>
    <published>2019-10-25T07:15:24.000Z</published>
    <updated>2019-10-25T09:09:29.504Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ACL2019<br>【链接】：<a href="https://arxiv.org/abs/1907.10371" target="_blank" rel="noopener">https://arxiv.org/abs/1907.10371</a><br>【代码、数据集】：<a href="https://github.com/Walleclipse/AGPC" target="_blank" rel="noopener">https://github.com/Walleclipse/AGPC</a></p></blockquote><a id="more"></a><p>北京大学发表在ACL2019的论文。论文的研究内容是基于User profile的评论生成。</p><h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><p>论文从微博收集了一个中文数据集，没有公开，但给出了部分样例数据。这个数据集可以看作基于persona的单轮对话数据集。将微博的博文看作对话历史，将用户的评论看作回复。用户画像包括两个部分，键值对形式的人口统计特征属性（年龄、性别、地区等）和 句子形式的个人描述（微博签名）。</p><div align="center"><img src="/images/PCdata.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.1. 数据样例</font></div><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>给定博文 $X = \lbrace{x_1,…,x_n}\rbrace$和用户画像 $U = \lbrace{F,D}\rbrace$，其中$F = \lbrace{f_1,…,f_k}\rbrace$是用户的数值化属性特征，$D = \lbrace{d_1,…,d_l}\rbrace$是句子形式的个人描述。要求生成与personal profile一致的回复$Y = \lbrace{y_1,…,y_m}\rbrace$。$$Y^* = \underset{Y}{argmax}(Y|X,U)$$</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><div align="center"><img src="/images/PersonalComment.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.2.模型结构的示意图</font></div><h4 id="encoder-decoder框架"><a href="#encoder-decoder框架" class="headerlink" title="encoder-decoder框架"></a>encoder-decoder框架</h4><p>基本框架当然还是<code>seq2seq模型 + attention机制</code>。$X= \{x_1,…,x_n\}$经过encoder转换为向量表示$h^X = \{h^X_1,…,h_n^X\}$，encoder采用Bi-LSTM。$$h_t^X = LSTM_{enc}^X(h_{t-1}^X,x_t)$$ decoder采用单向LSTM，decoder的隐藏状态更新公式为：$$s_t = LSTM_{dec}(s_{t-1},[c_t^X;e(y_{t-1})]) \tag{1}$$其中$c_t^X$表示时间步t，在所有encoder hidden states $h^X = \{h^X_1,…,h_n^X\}$上使用attention得到的context vector。<br>则decoder生成词的概率分布为：$$p(y_t) = softmax(W_os_t)$$ 采用负对数似然函数作为目标函数，模型最大化真实回复 $Y^* = \{y_1,…,y_m\}$ 的似然函数：$$Loss = -\sum_{t=1}^m log\Bigl(p(y_t|y_{&lt;t},X,U)\Bigr)$$</p><h4 id="User-Feature-Embedding-with-Gated-Memory"><a href="#User-Feature-Embedding-with-Gated-Memory" class="headerlink" title="User Feature Embedding with Gated Memory"></a>User Feature Embedding with Gated Memory</h4><p>将用户的数值化特征属性$F = \{f_1,…,f_k\}$经过一个全连接层，得到向量表示$v_u$。$v_u$可以看作是<code>user feature embedding</code>，表明用户的个人特征。如果user feature embedding是静态的，在decode过程中会影响生成回复的语法性。为了解决这个问题，设计了一个<code>gated memory</code>来动态地表达用户的个人特征。<br>在decode过程中，保持一个<code>Internal personal state</code>$M_t$，在decode过程中$M_t$逐渐衰减，decode结束，$M_t$衰减为0，表示用户的个人特征完全表达了。$M_0$的初始值设为$v_u$。 $$g_t^u = sigmoid(W_g^us_t)$$ $$M_0 = v_u$$ $$M_t = g_t^u \cdot M_{t-1}, &emsp; t&gt;0$$ 引入输出门机制$g_t^o$来充值persona信息的流动：$$g_t^o = sigmoid(W_g^o[s_{t-1};e(y_{t-1});c_t^X])$$ 则时间步t，personal information为：$$M_t^o = g_t^o\cdot M_t$$</p><h4 id="Blog-User-Co-Attention"><a href="#Blog-User-Co-Attention" class="headerlink" title="Blog-User Co-Attention"></a>Blog-User Co-Attention</h4><p>实质上是在用户的个人描述$D = \{d_1,…,d_l\}$上使用attention机制。先用另一个persona encoder来编码$D = \{d_1,…,d_l\}$，得到向量表示$\{h_1^D,…,h_l^D\}$。persona encoder采用LSTM: $$h_t^D = LSTM_{enc}^D(h_{t-1}^D,d_t)$$ 在$\{h_1^D,…,h_l^D\}$上使用attention机制，得到总的persona context vector $c_t^D$ $$c_t^D = \sum_{j=1}^k\alpha_{tj}h_j^D$$ $$\alpha_{tj} = softmax(\beta_{tj})$$ $$\beta_{tj} = score(s_{t-1},h_j^D) = s_{t-1}W_ah_j^D$$ 结合$c_t^D$和$c_t^X$作为时间步t总的context vector $c_t$: $$c_t = [c_t^X,c_t^D]$$ 则式（1）中decoder的隐藏状态更新公式变为：$$s_t = LSTM_{dec}(s_{t-1},[c_t;e(y_{t-1});M_t^o])$$</p><h4 id="External-Personal-Expression"><a href="#External-Personal-Expression" class="headerlink" title="External Personal Expression"></a>External Personal Expression</h4><p>通过将<code>internal persona state</code>$M_t$和persona context vector $c_t^D$作为decoder隐藏状态更新的输入，来结合persona信息，进而影响decode过程。这种影响是隐性的，为了更明确地利用用户信息来指导word的生成，将用户信息直接作为输出层的输入。先计算一个<code>user representation</code> $r_t^u$: $$r_t^u = W_r[v_u;c_t^D]$$ 将$r_t^u$作为输出层的输入，则生成词的概率分布为：$$p(y_t) = softmax(W_o[s_t;r_t^u])$$</p><h3 id="相似论文"><a href="#相似论文" class="headerlink" title="相似论文"></a>相似论文</h3><ul><li><a href="https://arxiv.org/abs/1901.09672" target="_blank" rel="noopener">《Personalized Dialogue Generation with Diversified Traits》</a> <ul><li><a href="https://spring-quan.github.io/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8APersonalized-Dialogue-Generation-with-Diversified-Traits%E3%80%8B/" target="_blank" rel="noopener">笔记链接</a></li><li>异同点比较：<ul><li>不同点是：本篇论文用internal persona state $M_t$和personal context vector $c_t^D$来作为decoder隐藏状态$s_t$更新的输入，进而影响word的生成。<br>而相似的这篇论文中，将personal vector $v_p$ 作为attention机制的query，来attend对话历史。含义是用persona vector $v_p$来选择context相关的信息。</li><li>相同点是：两篇论文都把persona information 作为输出层的输入，来明确地影响word的生成。</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1907.10371&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1907.10371&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/Walleclipse/AGPC&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Walleclipse/AGPC&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Personalized Dialogue Generation with Diversified Traits》</title>
    <link href="http://yoursite.com/2019/10/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8APersonalized-Dialogue-Generation-with-Diversified-Traits%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/25/论文笔记《Personalized-Dialogue-Generation-with-Diversified-Traits》/</id>
    <published>2019-10-25T02:51:12.000Z</published>
    <updated>2019-10-25T06:33:49.351Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【链接】：<a href="https://arxiv.org/abs/1901.09672" target="_blank" rel="noopener">https://arxiv.org/abs/1901.09672</a><br>【代码、数据集】：无</p></blockquote><a id="more"></a><p>三星电子中国、清华大学黄明烈教授提交到<code>ARXIV</code>的论文。论文的研究内容是基于persona的单轮对话系统。<br><strong>数据集描述</strong><br>论文中构建了一个<code>PersonaDialog</code>的数据集，但数据集没有公开。数据是从微博上爬取，把用户的博文作为对话的post，把用户的评论作为回复。persona是用键值对来描述用户的属性（年龄、性别、地址、兴趣标签等）。而不是用几句话的文本来描述用户画像的。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>给定单句的对话历史$X = \lbrace{x_1,…,x_n\rbrace}$，以及回复者的N个属性$T = \lbrace{t_1,…,t_N\rbrace}$，其中$t_i = &ensp; &lt;k_i,v_i&gt;$为键值对。要求生成与用户的画像相一致的回复$Y = \lbrace{y_1,y_2,…,y_m\rbrace}$。$$Y^* = \underset{Y}{arg max} P(Y|X,T)$$</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>基本框架当然是<code>seq2seq模型+attention机制</code>。将对话历史$X = \lbrace{x_1,…,x_n}\rbrace$经过encoder编码为$\lbrace{h_1,…,h_n}\rbrace$。设时间步t，decoder的上一个隐藏状态为$s_{t-1}$，计算attention得到总的context vector $c_t$：$$c_t = \sum_{i=1}^n\alpha_ih_i$$ $$alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i) = V^T\cdot tanh(W^1_{\alpha}s_{t-1} + W^2_{\alpha}h_i) \tag{1}$$decoder RNN的隐藏状态更新公式是：$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 生成$y_t$的概率分布为：$$p(y_t) = softmax(W_os_t + b_{out})$$</p><div align="center"><img src="/images/PersonaTraitFusion.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型结构的示意图</font></div><h3 id="Personality-Trait-Fusion"><a href="#Personality-Trait-Fusion" class="headerlink" title="Personality Trait Fusion"></a>Personality Trait Fusion</h3><p>这个模块用来把整合persona得到persona representation $v_p$，并用$v_p$来影响decode过程。<br>将回复者的属性描述$T = \lbrace{t_1,…,t_N}\rbrace$经过embedding层，得到对应的向量表示$\lbrace{v_{t_1},…,v_{t_N}}\rbrace$。论文提出了三种整合persona Trait的方法。</p><h4 id="Trait-Attention"><a href="#Trait-Attention" class="headerlink" title="Trait Attention"></a>Trait Attention</h4><p>计算decoder的隐藏状态$s_{t-1}$在Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$上的attention，来整合persona Trait，得到persona representation $v_p$。$$v_p = \sum_{i=1}^N\alpha_i^pv_{t_i}$$ $$\alpha_i^p = softmax(\beta_i^p)$$ $$\beta_i^p = score(s_{t-1},v_{t_i}) = V_p^T \cdot tanh(W_p^1s_{t-1} + W_p^2v_{t_i})$$</p><h4 id="Trait-Average"><a href="#Trait-Average" class="headerlink" title="Trait Average"></a>Trait Average</h4><p>直接在Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$上取平均值，来作为persona representation $v_p$：$$v_p = \frac{1}{N}\sum_{i=1}^Nv_{t_i}$$</p><h4 id="Trait-Concatenation"><a href="#Trait-Concatenation" class="headerlink" title="Trait  Concatenation"></a>Trait  Concatenation</h4><p>直接把Trait representation $\lbrace{v_{t_1},…,v_{t_N}}\rbrace$做连接操作，作为persona representation $v_p$。</p><h3 id="使用persona-representation-v-p-进行decode"><a href="#使用persona-representation-v-p-进行decode" class="headerlink" title="使用persona representation $v_p$进行decode"></a>使用persona representation $v_p$进行decode</h3><h4 id="Persona-Aware-Attention"><a href="#Persona-Aware-Attention" class="headerlink" title="Persona-Aware Attention"></a>Persona-Aware Attention</h4><p>让persona representation $v_p$来影响式（1）中attention权重的计算。这种方法可以获得基于persona representation $v_p$的context vector $c_t$。$$\beta_i = f(s_{t-1},h_i,v_p) = V^T \cdot tanh(W^1_{\alpha}s_{t-1} + W^2_{\alpha}h_i + W^3_{\alpha}v_p)$$</p><h4 id="Persona-Aware-Bias"><a href="#Persona-Aware-Bias" class="headerlink" title="Persona-Aware Bias"></a>Persona-Aware Bias</h4><p>在输出层结合$v_p$，来影响decode过程。$$p(y_t) = softmax(a_t\cdot W_o^1s_t + (1-s_t)\cdot W_o^2v_p + b_{out})$$ $$a_t = \sigma(V_o^T\cdot s_t)$$ 其中$a_t\in [0,1]$作为一个门机制，来控制生成persona相关的词，或者语义相关的词。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1901.09672&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1901.09672&lt;/a&gt;&lt;br&gt;【代码、数据集】：无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Personalization" scheme="http://yoursite.com/tags/Personalization/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》</title>
    <link href="http://yoursite.com/2019/10/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ADEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/24/论文笔记《DEEPCOPY-Grounded-Response-Generation-with-Hierarchical-Pointer-Networks》/</id>
    <published>2019-10-24T08:19:48.000Z</published>
    <updated>2019-10-25T03:08:22.933Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【链接】：<a href="https://arxiv.org/abs/1908.10731" target="_blank" rel="noopener">https://arxiv.org/abs/1908.10731</a><br>【代码、数据集】：无</p></blockquote><a id="more"></a><p>论文的研究内容是conditional text generation，基于knowledge facts的单轮对话。基于给定的对话历史和外部知识，生成合适的回复。论文中将K句话的个人描述作为knowledge facts。<br>论文采用的模型是seq2seq模型 + attention机制 + 分级pointer network。<br>论文的亮点是对比模型的思路，可以重点学习一下如何设计对比模型。</p><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>输入分为两部分：对话历史$X = (x_1,…,x_n)$和K个相关的knowledge facts，其中第i个knowledge fact为$f^i = (f^i_1,…,f^i_{n_i}),i\in \lbrack 1,K \rbrack$。要求生成输出$Y = (y_1,…,y_m)$。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="baseline-seq2seq模型-attention机制"><a href="#baseline-seq2seq模型-attention机制" class="headerlink" title="baseline: seq2seq模型 + attention机制"></a>baseline: seq2seq模型 + attention机制</h3><p>seq2seq模型是基于encoder-decoder框架的。encoder包括embedding层和LSTM层，对于输入$X$，经过encoder得到相应的向量表示 $\lbrace{h_1,…,h_n}\rbrace$。decoder采用单向LSTM，设时间步t的隐藏状态为$s_t$，隐藏状态更新公式为：$$s_t = f(s_{t-1},y_t,c_t)$$ 其中$s_{t-1}$是上一个时间步decoder的隐藏状态，$y_{t-1}$是上一个时间步的输出。c_t为用attention机制计算得到的context vector。计算过程如下：$$c_t = \sum_{i=1}^{n}\alpha_ih_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i)$$ 其中$score(s,h)$是计算$s$和$h$之间相似度的函数。<br>在时间步t，decoder的隐藏状态$s_t$和对应的context vector $c_t$，经过线性层和softmax层，得到在固定词汇表上的概率分布。$$p_g(y_t) = softmax(W[h_t,c_t] + b)$$</p><p>可以看出“seq2seq模型 + attention机制”可以用来完成”text-to-text”的text generation的任务。输入是text，没有其他的附加信息（比如knowledge，persona，context等），输出也是text。根据输入的不同，可以得到以下三个模型。</p><ol><li><strong>SEQ2SEQ + NOFACT</strong><br> 只把对话历史$X$作为encoder的输入。</li><li><strong>SEQ2SEQ + BESTFACTCONTEXT</strong><ul><li>先从K个knowledge facts $\lbrace{f^1,…,f^K}\rbrace$中选择与dialog context $X$最相似的fact $f^c$</li><li>再将$f^c$与dialog context $X$连接起来的$[X;f^c]$，作为encoder的输入。</li></ul></li><li><strong>SEQ2SEQ + BESTFACTCONTEXT</strong><ul><li>先从K个knowledge facts $\lbrace{f^1,…,f^K}\rbrace$中选择与truth response $Y$最相似的fact $f^r$</li><li>再将$f^r$与dialog context $X$连接起来的$[X;f^r]$，作为encoder的输入。</li></ul></li></ol><p>从这三个对比试验，可以表明是否添加knowledge fact，以及knowledge fact的不同的选择，对回复生成的影响。</p><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>“seq2seq模型 + attention机制”容易遇到 generic response的问题，需要添加附加信息作为额外的输入，来得到信息更丰富的回复。与直接把knowledge fact $f$与dialog context $X$的连接$[X;f]$作为encoder的输入不同，用Memory Network可以更好地结合knowledge facts这样的附加信息。</p><p>Memory Network的作用是可以更有效地结合knowledge facts，persona description，dialog context这样的附加信息。Memory Network的工作原理可以分成两个部分：Memory representation 和 read Memory。</p><ul><li><strong>Memory representation</strong><br>  实质上是对附加信息通过另一个facts encoder的向量化表示。$\lbrace{f^1,…,f^K}\rbrace$通过另外一个encoder来编码，经过线性变换分别得到key vectors $\lbrace{k_1,…,k_K}\rbrace$和value vectors$\lbrace{m_1,m_2,…,m_K}\rbrace$。</li><li><strong>read Memory</strong><br>  实质上是计算在$\lbrace{f^1,…,f^K}\rbrace$的attention。论文中用context encoder的最后一个隐藏状态$u$作为query，计算得到总的memory representation： $$o = \sum_{i=1}^K\alpha_im_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(u,k^i)$$ 最后把context encoder的最后一个隐藏状态$u$和总的memory representation $o$组合起来，$$\hat{u} = u + o$$ 接着用$\hat{u}$来初始化decoder的隐藏状态。</li></ul><p>根据是否使用attention机制，可以得到以下四个模型：</p><ol start="3"><li><strong>MEMNET</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 实际上相当于没有用attention机制的seq2seq模型。</li><li><strong>MEMNET + CONTEXTATTENTION</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在context encoder的输出context representation $\lbrace{h_1,…,h_n}\rbrace$上的attention，得到总的context vector $c_t^{(c)}$。$$c_t^{(c)} = \sum_{i=1}^{n}\alpha_ih_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},h_i)$$ 将$c_t^{(c)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(c)})$$</li><li><strong>MEMNET + FACTATTENTION</strong><br> 用Memory Network来结合附加信息knowledge facts，用$\hat{u}$来初始化decoder的隐藏状态。<br> 另外在decoder的每个时间步，用decoder的隐藏状态$s_{t-1}$作为query，计算在facts encoder的输出facts representation $\lbrace{m_1,…,m_K}\rbrace$上的attention，得到总的facts vector $c_t^{(f)}$。 $$c_t^{(f)} = \sum_{i=1}^K\alpha_im_i$$ $$\alpha_i = softmax(\beta_i)$$ $$\beta_i = score(s_{t-1},k_i)$$ 将$c_t^{(f)}$作为decoder隐藏状态更新的输入：$$s_t = f(s_{t-1},y_{t-1},c_t^{(f)})$$</li><li><strong>MEMNET + FULLATTENTION</strong><br> 同时在context representation $\lbrace{h_1,…,h_n}\rbrace$和facts representation $\lbrace{m_1,…,m_K}\rbrace$上用attention，得到context vector $c_t^{(c)}$和facts vector $c_t^{(f)}$。把二者连接起来，作为decoder隐藏状态更新的输入。$$s_t = f(s_{t-1},y_{t-1},[c_t^{(c)},c_t^{(f)}])$$</li></ol><h3 id="seq2seq模型-copy机制"><a href="#seq2seq模型-copy机制" class="headerlink" title="seq2seq模型 + copy机制"></a>seq2seq模型 + copy机制</h3><p>seq2seq模型只能从固定的词汇表中生成word。Pointer Network的作用是可以从source input中来复制word。用Pointer Network来实现copy机制也可以分为两个部分。</p><ul><li>计算在所有input tokens $\lbrace{x_1,…,x_n}\rbrace$上的attention权重分布，作为在 $\lbrace{x_1,…,x_n}\rbrace$上的概率分布。</li><li>使用一个“soft switch”机制，在copy模式时，从source input $\lbrace{x_1,…,x_n}\rbrace$中复制word；当在generation模式时，从固定的词汇表中生成word。</li></ul><p>单纯的“seq2seq模型 + attention机制”，再结合copy机制可以得到以下三种模型：</p><ol start="7"><li><strong>SEQ2SEQ + NOFACT + COPY</strong></li><li><strong>SEQ2SEQ + BESTFACTCONTEXT + COPY</strong></li><li><strong>SEQ2SEQ + BESTFACTRESPONSE + COPY</strong></li></ol><h3 id="分级Pointer-Network"><a href="#分级Pointer-Network" class="headerlink" title="分级Pointer Network"></a>分级Pointer Network</h3><p>单纯的Pointer Network可以从单句话中复制word，使用分级Pointer Network可以从K句话中复制word。</p><div align="center"><img src="/images/overall_HPN.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 系统的总体框架图</font></div><h4 id="从dialog-context中复制word"><a href="#从dialog-context中复制word" class="headerlink" title="从dialog context中复制word"></a>从dialog context中复制word</h4><p>dialog context $X = \lbrace{x_1,…,x_n}\rbrace$经过context encoder后的context representation为$\lbrace{h_1,…,h_n}\rbrace$，设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用attention机制：$$c_t^{(x)} = \sum_{i=1}^n\alpha_i^{c}h_i$$ $$\alpha_i^{(x)} = softmax(\beta_i^{(x)})$$ $$\beta_i^{(x)} = score(s_{t-1},h_i)$$ 则从dialog context $X = \lbrace{x_1,…,x_n}\rbrace$中复制word的概率分布为$$p_{copy}^{(x)}(y_t) = \alpha_i^{(x)}  &emsp; i \in [1,n]$$</p><h4 id="从knowledge-facts中复制word"><a href="#从knowledge-facts中复制word" class="headerlink" title="从knowledge facts中复制word"></a>从knowledge facts中复制word</h4><div align="center"><img src="/images/HierPointerNetwork.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. 分级Pointer Network的示意图</font></div><p>knowledge facts包含K个句子$\lbrace{f^1,…,f^K}\rbrace$，其中$f^i = \lbrace{f^i_1,…,f^i_{n_i}}\rbrace$。经过facts encoder后，再经过线性转换，得到词级别的keys vector和values vector分别为$\lbrace{k_1^{f^i},…,k_{n_i}^{f^i}}\rbrace$和$\lbrace{m_1^{f^i},…,m_{n_i}^{f^i}}\rbrace$。设时间步t,decoder的上一个隐藏状态为$s_{t-1}$。用词级别的attention机制：$$c_t^{f^i} = \sum_{j=1}^{n_i}\alpha_j^{f^i}m_j^{f^i} &emsp; i\in \lbrack {1,K}\rbrack $$ $$\alpha_j^{f^i} = softmax(score(s_{t-1},k_j^{f^i})) &emsp; i\in [1,K],j\in [1,n_i]$$ 其中$c_t^{f^i}, i\in [1,K]$是K个句子$\lbrace{f^1,…,f^K}\rbrace$句子级别的向量表示。使用句子级别的attention机制：$$c_t^{(f)} = \sum_{i=1}^{K} = \beta_i^fc_t^{f^i}$$ $$\beta_i^f = softmax(score(s_{t-1},c_t^{f^i})) &emsp; i\in [1,K]$$ 其中从$c_t^{(f)}$是knowledge facts总的向量表示。<br>则从knowledge facts中复制word的概率分布为：$$p_{copy}^{(f)}(y_t) = \beta_i^f\alpha_j^{f^i} &emsp; i\in [1,K],j\in [1,n_i]$$</p><h4 id="总的复制word的概率分布"><a href="#总的复制word的概率分布" class="headerlink" title="总的复制word的概率分布"></a>总的复制word的概率分布</h4><p>为了把复制word的两个概率分布$p_{copy}^{(x)}$和$p_{copy}^{(f)}$结合起来，使用decoder的隐藏状态$s_t$在context representation $c_t^{(x)}$和总的facts representation $c_t^{(f)}$上用attention机制。得到attention权重分布为$\lbrack{\gamma,1- \gamma}\rbrack$。$$c_t = \gamma c_t^{(x)} + (1 - \gamma) c_t^{(f)}$$ $$\lbrack{\gamma,1-\gamma}\rbrack = softmax(\lbrack{score(s_{t-1},c_t^{(x)}),score(s_{t-1},c_t^{(f)})\rbrack})$$其中$c_t$既包含了dialog context的信息，也包含了knowledge facts的信息，可以用来更新decoder的隐藏状态。$$s_t = f(s_{t-1},y_{t-1},c_t)$$ 则总的复制word的概率分布为$$p_{copy}(y_t) = \gamma \cdot p_{copy}^{(x)} + (1 - \gamma) \cdot p_{copy}^{(f)}$$</p><h4 id="soft-switch"><a href="#soft-switch" class="headerlink" title="soft switch"></a>soft switch</h4><p>soft switch可以把copy模式和generation这两种模式结合起来。用一个门机制$p_{gen}$来控制是从固定的词汇表中生成词，或者从dialog context和knowledge facts中复制词。$$p_{gen} = sigmoid(W\lbrack{s_t,c_t}\rbrack)$$ $$p(y_t) = p_{gen} \cdot p_g(y_t) + (1 - p_{gen}) \cdot p_{copy}(y_t)$$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>采用负对数似然函数作为优化的目标函数，对于单个word的loss函数为：$$loss(\Theta) = -log(p(y_t|y_{&lt;t},X,\lbrace{f^i}\rbrace_{i=1}^K))$$其中$\Theta$表示模型所有的可训练参数，$y_{&lt;t}$表示$y_t$之前所有的word。则对于一个训练样本的loss函数为：$$J_{loss}(\Theta) = -\frac{1}{|Y|}\sum_{t=1}^{|Y|}log(p(y_t|y_{&lt;t},X,\lbrace{f^i}\rbrace_{i=1}^K))$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1908.10731&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1908.10731&lt;/a&gt;&lt;br&gt;【代码、数据集】：无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
      <category term="Dialog System" scheme="http://yoursite.com/tags/Dialog-System/"/>
    
      <category term="Copy Mechanism" scheme="http://yoursite.com/tags/Copy-Mechanism/"/>
    
      <category term="Pointer Network" scheme="http://yoursite.com/tags/Pointer-Network/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：《(BiDAF)Bi-Directional Attention Flow for Machine Comprehension》</title>
    <link href="http://yoursite.com/2019/10/23/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension%E3%80%8B/"/>
    <id>http://yoursite.com/2019/10/23/论文笔记《BiDAF-Bi-Directional-Attention-Flow-for-Machine-Comprehension》/</id>
    <published>2019-10-23T01:48:41.000Z</published>
    <updated>2019-10-25T14:42:34.397Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ICLR2017<br>【链接】：<a href="https://arxiv.org/abs/1611.01603" target="_blank" rel="noopener">https://arxiv.org/abs/1611.01603</a><br>【代码、数据集】： <a href="https://github.com/allenai/bi-att-flow" target="_blank" rel="noopener">https://github.com/allenai/bi-att-flow</a></p></blockquote><a id="more"></a><p>这是由华盛顿大学和艾伦人工智能研究所发表的论文。艾伦人工智能研究所是大名鼎鼎的微软联合创始人保罗·艾伦创建的。<br>这是一篇经典的论文，截至目前被引次数高达678次。论文最大的贡献是在阅读理解任务中提出了双向attention机制（BiDirectional attention flow, BiDAF），BiDAF也可以用在其他任务中。</p><h2 id="阅读理解任务定义"><a href="#阅读理解任务定义" class="headerlink" title="阅读理解任务定义"></a>阅读理解任务定义</h2><p>给定文章context $\lbrace{x_1,x_2,…,x_T}\rbrace$及query $\lbrace{q_1,q_2,…,q_J}\rbrace$，在文章context中找到某个段span作为query的答案。输出其实是这个span的起始坐标和结束坐标。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>设encoder包含embedding层和Bi-LSTM层，经过encoder后的context representation为$H = \lbrace{h_1,h_2,…,h_T}\rbrace$，query representation为$U = \lbrace{u_1,u_2,…,u_J}\rbrace$。其中$H \in R^{2d\times  T}, U \in R^{2d\times  J}$。</p><h3 id="传统attention机制的几个特征"><a href="#传统attention机制的几个特征" class="headerlink" title="传统attention机制的几个特征"></a>传统attention机制的几个特征</h3><p>先介绍传统attention的计算方式。在时间步t计算传统attention时，需要用到上个时间步t-1的decoder RNN的隐藏状态$s_{t-1}$。decoder RNN的隐藏状态更新公式为：$$s_t = f(s_{t-1},y_{t-1},c_t)$$其中c_t为context vector，计算方式为：$$c_t = \sum_{i=1}^{T}\alpha_{t,i}h_i$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$\beta_{t,i} = score(s_{t-1},h_t)$$其中score(s,h)函数计算s与t之间的相似度。</p><p>从传统attention的计算方式可以看出，传统attention有以下几个特征：</p><ul><li>attention权重用来将所有的context representation $\lbrace{h_1,h_2,…,h_T}\rbrace$总结为一个固定维度的向量$c_t$。<em>这个过程不可避免地会带来信息丢失。</em></li><li>时间步t的attention权重$\alpha_{t_i}$计算 依赖于上一个时间步的向量$s_{t-1}$。<em>这里可以看出，attention权重的计算是有记忆的。</em></li><li>attention的计算是单向的。</li></ul><div align="center"><img src="/images/BiDAF.png" width="100%" height="100%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font></div><h3 id="双向attention机制"><a href="#双向attention机制" class="headerlink" title="双向attention机制"></a>双向attention机制</h3><p>论文中模型分为了6层，这里只介绍最关键的一层：attention flow layer。该层的输入是context representation $H$和query representation $U$，输出是意识到query的context words representation $G$,以及前一层的context representation。</p><ol><li><strong>相似度矩阵S</strong><br>先定义一个 $H\in R^{2d\times T}$和 $U\in R^{2d\times J}$之间的共享相似度矩阵$S\in R^{T\times J}$。其中$S_{tj}$衡量了第t个context word与第j个query word之间的相似度。$$S_{tj} = \alpha(H_{:t},U_{:j}) \in R$$ 其中$H_{:t}\in R^{2d}$是H的第t个列向量，$U_{:j}\in R^{2d}$是U的第j个列向量。$\alpha()$是一个计算相似度的函数：$$\alpha(h,u) = w_{(S)}[h;u;h·u]$$</li><li><strong>context-to-query attention</strong><br>表示对于每个context word，哪个query word是最相关的。<br>对于第t个context word，在所有query words $\{q_1,q_2,…,q_J\}$上的attention权重为$a_t\in R^{J}$，有$$\sum_{j}a_{tj} = 1$$<br>attention权重$a_t$的计算方式为：$$a_t = softmax(S_{t:}) \in R^{J}$$ 对于第t个context word的attended query vector为$$\widetilde{U_{:t}} = \sum_{j}a_{tj}U_{:j} \in R^{2d}$$ 对于所有的context words $\{x_1,x_2,…,x_T\}$,则有$\widetilde{U} \in R^{2d\times T}$</li><li><strong>query-to-context attention</strong><br>表示对于每个query words，哪个context word是最相似的，对于回答query最重要。<br>计算在所有context words ${x_1,…,x_T}$上的attention权重为 $$b = softmax(max_{col}(S)) \in R^{T}$$其中$max_{col}(S) \in R^{T}$函数表示在矩阵$S \in R^{T\times J}$的列上取最大值。<br>则attended context vector为$$\widetilde{h} = \sum_{t}b_{t}H_{:t} \in R^{2d}$$ 这个向量的含义是对于query所有重要的context words的加权和。<br>把$\widetilde{h}$在列上复制T次，得到了$\widetilde{H} \in R^{2d\times T}$</li><li><strong>输出融合</strong><br>把上一层的context representation $H$和attended vector $\widetilde{H}$和$\widetilde{U}$总结组合起来得到<em>意识到query的context words representation</em> $G$，计算方式为：$$G_{:t} = \beta(H_{:t},\widetilde{U_{:t}},\widetilde{H_{:t}}) \in R^{d_{G}}$$ 其中$\beta()$函数可以是任意神经网络，比如MLP多层感知机。论文中采用了简单的连接操作，将$\beta()$函数定义为：$$\beta(h,\widetilde{h},\widetilde{u}) = [h;\widetilde{u};h \circ \widetilde{u};h \circ \widetilde{h}] \in R^{8d\times T}$$</li></ol><p>从双向attention机制的计算可以看出，双向attention机制有以下几个特征：</p><ul><li>与传统attention将所有context representation $\lbrace{h_1,h_2,…,h_T}\rbrace$总结为一个固定维度的向量$c_t$不同。双向attention机制为每个时间步都计算attention，并将attended vector $\widetilde{H}$、$\widetilde{U}$和前一层的context representation $H$流动到下一层。这样减少了提前总结为固定维度的向量带来的信息损失。</li><li>这是无记忆的attention机制。当前时间步的attention计算只取决于当前的context representation $H$和query representation $U$，而不依赖于上一个时间步的attention。 这种无记忆的attention机制将<em>attention layer</em>和<em>model layer</em>分隔开，迫使<em>attention layer</em>专注于学习context与query之间的attention，而<em>model layer</em>专注于学习attention layer输出内部之间的联系。</li><li>双向attention机制是双向的，包含query-to-context attention和context-to-query attention，可以彼此之间相互补充。</li></ul><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://allenai.github.io/bi-att-flow/" target="_blank" rel="noopener">BiDAF</a></li><li><a href="https://zhuanlan.zhihu.com/p/53626872" target="_blank" rel="noopener">机器阅读理解之双向注意力流||Bidirectional Attention Flow for Machine Comprehension</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ICLR2017&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1611.01603&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1611.01603&lt;/a&gt;&lt;br&gt;【代码、数据集】： &lt;a href=&quot;https://github.com/allenai/bi-att-flow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/allenai/bi-att-flow&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="BiDAF" scheme="http://yoursite.com/tags/BiDAF/"/>
    
      <category term="Machine Comprehension" scheme="http://yoursite.com/tags/Machine-Comprehension/"/>
    
  </entry>
  
  <entry>
    <title>对话系统的数据集</title>
    <link href="http://yoursite.com/2019/09/06/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>http://yoursite.com/2019/09/06/对话系统的数据集/</id>
    <published>2019-09-06T02:07:32.000Z</published>
    <updated>2019-09-06T02:11:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在读论文的过程中，积累记录一些论文中用到的数据集，并对数据集的大小、样例、获取链接作简单介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="对话系统" scheme="http://yoursite.com/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="对话系统" scheme="http://yoursite.com/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="数据集" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Bridging the Gap between Training and Inference for Neural Machine Translation》</title>
    <link href="http://yoursite.com/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8ABridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/02/论文笔记《Bridging-the-Gap-between-Training-and-Inference-for-Neural-Machine-Translation》/</id>
    <published>2019-08-02T06:46:00.000Z</published>
    <updated>2019-08-05T05:14:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：ACL2019<br>【链接】：<a href="https://arxiv.org/abs/1906.02448" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02448</a><br>【代码、数据集】： 无</p></blockquote><a id="more"></a><p>这篇论文由中科院发表，获得了ACL2019的 “best long paper”。</p><h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>在Neural Machine Translation(NMT)任务中，模型通常采用encoder-decoder框架，基于RNN 或 CNN 或attention。假设输入为$X = \lbrace{x_1,x_2,…,x_m}\rbrace$，真实输出为$Y = \lbrace{y_1^*,y_2^*,…,y_n^*}\rbrace$，预测输出为$Y’ = \lbrace {y_1’,y_2’,…,y_m’}\rbrace$。</p><p>第一个问题是：decoder会一个词一个词地生成整个回复。在train阶段，在时间步t生成$y_t’$时，decoder会根据之前真实的词$\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\rbrace$来预测$y_t’$。在infer阶段，由于不可能知道真实输出，在时间步生成$y_t’$时，decoder会根据之前预测的词$\lbrace{y_1’,y_2’,…,y_{t-1}’}\rbrace$来预测$y_t’$。<br>可以看到train阶段与infer阶段所依据的词是不同的，train阶段和infer阶段预测的词$y_t’$来自两个不同的概率分布，分别是数据分布(data distribution)和模型的分布(model distribution)，这种差别称为“爆炸偏差(exposure bias)”。随着预测序列的长度增加，错误会逐渐累积。<br>为了解决第一个问题，消除train阶段和infer阶段的这种差别，一个可能的解决方法是：在train阶段，decoder同时根据真实的词$\lbrace{y_1^*,y_2^*,…,y_{t-1}^*}\rbrace$和预测的词$\lbrace{y_1’,y_2’,…,y_{t-1}’}\rbrace$来生成$y_t’$。</p><p>第二个问题是: NMT模型通常最优化$Y与Y’$之间的交叉熵目标函数来更新模型参数，但交叉熵函数会严格匹配预测输出$Y’$与真实的输出$Y$。但在NMT任务中，一句话可以有多个不同但合理的翻译。一旦预测输出$Y’$的某个词与$Y$不同，尽管它是合理的，也会被交叉熵函数纠正。这种情况称为“过度纠正的现象”。</p><p>为了消除train阶段与infer阶段的差别，论文提出了一种在train阶段做改进的解决方案。首先，从预测的词中选择oracle word $y_{j-1}^{oracle}$，设真实输出中上一个词为$y_{j-1}^{*}$。接着从$\lbrace{y_{j-1}^{oracle},y_{j-1}^{*}}\rbrace$中抽样一个词，抽中$y_{j-1}^{*}$的概率为$p$，抽中$y_{j-1}^{oracle}$的概率为$1-p$。最后，decoder根据抽样的这个词来预测$y_j$。</p><p>在train阶段刚开始时，抽中真实的词$y_{j-1}^{*}$的概率比较大，随着模型逐渐收敛，抽中预测的词$y_{j-1}^{oracle}$的概率变大，让模型有能力处理”过度纠正的问题”。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/oracle.png" alt="Fig.1 论文提出的方法的结构图" title>                </div>                <div class="image-caption">Fig.1 论文提出的方法的结构图</div>            </figure><h3 id="RNN-based-NMT-Model"><a href="#RNN-based-NMT-Model" class="headerlink" title="RNN-based NMT Model"></a>RNN-based NMT Model</h3><p>NMT任务常采用encoder-decoder框架，可以基于RNN或CNN或纯attention。论文提出的消除train阶段和infer阶段差别的方法，可以用于任何NMT模型。论文以基于RNN的NMT模型为例，来介绍这种方法。这一节先介绍RNN-based NMT模型。下一节介绍NMT模型如何结合这种方法。</p><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>记输入为$X = \lbrace{x_1,x_2,…,x_m}\rbrace$，真实输出为$Y = \lbrace{y_1,y_2,…,y_n}\rbrace$。encoder采用bi-GRU分别获取正向和反向的隐藏状态$\overrightarrow{h_i},\overleftarrow{h_i}$。$x_i$的embedding向量为$e_{x_i}$。$$\overrightarrow{h_i} = GRU(e_{x_i},h_{i-1})$$ $$\overleftarrow{h_i} = GRU(e_{x_i},h_{i+1})$$ 将$\overrightarrow{h_i},\overleftarrow{h_i}$连接起来，作为$x_i$对应的隐藏状态：$$h_i = [\overrightarrow{h_i},\overleftarrow{h_i}] \tag{1}$$</p><h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><p>attention机制用来联系encoder和decoder，更好地捕捉source sequence的信息。也就是在时间步t,通过encoder所有的隐藏状态$\lbrace h_1,h_2,…,h_m \rbrace$来计算context vector $c_t$。记decoder上一时间步的隐藏状态为$s_{t-1}$。 $c_t$是encoder所有隐藏状态$\lbrace h_1,h_2,…,h_m \rbrace$的加权和：$$c_t = \sum_{i=1}^{m}\alpha_{ti}h_i \tag{2}$$ 其中$\alpha_{ti}$是attention权重，计算方式为:$$\beta_{ti} = v_a^\top tanh(W_as_{t-1} + U_ah_i) \tag{3}$$ $$\alpha_{ti} = softmax(\beta_{ti}) = \frac{exp(\beta_{ti})}{\sum_jexp(\beta_{tj})}$$</p><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>decoder采用单向GRU的变体，隐藏状态更新公式为:$$s_t = GRU(s_{t-1},e_{y_{t-1}^*},c_t) \tag{4}$$ 最后根据e_{y_{t-1}^*}，decoder的隐藏状态$s_t$，对应的context vector $c_t$来预测$y_t$。 $$o_t = W_og(e_{y_{t-1}^*},s_t,c_t) \tag{5}$$ 在词汇表上的概率分布为：$$P_t(y_t = w) = softmax(o_t) \tag{6}$$</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>为了消除或减轻train阶段和infer阶段的差别，论文提出 从真实的词$y_{t-1}*$和$y_{t-1}^{oracle}$预测的词中抽样，decoder根据抽样的词来预测下一个词$y_t$。使用论文提出的方法，在时间步t预测$y_t$分为三步：</p><ol><li>先从预测的词中选择$y_{t-1}^{oracle}$。 论文提出了两种方法来选择oracle word，分别是词级别的方法和句子级别的方法。</li><li>从$\lbrace{y_{t-1}^{oracle},y_{t-1}*}\rbrace$中抽样得到$y_{t-1}$，抽中$y_{t-1}*$的概率为$p$，抽中$y_{t-1}^{oracle}$的概率为$1-p$。</li><li>用抽样的词$y_{t-1}$来替换公式$(4)(5)$中的$y_{t-1}^*$来预测下一个词。</li></ol><h4 id="oracle-word的选择"><a href="#oracle-word的选择" class="headerlink" title="oracle word的选择"></a>oracle word的选择</h4><p>传统的方法中，decoder会根据上一个时间步真实的$y_{t-1}^*$来预测$y_t$。为了消除train阶段的infer阶段的差别，可以从预测的词中选择oracle word $y_{t-1}^{oracle}$来代替$y_{t-1}^*$。一种方法是每个时间步采用词级别的greedy search来生成oracle word，称为word-level oracle(WO)，另一种方法是采用beam-search，扩大搜索空间，用句子级的衡量指标(如：BLEU)对beam-search的结果进行排序，称为sentence-level oracle(SO).</p><h5 id="word-level-oracle"><a href="#word-level-oracle" class="headerlink" title="word-level oracle"></a>word-level oracle</h5><p>选择$y_{t-1}^{oracle}$最简单直观的方法是，在时间步t-1，选择公式$P_{t-1}$中概率最高的词作为$y_{t-1}^{oracle}$，如Fig.2所示。 为了获得更健壮的$y_{t-1}^{oracle}$，更好地选择是使用<a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">gumbel max技术</a>来冲离散分布中进行抽样，如Fig.3所示。<br>具体地讲，将gumbel noise $\eta$作为正则化项加到公式(5)中的$o_{t-1}$，再进行softmax操作得到$y_{t-1}$的概率分布。$$\eta = -log(-log(u)) $$ $$\tilde{o_{t-1}} = \frac{o_{t-1} + \eta}{\tau} \tag{7}$$ $$\tilde{P_{t-1}} = softmax(\tilde{o_{t-1}}) \tag{8}$$ 其中变量$u \sim U(0,1)$服从均匀分布。$\tau$为温度系数，当$\tau \to 0$时，公式(8)的softmax()逐渐相当于argmax()函数；当$\tau \to \infty$时，softmax()函数逐渐相当于均匀分布。<br>则$y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{WO} =argmax(\tilde{P_{t-1}}) \tag{9}$$需要注意的是gumbel noise $\eta$只用来选择oracle word，而不会影响train阶段的目标函数。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/word_level_oracle_without_gumbel_noise.png" alt="Fig.2. word level oracle without gumbel noise" title>                </div>                <div class="image-caption">Fig.2. word level oracle without gumbel noise</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/word_level_oracle_with_gumbel_noise.png" alt="Fig.3. word level oracle with gumbel noise" title>                </div>                <div class="image-caption">Fig.3. word level oracle with gumbel noise</div>            </figure><h5 id="sentence-level-oracle"><a href="#sentence-level-oracle" class="headerlink" title="sentence-level oracle"></a>sentence-level oracle</h5><p>为了选择sentence-level oracle word，首先要进行beam-search解码，设beam size为k，得到k个candidate句子。在beam-search解码的过程中，生成每个词时也应用gumbel max技术。<br>接着，得到k个candidate句子后，用句子级衡量指标BLEU来给这k个句子打分，得分最高的句子为oracle sentence $Y^S = \lbrace{y_1^S,y_2^S,..,y_{|y^S|}^S}\rbrace$。<br>则时间步t解码对应的oracle word $y_{t-1}^{oracle}$为$$y_{t-1}^{oracle} = y_{t-1}^{SO} = y_{t-1}^{S} \tag{10}$$ 当模型从真实输出$Y$和sentence oracle $Y^S$抽样，这有一个前提是，这两个序列的长度需要是一致的。但beam-search decode不能保证解码序列的长度。为了保证这两个序列长度一致，论文提出了<em>force decoding</em>的解决方法。</p><p><strong>force decoding</strong><br>设真实输出$Y = \lbrace{y_1,y_2,…,y_n}\rbrace$的序列长度为n。<em>force decoding</em>需要解码得到长度同样为n的序列，以特殊字符”EOS”结束。设beam search decode时，时间步t对应的概率分布为$P_t$。</p><ul><li>当$t&lt; n$时，对于概率分布$P_t$，即使字符”EOS”是概率最高的词，那么生成概率次高的词。</li><li>当$t = n+1$时，对于概率分布$P_{n+1}$，即使字符”EOS”不是概率最高的词，也要生成”EOS”。</li></ul><p>果真是强制生成长度为n的序列。这样beam-search decode得到的序列与真实输出序列的长度就是一致的，都为n。</p><h4 id="递减抽样"><a href="#递减抽样" class="headerlink" title="递减抽样"></a>递减抽样</h4><p>根据公式(9)或(10)得到$y_{t-1}^{oracle}$后，下一步是从$\lbrace{y_{t-1}^{oracel},y_{t-1}^*}\rbrace$中抽样，抽中$y_{t-1}^*$的概率是p，抽中$y_{t-1}^{oracle}$的概率是1-p。在训练的初始阶段，如果过多地选择$y_{t-1}^{oracle}$，会导致模型收敛速度慢；在训练的后期阶段，如果过多地选择$y_{t-1}^*$，会导致模型在train阶段没有学习到如何处理infer阶段的差别。<br>因此，好的选择是：在训练的初始阶段，更大概率地选择$y_{t-1}^*$来加快模型收敛，当模型逐渐收敛后，以更大概率选择$y_{t-1}^{oracle}$，来让模型学习到如何处理infer阶段的差别。从数学表示上，概率$p$先大后逐渐衰减，$p$随着训练轮数$e$的增大而逐渐变小。$$p = \frac{\mu}{\mu + exp(\frac{e}{\mu})} \tag{11}$$其中，$\mu$是超参数。$p$是轮数$e$的单调递减函数。$e$从0开始，此时，$p=1$。</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>将采样得到的$y_{t-1}$代替公式(4)-(6)中的$y_{t-1}^*$来预测$y_t$在词汇表上的概率分布。采用最大似然估计，相当于最小化以下目标函数：$$L(\theta) = -\sum_{n=1}^{N}\sum_{j=1}^{|y_n|}logP_j^n[y_j^n]$$</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：ACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/abs/1906.02448&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/1906.02448&lt;/a&gt;&lt;br&gt;【代码、数据集】： 无&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="ACL2019" scheme="http://yoursite.com/tags/ACL2019/"/>
    
      <category term="Neural Machine Translation" scheme="http://yoursite.com/tags/Neural-Machine-Translation/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Multi-Level Memory for Task Oriented Dialogs》</title>
    <link href="http://yoursite.com/2019/08/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AMulti-Level-Memory-for-Task-Oriented-Dialogs%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/</id>
    <published>2019-08-01T06:14:37.000Z</published>
    <updated>2019-10-25T07:26:40.134Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.10647.pdf</a><br>【代码、数据集】：<a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></p></blockquote><a id="more"></a><p>已有工作中，端到端的任务型对话系统采用memory network来结合外部的知识库(knowledgt base) 和 对话历史(context)。为了使用从跑一趟 network，通常将二者放在同一个memory中。这样带来的问题是：memory变得太大，模型在读取memory时需要区分外部知识库和对话历史，并且在memory上的推理变得很难。为了解决这个问题，论文将外部知识库和对话历史区分开，另外，将外部知识库保存为分层的memory。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型主要包括三个部分。 </p><ul><li>分级encoder：<br>  分别编码对话历史中的句子。</li><li>milti-level memory<br>  保存了目前为止所有的query以及对应的知识库查询结果，是以分级的方式保存在memory中的。</li><li>copy机制增强的decoder：<br>  从词汇表中生成词，或者从知识库multi-level memory中复制词，或者从对话历史(context)中复制词。</li></ul><div align="center"><img src="/images/multi-memory-model.jpg" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div><h4 id="分级encoder"><a href="#分级encoder" class="headerlink" title="分级encoder"></a>分级encoder</h4><p>在第t轮，对话历史共有2t-1个句子$\lbrace{c_1,c_2,…,c_{2t-1}}\rbrace$，其中用户对话为t轮，回复对话为t-1轮。 每个句子$c_i$都是词序列$\lbrace{w_{i1},w_{i2},…,w_{im}}\rbrace$。<br>每个句子$c_i$先经过embedding layer得到词向量表示，再经过单层bi-GRU得到句子的向量表示$\varphi(c_i)$。$h_{ij}^e$表示词$w_{ij}$对应的隐藏状态。<br>再将$\varphi{c_i}$经过另一个单词GRU来得到context的向量表示$c$。</p><h4 id="multi-level-memory"><a href="#multi-level-memory" class="headerlink" title="multi-level memory"></a>multi-level memory</h4><p>memory的关键是分级的分为三级：query $\to$ result $\to$ result key和result value。见Fig.2。<br>记本轮对话之前所有的知识库query为$q_1,…,q_k$。每个query $q_i$是一个(key,value)对，$q_i = \lbrace{k_a^{q_i}:v_a^{q_i},0&lt; a&lt; n_{q_i}}\rbrace $。其中key和value分别对应query的槽(slots)和槽值，$n_{q_i}$是query $q_i$的槽值个数。<br>第j轮对话，用query $q_i$查询知识库的返回结果为result $r_{ij}$。$r_{ij}$也是一个key-value对，$r_{ij} = \lbrace{k_a^{r_{ij}}:v_a^{r_{ij}},0&lt; a &lt; n_{r_{ij}}}\rbrace$。其中$n_{r_{ij}}$是key-value对的个数。</p><div align="center"><img src="/images/multi-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. multi memory</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div>第一级memory是query的向量表示。 $q_i$的向量表示为$q_i^v$，$q_i^v$为所有values $v_a^{q_i}$的词袋(bag of words)向量表示。第二级memory是result的向量表示。同样地，$r_{ij}$的向量表示为$r_{ij}^v$，$r_{ij}^v$为所有values $v_a^{r_{ij}}$的词袋(BOW)向量表示。第三级memory是result的key-value对，$(k_a^{r_{ij}}:v_a^{r_{ij}})$，其中value $v_a^{r_{ij}}$可能会被复制到回复中。<h4 id="copy机制增强的decoder"><a href="#copy机制增强的decoder" class="headerlink" title="copy机制增强的decoder"></a>copy机制增强的decoder</h4><p>decoder一个词一个词地生成回复。在时间步t生成词$y_t$时，可能从词汇表中生成，也能从两个分开的memory上复制。用门$g_1$来选择是从词汇表上生成，还是从memory中复制。如果是后者，用另一个门$g_2$来选择是从context中复制，还是从知识库复制。</p><ol><li><strong>从词汇表生成词</strong><br> 时间步t，decoder的隐藏状态$h_t$为$$h_t = GRU(y_{t-1},s_{t-1})$$用$h_t$计算在encoder的所有隐藏状态上的attention权重，采用”concat attention”机制：$$a_{ij} = softmax(w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))) = \frac{w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}{\sum_{ij}w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}$$则context vector为$$d_t = \sum_{ij}a_{ij}h^e_{ij}$$ $h_t$和$d_t$连接后经过线性层和softmax层得到在词汇表上的概率分布：$$P_g(y_t) = softmax(W_1[h_t,d_t] + b_1)$$</li><li><strong>从context memory中复制词</strong><br> 直接将计算context vector时的attention权重，作为在context所有词$w_{ij}$上的概率分布：$$P_{con}(y_t = w) = \sum_{ij:w_{ij}=w}a_{ij}$$</li><li><strong>从KB memory中复制实体</strong><br> 时间步t的隐藏状态$h_t$和context vector $d_t$用来计算在所有query上的attention权重。第一级在所有query $q_1,q_2,…,q_k$的attention权重为$$\alpha_i = softmax(w_2^\top tanh(W_4[h_t,d_t,q_i^v])) = \frac{w_2^\top tanh(W_4[h_t,d_t,q_i^v])}{\sum_{i}w_2^\top tanh(W_4[h_t,d_t,q_i^v])}$$<br> 第二级$\beta_i$在$q_i$对应的$r_i$上的attention权重为$$\beta_{ij} = softmax(w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])) = \frac{w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}{\sum_{j}w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}$$<br> 第一级attention和第二级attention的乘积是在所有result上的attention权重分布。则memory总的向量表示为$$m_t = \sum_{i}\sum_j\alpha_i\beta_{ij}r_{ij}^v$$<br> 第三级memory为result的key-value对$(k_a^{r_{ij}}:v_a^{r_{ij}})$，类似于<a href="https://arxiv.org/abs/1705.05414" target="_blank" rel="noopener">(Eric and Manning, 2017)</a>，用key $k_a^{r_{ij}}$来计算attention权重，将对应的value $v_a^{r_{ij}}$复制到回复中。在$r_{ij}$所有keys上的attention权重为$$\gamma_{ijl} = softmax(w_4^\top tanh(W_6[h_t,d_t,m_t,k_l^{r_{ij}}]))$$则在所有values $v_a^{r_{ij}}$的概率分布为:$$P_{kb}(y_t = w) = \sum_{ijl:v_l^{r_{ij}}=w}\alpha_i\beta_{ij}\gamma_{ijl}$$</li><li><strong>decoding</strong><br> 我们用门机制$g_2$来来结合$P_{con}(y_t)$和$P_{kb}(y_t)$，得到memory上的copy概率分布$P_c(y_t)$。$$g_2 = sigmoid(W_7[h_t,d_t,m_t]+b_2)$$ $$P_c(y_t) = g_2P_{kb}(y_t) + (1-g_2)P_{con}(y_t)$$ 用门机制$g_1$来结合$P_{c}(y_t)$和$P_{g}(y_t)$来得到总的概率分布$P(y_t)$：$$g_1 = sigmoid(W_8[h_t,d_t,m_t]+b_3)$$ $$P(y_t) = g_1P_g(y_t) + (1-g_1)P_c(y_t)$$</li></ol><h3 id="相似论文"><a href="#相似论文" class="headerlink" title="相似论文"></a>相似论文</h3><ul><li><a href="https://arxiv.org/abs/1810.10647" target="_blank" rel="noopener">《Multi-level Memory for Task Oriented Dialogs》</a><ul><li>发表在NAACL2019</li><li>github: <a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></li></ul></li><li><a href="https://arxiv.org/abs/1804.08217" target="_blank" rel="noopener">《Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems》</a><ul><li>发表在ACL2018</li><li>github: <a href="https://github.com/HLTCHKUST/Mem2Seq" target="_blank" rel="noopener">https://github.com/HLTCHKUST/Mem2Seq</a></li></ul></li><li><a href="https://www.ijcai.org/proceedings/2018/643" target="_blank" rel="noopener">《Commonsense Knowledge Aware Conversation Generation with Graph Attention》</a><ul><li>发表在IJCAI2018</li><li>github： <a href="https://github.com/tuxchow/ccm" target="_blank" rel="noopener">https://github.com/tuxchow/ccm</a></li></ul></li><li><a href="https://arxiv.org/abs/1908.10731" target="_blank" rel="noopener">《DEEPCOPY: Grounded Response Generation with Hierarchical Pointer Networks》</a><ul><li>发表于2019年</li><li>代码：无</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/pdf/1810.10647.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1810.10647.pdf&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/DineshRaghu/multi-level-memory-network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/DineshRaghu/multi-level-memory-network&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>Neural Turing Machines与Memory Network</title>
    <link href="http://yoursite.com/2019/07/26/Neural-Turing-Machines%E4%B8%8EMemory-Network/"/>
    <id>http://yoursite.com/2019/07/26/Neural-Turing-Machines与Memory-Network/</id>
    <published>2019-07-26T03:03:15.000Z</published>
    <updated>2019-08-01T02:30:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Memory Networks。</p><a id="more"></a><h3 id="Neural-Turing-Machines-神经图灵机"><a href="#Neural-Turing-Machines-神经图灵机" class="headerlink" title="Neural Turing Machines-神经图灵机"></a>Neural Turing Machines-神经图灵机</h3><p>Google DeepMind团队在<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>提出Neural Turing Machines，第一次提出用external memory来提高神经网络的记忆能力。这之后又出现了多篇关于Memory Networks的论文。我们先看看Turing Machines的概念。</p><h4 id="Turing-Machines-图灵机"><a href="#Turing-Machines-图灵机" class="headerlink" title="Turing Machines-图灵机"></a>Turing Machines-图灵机</h4><p>计算机先驱<a href="https://baike.baidu.com/item/%E8%89%BE%E4%BC%A6%C2%B7%E9%BA%A6%E5%B8%AD%E6%A3%AE%C2%B7%E5%9B%BE%E7%81%B5/3940576?fromtitle=%E5%9B%BE%E7%81%B5&fromid=121208" target="_blank" rel="noopener">turing</a>在1936年提出了Turing Machines这样一个计算模型。它由三个基本的组件：</p><ul><li>tape: 一个无限长的纸带作为memory，包含无数个symbols，每个symbol的值为0、1或”$\space$”。</li><li>head: 读写头，对tape上的symbols进行读操作和写操作。</li><li>controller： 根据当前状态来控制head的操作。</li></ul><p>理论上Turing Machines可以模拟任何一个计算算法，不管这个算法多么复杂。但现实中，计算机不可能有无限大的memory space，因此Turing Machines只是数学意义上的计算模型。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/turing-machine.jpg" alt="Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)" title>                </div>                <div class="image-caption">Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)</div>            </figure><h4 id="Neural-Turing-Machines"><a href="#Neural-Turing-Machines" class="headerlink" title="Neural Turing Machines"></a>Neural Turing Machines</h4><p>Neural Turing Machines(NTM,<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>)用external memory来提高神经网络的记忆能力。<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM(Long and short memory)</a>通过门机制有效缓解了RNN的’梯度消失和梯度爆炸问题’，可以通过internal memory实现长期记忆。当LSTM的internal memory的记忆能力有限，需要用external memory来提高神经网络的记忆能力。</p><p>Neural Turing Machines包含两个基本组件：<em>a neural network controller</em>和<em>memory bank</em>。<em>memory</em>是一个 $N\cdot M$阶的矩阵，包含N个向量，每个向量的维度是M。我们把每个memory vector称为memory location。<em>controller</em>控制<em>heads</em>对<em>memory</em>进行读写操作。</p><p>如何对<em>memory matrix</em>进行读写操作呢？关键问题是如何让读写操作是可微的，这样才能用梯度下降法来更新模型参数。具体来说，问题是让模型关于memory location是可微的，但memory locations是离散的。Neural Turing Machines用了一个很聪明的方法来解决这个问题：不是对单独某个memory location进行读写操作，而是对所有的memory locations进行不同程度的读写操作，这个程度是通过attention的权重分布来控制的。</p><div align="center"><img src="/images/NTM.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig. 2. Neural Turing Machine Architecture</font></div><h5 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h5><p>记时间步t <em>memory matrix</em>为$N\cdot M$阶矩阵$M_t$，$w_t$是在N个memory向量上的权重分布，是一个N维向量。则时间步t的read vector $r_t$为$$r_t = \sum_{i=1}^{N}w_t(i)\cdot M_t(i)$$ $$where: \sum_{i=1}^{N}w_t(i) = 1; 0 \le w_t(i) \le 1,\forall i $$其中，$w_t(i)$是$w_t$的第i个元素，$M_t(i)$是$M_t$的第i个行向量。</p><h5 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h5><p>受LSTM门机制的启发，将写操作分成两步：先<em>erase</em>，再<em>add</em>。先根据<em>erase vector $e_t$</em>擦去旧的内容，再根据<em>add vector $a_t$</em>添加新的内容。</p><ol><li>先erase：<br> 在时间步t，attention权重分布为$w_t$，<em>erase vector $e_t$</em>是一个M维向量，每个元素取值[0,1]，上一个时间步的<em>memory vector</em>为$M_{t-1}$。则erase操作为$$\tilde{M_{t}}(i) = M_{t-1}(i)[\vec{1}-w_t(i)e_t]$$ $\vec{1}$是一个M维的全1向量。对memory vector的erase操作是逐点进行的。当$e_t$的元素和memory location对应权重$w_t(i)$的元素值都是1时，memory vector $M_t(i)$的元素值才会置为0。如果$e_t$或$w_t(i)$的元素值为0时，memory vector $M_t(i)$的元素值保持不变。</li><li>再add:<br> 每个<em>write head</em>会产生一个M维的<em>add vector a_t</em>，则：$$M_t(i) = \tilde{M_{t}}(i) + w_t(i)a_t$$至此，就完成了写操作。</li></ol><h5 id="寻址机制"><a href="#寻址机制" class="headerlink" title="寻址机制"></a>寻址机制</h5><p>进行读写操作前，要搞清楚对哪个memory location进行读写呢？这就是寻址。为了让模型关于memory locatios可微，Neural Turing Machines不是对某个单独的memory location进行读写操作，而是对所有memory locations进行不同程度的读写操作，这个程度就是由权重分布$w_t$来控制的。模型结合并同时使用了content-based和location-based两种寻址方式来计算这个权重分布$w_t$。具体地，权重计算分为以下几步：</p><ol><li>content-based addressing<br> 时间步t，每个head产出一个M维的<em>key vector $k_t$</em>，通过$k_t$与memory vectors $M_t(i)$之间的相似性来计算content-based attention权重分布$w_{t}^{c}$。相似性是通过余弦相似度来衡量的。$$w_{t}^{c} = softmax(\beta_tK(k_t,M_t(i))) = \frac{\beta_tK(k_t,M_t(i))}{\sum_{j}K(k_t,M_t(j))}$$ $$K(u,v) = \frac{u\cdot v}{|u|\cdot |v|}$$<br> $\beta_t$可以放大或缩小权重的精度。</li><li>内插法<br> 每个head产生一个<em>interpolation gate $g_t$</em>，取值[0,1]。content-based attention权重分布为$w_t^{c}$，上一个时间步的attention权重分布为$w_{t-1}$。则门控制的权重分布$w_t^g$为：$$w_t^g = g_tw_t^c + (1-g_t)w_{t-1}$$当$g_t$为0时，采用上一个时间步的权重分布$w_{t-1}$，当$g_t$为1时，采用content-based attention权重分布$w_t^c$。</li><li>循环卷积<br> 对经过插值后的权重分布$w_t^g$进行循环卷积，主要功能是对权重进行旋转位移。比如当权重分布关注某个memory location时，经过循环卷积就会扩展到附近的memory locations，也会对附近的memory locations进行少量的读写操作。每个head产生的转移权重为$s_t$,循环卷积的操作为:$$\tilde{w_t(i)} = \sum_{j=0}^{N-1}w_t^g(i)s_t(i-j)$$<br> 关于$s_t$的详细介绍可以见<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a>;<br> 循环卷积的详细介绍可以见<a href="https://blog.csdn.net/rtygbwwwerr/article/details/50548311" target="_blank" rel="noopener">Neural Turing Machines-NTM系列（一）简述</a></li><li>锐化<br> 循环卷积往往会造成权重泄漏和分散，为了解决这个问题，需要最后进行锐化操作。$$w_t(i) = \frac{\tilde{w_t(i)^{\gamma_t}}}{\sum_j\tilde{w_t(j)^{\gamma_t}}}$$其中$\gamma_t &gt;1$。至此，就得到了时间步t的权重分布$w_t$。可以根据这个权重分布$w_t$对memory matrix进行读写操作。</li></ol><p>总结以下这4步操作。第一步content-based addressing根据输入得到关于memory locations的相似度；后三步实现了location-based addressing。第二步插值操作引入了上一个时间步的权重分布，对content-based 权重进行修正；第三步循环卷积将每个位置的权重向两边分散；第四步锐化操作将权重突出化，大的更大，小的更小。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/50.jpg" alt="Fig.3. 寻址机制的4步操作" title>                </div>                <div class="image-caption">Fig.3. 寻址机制的4步操作</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/NTM-flow-addressing.png" alt="Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)" title>                </div>                <div class="image-caption">Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)</div>            </figure><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a><br>  用动图直观地表现Neural Turing Machines的计算过程。推荐！👍</li><li><a href="https://zhuanlan.zhihu.com/p/30383994" target="_blank" rel="noopener">记忆网络之Neural Turing Machines</a>，中文</li><li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a></li></ul><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>在Neural Turing Machines提出仅仅五天后，Facebook研究员<a href="http://www.thespermwhale.com/jaseweston/" target="_blank" rel="noopener">Jason Weston</a>发表了<a href="http://arxiv.org/abs/1410.3916" target="_blank" rel="noopener">MEMORY NETWORKS</a>。在QA系统的领域，应用memory network。虽然RNN或LSTM可以通过hidden state和weights来进行短期记忆，但它们的记忆能力是有限的。要实现长期记忆，需要memory network。</p><h4 id="Memory-Network的一般框架"><a href="#Memory-Network的一般框架" class="headerlink" title="Memory Network的一般框架"></a>Memory Network的一般框架</h4><p>memory network包括一个记忆单元memory，和四个基本组件：</p><ul><li>I(input feature map):<br>  将input <em>x</em>进行向量化表示，编码为feature representation <em>I(x)</em>。</li><li>G(generalization):<br>  对memory进行写操作。根据input 来更新memory <em>$m_i$</em>。$m_i = G(m_i,I(x),m)$</li><li>O(output feature map):<br>  对memory进行读操作。根据input和memory生成output feature。$o = O(I(x),m)$</li><li>R(response):<br>  根据output feature o来生成response。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/memn.jpg" alt="Fig.5. memory network的框架图" title>                </div>                <div class="image-caption">Fig.5. memory network的框架图</div>            </figure><h4 id="memory-network框架的实现–MemNNs"><a href="#memory-network框架的实现–MemNNs" class="headerlink" title="memory network框架的实现–MemNNs"></a>memory network框架的实现–MemNNs</h4><p>在I模块将input $x_i$编码为$I(x_i)$后，G模块之间将$I(x_i)$保存到下一个空的memory slot中，而不更新旧的memory slots。真正实现inference的核心模块是O和R。</p><p>O模块在给定x的条件下，依次找到与x最相关的k个memory slots。论文中采用k = 2。先找到第一个最相关的memory slot：$$m_{o1} = \mathop{argmax}\limits_{i = 1,…,N} s_{o1}(x,m_i)$$其中$s_o()$是一个匹配函数，计算x与$m_i$之间的相关程度。接着，根据x和第一个memory找到下一个memory：$$m_{o2} = \mathop{argmax}\limits_{i = 1,…,N} s_{o2}([x,m_{o1}],m_i)$$将output feature o = $[x,m_{o1},m_{o2}]$作为R模块的输入。</p><p>R模块将词汇表中所有词与output feature进行匹配，选择匹配度最高的词作为response。这样生成的response只有一个词。$$r = \mathop{argmax}\limits_{w \in W}s_R([x,m_{o1},m_{o2}],w)$$其中$s_R()$是一个匹配函数。</p><p>匹配函数$s_O$和$s_R$都采用以下函数：$$s(x,y) = \Phi_x(x)^\top U^\top U \Phi_y(y)$$其中$\Phi_x(x),\Phi_y(y)$分别将x/y编码为向量。<br><strong>目标函数</strong><br>在训练阶段采用最大边缘目标函数，设对于question x，真实的label为r，对应的memory为$m_{o1},m_{o2}$。则最大边缘目标函数为：$$\sum_{m_i\ne m_{o1}}max(0,\gamma - s_{O1}(x,m_{o1}) + s_{O1}(x,m_i)) + $$ $$\sum_{m_j\ne m_{o2}}max(0,\gamma - s_{O2}([x,m_{o1}],m_{o2}) + s_{O2}([x,m_{o1}],m_j)) + $$ $$\sum_{r’ \ne r}max(0,\gamma - s_{R}([x,m_{o1},m_{o2}],r) + s_{R}([x,m_{o1},m_{o2}],r’))$$</p><p>由于argmax()函数的存在，这个模型是不可微的。而且中间过程找到相关memory需要监督，这个模型不是端到端的。<br>总的来说，这个memory network是一种普适性的架构，是很初级很简单的，很多部分还不完善，不足以应用具体的任务上。不过，通过多跳方式找到相关memory的思路是很值得学习的。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>Jason Weston作为三作的<a href="http://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">Sainbayar Sukhbaatar2015</a>对Memory network工作的改进，主要改进是实现了端到端，减少了监督。End-to-End Memory Network采用soft attention而不是hard attention来read memory，因此是端到端的。另外不需要对相关memory进行监督。提高memory network的可用性。<br>假设多个句子input $x_1,…,x_n$作为memory，对于query q，输出对应的answer a。给定query q，经过多跳找到相关的memory，并生成对应的answer a。</p><h4 id="single-layer"><a href="#single-layer" class="headerlink" title="single layer"></a>single layer</h4><p>给定input $x_1,x_2,…,x_n$，采用两个不同的embedding matrix A和C分别编码为向量$\lbrace{m_1,…,m_n}\rbrace$，$\lbrace{c_1,…,c_n}\rbrace$,分别对应attention机制的keys和values。将query q经过embedding matrix B编码为向量表示u。</p><p>采用dot-product attention计算权重：$$p_i = softmax(u^\top m_i) = \frac{exp(u^\top m_i)}{\sum_{j}exp(u^\top m_j)}$$<br>则memory representation为：$$o = \sum_{i}p_i m_i$$<br>根据u和o来进行预测：$$\hat{a} = softmax(W (o + u))$$<br>通过最小化a与$\hat{a}$之间的交叉熵来训练模型参数A,B,C,W。这个single layer end-to-end Memory network是简单而直观的。核心是用soft attention来read memory，找到相关的memory，并进行inference。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/mem.png" alt="Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)" title>                </div>                <div class="image-caption">Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)</div>            </figure></p><h4 id="multi-layers"><a href="#multi-layers" class="headerlink" title="multi layers"></a>multi layers</h4><p>将K层single layer进行stack得到K层memory network，进行K跳memory查询操作。具体地stack方式为：</p><ul><li>将第k层的输入$u^k$和memory representation $o^k$相加作为第k+1层的输入:$$u^{k+1} = u^k + o^k$$</li><li>每一层都有单独的embedding matrix $A^k$和$C^k$</li><li>最后一层的预测输出为：$$\hat{a} = softmax(W u^{K+1}) = softmax(W(u^K + o^K))$$</li></ul><p>为了减少参数量，有两种方法：</p><ul><li>adjacent:<br>  让相邻层的embedding matrix A=C，共享参数。即：$C^k = A^{k+1}$，对第一层有$A^1 = B$，最后一层有：$C^K = W$。这样就减少了一半的参数量。</li><li>RNN-like:<br>  跟RNN一样，采用完全参数共享的方法，$A^1 = A^2 = … = A^K$;$C^1 = C^2 = … = C^K$。参数数量大大减少导致模型效果变差，在层与层之间添加一个线性映射：$u^{k+1} = Hu^k + o^k$</li></ul><h3 id="key-value-Memory-Networks"><a href="#key-value-Memory-Networks" class="headerlink" title="key-value Memory Networks"></a>key-value Memory Networks</h3><p>Jason Weston作为作者之一的<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">Alexander Miller2016</a>在End-to-End Memory networks的基础上继续推进，可以更好的通过memory来编码和利用先验知识，并且具体地应用到了QA系统中。</p><p>作为memory的先验知识可以是结构化的三元组知识库，也可以是非结构化的文本。</p><ul><li>三元组知识库。三元组的形式是”实体-关系-实体”，或”主语-谓语-宾语”。三元组知识库的优点是结构化的，便于机器处理。但缺点是与一句完整的话比较，三元组缺少了一些信息。由于三元组知识库是人工构建的，难免会有覆盖不到的知识，对于某个问题可能知识库中根本就没有对应的知识。另外，三元组中的实体可以有多种不同的表达，比如知识库中有三元组”中国-首都-北京”。当问题是“中华人民共和国的首都是？”时，可能就不能很好地回答。</li><li>像“维基百科”这样的非结构化文本。优点时覆盖面广，几乎包含所有问题的知识。缺点是非结构化的，有歧义，需要经过复杂的推理才能找到答案。</li></ul><p>作为先验知识的memory是(key,value)形式的。</p><ul><li>key memory用于寻址(addressing/lookup)阶段，通过计算query与key memory的相关程度来计算attention权重，因此在设计key memory时，key memory的特征应该更好地匹配query。</li><li>value memory用于read阶段，将value memory的加权和作为memory总的向量表示，因此在涉及value memory时，value memory的特征应该更好地匹配response。</li></ul><p>比较一下end-to-end memory network与key-value memory network的区别：</p><ul><li>前者是将相同的输入经过两个不同的embedding matrix编码分为作为key memory和value memory。而后者可以将不同的知识(key,value)分别编码为key memory和value memory，可以更灵活地利用先验知识。</li><li>后者的每个hop之间添加了用$R_j$来进行线性映射。</li></ul><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>在问答系统中，记memory slots为$(k_1,v_1),…,(k_M,v_M)$，问题query为x，真实回复为a，预测回复为$\hat{a}$。$\Phi_{X},\Phi_{Y},\Phi_{K},\Phi_{V}$分别是x,a,key,value的embedding matrix，将文本编码为向量表示。</p><p>则单次memory的寻址和读取可以分为三步：</p><ul><li>key hashing:<br>  当知识库很大时，这一步是非常必要的。根据query从知识库中检索筛选出相关的facts $ (k_{h_1},v_{h_1}),(k_{h_2},v_{h_2}),…,(k_{h_N},v_{h_N})$，筛选条件可以是key中至少包含query中一个相同的词（去除停用词）。这一步可以在数据预处理时进行，直接将query和相关的facts作为模型的输入。</li><li>key addressing(寻址阶段)<br>  计算query与memory的相关程度来分配在memory上的概率分布：$$p_{h_i} = softmax(A\Phi_{X}(x) \cdot A\Phi_{K}(k_{h_i}))$$其中$\Phi$将文本编码为D维向量，A是一个$d\times D$的可训练矩阵。</li><li>value reading：<br>  将value的加权求和作为memory总的向量表示。$$o = \sum_{i}p_{h_i}A\Phi_{V}(v_{h_i})$$</li></ul><p>memory的读取过程是由controller神经网络通过query $q = A\Phi_{X}(x)$来控制的。模型会利用query $q$与上一跳(hop)的$o$来更新query，进而迭代地寻址和读取memory，这个迭代的过程称为多跳(hops)。<br>用多跳方式来迭代地寻址和读取memory，可以这样来理解：浅层神经网络可以学习到低级的特征，随着神经网络层数增多就可以学习到更高级的特征。类比CNN处理人脸图片时，第一层可以学习到一些边缘特征，第二层可以学习到眼睛、鼻子、嘴巴这样的特征，最后一层得到整个人脸的特征。同样地，用多跳方式来寻址和读取memory，可以得到更相关更突出的memory，同时可以起到推理的作用。</p><p>query的更新公式为:$$q_2 = R_1(q + o)$$其中R是一个$d\times d$的可训练矩阵。每一跳使用不同的矩阵$R_j$。<br>则第j跳更新query后，寻址阶段的计算公式为$$p_{h_i} = softmax(q_{j+1}^\top \cdot A\Phi_{K}(k_{h_i}))$$<br>在经过H跳之后，用controller神经网络的最终状态进行预测:$$\hat{a} = argmax_{i=1,…,C}softmax(q_{H+1}B\Phi_{y}(y_i))$$其中B是一个$d\times D$的可训练矩阵，形状跟A一样。$y_i$可以是知识库中的实体，或者候选句子。</p><p>模型的目标函数为预测回复$\hat{a}$与真实回复$a$之间的交叉熵，用梯度下降的方法来更新模型参数：$A,B,R_1,…,R_H$</p><div align="center"><img src="/images/key-value-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.7. 问答系统key-value memory networks的模型框架</font><br><a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener"><font color="grey" size="2">来源:Alexander Miller2016</font></a></div><h4 id="key-value的选择与编码方式"><a href="#key-value的选择与编码方式" class="headerlink" title="key-value的选择与编码方式"></a>key-value的选择与编码方式</h4><p>论文根据不同形式的先验知识，提出了key-value不同的编码方式：</p><ul><li>知识库三元组。三元组形式为”subject-relation-object”，将”subject-relation”作为寻址的key，将”object”作为记忆的value。</li><li>sentence level。直接将句子的词袋向量表示作为key和value，key和value是一样的。每个memory slot存一个句子。</li><li>window level。以大小为W的窗口对文档进行分割（只保留中心词为实体的窗口），将单个窗口内的词作为寻址的key，将窗口的中心词作为value。</li></ul><h3 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://zhuanlan.zhihu.com/c_129532277" target="_blank" rel="noopener">记忆网络-Memory Network</a></li><li><a href="https://jhui.github.io/2017/03/15/Memory-network/" target="_blank" rel="noopener">Memory network (MemNN) &amp; End to end memory network (MemN2N), Dynamic memory network</a></li><li><a href="http://thespermwhale.com/jaseweston/icml2016/" target="_blank" rel="noopener">Memory Networks for Language Understanding, ICML Tutorial 2016</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍Memory Networks。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Neural Turing Machines" scheme="http://yoursite.com/tags/Neural-Turing-Machines/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
</feed>
