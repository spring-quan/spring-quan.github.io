<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>spring&#39;s Blog</title>
  
  <subtitle>游龙当归海，海不迎我自来也。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-01T08:32:54.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>spring</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文笔记《Multi-Level Memory for Task Oriented Dialogs》</title>
    <link href="http://yoursite.com/2019/08/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E3%80%8AMulti-Level-Memory-for-Task-Oriented-Dialogs%E3%80%8B/"/>
    <id>http://yoursite.com/2019/08/01/论文笔记《Multi-Level-Memory-for-Task-Oriented-Dialogs》/</id>
    <published>2019-08-01T06:14:37.000Z</published>
    <updated>2019-08-01T08:32:54.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>【来源】：NAACL2019<br>【链接】：<a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1810.10647.pdf</a><br>【代码、数据集】：<a href="https://github.com/DineshRaghu/multi-level-memory-network" target="_blank" rel="noopener">https://github.com/DineshRaghu/multi-level-memory-network</a></p></blockquote><a id="more"></a><p>已有工作中，端到端的任务型对话系统采用memory network来结合外部的知识库(knowledgt base) 和 对话历史(context)。为了使用从跑一趟 network，通常将二者放在同一个memory中。这样带来的问题是：memory变得太大，模型在读取memory时需要区分外部知识库和对话历史，并且在memory上的推理变得很难。为了解决这个问题，论文将外部知识库和对话历史区分开，另外，将外部知识库保存为分层的memory。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型主要包括三个部分。 </p><ul><li>分级encoder：<br>  分别编码对话历史中的句子。</li><li>milti-level memory<br>  保存了目前为止所有的query以及对应的知识库查询结果，是以分级的方式保存在memory中的。</li><li>copy机制增强的decoder：<br>  从词汇表中生成词，或者从知识库multi-level memory中复制词，或者从对话历史(context)中复制词。</li></ul><div align="center"><img src="/images/multi-memory-model.jpg" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.1. 模型的整体框架图</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div><h4 id="分级encoder"><a href="#分级encoder" class="headerlink" title="分级encoder"></a>分级encoder</h4><p>在第t轮，对话历史共有2t-1个句子$\lbrace{c_1,c_2,…,c_{2t-1}}\rbrace$，其中用户对话为t轮，回复对话为t-1轮。 每个句子$c_i$都是词序列$\lbrace{w_{i1},w_{i2},…,w_{im}}\rbrace$。<br>每个句子$c_i$先经过embedding layer得到词向量表示，再经过单层bi-GRU得到句子的向量表示$\varphi(c_i)$。$h_{ij}^e$表示词$w_{ij}$对应的隐藏状态。<br>再将$\varphi{c_i}$经过另一个单词GRU来得到context的向量表示$c$。</p><h4 id="multi-level-memory"><a href="#multi-level-memory" class="headerlink" title="multi-level memory"></a>multi-level memory</h4><p>memory的关键是分级的分为三级：query $\to$ result $\to$ result key和result value。见Fig.2。<br>记本轮对话之前所有的知识库query为$q_1,…,q_k$。每个query $q_i$是一个(key,value)对，$q_i = \lbrace{k_a^{q_i}:v_a^{q_i},0&lt; a&lt; n_{q_i}}\rbrace $。其中key和value分别对应query的槽(slots)和槽值，$n_{q_i}$是query $q_i$的槽值个数。<br>第j轮对话，用query $q_i$查询知识库的返回结果为result $r_{ij}$。$r_{ij}$也是一个key-value对，$r_{ij} = \lbrace{k_a^{r_{ij}}:v_a^{r_{ij}},0&lt; a &lt; n_{r_{ij}}}\rbrace$。其中$n_{r_{ij}}$是key-value对的个数。</p><div align="center"><img src="/images/multi-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.2. multi memory</font><br><a href="https://arxiv.org/pdf/1810.10647.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Revanth Reddy2019</font></a></div>第一级memory是query的向量表示。 $q_i$的向量表示为$q_i^v$，$q_i^v$为所有values $v_a^{q_i}$的词袋(bag of words)向量表示。第二级memory是result的向量表示。同样地，$r_{ij}$的向量表示为$r_{ij}^v$，$r_{ij}^v$为所有values $v_a^{r_{ij}}$的词袋(BOW)向量表示。第三级memory是result的key-value对，$(k_a^{r_{ij}}:v_a^{r_{ij}})$，其中value $v_a^{r_{ij}}$可能会被复制到回复中。<h4 id="copy机制增强的decoder"><a href="#copy机制增强的decoder" class="headerlink" title="copy机制增强的decoder"></a>copy机制增强的decoder</h4><p>decoder一个词一个词地生成回复。在时间步t生成词$y_t$时，可能从词汇表中生成，也能从两个分开的memory上复制。用门$g_1$来选择是从词汇表上生成，还是从memory中复制。如果是后者，用另一个门$g_2$来选择是从context中复制，还是从知识库复制。</p><ol><li><strong>从词汇表生成词</strong><br> 时间步t，decoder的隐藏状态$h_t$为$$h_t = GRU(y_{t-1},s_{t-1})$$用$h_t$计算在encoder的所有隐藏状态上的attention权重，采用”concat attention”机制：$$a_{ij} = softmax(w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))) = \frac{w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}{\sum_{ij}w_1^\top tanh(W_2tanh(W_3[h_t,h_{ij}^e]))}$$则context vector为$$d_t = \sum_{ij}a_{ij}h^e_{ij}$$ $h_t$和$d_t$连接后经过线性层和softmax层得到在词汇表上的概率分布：$$P_g(y_t) = softmax(W_1[h_t,d_t] + b_1)$$</li><li><strong>从context memory中复制词</strong><br> 直接将计算context vector时的attention权重，作为在context所有词$w_{ij}$上的概率分布：$$P_{con}(y_t = w) = \sum_{ij:w_{ij}=w}a_{ij}$$</li><li><strong>从KB memory中复制实体</strong><br> 时间步t的隐藏状态$h_t$和context vector $d_t$用来计算在所有query上的attention权重。第一级在所有query $q_1,q_2,…,q_k$的attention权重为$$\alpha_i = softmax(w_2^\top tanh(W_4[h_t,d_t,q_i^v])) = \frac{w_2^\top tanh(W_4[h_t,d_t,q_i^v])}{\sum_{i}w_2^\top tanh(W_4[h_t,d_t,q_i^v])}$$<br> 第二级$\beta_i$在$q_i$对应的$r_i$上的attention权重为$$\beta_{ij} = softmax(w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])) = \frac{w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}{\sum_{j}w_3^\top tanh(W_5[h_t,d_t,r_{ij}^v])}$$<br> 第一级attention和第二级attention的乘积是在所有result上的attention权重分布。则memory总的向量表示为$$m_t = \sum_{i}\sum_j\alpha_i\beta_{ij}r_{ij}^v$$<br> 第三级memory为result的key-value对$(k_a^{r_{ij}}:v_a^{r_{ij}})$，类似于<a href="https://arxiv.org/abs/1705.05414" target="_blank" rel="noopener">(Eric and Manning, 2017)</a>，用key $k_a^{r_{ij}}$来计算attention权重，将对应的value $v_a^{r_{ij}}$复制到回复中。在$r_{ij}$所有keys上的attention权重为$$\gamma_{ijl} = softmax(w_4^\top tanh(W_6[h_t,d_t,m_t,k_l^{r_{ij}}]))$$则在所有values $v_a^{r_{ij}}$的概率分布为:$$P_{kb}(y_t = w) = \sum_{ijl:v_l^{r_{ij}}=w}\alpha_i\beta_{ij}\gamma_{ijl}$$</li><li><strong>decoding</strong><br> 我们用门机制$g_2$来来结合$P_{con}(y_t)$和$P_{kb}(y_t)$，得到memory上的copy概率分布$P_c(y_t)$。$$g_2 = sigmoid(W_7[h_t,d_t,m_t]+b_2)$$ $$P_c(y_t) = g_2P_{kb}(y_t) + (1-g_2)P_{con}(y_t)$$ 用门机制$g_1$来结合$P_{c}(y_t)$和$P_{g}(y_t)$来得到总的概率分布$P(y_t)$：$$g_1 = sigmoid(W_8[h_t,d_t,m_t]+b_3)$$ $$P(y_t) = g_1P_g(y_t) + (1-g_1)P_c(y_t)$$</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【来源】：NAACL2019&lt;br&gt;【链接】：&lt;a href=&quot;https://arxiv.org/pdf/1810.10647.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1810.10647.pdf&lt;/a&gt;&lt;br&gt;【代码、数据集】：&lt;a href=&quot;https://github.com/DineshRaghu/multi-level-memory-network&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/DineshRaghu/multi-level-memory-network&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>Neural Turing Machines与Memory Network</title>
    <link href="http://yoursite.com/2019/07/26/Neural-Turing-Machines%E4%B8%8EMemory-Network/"/>
    <id>http://yoursite.com/2019/07/26/Neural-Turing-Machines与Memory-Network/</id>
    <published>2019-07-26T03:03:15.000Z</published>
    <updated>2019-08-01T02:30:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍Memory Networks。</p><a id="more"></a><h3 id="Neural-Turing-Machines-神经图灵机"><a href="#Neural-Turing-Machines-神经图灵机" class="headerlink" title="Neural Turing Machines-神经图灵机"></a>Neural Turing Machines-神经图灵机</h3><p>Google DeepMind团队在<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>提出Neural Turing Machines，第一次提出用external memory来提高神经网络的记忆能力。这之后又出现了多篇关于Memory Networks的论文。我们先看看Turing Machines的概念。</p><h4 id="Turing-Machines-图灵机"><a href="#Turing-Machines-图灵机" class="headerlink" title="Turing Machines-图灵机"></a>Turing Machines-图灵机</h4><p>计算机先驱<a href="https://baike.baidu.com/item/%E8%89%BE%E4%BC%A6%C2%B7%E9%BA%A6%E5%B8%AD%E6%A3%AE%C2%B7%E5%9B%BE%E7%81%B5/3940576?fromtitle=%E5%9B%BE%E7%81%B5&fromid=121208" target="_blank" rel="noopener">turing</a>在1936年提出了Turing Machines这样一个计算模型。它由三个基本的组件：</p><ul><li>tape: 一个无限长的纸带作为memory，包含无数个symbols，每个symbol的值为0、1或”$\space$”。</li><li>head: 读写头，对tape上的symbols进行读操作和写操作。</li><li>controller： 根据当前状态来控制head的操作。</li></ul><p>理论上Turing Machines可以模拟任何一个计算算法，不管这个算法多么复杂。但现实中，计算机不可能有无限大的memory space，因此Turing Machines只是数学意义上的计算模型。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/turing-machine.jpg" alt="Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)" title>                </div>                <div class="image-caption">Fig. 1. How a Turing machine looks like.(来源: http://aturingmachine.com/)</div>            </figure><h4 id="Neural-Turing-Machines"><a href="#Neural-Turing-Machines" class="headerlink" title="Neural Turing Machines"></a>Neural Turing Machines</h4><p>Neural Turing Machines(NTM,<a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Alex Graves2014</a>)用external memory来提高神经网络的记忆能力。<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM(Long and short memory)</a>通过门机制有效缓解了RNN的’梯度消失和梯度爆炸问题’，可以通过internal memory实现长期记忆。当LSTM的internal memory的记忆能力有限，需要用external memory来提高神经网络的记忆能力。</p><p>Neural Turing Machines包含两个基本组件：<em>a neural network controller</em>和<em>memory bank</em>。<em>memory</em>是一个 $N\cdot M$阶的矩阵，包含N个向量，每个向量的维度是M。我们把每个memory vector称为memory location。<em>controller</em>控制<em>heads</em>对<em>memory</em>进行读写操作。</p><p>如何对<em>memory matrix</em>进行读写操作呢？关键问题是如何让读写操作是可微的，这样才能用梯度下降法来更新模型参数。具体来说，问题是让模型关于memory location是可微的，但memory locations是离散的。Neural Turing Machines用了一个很聪明的方法来解决这个问题：不是对单独某个memory location进行读写操作，而是对所有的memory locations进行不同程度的读写操作，这个程度是通过attention的权重分布来控制的。</p><div align="center"><img src="/images/NTM.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig. 2. Neural Turing Machine Architecture</font></div><h5 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h5><p>记时间步t <em>memory matrix</em>为$N\cdot M$阶矩阵$M_t$，$w_t$是在N个memory向量上的权重分布，是一个N维向量。则时间步t的read vector $r_t$为$$r_t = \sum_{i=1}^{N}w_t(i)\cdot M_t(i)$$ $$where: \sum_{i=1}^{N}w_t(i) = 1; 0 \le w_t(i) \le 1,\forall i $$其中，$w_t(i)$是$w_t$的第i个元素，$M_t(i)$是$M_t$的第i个行向量。</p><h5 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h5><p>受LSTM门机制的启发，将写操作分成两步：先<em>erase</em>，再<em>add</em>。先根据<em>erase vector $e_t$</em>擦去旧的内容，再根据<em>add vector $a_t$</em>添加新的内容。</p><ol><li>先erase：<br> 在时间步t，attention权重分布为$w_t$，<em>erase vector $e_t$</em>是一个M维向量，每个元素取值[0,1]，上一个时间步的<em>memory vector</em>为$M_{t-1}$。则erase操作为$$\tilde{M_{t}}(i) = M_{t-1}(i)[\vec{1}-w_t(i)e_t]$$ $\vec{1}$是一个M维的全1向量。对memory vector的erase操作是逐点进行的。当$e_t$的元素和memory location对应权重$w_t(i)$的元素值都是1时，memory vector $M_t(i)$的元素值才会置为0。如果$e_t$或$w_t(i)$的元素值为0时，memory vector $M_t(i)$的元素值保持不变。</li><li>再add:<br> 每个<em>write head</em>会产生一个M维的<em>add vector a_t</em>，则：$$M_t(i) = \tilde{M_{t}}(i) + w_t(i)a_t$$至此，就完成了写操作。</li></ol><h5 id="寻址机制"><a href="#寻址机制" class="headerlink" title="寻址机制"></a>寻址机制</h5><p>进行读写操作前，要搞清楚对哪个memory location进行读写呢？这就是寻址。为了让模型关于memory locatios可微，Neural Turing Machines不是对某个单独的memory location进行读写操作，而是对所有memory locations进行不同程度的读写操作，这个程度就是由权重分布$w_t$来控制的。模型结合并同时使用了content-based和location-based两种寻址方式来计算这个权重分布$w_t$。具体地，权重计算分为以下几步：</p><ol><li>content-based addressing<br> 时间步t，每个head产出一个M维的<em>key vector $k_t$</em>，通过$k_t$与memory vectors $M_t(i)$之间的相似性来计算content-based attention权重分布$w_{t}^{c}$。相似性是通过余弦相似度来衡量的。$$w_{t}^{c} = softmax(\beta_tK(k_t,M_t(i))) = \frac{\beta_tK(k_t,M_t(i))}{\sum_{j}K(k_t,M_t(j))}$$ $$K(u,v) = \frac{u\cdot v}{|u|\cdot |v|}$$<br> $\beta_t$可以放大或缩小权重的精度。</li><li>内插法<br> 每个head产生一个<em>interpolation gate $g_t$</em>，取值[0,1]。content-based attention权重分布为$w_t^{c}$，上一个时间步的attention权重分布为$w_{t-1}$。则门控制的权重分布$w_t^g$为：$$w_t^g = g_tw_t^c + (1-g_t)w_{t-1}$$当$g_t$为0时，采用上一个时间步的权重分布$w_{t-1}$，当$g_t$为1时，采用content-based attention权重分布$w_t^c$。</li><li>循环卷积<br> 对经过插值后的权重分布$w_t^g$进行循环卷积，主要功能是对权重进行旋转位移。比如当权重分布关注某个memory location时，经过循环卷积就会扩展到附近的memory locations，也会对附近的memory locations进行少量的读写操作。每个head产生的转移权重为$s_t$,循环卷积的操作为:$$\tilde{w_t(i)} = \sum_{j=0}^{N-1}w_t^g(i)s_t(i-j)$$<br> 关于$s_t$的详细介绍可以见<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a>;<br> 循环卷积的详细介绍可以见<a href="https://blog.csdn.net/rtygbwwwerr/article/details/50548311" target="_blank" rel="noopener">Neural Turing Machines-NTM系列（一）简述</a></li><li>锐化<br> 循环卷积往往会造成权重泄漏和分散，为了解决这个问题，需要最后进行锐化操作。$$w_t(i) = \frac{\tilde{w_t(i)^{\gamma_t}}}{\sum_j\tilde{w_t(j)^{\gamma_t}}}$$其中$\gamma_t &gt;1$。至此，就得到了时间步t的权重分布$w_t$。可以根据这个权重分布$w_t$对memory matrix进行读写操作。</li></ol><p>总结以下这4步操作。第一步content-based addressing根据输入得到关于memory locations的相似度；后三步实现了location-based addressing。第二步插值操作引入了上一个时间步的权重分布，对content-based 权重进行修正；第三步循环卷积将每个位置的权重向两边分散；第四步锐化操作将权重突出化，大的更大，小的更小。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/50.jpg" alt="Fig.3. 寻址机制的4步操作" title>                </div>                <div class="image-caption">Fig.3. 寻址机制的4步操作</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/NTM-flow-addressing.png" alt="Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)" title>                </div>                <div class="image-caption">Fig.4. 寻址机制的4步操作<br>来源：[Alex Graves2014](https://arxiv.org/abs/1410.5401)</div>            </figure><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><ul><li><a href="https://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a><br>  用动图直观地表现Neural Turing Machines的计算过程。推荐！👍</li><li><a href="https://zhuanlan.zhihu.com/p/30383994" target="_blank" rel="noopener">记忆网络之Neural Turing Machines</a>，中文</li><li><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention?attenion!</a></li></ul><h3 id="Memory-Network"><a href="#Memory-Network" class="headerlink" title="Memory Network"></a>Memory Network</h3><p>在Neural Turing Machines提出仅仅五天后，Facebook研究员<a href="http://www.thespermwhale.com/jaseweston/" target="_blank" rel="noopener">Jason Weston</a>发表了<a href="http://arxiv.org/abs/1410.3916" target="_blank" rel="noopener">MEMORY NETWORKS</a>。在QA系统的领域，应用memory network。虽然RNN或LSTM可以通过hidden state和weights来进行短期记忆，但它们的记忆能力是有限的。要实现长期记忆，需要memory network。</p><h4 id="Memory-Network的一般框架"><a href="#Memory-Network的一般框架" class="headerlink" title="Memory Network的一般框架"></a>Memory Network的一般框架</h4><p>memory network包括一个记忆单元memory，和四个基本组件：</p><ul><li>I(input feature map):<br>  将input <em>x</em>进行向量化表示，编码为feature representation <em>I(x)</em>。</li><li>G(generalization):<br>  对memory进行写操作。根据input 来更新memory <em>$m_i$</em>。$m_i = G(m_i,I(x),m)$</li><li>O(output feature map):<br>  对memory进行读操作。根据input和memory生成output feature。$o = O(I(x),m)$</li><li>R(response):<br>  根据output feature o来生成response。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/memn.jpg" alt="Fig.5. memory network的框架图" title>                </div>                <div class="image-caption">Fig.5. memory network的框架图</div>            </figure><h4 id="memory-network框架的实现–MemNNs"><a href="#memory-network框架的实现–MemNNs" class="headerlink" title="memory network框架的实现–MemNNs"></a>memory network框架的实现–MemNNs</h4><p>在I模块将input $x_i$编码为$I(x_i)$后，G模块之间将$I(x_i)$保存到下一个空的memory slot中，而不更新旧的memory slots。真正实现inference的核心模块是O和R。</p><p>O模块在给定x的条件下，依次找到与x最相关的k个memory slots。论文中采用k = 2。先找到第一个最相关的memory slot：$$m_{o1} = \mathop{argmax}\limits_{i = 1,…,N} s_{o1}(x,m_i)$$其中$s_o()$是一个匹配函数，计算x与$m_i$之间的相关程度。接着，根据x和第一个memory找到下一个memory：$$m_{o2} = \mathop{argmax}\limits_{i = 1,…,N} s_{o2}([x,m_{o1}],m_i)$$将output feature o = $[x,m_{o1},m_{o2}]$作为R模块的输入。</p><p>R模块将词汇表中所有词与output feature进行匹配，选择匹配度最高的词作为response。这样生成的response只有一个词。$$r = \mathop{argmax}\limits_{w \in W}s_R([x,m_{o1},m_{o2}],w)$$其中$s_R()$是一个匹配函数。</p><p>匹配函数$s_O$和$s_R$都采用以下函数：$$s(x,y) = \Phi_x(x)^\top U^\top U \Phi_y(y)$$其中$\Phi_x(x),\Phi_y(y)$分别将x/y编码为向量。<br><strong>目标函数</strong><br>在训练阶段采用最大边缘目标函数，设对于question x，真实的label为r，对应的memory为$m_{o1},m_{o2}$。则最大边缘目标函数为：$$\sum_{m_i\ne m_{o1}}max(0,\gamma - s_{O1}(x,m_{o1}) + s_{O1}(x,m_i)) + $$ $$\sum_{m_j\ne m_{o2}}max(0,\gamma - s_{O2}([x,m_{o1}],m_{o2}) + s_{O2}([x,m_{o1}],m_j)) + $$ $$\sum_{r’ \ne r}max(0,\gamma - s_{R}([x,m_{o1},m_{o2}],r) + s_{R}([x,m_{o1},m_{o2}],r’))$$</p><p>由于argmax()函数的存在，这个模型是不可微的。而且中间过程找到相关memory需要监督，这个模型不是端到端的。<br>总的来说，这个memory network是一种普适性的架构，是很初级很简单的，很多部分还不完善，不足以应用具体的任务上。不过，通过多跳方式找到相关memory的思路是很值得学习的。</p><h3 id="End-to-End-Memory-Network"><a href="#End-to-End-Memory-Network" class="headerlink" title="End-to-End Memory Network"></a>End-to-End Memory Network</h3><p>Jason Weston作为三作的<a href="http://arxiv.org/abs/1503.08895" target="_blank" rel="noopener">Sainbayar Sukhbaatar2015</a>对Memory network工作的改进，主要改进是实现了端到端，减少了监督。End-to-End Memory Network采用soft attention而不是hard attention来read memory，因此是端到端的。另外不需要对相关memory进行监督。提高memory network的可用性。<br>假设多个句子input $x_1,…,x_n$作为memory，对于query q，输出对应的answer a。给定query q，经过多跳找到相关的memory，并生成对应的answer a。</p><h4 id="single-layer"><a href="#single-layer" class="headerlink" title="single layer"></a>single layer</h4><p>给定input $x_1,x_2,…,x_n$，采用两个不同的embedding matrix A和C分别编码为向量$\lbrace{m_1,…,m_n}\rbrace$，$\lbrace{c_1,…,c_n}\rbrace$,分别对应attention机制的keys和values。将query q经过embedding matrix B编码为向量表示u。</p><p>采用dot-product attention计算权重：$$p_i = softmax(u^\top m_i) = \frac{exp(u^\top m_i)}{\sum_{j}exp(u^\top m_j)}$$<br>则memory representation为：$$o = \sum_{i}p_i m_i$$<br>根据u和o来进行预测：$$\hat{a} = softmax(W (o + u))$$<br>通过最小化a与$\hat{a}$之间的交叉熵来训练模型参数A,B,C,W。这个single layer end-to-end Memory network是简单而直观的。核心是用soft attention来read memory，找到相关的memory，并进行inference。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/mem.png" alt="Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)" title>                </div>                <div class="image-caption">Fig.6.左:single layer;右:multi layers<br>来源：[Sainbayar Sukhbaatar2015](http://arxiv.org/abs/1503.08895)</div>            </figure></p><h4 id="multi-layers"><a href="#multi-layers" class="headerlink" title="multi layers"></a>multi layers</h4><p>将K层single layer进行stack得到K层memory network，进行K跳memory查询操作。具体地stack方式为：</p><ul><li>将第k层的输入$u^k$和memory representation $o^k$相加作为第k+1层的输入:$$u^{k+1} = u^k + o^k$$</li><li>每一层都有单独的embedding matrix $A^k$和$C^k$</li><li>最后一层的预测输出为：$$\hat{a} = softmax(W u^{K+1}) = softmax(W(u^K + o^K))$$</li></ul><p>为了减少参数量，有两种方法：</p><ul><li>adjacent:<br>  让相邻层的embedding matrix A=C，共享参数。即：$C^k = A^{k+1}$，对第一层有$A^1 = B$，最后一层有：$C^K = W$。这样就减少了一半的参数量。</li><li>RNN-like:<br>  跟RNN一样，采用完全参数共享的方法，$A^1 = A^2 = … = A^K$;$C^1 = C^2 = … = C^K$。参数数量大大减少导致模型效果变差，在层与层之间添加一个线性映射：$u^{k+1} = Hu^k + o^k$</li></ul><h3 id="key-value-Memory-Networks"><a href="#key-value-Memory-Networks" class="headerlink" title="key-value Memory Networks"></a>key-value Memory Networks</h3><p>Jason Weston作为作者之一的<a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener">Alexander Miller2016</a>在End-to-End Memory networks的基础上继续推进，可以更好的通过memory来编码和利用先验知识，并且具体地应用到了QA系统中。</p><p>作为memory的先验知识可以是结构化的三元组知识库，也可以是非结构化的文本。</p><ul><li>三元组知识库。三元组的形式是”实体-关系-实体”，或”主语-谓语-宾语”。三元组知识库的优点是结构化的，便于机器处理。但缺点是与一句完整的话比较，三元组缺少了一些信息。由于三元组知识库是人工构建的，难免会有覆盖不到的知识，对于某个问题可能知识库中根本就没有对应的知识。另外，三元组中的实体可以有多种不同的表达，比如知识库中有三元组”中国-首都-北京”。当问题是“中华人民共和国的首都是？”时，可能就不能很好地回答。</li><li>像“维基百科”这样的非结构化文本。优点时覆盖面广，几乎包含所有问题的知识。缺点是非结构化的，有歧义，需要经过复杂的推理才能找到答案。</li></ul><p>作为先验知识的memory是(key,value)形式的。</p><ul><li>key memory用于寻址(addressing/lookup)阶段，通过计算query与key memory的相关程度来计算attention权重，因此在设计key memory时，key memory的特征应该更好地匹配query。</li><li>value memory用于read阶段，将value memory的加权和作为memory总的向量表示，因此在涉及value memory时，value memory的特征应该更好地匹配response。</li></ul><p>比较一下end-to-end memory network与key-value memory network的区别：</p><ul><li>前者是将相同的输入经过两个不同的embedding matrix编码分为作为key memory和value memory。而后者可以将不同的知识(key,value)分别编码为key memory和value memory，可以更灵活地利用先验知识。</li><li>后者的每个hop之间添加了用$R_j$来进行线性映射。</li></ul><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>在问答系统中，记memory slots为$(k_1,v_1),…,(k_M,v_M)$，问题query为x，真实回复为a，预测回复为$\hat{a}$。$\Phi_{X},\Phi_{Y},\Phi_{K},\Phi_{V}$分别是x,a,key,value的embedding matrix，将文本编码为向量表示。</p><p>则单次memory的寻址和读取可以分为三步：</p><ul><li>key hashing:<br>  当知识库很大时，这一步是非常必要的。根据query从知识库中检索筛选出相关的facts $ (k_{h_1},v_{h_1}),(k_{h_2},v_{h_2}),…,(k_{h_N},v_{h_N})$，筛选条件可以是key中至少包含query中一个相同的词（去除停用词）。这一步可以在数据预处理时进行，直接将query和相关的facts作为模型的输入。</li><li>key addressing(寻址阶段)<br>  计算query与memory的相关程度来分配在memory上的概率分布：$$p_{h_i} = softmax(A\Phi_{X}(x) \cdot A\Phi_{K}(k_{h_i}))$$其中$\Phi$将文本编码为D维向量，A是一个$d\times D$的可训练矩阵。</li><li>value reading：<br>  将value的加权求和作为memory总的向量表示。$$o = \sum_{i}p_{h_i}A\Phi_{V}(v_{h_i})$$</li></ul><p>memory的读取过程是由controller神经网络通过query $q = A\Phi_{X}(x)$来控制的。模型会利用query $q$与上一跳(hop)的$o$来更新query，进而迭代地寻址和读取memory，这个迭代的过程称为多跳(hops)。<br>用多跳方式来迭代地寻址和读取memory，可以这样来理解：浅层神经网络可以学习到低级的特征，随着神经网络层数增多就可以学习到更高级的特征。类比CNN处理人脸图片时，第一层可以学习到一些边缘特征，第二层可以学习到眼睛、鼻子、嘴巴这样的特征，最后一层得到整个人脸的特征。同样地，用多跳方式来寻址和读取memory，可以得到更相关更突出的memory，同时可以起到推理的作用。</p><p>query的更新公式为:$$q_2 = R_1(q + o)$$其中R是一个$d\times d$的可训练矩阵。每一跳使用不同的矩阵$R_j$。<br>则第j跳更新query后，寻址阶段的计算公式为$$p_{h_i} = softmax(q_{j+1}^\top \cdot A\Phi_{K}(k_{h_i}))$$<br>在经过H跳之后，用controller神经网络的最终状态进行预测:$$\hat{a} = argmax_{i=1,…,C}softmax(q_{H+1}B\Phi_{y}(y_i))$$其中B是一个$d\times D$的可训练矩阵，形状跟A一样。$y_i$可以是知识库中的实体，或者候选句子。</p><p>模型的目标函数为预测回复$\hat{a}$与真实回复$a$之间的交叉熵，用梯度下降的方法来更新模型参数：$A,B,R_1,…,R_H$</p><div align="center"><img src="/images/key-value-memory.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.7. 问答系统key-value memory networks的模型框架</font><br><a href="https://arxiv.org/abs/1606.03126" target="_blank" rel="noopener"><font color="grey" size="2">来源:Alexander Miller2016</font></a></div><h4 id="key-value的选择与编码方式"><a href="#key-value的选择与编码方式" class="headerlink" title="key-value的选择与编码方式"></a>key-value的选择与编码方式</h4><p>论文根据不同形式的先验知识，提出了key-value不同的编码方式：</p><ul><li>知识库三元组。三元组形式为”subject-relation-object”，将”subject-relation”作为寻址的key，将”object”作为记忆的value。</li><li>sentence level。直接将句子的词袋向量表示作为key和value，key和value是一样的。每个memory slot存一个句子。</li><li>window level。以大小为W的窗口对文档进行分割（只保留中心词为实体的窗口），将单个窗口内的词作为寻址的key，将窗口的中心词作为value。</li></ul><h3 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://zhuanlan.zhihu.com/c_129532277" target="_blank" rel="noopener">记忆网络-Memory Network</a></li><li><a href="https://jhui.github.io/2017/03/15/Memory-network/" target="_blank" rel="noopener">Memory network (MemNN) &amp; End to end memory network (MemN2N), Dynamic memory network</a></li><li><a href="http://thespermwhale.com/jaseweston/icml2016/" target="_blank" rel="noopener">Memory Networks for Language Understanding, ICML Tutorial 2016</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍Memory Networks。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Neural Turing Machines" scheme="http://yoursite.com/tags/Neural-Turing-Machines/"/>
    
      <category term="Memory Network" scheme="http://yoursite.com/tags/Memory-Network/"/>
    
  </entry>
  
  <entry>
    <title>attention? attention!</title>
    <link href="http://yoursite.com/2019/07/23/attention-attention/"/>
    <id>http://yoursite.com/2019/07/23/attention-attention/</id>
    <published>2019-07-23T08:20:18.000Z</published>
    <updated>2019-07-30T03:41:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>读了博主<a href="https://lilianweng.github.io/lil-log/contact.html" target="_blank" rel="noopener">Weng, Lilian</a>的文章<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines" target="_blank" rel="noopener">attention? attention!</a>，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。<br>attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。</p><a id="more"></a><h3 id="why-we-need-attention-从seq2seq模型谈起"><a href="#why-we-need-attention-从seq2seq模型谈起" class="headerlink" title="why we need attention?从seq2seq模型谈起"></a>why we need attention?从seq2seq模型谈起</h3><p><strong>seq2seq模型</strong>与14年提出(<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever, et al. 2014</a>)，实现输入序列(source sequence)到输出序列(target sequence)的映射，这两个序列的长度都是可变的。序列到序列映射的任务包括机器翻译、问答系统、对话系统、摘要生成等。</p><p>用数学语言来定义序列到序列的任务，给定输入序列(source sequence) $X = \lbrace{ x_1,x_2,…,x_n \rbrace}$，需要生成输出序列(target sequence) $Y = \lbrace{ y_1,y_2,…,y_m \rbrace}$，其中source sequence长度为$n$,target sequence长度为$m$。</p><p><strong>seq2seq模型</strong>基于encoder-decoder框架，包括2个部分：</p><ol><li><p><strong>encoder</strong>将source sequence编码（映射）为一个固定维度的向量表示(context vector,或称为sentence embedding)，我们希望这个向量表示可以很好的表示source sequence的意思。<br> encoder可以采用卷积神经网络CNN，也可以采用循环神经网络RNN，但用的更多的效果也更好的还是RNN。通常使用<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">LSTM 或 GRU</a>。<br> encoder RNN的隐藏状态更新公式为：$$\begin{gather}h_t = f(h_{t-1},x_t)\end{gather}$$其中$h_t$为RNN在时间步t的隐藏状态，f为LSTM 或GRU.<br> 对于长度为n的source sequence，一个词接一个词地输入RNN后，可以得到n个隐藏状态$(h_1,h_2,…,h_n)$，通常将最后一个时间步最后一个词对应的隐藏状态$h_t$作为source sequence的向量表示，也就是context vector，记为$c$。</p></li><li><p><strong>decoder</strong>根据source sequence的向量表示context vector，来一个词一个词的生成target sequence。<br> decoder采用单向RNN，decoder RNN隐藏状态的更新公式为:$$\begin{gather}s_t = f(s_{t-1},y_{t-1},c)\end{gather}$$其中$s_t$为decoder在时间步t的隐藏状态，$y_{t-1}$为target sequence中的上一个词，在train阶段，$y_{n-1}$为真实target sequence中的上一个词，在infer阶段，$y_{t-1}$为预测输出的上一个词；c为context vector。<br> 时间步t，隐藏状态$s_t$再经过线性层和softmax得到在词表上的概率分布，将概率最大的词作为prediction word $y_t$。迭代循环直到输出整个target sequence。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/43.png" alt="Fig.1. seq2seq模型的框架图" title>                </div>                <div class="image-caption">Fig.1. seq2seq模型的框架图</div>            </figure><p>我们可以看到当生成不同的$y_t$时，所依据的context vector都是固定不变的。固定的context vector有一个缺点是：当encoder编码完整个source sequence时，会偏向于最近的词，而遗忘了距离更远的最开始的一些词。<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">(Bahdanau et al., 2015)</a>提出了attention机制来解决这个问题。</p><h3 id="attention机制-born-for-Translation"><a href="#attention机制-born-for-Translation" class="headerlink" title="attention机制:born for Translation"></a>attention机制:born for Translation</h3><p>attention机制最先在机器翻译(neural machine translation,NMT)任务上提出。从解决长期依赖问题的角度，attention可以实现长距离的记忆；从注意力的角度，attention机制可以实现对齐(alignment)，用更多的注意力关注到相关的部分，而忽略或低注意力关注到不相关的部分。</p><p>上文中提到，在生成不同的$y_t$时，直接将encoder最后一个时间步的隐藏状态$h_n$作为固定context vector。不同于这种方法，attention机制将所有encoder隐藏状态$\lbrace{ h_1,h_2,…,h_n }\rbrace$的加权和作为context vector，这样在每个时间步t生成$y_t$时，所依据的context vector都是专门针对于$y_t$的。<br>一方面，context vector可以获取到所有隐藏状态，也就是整个source sequence的信息，这样就可以实现长距离的记忆。另一方面，source sequence 与target sequence之间的语义对齐(aligenment)是也是通过context vector实现的。在计算时间步t生成$y_t$对应的context vector $c_t$的计算需要三个部分的信息：</p><ul><li>所有的encoder隐藏状态： $\lbrace{ h_1,h_2,…,h_n }\rbrace$</li><li>上个时间步t-1的decoder 隐藏状态： $s_{t-1}$</li><li>source与target之间的alignment.<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/44.png" alt="Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)" title>                </div>                <div class="image-caption">Fig.2.有attention机制的encoder-decoder模型，来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)</div>            </figure></li></ul><h4 id="attention的数学定义"><a href="#attention的数学定义" class="headerlink" title="attention的数学定义"></a>attention的数学定义</h4><p>在计算时间步t生成$y_t$对应的context vector $c_t$时，encoder的所有隐藏状态为 $\lbrace{ h_1,h_2,…,h_n }\rbrace$，时间步t-1的decoder隐藏状态为 $s_{t-1}$，decoder RNN的隐藏状态更新公式变为：$$\begin{gather}s_t = f(s_{t-1},y_{t-1},c_t)\end{gather}$$ context vector $c_t$为encoder hidden state的加权和：<br>$$c_t = \sum_{i=1}^{n}\alpha_{t,i}h_i$$ $$\alpha_{t,i} = softmax(\beta_{t,i}) = \frac{exp(\beta_{t,i})}{\sum_{j = 1}^{n}exp(\beta_{t,j})}$$ $$\beta_{t,i} = score(s_{t-1},h_i)$$<br>其中权重$\alpha_{t,i}$是时间步t生成$y_t$与隐藏状态$h_i$之间的score，从某种意义上说，$h_i$可以看作是$x_i$的表示，也可以看作是$\lbrace{x_1,x_2,…,x_{i}}\rbrace$的表示。因此，$\alpha_{t,i}$可以看作是$y_t$与$x_i$之间联系（相关性）的score。所有权重$\lbrace{\alpha_{t,1},\alpha_{t,2},…,\alpha_{t,n}}\rbrace$衡量了生成$y_t$时应该如何关注到所有的encoder hidden state。</p><p>score()为打分函数，有多种计算方法，下文会详细介绍。在<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015.</a>中，score()采用前馈神经网络，采用非线性激活函数$tanh()$,score()的数学形式为：$$score(s_{t},h_i) = v_a^\top tanh(W_a[s_t;h_i])$$<br>其中$v_a,W_a$是可训练参数。<br>attention权重可视化矩阵很直观地表明了source words与target words之间的关联关系:</p><div align="center"><img src="/images/45.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.3.来源:[Bahdanau et al., 2015.](https://arxiv.org/pdf/1409.0473.pdf)</font></div><h3 id="各种attention机制"><a href="#各种attention机制" class="headerlink" title="各种attention机制"></a>各种attention机制</h3><h4 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h4><p>下表总结了使用比较广泛的attention机制，及其对应的alignment score function。</p><table class="table table-bordered table-striped table-condensed">   <tr>      <th width="25">名字</th>      <th>alignment score funtion</th>      <th width="25">来源</th>   </tr>   <tr>      <td>content-based attention</td>      <td>$score(s_t,h_i) = cosine(s_t,h_i)$</td>      <td><a href="https://arxiv.org/abs/1410.5401" target="_blank" rel="noopener">Graves2014</a></td>   </tr>   <tr>      <td>concat/additive</td>      <td>$score(s_{t},h_i) = v_a^\top tanh(W_a[s_t;h_i])$</td>      <td><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a></td>   </tr>   <tr>      <td>location-based</td>      <td>$\alpha_{t,i} = softmax(W_as_t)$<br><font color="grey" size="2">将alignment简化为只依赖于target position</font></td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>general</td>      <td>$score(s_{t},h_i) = s_t^\top W_ah_i$</td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>dot-product</td>      <td>$score(s_{t},h_i) = s_t^\top h_i$<br><font color="grey" size="2">note:当general attention的$W_a$为单位矩阵时，就退出为dot-product attention</font></td>      <td><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr>   <tr>      <td>scaled <br>dot-product(*)</td>      <td>$score(s_{t},h_i) = \frac{s_t^\top h_i}{\sqrt{n}}$<br><font color="grey" size="2">note:跟dot-product attention很像，n是encoder hidden state $h_i$的维度</font></td>      <td><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani2017</a></td>   </tr></table>(*)scaled dot-product attention机制添加了比例因子$/frac{1}{/sqrt{n}}$，动机是：对于softmax()函数，当输入很大时，对应的梯度很小（梯度逐渐消失），难以进行高效的优化和学习。因此，添加比例因子可以减小$score(s_t,h_i)$。<p>下表列出了更广范畴上的attention机制。</p><table class="table table-bordered table-striped table-condensed">   <tr>      <th width="25">名字</th>      <th>定义</th>      <th width="25">来源</th>   </tr>   <tr>      <td>self attention(&)</td>      <td><font color="grey" size="2">将input sequence的不同部分联系起来，只用到input sequence本身，而不用target sequence。<br>可以使用上表中的所有score function，只要将target sequence替换为input sequence即可。</font></td>      <td><a href="https://arxiv.org/pdf/1601.06733.pdf" target="_blank" rel="noopener">Cheng2016</a></td>   </tr>   <tr>      <td>global/soft attention</td>      <td><font color="grey" size="2">context vector是整个input sequence的加权和，注意到整个input sequence</font></td>      <td><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu2015</a></td>   </tr>   <tr>      <td>local/hard attention</td>      <td><font color="grey" size="2">context vector是局部input sequence的加权和，注意到局部input sequence</font></td>      <td><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu2015</a>，<br><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></td>   </tr></table>(&)self-attention在一些论文中也被称为intra-attention.<h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p>self-attention,最先在<a href="https://arxiv.org/pdf/1601.06733.pdf" target="_blank" rel="noopener">Cheng2016</a>提出称为”intra-attention”，后来在大作<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">attention is all you need</a>中发挥了更大的影响力。self-attention将同一个sequence的不同位置的tokens联系起来，建模tokens之间的关系，计算这个sequence的向量表示。[Cheng2016]提出self-attention的动机是什么呢？</p><p>我们先看以下LSTM的局限。LSTM在编码sequence的向量表示时，隐藏状态更新公式为：$$h_t = f(h_{t-1},x_t)$$从这个更新公式可以看到：在给定$h_t$的条件下，$h_{t+1}$与之前的状态$\lbrace{h_1,h_2,…,h_{t-1}}\rbrace$及之前的tokens $\lbrace{x_1,x_2,…,x_t}\rbrace$是条件独立的。LSTM的潜在假设是当前状态$h_t$包含了之前所有tokens的信息，这相当于假设LSTM有无限大的memory，这个假设实际上是不成立的。实际上LSTM会偏向于更近的tokens，而逐渐遗忘距离更远的tokens。另一方面，LSTM在编码token的隐藏状态时，没有建模tokens之间的关系。而这恰恰就是self-attention要解决的问题，也就是self-attention的核心思想：在计算sequence的向量表示时，引入tokens之间的关系。</p><p>接下来看self-attention的数学表示。对于sequence $\lbrace{x_1,x_2,…,x_n}\rbrace$，每个token $x_t$分别对应一个hidden vector 和memory vector。当前的memory tape $C_{t-1} = \lbrace{c_1,c_2,…,c_{t-1}}\rbrace$，hidden state tape为$H_{t-1} = \lbrace{h_1,h_2,…,h_{t-1}}\rbrace$。self-attention计算$x_t$与$\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$之间的关系：$$\beta_{t,i} = score(x_t,h_i) = v^\top tanh(W_hh_i,W_xx_t,W_{\tilde{h}}\tilde{h_{t-1}})$$ $$\alpha_{t,i} = softmax(\beta_{t,i})  ;i\in[1,t-1]$$</p><p>attention权重$\alpha_{t,i}$是t时间步x_t在之前的tokens $\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$对应的hidden vector上的概率分布。<div align="center"><img src="/images/46.png" width="60%" height="60%"></div></p><div align="center"><font color="grey" size="2">Fig.4.红色表示当前token，蓝色的深浅表示相关程度。<br>来源:[Cheng2016](https://arxiv.org/pdf/1601.06733.pdf)</font></div><p>比较一下self-attention机制与传统attention机制的区别：</p><ul><li>传统的attention机制是将target sequence与source sequence联系起来，attention权重$\lbrace{\alpha_{t,1},\alpha_{t,2},…,\alpha_{t,n}}\rbrace$是在encoder hidden states $\lbrace{h_1,h_2,…,h_n}\rbrace$上的概率分布。而self-attention是将同个sequence不同位置的tokens联系起来，attention权重$\alpha_{t,i}$是t时间步$x_t$在之前的tokens $\lbrace{x_1,x_2,…,x_{t-1}}\rbrace$对应的hidden vector上的概率分布。</li><li>传统的attention机制常与RNN联合使用，在transformer中self-attention可以与RNN解耦开（也就是分开使用），单独用self-attention也可以编码sequence的表示向量。</li></ul><h4 id="soft-vs-hard-attention"><a href="#soft-vs-hard-attention" class="headerlink" title="soft vs hard attention"></a>soft <em>vs</em> hard attention</h4><p><a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Show, Attend and Tell,Kelvin Xu2015</a>将attention机制用到了”给图片生成描述”的任务，第一次明确区分了hard attention与soft attention，区分的依据是attention是关注到整张图片，还是图片的局部。</p><ul><li>soft attention：attention关注到整张图片，或者是整个序列。alignment 权重$\alpha_{t,i}$是在整个序列上的概率分布。就像普通的attention一样。<ul><li>好处：模型是可微的。</li><li>坏处：计算量比较大。</li></ul></li><li>hard attention：attention关注到图片的局部，或者是序列的一部分。<ul><li>好处：减少了计算量。</li><li>坏处：模型不可微，需要用更复杂的技术，比如强化学习或者方差缩减来训练模型。<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a></li></ul></li></ul><h4 id="global-vs-local-attention"><a href="#global-vs-local-attention" class="headerlink" title="global vs local attention"></a>global <em>vs</em> local attention</h4><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>在NMT任务上提出了global 和local attention的概念。区分的依据是attention是关注到整个序列，还是关注到序列的一部分。</p><ul><li>global attention。 类似于soft attention，关注到整个序列。这里比较下<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>的global attention与<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a>中attention的区别。<ul><li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau2015</a>中attention的计算路径是：$s_{t-1} \to \alpha_{t} \to c_t \to s_t$<br>$$\beta_{t,i} = score(s_{t-1},h_i)$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i = 1}^{n}\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = g(y_{t-1},c_t,s_t)$$</li><li><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong2015</a>的global attention的计算路径是：$s_t \to \alpha_{t} \to c_t \to \tilde{s_t}$<br>$$\beta_{t,i} = score(s_{t},h_i)$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i = 1}^{n}\alpha_{t,i}h_i$$ $$RNN更新公式：s_t = f(s_{t-1},y_{t-1},c_t)$$ $$\tilde{s_t} = tanh(W_c[c_t,s_t])$$ $$y_t预测公式:p(y_t|y_{&lt; t},x) = softmax(W_s\tilde{s_t})$$</li></ul></li><li>local attention。是soft 与hard attention的结合，关注到序列的一部分。对hard attention进行改进，使得模型可微，训练和计算变得更容易。改进的方法如下：<ol><li>对于时间步t的target token $y_t$先用模型预测，生成一个对齐的位置$p_t$，</li><li>再根据固定窗口大小内$[p_t - D,p_t + D]$的encoder hidden state来计算context vector。D是窗口大小，是按经验定义好的。</li></ol></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/47.png" alt="Fig.5. global and local attenion.<br>来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)" title>                </div>                <div class="image-caption">Fig.5. global and local attenion.<br>来源：[Luong2015](https://arxiv.org/pdf/1508.04025.pdf)</div>            </figure><h3 id="pointer-network"><a href="#pointer-network" class="headerlink" title="pointer network"></a>pointer network</h3><p>对于输出序列的类别数依赖于输入序列的长度的问题，seq2seq模型或神经图灵机不能解决。因为这类问题中，输出的类别数是可变的，而seq2seq模型的decoder只能在固定数目的类别上生成一个概率分布。<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Vinyals2017</a>提出了pointer network（Pr_Net）来解决输出词表可变的问题。pointer network实际上是以attention为基础的。</p><p>我们比较下attention机制与pointer network的区别。<br>记输入序列$X = \lbrace{x_1,…,x_n}\rbrace$,输出序列$Y = {y_1,…,y_m}$，$y_j$是X的位置索引，$y_i \in [1,n] $。<br>encoder的所有hidden state为$\lbrace{h_1,h_2,…,h_n}\rbrace$，decoder在时间步t的隐藏状态为$s_t$，则：</p><ul><li>attention机制用alignment权重来计算context vector：<br> $$\beta_{t,i} = score(s_t,h_i) = v^\top tanh(W_ss_t,W_hh_i); i \in [1,n]$$ $$\alpha_{t,i} = softmax(\beta_{t,i})$$ $$c_t = \sum_{i=1}^{n}\alpha_{t,i}h_i$$</li><li>pointer network则用alignment权重在作为在输入序列上的概率分布，将输入序列中的token直接复制到输出序列中：<br> $$\beta_{t,i} = score(s_t,h_i) = v^\top tanh(W_ss_t,W_hh_i); i \in [1,n]$$ $$p(y_i|y_{&lt; i},X) = softmax(\beta_{t,i})$$<div align="center"><img src="/images/49.png" width="60%" height="60%"></div><div align="center"><font color="grey" size="2">Fig.6.Pointer Network model<br>来源:[Vinyals2017](https://arxiv.org/abs/1506.03134)</font></div></li></ul><h4 id="pointer-network解决OOV问题"><a href="#pointer-network解决OOV问题" class="headerlink" title="pointer network解决OOV问题"></a>pointer network解决OOV问题</h4><p>什么是OOV（out of vocabulary）问题？在序列（source sequence）到序列（target sequence）的映射问题（对话系统，问答系统）中，会根据训练集语料来构建词表，根据完成$word \to index \to embedding$的向量化表示。而在测试集的source sequence中难免会出现一些词表中没有的词，通常会将这些out of vocabulary的词映射到一个特定的字符”UNK”，而decoder在生成response时也可能生成”UNK”这个特殊字符。这就是OOV问题。</p><p>pointer network是解决OOV问题的有效方法。当source sequence中出现不在词表中的词时，pointer network可以直接将这个生词从输入序列复制到输出序列中。<a href="https://arxiv.org/abs/1704.04368" target="_blank" rel="noopener">Abigail See2017</a>就用了pointer network来解决OOV问题。</p><p>记时间步t decoder的隐藏状态为$s_t$,对应的context vector为$c_t$,alignment权重为$\alpha_{t,i},i \in [1,n]$</p><ol><li>在词汇表上的概率分布为:$p_{vocab} = softmax(W[s_t,c_t] + b)$</li><li>在输入序列的概率分布为:$p_{ptr} = \alpha_{t,i} = softmax(\beta_{t_i})$</li><li>选择开关为: $p_{gen} = sigmoid(W_ss_t + W_cc_t + W_xx_t + b)$<br>为逻辑回归，取值为[0,1]</li><li>最终在extend vocabulary上的概率分布为:$p(w) = p_{gen}p_{vocab} + (1-p_{gen})p_{ptr}$.<br>当$p_{gen}$为1时，从词汇表中生成word；当$p_{gen}$为0时，将输入序列的词复制到输出序列中。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/48.png" alt="Fig.7.Pointer-generator model.<br>来源：[Abigail See2017](https://arxiv.org/abs/1704.04368)" title>                </div>                <div class="image-caption">Fig.7.Pointer-generator model.<br>来源：[Abigail See2017](https://arxiv.org/abs/1704.04368)</div>            </figure> <p>类似的论文还有：<a href="https://arxiv.org/pdf/1603.06393v3.pdf" target="_blank" rel="noopener">CopyNet,Jiatao Gu2016</a></p><h3 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h3><p><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">attention is all you need!</a>(Vaswani, et al., 2017)提出了transformer。transformer也是基于encoder-decoder框架的，也可以看作是一个seq2seq模型。但不同于encoder和decoder都采用RNN的seq2seq模型，transformer完全依赖self-attention机制来计算input和output的向量表示，而不使用RNN或CNN。一般来说，attention机制是与RNN联合使用的，transformer把attention和RNN解耦开了，只使用attention机制。</p><p>为什么transformer用self-attention来编码input和output，而不使用RNN呢？<br>一方面，由RNN的更新公式$s_t = f(s_{t-1},x_t)$可以看到，RNN处理序列时是串行计算的，尤其是处理长序列时更费时间。不利于并行化，计算效率低。而transformer采用attention来编码计算向量，可以进行并行化计算，提高计算效率。<br>另一方面，RNN在编码长序列时，随着距离的增大，往往会偏向于最近的部分，而学习不到长期依赖。但self-attention机制不受距离的限制，可以有效地学习到长期依赖。</p><h4 id="scaled-dot-product-attention与key-value-query"><a href="#scaled-dot-product-attention与key-value-query" class="headerlink" title="scaled dot-product attention与key,value,query"></a>scaled dot-product attention与key,value,query</h4><p>transformer的主要组件是<em>multi-heads self-attenion mechanism</em>，这个组件用到了scaled dot-product attention机制。一般地，attention机制将query和(key,value)映射为output，其中query,key,value,output都是vector。output是所有values的加权和，权重是通过计算query与对应key之间的关联度得到。</p><p>具体来说，将什么作为key,value,query？分两种情况，从框图可以直观的看到：</p><ul><li>在encoder-decoder框架中，联系encoder与decoder的attention机制通常将encoder的所有hidden states乘以两个不同的矩阵$W^Q,W^K$分别作为<em>keys</em>和<em>values</em>。将decoder上一个时间步的hidden state作为query。</li><li>在encoder模块，self-attention机制将input词级别的向量表示乘以三个不同的矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算input总的句子级别的向量表示。同样地，在decoder模块中self-attention机制将output词级别的向量表示分别乘以三个不同矩阵$W^Q,W^K,W^V$分别作为query,key,value。进而计算output总的句子级别的向量表示。</li></ul><p>有了具体的key,value,query后，scaled dot-product attention机制怎么来计算output呢？<br>记query,key,value的矩阵形式分别为Q,K,V。query和key维度为$d_k$,value维度为$d_v$。则output的计算方式为：$$Attention(Q,K,V) = softmax(\frac{QK^\top}{\sqrt{d_k}})V$$</p><p>多种attention机制中，transformer为什么选择采用scale dot-product attention机制呢？<br>最常用的两种attention机制是dot-product和additive attention机制。dot-product attention用点乘来做打分函数，additive attenion将有一层隐藏层的前馈网络作为打分函数。理论上来说，这两种attention的计算复杂度是一样的；但实际上，dot-product attention计算更快，占用内存更小。因为dot-product attention机制可以采用高度优化的矩阵乘法代码。<br>当维度$d_k$较小时，这两种attention机制的效果是差不多的。当维度$d_k$更大时，additive attention的效果要好于dot-product attention。这可能是因为当维度$d_k$变大时，点乘的值变得过大，而softmax()函数在值过大的范围梯度是很小的，类似于梯度消失问题。因此，添加比例因子$\frac{1}{\sqrt{d_k}}$来减小点乘的值。</p><h4 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h4><p>并不是只用一次attention机制，将维度为$d_{model}$的key,value,query映射为output。而是将query,key,value映射到维度为$d_k,d_k,d_v$不同的子向量空间，并行的计算$h$次，分别得到output做concat操作，得到总的output。$h$为head的个数，也就是并行attention layer的层数。有关系$d_{model} = d_v \cdot h$，论文中采用$d_{model} =512,h=8,d_k = d_v = \frac{d_{model}}{h} = 64$ $$MultiHead(Q,K,V) = concat(head_1,head_2,…,head_h)W^O$$ $$where \quad head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$ 其中$W_i^Q \in R^{d_{model} \cdot d_k},W_i^K \in R^{d_{model} \cdot d_k},W_i^V \in R^{d_{model} \cdot d_v},W^O \in R^{hd_{v} \cdot d_{model}}$</p><div align="center"><img src="/images/multi-head-attention.png" width="40%" height="40%"></div><div align="center"><font color="grey" size="2">Fig.7.multi-head scaled dot-product attention</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h4><p>encoder将input text编码为基于attention的包含位置信息的向量表示。</p><ul><li>由6个完全相同的层堆叠起来。</li><li>每一层包含两个子层。第一子层是multi-head attention层，第二层是一个简单的全连接层。</li><li>两个子层之间采用<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">残差连接</a>，并进行归一化。这样所有子层的输出都有相同的维度$d_{model} = 512$</li></ul><div align="center"><img src="/images/transformer-encoder.png" width="70%" height="70%"></div><div align="center"><font color="grey" size="2">Fig.9. transformer encoder</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h4><p>从encoder output得到总的context vector，并据此生成response。</p><ul><li>与encoder相同，由6个完全相同的层堆叠起来。</li><li>每一层除了encoder中的两个子层外，还插入了一个multi-head layer来在所有encoder output上进行attention操作。</li><li>两个子层之间采用<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">残差连接</a>，并进行归一化。</li><li>第一个multi-head attention sub-layer进行mask操作，mask掉output当前时间步后所有的tokens。防止attention机制看到未来的信息。</li></ul><div align="center"><img src="/images/transformer-encoder.png" width="70%" height="70%"></div><div align="center"><font color="grey" size="2">Fig.10. transformer decoder</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h4 id="transformer的总体结构"><a href="#transformer的总体结构" class="headerlink" title="transformer的总体结构"></a>transformer的总体结构</h4><ul><li>input和output都先经过一个embedding layer得到各自的向量表示，维度为$d_{model} = 512$</li><li>由于self-attention不能像RNN一样自动地编码位置信息，因此需要额外地将位置信息添加到输入。</li><li>在最后decoder的输出外接一个线性层和softmax层。</li></ul><div align="center"><img src="/images/transformer.png" width="150%" height="150%"></div><div align="center"><font color="grey" size="2">Fig.11. transformer的整体框架</font><br><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener"><font color="grey" size="2">来源:Vaswani, et al., 2017</font></a></div><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" target="_blank" rel="noopener">Attention? Attention!</a>  by Weng, Lilian</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;读了博主&lt;a href=&quot;https://lilianweng.github.io/lil-log/contact.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Weng, Lilian&lt;/a&gt;的文章&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;attention? attention!&lt;/a&gt;，是一篇很好的文章。打算按照这篇文章的思路，进行翻译，并添加自己的理解。&lt;br&gt;attention机制在深度学习中被广为使用，本文介绍attention机制的提出，不同的attention机制，及attention机制的进一步探索和应用。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="rnn" scheme="http://yoursite.com/tags/rnn/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>pip安装python模块报错</title>
    <link href="http://yoursite.com/2019/07/12/pip%E5%AE%89%E8%A3%85python%E6%A8%A1%E5%9D%97%E6%8A%A5%E9%94%99/"/>
    <id>http://yoursite.com/2019/07/12/pip安装python模块报错/</id>
    <published>2019-07-12T01:51:58.000Z</published>
    <updated>2019-07-12T02:11:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>在使用<code>pip install</code>命令安装python模块时，报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot uninstall &apos;PyYAML&apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h3><p>报错信息告诉我们：“不能卸载‘pyyaml’模块，因为这个模块是<code>distutils</code>方式安装的，不能确定哪些文件属于这个模块，因此不能完整地卸载这个模块。”</p><p><a href="https://cloud.tencent.com/developer/section/1371690" target="_blank" rel="noopener">distutils</a>是python最初的模块安装和分发系统，distutils不会保留哪些文件属于哪个安装包的信息，甚至不会保留安装包之间的依赖关系。直接使用<code>distutils</code>的方式已经被淘汰，取而代之的是<a href="https://setuptools.readthedocs.io/en/latest/" target="_blank" rel="noopener">setuptools</a>.<br>    所谓模块的分发，就是开发者打包并发布自己的模块，供其他人使用。</p><p>这样我们就知道了，因为<code>pyyaml</code>模块时通过<code>distutils</code>方式安装的，因此不能明确文件与包之间的隶属关系，不能正确卸载。</p><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>使用下面的命令忽略已安装的模块，强制安装和更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install &lt;package-name&gt; --ignore-installed &lt;pyyaml&gt; --upgrade</span><br></pre></td></tr></table></figure><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.jianshu.com/p/94caf01dd9a6" target="_blank" rel="noopener">强制安装和更新</a></li><li><a href="https://cloud.tencent.com/developer/ask/196670" target="_blank" rel="noopener">如何在Windows操作系统中升级/卸载distutils软件包（PyYAML）？</a></li><li><a href="https://docs.python.org/zh-cn/3/installing/index.html" target="_blank" rel="noopener">python官方手册-安装python模块</a></li><li><a href="https://cloud.tencent.com/developer/section/1371690" target="_blank" rel="noopener">setuptools与distutils</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在使用&lt;code&gt;pip install&lt;/code&gt;命令安装python模块时，报错：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Cannot uninstall &amp;apos;PyYAML&amp;apos;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>NAACL2019-对话系统</title>
    <link href="http://yoursite.com/2019/07/10/NAACL2019-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2019/07/10/NAACL2019-对话系统/</id>
    <published>2019-07-10T12:55:17.000Z</published>
    <updated>2019-07-21T15:03:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。<br>NAACL2019的会议列表链接：<a href="https://naacl2019.org/program/accepted/" target="_blank" rel="noopener">https://naacl2019.org/program/accepted/</a></p><a id="more"></a><h3 id="《Evaluating-Coherence-in-Dialogue-Systems-using-Entailment》"><a href="#《Evaluating-Coherence-in-Dialogue-Systems-using-Entailment》" class="headerlink" title="《Evaluating Coherence in Dialogue Systems using Entailment》"></a>《Evaluating Coherence in Dialogue Systems using Entailment》</h3><p>【链接】<a href="https://arxiv.org/abs/1904.03371" target="_blank" rel="noopener">https://arxiv.org/abs/1904.03371</a><br>【代码】<a href="https://github.com/nouhadziri/DialogEntailment" target="_blank" rel="noopener">https://github.com/nouhadziri/DialogEntailment</a></p><p>加拿大阿尔伯塔大学发表的论文。论文提出了一种评估对话系统生成回复好坏的指标。<br>这篇论文的想法来源于：发表在ACL2019上的论文<a href="https://arxiv.org/abs/1811.00671" target="_blank" rel="noopener">《Dialogue Natural Language Inference》</a>提出利用NLI(natural language inference)任务来提高对话系统生成回复的一致性。<br>本文的作者则想到用NLI任务来评估对话系统生成回复的好坏。具体地，论文用了BERT<a href="https://arxiv.org/abs/1609.06038" target="_blank" rel="noopener">[Devlin et al., 2018]</a>和The Enhanced Sequential Inference Model(ESIM)<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">[Chen et al., 2016]</a> 这两种方法来训练NLI模型。另外论文还公开了一个用于NLI任务的数据集。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录NAACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;NAACL2019的会议列表链接：&lt;a href=&quot;https://naacl2019.org/program/accepted/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://naacl2019.org/program/accepted/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
      <category term="NAACL2019" scheme="http://yoursite.com/tags/NAACL2019/"/>
    
  </entry>
  
  <entry>
    <title>ACL2019-对话系统</title>
    <link href="http://yoursite.com/2019/07/07/ACL2019-%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2019/07/07/ACL2019-对话系统/</id>
    <published>2019-07-07T11:19:27.000Z</published>
    <updated>2019-07-31T03:40:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。<br>ACL2019的会议列表链接：<a href="http://www.acl2019.org/EN/program.xhtml" target="_blank" rel="noopener">http://www.acl2019.org/EN/program.xhtml</a></p><a id="more"></a><h3 id="《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》"><a href="#《Memory-Consolidation-for-Contextual-Spoken-Language-Understanding-with-Dialogue-Logistic-Inference》" class="headerlink" title="《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》"></a>《Memory Consolidation for Contextual Spoken Language Understanding with Dialogue Logistic Inference》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.01788" target="_blank" rel="noopener">https://arxiv.org/abs/1906.01788</a><br>【源码】：无</p><p>中科院自动化所发表的短论文。在多轮对话中，对话历史（context information）对回复（response）的生成有重要作用。任务型对话中的管道模型分为4个模块：NLU、对话状态追踪、对话策略学习 及NLG。对话状态追踪又包含任务：domain classification、intent detection和slot filling。domain classification和intent detection任务当做分类任务来处理，常采用SVM或深度神经网络的方法；slot filling任务被当做序列标注任务来处理，常采用BiLSTM+CRF模型。NLU能否充分利用context information，对这三个下游任务有很大影响。<br>为了更好的利用context information，本文提出了对话逻辑推断任务（DLI,dialog logic inference），任务定义为：将打乱顺序的多轮对话重新排序；输入之前的对话，从剩余的utterance candidates中选中下一句对话。NLU任务采用了所谓的memory network，其实就是采用多个encoder对context information进行编码，再用attention机制或别的方法得到context information总的向量化表示。本文联合训练DLI任务和NLU任务，通过两个任务共享encoder和memory retrieve模块，来让NLU任务更好地利用context information。其实是得到context information更合理的向量化表示，来作为下游domain classification、intent detection和slot filling任务的输入。</p><p>论文提出的将打乱顺序的对话重新排序的DLI任务，可以进一步深入，将句子切分为几段打乱顺序再重新排序；可以应用到闲聊式对话系统中。</p><h3 id="《Dialogue-Natural-Language-Inference》"><a href="#《Dialogue-Natural-Language-Inference》" class="headerlink" title="《Dialogue Natural Language Inference》"></a>《Dialogue Natural Language Inference》</h3><p>【链接】：<a href="https://arxiv.org/abs/1811.00671" target="_blank" rel="noopener">https://arxiv.org/abs/1811.00671</a><br>【代码】：无<br>【数据集】：<a href="https://wellecks.github.io/dialogue_nli/" target="_blank" rel="noopener">https://wellecks.github.io/dialogue_nli/</a></p><p>加利福尼亚大学、Facebook AI Lab发表的论文。核心是提出用NLI(natural language inference)任务来提高persona-based dialog system的一致性。这里就要先搞清楚NLI任务和一致性问题两个概念。</p><ul><li><p>先从问题出发，所谓对话的一致性问题。可以分为两类：</p><ul><li><p>logical contradiction，逻辑矛盾。比如同一个人的两句话:”我有一只狗”，”我没养过狗”。就是逻辑矛盾的。</p></li><li><p>比较模糊的非逻辑矛盾。同一个人不可能说出的两句话：“我从来不运动”，“我去篮球了”。就是这种非逻辑矛盾。真香警告。</p><p>至于persona一致性问题，就是回复的utterance不能与说话人的persona矛盾，也不能与之前的回复有矛盾。</p></li></ul></li><li><p>具体介绍NLI任务。这其实是一个分类问题。论文公开了一个自己标注的NLI数据集。</p><ul><li>训练阶段：训练集形式是 {$（s_1,s_2）$,label }，对应labels $\in$（一致、无关、矛盾）。</li><li>在test阶段，给定一个句子对（句子1，句子2）来判断对应的label。</li></ul></li></ul><p>论文的最终目的是通过NLI任务训练的模型来提高persona dialog system的一致性。这是如何来实现的呢？对于一个dialog system，给定对话历史$（u_1,u_2,…,u_t）$ 及说话人的persona文本描述$（p_1,p_2,…,p_n）$,从response candidates$（y_1,y_2,…,y_m）$中选择一个$u_{t+1}$（如何生成多个responses不是这篇论文要解决的）。<br>用NLI任务的模型来预测$(y_i,u_j),(y_i,p_k)其中：i\in [1,m],j\in [1,t],k \in [1,n]$对应的label，如果句子之间是矛盾的，则添加惩罚项。从而得到一致性最好的utterance作为response。</p><h3 id="《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》"><a href="#《ReCoSa-Detecting-the-Relevant-Contexts-with-Self-Attention-for-Multi-turn-Dialogue-Generation》" class="headerlink" title="《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》"></a>《ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation》</h3><p>【链接】：<a href="https://arxiv.org/abs/1907.05339" target="_blank" rel="noopener">https://arxiv.org/abs/1907.05339</a><br>【数据集】：<a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" target="_blank" rel="noopener">English Ubuntu dialogue corpus</a><br>【代码】：<a href="https://github.com/zhanghainan/ReCoSa" target="_blank" rel="noopener">https://github.com/zhanghainan/ReCoSa</a></p><p>中科院发表的论文。<br>在多轮对话中，生成response时，对话历史中最相关的部分起着重要的作用。论文要解决的问题：如何更准确地找到并利用relevant context来生成response。<br>多轮对话中广泛使用的HRED模型,[<a href="https://arxiv.org/abs/1507.04808" target="_blank" rel="noopener">(Serban et al.,2016;</a>,<a href="https://arxiv.org/abs/1507.02221" target="_blank" rel="noopener">Sordoni et al., 2015</a>]无差别地利用context information，忽略了relevant context。虽然有利用relevant context的相关工作，但这些工作都有各自的问题。[<a href="https://www.aclweb.org/anthology/P17-2036" target="_blank" rel="noopener">Tian et al., 2017</a>]提出计算context 与post之间的cosine similarity来衡量context relevance，其假设是context与response之间的relevance等价于post与response之间的relevance，这个假设是站不住脚的。[<a href>Xing et al., 2018</a>]向HRED模型引入了attention机制，但attention机制定位relevant context时会产生偏差，因为基于RNN的attention机制倾向于最靠近的context（close context）。论文提出了自己的解决办法，用self-attention机制来衡量context于response之间的relevance。self-attention机制的优点是可以有效捕捉到长距离的依赖关系。</p><p>模型分为三个部分：<br>context包含N轮对话： ${s_1,s_2,…,s_N}$其中，$s_i = {x_1,x_2,…,x_M}$，M为句子长度。<br>response为$Y = {y_1,y_2,…,y_M}$</p><ol><li><strong>context representation encoder</strong>：<br> 将context encode为vector。<ol><li>word-level encoder：<br> 用LSTM对sentence编码，将LSTM最后一个时间步的hidden state作为sentence representation: $h^{s_i}$；<br> 由于self-attention机制不能区分word位置信息，还需要添加position embedding: $p^{s_i}$,<br> 把两个向量做concatenate操作，得到总得sentence representation:$(h^{s_i},p^{s_i})$。<br> 对于context中的N个句子有${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$</li><li>context self-attention:<br> 采用multi-head self-attention机制，将${(h^{s_1},p^{s_1}),…,(h^{s_N},p^{s_N})}$经过不同的线性变换作为query、keys、values matrix,由N个sentence representation得到总的context representation $O_s$。</li></ol></li><li><strong>response representation encoder</strong><br> 同样用multi-head self-attention机制,将response的word embedding及position embedding ${(w_1,p_1),…,(w_{t-1},p_{t-1})}$经过不同的线性变换作为query、keys、values matrix，得到response representation $O_r$。<ol><li>在train阶段<br> 采用mask操作，在时间步t对于word $y_t$，mask掉${y_t,y_{t+1},…,y_M}$，只保留${y_1,y_2,…,y_{t-1}}$来计算response representation。</li><li>在infer阶段<br> 在生成response的时间步t，将生成的response ${g_1,…,g_{t-1}}$，来作为response representation。</li></ol></li><li><strong>context-response attention decoder</strong><br> 采用multi-head self-attention机制，将context attention representation $O_s$作为keys、values matrix，将response hidden representation $O_r$作为query matrix。得到输出$O_d$. <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/42.png" alt="模型框架图" title>                </div>                <div class="image-caption">模型框架图</div>            </figure></li></ol><h3 id="《Persuasion-for-Good-Towards-a-Personalized-Persuasive-Dialogue-System-for-Social-Good》"><a href="#《Persuasion-for-Good-Towards-a-Personalized-Persuasive-Dialogue-System-for-Social-Good》" class="headerlink" title="《Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good》"></a>《Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.06725" target="_blank" rel="noopener">https://arxiv.org/abs/1906.06725</a><br>【代码、数据集】：<a href="https://gitlab.com/ucdavisnlp/persuasionforgood/tree/master" target="_blank" rel="noopener">https://gitlab.com/ucdavisnlp/persuasionforgood/tree/master</a></p><p>浙江大学、加利福尼亚大学发表。获得ACL2019 best paper提名。<br>论文的主要贡献是公开了一个包含说话人个人信息的劝说数据集，在子集上标注了十种不同的劝说策略。并训练了用于分类不同劝说策略的分类器。</p><h3 id="《Do-Neural-Dialog-Systems-Use-the-Conversation-History-Effectively-An-Empirical-Study》"><a href="#《Do-Neural-Dialog-Systems-Use-the-Conversation-History-Effectively-An-Empirical-Study》" class="headerlink" title="《Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study》"></a>《Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study》</h3><p>【链接】：<a href="https://arxiv.org/abs/1906.01603" target="_blank" rel="noopener">https://arxiv.org/abs/1906.01603</a><br>【代码】：<a href="https://github.com/chinnadhurai/ParlAI/" target="_blank" rel="noopener">https://github.com/chinnadhurai/ParlAI/</a></p><p>论文获得ACL2019 best short paper提名。<br>论文的研究点是：生成式对话系统是否有效利用或正确理解了对话历史？论文通过向对话历史中引入不同类型的扰动，来研究生成式对话系统生成回复的困惑度变化。这个方法的一个前提是如果生成式对话系统对某种信息的扰动不敏感，那么它没有有效利用这种信息。</p><p>论文在比较了三种模型。</p><ul><li>基于LSTM的seq2seq模型。</li><li>基于LSTM的seq2seq模型 + attention机制。</li><li>基于transformer的seq2seq模型。</li></ul><p>论文在四个多轮对话数据集上进行实验。</p><ul><li>bAbI dialog。<a href="https://arxiv.org/abs/1605.07683" target="_blank" rel="noopener">(Bordes and Weston, 2016)</a></li><li>Persona Chat。<a href="https://www.aclweb.org/anthology/papers/P/P18/P18-1205/" target="_blank" rel="noopener">(Zhang et al., 2018)</a></li><li>Dailydialog。 <a href="https://arxiv.org/abs/1710.03957" target="_blank" rel="noopener">(Li et al., 2017)</a></li><li>MutualFriends。<a href="https://arxiv.org/abs/1704.07130" target="_blank" rel="noopener">(He et al., 2017)</a></li></ul><p>论文向对话历史引入了不同的扰动。</p><ul><li>句子级别的扰动：<ul><li>随机打乱对话历史中句子的顺序。</li><li>倒序对话历史中句子的顺序。</li><li>随机去掉对话历史中特定的句子。</li><li>对话历史中有n个句子，只保留最近的k个句子$(k \le n)$。</li></ul></li><li>词级别的扰动：<ul><li>随机打乱一个句子中词的顺序。</li><li>倒序一个句子中词的顺序。</li><li>随机去掉对话历史中30%的词。</li><li>去掉所有的名词。</li><li>去掉所有的动词。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录ACL2019对话系统相关的论文阅读笔记。包括任务型和闲聊式对话系统，对论文的思路和模型做简单介绍，值得反复精读的论文会单独开一篇博文来写。&lt;br&gt;ACL2019的会议列表链接：&lt;a href=&quot;http://www.acl2019.org/EN/program.xhtml&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.acl2019.org/EN/program.xhtml&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="ACL2019" scheme="http://yoursite.com/tags/ACL2019/"/>
    
      <category term="dialog system" scheme="http://yoursite.com/tags/dialog-system/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理---会议列表</title>
    <link href="http://yoursite.com/2019/04/24/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-%E4%BC%9A%E8%AE%AE%E5%88%97%E8%A1%A8/"/>
    <id>http://yoursite.com/2019/04/24/自然语言处理-会议列表/</id>
    <published>2019-04-24T06:55:43.000Z</published>
    <updated>2019-07-07T09:00:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录自然语言处理方向的国际会议列表。</p><a id="more"></a><h2 id="A类会议"><a href="#A类会议" class="headerlink" title="A类会议"></a>A类会议</h2><p><a href="http://www.aaai.org/" target="_blank" rel="noopener">AAAI</a>，Association for the Advancement of Artificial Intelligence</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><a href="https://iclr.cc/" target="_blank" rel="noopener">ICLR</a>，The International Conference on Learning Representations，国际学习表征会议<br>2013年才刚刚成立了第一届。这个一年一度的会议已经被学术研究者们广泛认可，被认为「深度学习的顶级会议」。<br>这个会议的来头不小，由位列深度学习三大巨头之二的 Yoshua Bengio 和 Yann LeCun 牵头创办。Yoshua Bengio 是蒙特利尔大学教授，深度学习三巨头之一，他领导蒙特利尔大学的人工智能实验室（MILA）进行 AI 技术的学术研究。MILA 是世界上最大的人工智能研究中心之一，与谷歌也有着密切的合作。而 Yann LeCun 就自不用提，同为深度学习三巨头之一的他现任 Facebook 人工智能研究院（FAIR）院长、纽约大学教授。作为卷积神经网络之父，他为深度学习的发展和创新作出了重要贡献。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://www.ccf.org.cn/xspj/gyml/" target="_blank" rel="noopener">中国计算机学会推荐国际学术会议和期刊目录</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录自然语言处理方向的国际会议列表。&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="会议列表" scheme="http://yoursite.com/tags/%E4%BC%9A%E8%AE%AE%E5%88%97%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>python读写csv文件</title>
    <link href="http://yoursite.com/2019/04/19/python%E8%AF%BB%E5%86%99csv%E6%96%87%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/04/19/python读写csv文件/</id>
    <published>2019-04-19T10:32:22.000Z</published>
    <updated>2019-07-21T15:22:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍csv文件的读写。</p><a id="more"></a><h2 id="csv模块"><a href="#csv模块" class="headerlink" title="csv模块"></a>csv模块</h2><h3 id="csv-writer-csvfile"><a href="#csv-writer-csvfile" class="headerlink" title="csv.writer(csvfile)"></a>csv.writer(csvfile)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import csv </span><br><span class="line">row = [&apos;Symbol&apos;,&apos;Price&apos;,&apos;Date&apos;,&apos;Time&apos;,&apos;Change&apos;,&apos;Volume&apos;]</span><br><span class="line">rows = [(&apos;AA&apos;, 39.48, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.18, 181800),</span><br><span class="line">         (&apos;AIG&apos;, 71.38, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.15, 195500),</span><br><span class="line">         (&apos;AXP&apos;, 62.58, &apos;6/11/2007&apos;, &apos;9:36am&apos;, -0.46, 935000),</span><br><span class="line">       ]</span><br><span class="line">with open(&apos;name.csv&apos;,&apos;w&apos;) as csvfile:</span><br><span class="line">    writer = csv.writer(csvfile,delimiter = &apos;\t&apos;,lineterminator = &apos;\n&apos;) #delimiter和lineterminator分别是分隔符，行结束符</span><br><span class="line">    writer.writerow(row) #写入单行</span><br><span class="line">    writer.writerows(rows) #写入多行</span><br></pre></td></tr></table></figure><h3 id="csv-reader-csvfile"><a href="#csv-reader-csvfile" class="headerlink" title="csv.reader(csvfile)"></a>csv.reader(csvfile)</h3><p>该函数接收一个可迭代对象，返回对象<code>reader</code>是一个生成器，不能直接用下标访问。可以用for循环和next()函数访问。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile:</span><br><span class="line">    reader = csv.reader(csvfile,delimiter = &apos;\t&apos;) #迭代器</span><br><span class="line">    rows = [row for row in reader] #用for循环访问：</span><br><span class="line">for row in rows:</span><br><span class="line">    print(row)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/36.png" alt title>                </div>                <div class="image-caption"></div>            </figure>如果要读取csv文件的某列，可以看下面的例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;name.csv&apos;,&apos;r&apos;) as csvfile:</span><br><span class="line">    reader = csv.reader(csvfile,delimiter = &apos;\t&apos;) #迭代器</span><br><span class="line">    column = [row[2] for row in reader] #用for循环访问：</span><br><span class="line">print(column)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/37.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h3 id="csv-DictReader-csvfile"><a href="#csv-DictReader-csvfile" class="headerlink" title="csv.DictReader(csvfile)"></a>csv.DictReader(csvfile)</h3><p>与csv.reader()函数相同，接收一个可迭代对象，返回一个生成器。不同之处是，返回的每个单元格放在字典的值中，字典的键就是这个单元格的列头。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./name.csv&apos;,&apos;r&apos;) as f:</span><br><span class="line">    reader = csv.DictReader(f,delimiter = &apos;\t&apos;)</span><br><span class="line">    rows = [row for row in reader]</span><br><span class="line">for row in rows:</span><br><span class="line">    print(rows)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/38.png" alt title>                </div>                <div class="image-caption"></div>            </figure>如果要读取csv文件的某列，可以看下面的例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./name.csv&apos;,&apos;r&apos;) as f:</span><br><span class="line">    reader = csv.DictReader(f,delimiter = &apos;\t&apos;)</span><br><span class="line">    column = [row[&apos;Time&apos;] for row in reader]</span><br><span class="line">print(column)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/39.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h3 id="csv-DictWriter-csvfile"><a href="#csv-DictWriter-csvfile" class="headerlink" title="csv.DictWriter(csvfile)"></a>csv.DictWriter(csvfile)</h3><h2 id="pandas读写csv"><a href="#pandas读写csv" class="headerlink" title="pandas读写csv"></a>pandas读写csv</h2><p>也可以直接用pandas的函数<code>read_csv()</code>来读取csv文件的列。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">f = pd.read_csv(&apos;name.csv&apos;,delimiter = &apos;\t&apos;)</span><br><span class="line">time = f.Time</span><br><span class="line">print(time)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/40.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://docs.python.org/zh-cn/3/library/csv.html" target="_blank" rel="noopener">python官方文档-csv模块</a></li><li><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p01_read_write_csv_data.html" target="_blank" rel="noopener">读写csv数据</a></li><li><a href="https://blog.csdn.net/Allyli0022/article/details/79125672" target="_blank" rel="noopener">使用python获取csv文本的某行或某列数据</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍csv文件的读写。&lt;/p&gt;
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="csv" scheme="http://yoursite.com/tags/csv/"/>
    
      <category term="panda" scheme="http://yoursite.com/tags/panda/"/>
    
  </entry>
  
  <entry>
    <title>torch.cuda.is_available()返回False,但nvidia-smi正常</title>
    <link href="http://yoursite.com/2019/04/16/torch-cuda-is-available-%E8%BF%94%E5%9B%9EFalse-%E4%BD%86nvidia-smi%E6%AD%A3%E5%B8%B8/"/>
    <id>http://yoursite.com/2019/04/16/torch-cuda-is-available-返回False-但nvidia-smi正常/</id>
    <published>2019-04-16T09:04:44.000Z</published>
    <updated>2019-07-22T01:27:54.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>torch.cuda.is_available()</code>返回False,但nvidia-smi可以正常运行。</p><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在pytorch用GPU来加速计算时发现。<code>torch.cuda.is_available()</code>返回False,但nvidia-smi可以正常运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; torch.cuda.is_available()</span><br><span class="line">False</span><br></pre></td></tr></table></figure><p>此时，<code>nvidia-smi</code>可以正常运行。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/34.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h2 id="可能原因"><a href="#可能原因" class="headerlink" title="可能原因"></a>可能原因</h2><p>在<code>nvidia-smi</code>的运行结果中可以看到，driver version是<code>390.xx</code>。可能是driver version版本太低，造成了这个问题，实际上也是如此。<br>driver version的常见版本是<code>384.xx</code>,<code>390.xx</code>,<code>396.xx</code>。接下来，把driver version升级到<code>396.xx</code>看能不能解决问题。</p><h2 id="升级nvidia-driver-version"><a href="#升级nvidia-driver-version" class="headerlink" title="升级nvidia driver version"></a>升级nvidia driver version</h2><ol><li><p>卸载旧版本的NVIDIA driver </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia-\*</span><br></pre></td></tr></table></figure></li><li><p>添加NVIDIA的ppa源.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br></pre></td></tr></table></figure></li><li><p>安装新版本的NVIDIA driver </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install nvidia-driver-396</span><br></pre></td></tr></table></figure></li></ol><p>此时，运行<code>nvidia-smi</code>，会报以下错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to initialize NVML: Driver/library version mismatch</span><br></pre></td></tr></table></figure><p>这是更新NVIDIA driver版本后的常见问题。这个问题出现的原因是kernel mod的NVIDIA driver版本没有更新。<br>执行以下命令可以查看nvidia kernel mod的version。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/driver/nvidia/version</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/32.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>可以看到已经加载的nvidia kernel mod的版本是还是旧版本<code>390.xx</code>。一般情况下，重启服务器就能解决问题。<br>如果由于某些原因不能重启，可以重新加载kernel mod。思路是先unload kernel mod，再reload kernel mod. 详见<a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a><br><code>nvidia-smi</code>可以正常运行后，问题就解决了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; torch.cuda.is_available()</span><br><span class="line">True</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://discuss.pytorch.org/t/torch-cuda-is-available-returns-false-nvidia-smi-is-working/20614" target="_blank" rel="noopener">Torch.cuda.is_available() returns False, nvidia-smi is working</a></li><li><a href="https://askubuntu.com/questions/1063871/how-can-i-update-the-nvidia-drivers-to-version-390-77" target="_blank" rel="noopener">How can I update the NVIDIA drivers to version 390.77?</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;torch.cuda.is_available()&lt;/code&gt;返回False,但nvidia-smi可以正常运行。&lt;/p&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="nvidia-smi" scheme="http://yoursite.com/tags/nvidia-smi/"/>
    
      <category term="GPU" scheme="http://yoursite.com/tags/GPU/"/>
    
      <category term="cuda" scheme="http://yoursite.com/tags/cuda/"/>
    
  </entry>
  
  <entry>
    <title>nvidia-smi返回错误信息‘Failed to initialize NVML: Driver/library version mismatch’</title>
    <link href="http://yoursite.com/2019/03/29/nvidia-smi%E8%BF%94%E5%9B%9E%E9%94%99%E8%AF%AF%E4%BF%A1%E6%81%AF%E2%80%98Failed-to-initialize-NVML-Driver-library-version-mismatch%E2%80%99/"/>
    <id>http://yoursite.com/2019/03/29/nvidia-smi返回错误信息‘Failed-to-initialize-NVML-Driver-library-version-mismatch’/</id>
    <published>2019-03-29T11:47:03.000Z</published>
    <updated>2019-07-21T15:23:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>Ubuntu运行命令<code>nvidia-smi</code>出错。</p><a id="more"></a> <h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在Ubuntu18.04的命令行中运行命令<code>nvidia-smi</code>，返回错误信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to initialize NVML: Driver/library version mismatch</span><br></pre></td></tr></table></figure><h2 id="方法1：重启解决大部分问题"><a href="#方法1：重启解决大部分问题" class="headerlink" title="方法1：重启解决大部分问题"></a>方法1：重启解决大部分问题</h2><p>博客<a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a>讲述的很清楚，这里就不再赘述。<br>或者参考大型交友网站stack overflow的问题<a href="https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch/45319156" target="_blank" rel="noopener">NVIDIA NVML Driver/library version mismatch</a></p><h2 id="方法2：重装驱动"><a href="#方法2：重装驱动" class="headerlink" title="方法2：重装驱动"></a>方法2：重装驱动</h2><p>看返回的错误信息，这个问题出现的原因是NVIDIA Driver的版本不匹配。如果重启不能解决问题，我们需要卸载重装NVIDIA driver。</p><ol><li><strong>查看驱动程序版本</strong></li></ol><ul><li><code>dpkg -l | grep nvidia</code> <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/31.png" alt title>                </div>                <div class="image-caption"></div>            </figure>可以看到驱动版本是<code>390.116</code></li><li><code>cat /proc/driver/nvidia/version</code> <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/32.png" alt title>                </div>                <div class="image-caption"></div>            </figure>这里NVRM的版本是<code>390.87</code>。错误信息就是这两个版本不匹配造成的。接下来先卸载NVIDIA driver，再重新安装。</li></ul><ol start="2"><li><p><strong>卸载旧版本的NVIDIA driver</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia-\*</span><br></pre></td></tr></table></figure></li><li><p><strong>添加NVIDIA的ppa源</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br></pre></td></tr></table></figure></li><li><p><strong>重新安装NVIDIA的驱动</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install nvidia-390</span><br></pre></td></tr></table></figure><p> 用你自己的版本号替换<code>390</code>。</p></li></ol><p>这时再用<code>cat /proc/driver/nvidia/version</code>查看NVIDIA driver的驱动。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/33.png" alt title>                </div>                <div class="image-caption"></div>            </figure> 可以看到NVRM的版本是<code>390.116</code>，这时版本就匹配了。<br>再次执行<code>nvidia-smi</code>,终于看到<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/34.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><p>在<a href="https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa" target="_blank" rel="noopener">nvidia driver各版本总览</a>可以看到NVIDIA driver的各个版本。</p><h2 id="额外的：update-与-upgrade"><a href="#额外的：update-与-upgrade" class="headerlink" title="额外的：update 与 upgrade"></a>额外的：update 与 upgrade</h2><p>记录下<code>sudo apt-get update</code>与<code>sudo apt-get upgrade</code>的区别。<br>在windows系统中安装软件，只需要有exe文件，双击即可安装了。Linux系统中则不同，Linux会维护一个自己的软件仓库，几乎所有软件都在这个仓库里，而且里面的软件完全安全，绝对可以安装。<br>我们自己的Ubuntu服务器上，维护一个软件源列表文件<code>/etc/apt/sources.list</code>,里面都是一些网址信息，每个网址就是一个软件源，这个地址指向的数据标识着有哪些软件可以安装使用。</p><ol><li><p>查看源列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li><li><p>更新软件列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><p> 这个命令对访问源列表里的每个网址，读取软件列表，保存到本地电脑。</p></li><li><p>更新软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure><p> 这个命令会把本地已安装的软件，与软件列表里对应软件做对比，如果有可更新版本就更新软件。<br>总的来说，<code>sudo apt-get update</code>是更新软件列表，<code>sudo apt-get upgrade</code>是更新软件。</p></li></ol><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a></li><li><a href="https://www.cnblogs.com/yizhichun/p/6397168.html" target="_blank" rel="noopener">Ubuntu配置GPU+CUDA+CAFFE</a></li><li><a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---Tensorflow-gpu%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85.html" target="_blank" rel="noopener">ubuntu下安装安装CUDA、cuDNN和tensotflow-gpu版本流程和问题总结</a></li><li><a href="https://wiki.ubuntu.org.cn/NVIDIA#PPA.E6.BA.90" target="_blank" rel="noopener">NVIDIA的wiki</a></li><li><a href="https://www.cnblogs.com/darkknightzh/p/5638185.html" target="_blank" rel="noopener">（原）Ubuntu16中安装nvidia的显卡驱动</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ubuntu运行命令&lt;code&gt;nvidia-smi&lt;/code&gt;出错。&lt;/p&gt;
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
      <category term="nvidia-smi" scheme="http://yoursite.com/tags/nvidia-smi/"/>
    
  </entry>
  
  <entry>
    <title>条件随机场CRF</title>
    <link href="http://yoursite.com/2019/03/23/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BACRF/"/>
    <id>http://yoursite.com/2019/03/23/条件随机场CRF/</id>
    <published>2019-03-23T05:50:20.000Z</published>
    <updated>2019-07-22T01:26:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习了条件随机场CRF，做下总结。主要参考<a href="https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/" target="_blank" rel="noopener">BiLSTM+CRF模型中的CRF层</a>为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。</p><a id="more"></a> <h2 id="从马尔科夫随机场到线性链条件随机场"><a href="#从马尔科夫随机场到线性链条件随机场" class="headerlink" title="从马尔科夫随机场到线性链条件随机场"></a>从马尔科夫随机场到线性链条件随机场</h2><ol><li><p><strong>概率图模型</strong><br> 概率图模型是用图G = (V,E)来表示概率分布。设有联合概率分布P(Y)，Y是一组随机变量。我们可以用无向图G = (V,E)来表示联合概率分布P(Y),节点$v \in V$表示随机变量$Y_v$，边$e \in E$表示随机变量之间的概率依赖关系。</p></li><li><p><strong>概率无向图模型，即马尔科夫随机场</strong><br> 设有联合概率分布P(Y),由无向图G=(V,E)表示。如果概率分布P(Y)满足<code>成对、局部、全局马尔科夫性</code>，那么称 这个联合概率分布p(Y)为概率无向图模型，或马尔科夫随机场(Markov random field)。</p><p> 成对马尔科夫性、局部马尔可夫性、全部马尔科夫性要表达的就是：在无向图中，没有边连接的节点之间没有概率依赖关系，也就是没有边连接的节点代表的随机变量之间是条件独立的。</p></li><li><p><strong>条件随机场</strong><br> 设X和Y是随机变量，P(Y|X)是给定X的条件下Y的条件概率分布。若给定X的条件下，Y构成一个马尔科夫随机场。则称条件概率分布P(Y|X)为条件随机场。<br> 我们可以看到，马尔科夫随机场是联合概率分布P(Y),而条件随机场是条件概率分布P(Y|X)。这是一点不同。</p></li><li><p><strong>线性链条件随机场</strong><br> 设$X =(X_1,X_2,…,X_n), Y =(Y_1,Y_2,…,Y_n)$是线性链表示的随机变量序列。若在给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔可夫性：$$P(Y_i|X,Y_1,Y_2,…,Y_{i-1},Y_{i+1},…,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})$$ 。则称条件概率分布p(Y|X)为线性链条件随机场。<br> <img src="/images/21.png" alt><br> 线性链条件随机场和隐马尔可夫模型都是序列模型，可以用于标注问题。这时，条件概率模型P(Y|X)中，X是输入变量序列，表示需要标注的观测序列；Y是输出变量，表示标记序列，或称状态序列。</p></li></ol><h2 id="BiLSTM-CRF模型"><a href="#BiLSTM-CRF模型" class="headerlink" title="BiLSTM+CRF模型"></a>BiLSTM+CRF模型</h2><p>BiLSTM+CRF模型是命名实体识别任务的常用模型。<br>假设我们训练集中有个由五个词组成的句子$X = (w_0,w_1,w_2,w_3,w_4)$,对应标签为$Y = [B-Person，I-Person,O,B-Organization,O]$。数据集中有五类标签： </p><table><thead><tr><th>类别</th><th align="center">B-Person</th><th align="center">I-Person</th><th align="center">B-Organization</th><th align="center">I-Organization</th><th align="center">O</th></tr></thead><tbody><tr><td>含义</td><td align="center">人名的开始部分</td><td align="center">人名的中间部分</td><td align="center">组织机构的开始部分</td><td align="center">组织机构的中间部分</td><td align="center">非实体信息</td></tr></tbody></table><p>先简单介绍下BiLSTM+CRF模型的结构。LSTM层的输入一般为每个词的Word-embedding，输出为每个词word在类别空间tag_space上的非归一化概率，也就是在单词对应每个类别的得分score。这些score作为CRF层的输入。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/22.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><h2 id="CRF的损失函数"><a href="#CRF的损失函数" class="headerlink" title="CRF的损失函数"></a>CRF的损失函数</h2><p>条件随机场中有两个重要的矩阵，转移概率矩阵和状态概率矩阵，分别对应转移特征和状态特征。</p><ul><li>状态概率矩阵。就是LSTM层的输出，作为CRF层的输入。矩阵的形状为[N,M],N为句子长度，M为可能状态数。</li><li>转移概率矩阵。矩阵形状为[M,M]，M为可能状态数。转移矩阵是模型参数，是随机初始化的，在训练过程中不断更新优化。</li></ul><p>给定转移矩阵T，随机变量序列X取值为x的条件下，随机变量序列取值为y的似然函数为：$$Likelihood(y|x,T) = \frac{ \sum_{i=0}^{n} P(x_i|y_i)T(y_i|y_{i-1})}{\sum_{y^<em>}\biggl(\sum_{i=0}^{n}P(x_i|y_i^</em>)T(y_i^<em>|y_{i-1}^</em>)\biggr)}  \cdots\cdots\cdots\cdots\cdots  (1)$$<br>上式中,</p><ul><li>$P(x_i|y_i)$表示当前状态为$y_i$时，产生观测值$x_i$的概率。对应状态分数。</li><li>$T(y_i|y_{i-1})$表示从上一个状态$y_{i-1}$转换到当前状态$y_i$的概率。对应转移分数。我们可以从转移矩阵中读出这个概率。</li><li>分子表示了单条路径y=[y_0,y_1,…,y_n]的分数score或概率。</li><li>分母表示了所有可能路径$y^*$的总分。注意计算分母时，我们要计算所有可能路径并求和。若序列长度为N,状态可能数为M,则所有可能路径数为$M^N$,这个数量是指数级的，非常大。我们的秘密武器是<code>前向后向算法</code>来高效地计算分母。</li></ul><p>进一步地，负对数似然函数为：<br>$$NegLogLikelihood(y|x,T) = \sum_{y^<em>}\biggl( \sum_{i=0}^{n} log(P(x_i|y_i^</em>)T(y^<em>_i|y^</em><em>{i-1}))\biggr) - \sum</em>{i=0}^{n}log(P(x_i|y_i)T(y_i|y_{i-1})) \cdots\cdots\cdots\cdots (2)$$</p><p>从式子(1)到式子(2)，直接对式子(1)取负对数是得到式子(2)可能比较令人费解。<br>需要留意的是：转移概率矩阵和状态概率矩阵中的概率都是对数概率（这很重要），这样计算路径概率时都是加法。对对数概率加上exp()运算我们能得到正常概率。<br>《统计学习方法》书中说，线性链条件随机场是对数线性模型，在对数空间中，对数概率可以直接相加，带来很大的方便。接下来，我们来看看：真实路径的分数和所有路径的总分是怎么计算的？</p><h3 id="真实路径分数"><a href="#真实路径分数" class="headerlink" title="真实路径分数"></a>真实路径分数</h3><p>数据集中有五类标签，再引入start和end作为序列开始和结束标志。</p><table><thead><tr><th>类别</th><th align="center">B-Person</th><th align="center">I-Person</th><th align="center">B-Organization</th><th align="center">I-Organization</th><th align="center">O</th><th align="center">start</th><th align="center">end</th></tr></thead><tbody><tr><td>索引</td><td align="center">0</td><td align="center">1</td><td align="center">2</td><td align="center">3</td><td align="center">4</td><td align="center">5</td><td align="center">6</td></tr></tbody></table><p>长度为5的序列，$X = (w_0,w_1,w_2,w_3,w_4)$,<br>对应类别为$Y = [B-Person，I-Person,O,B-Organization,O]$，<br>标注序列，也就是真实路径为为y = [0,1,4,3,4]。<br>真实路径的分数由两部分组成，状态分数和转移分数。状态矩阵就是LSTM层的输出。转移矩阵是模型参数，为$$[t_{ij}],i,j\in [0,6];i\neq 6,j\neq 5$$其中$t_ij$表示从上一状态转换到当前状态的概率。转移时，不能转移到start，不能从end转移。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/23.png" alt title>                </div>                <div class="image-caption"></div>            </figure>则真实路径的分数 = 转移分数 + 状态分数 = $1.5+0.4+0.1+0.2+0.5+t_{51}+t_{01}+t_{14}+t_{42}+t_{24}+t_{46}$</p><h3 id="所有路径分数-前向后向算法"><a href="#所有路径分数-前向后向算法" class="headerlink" title="所有路径分数-前向后向算法"></a>所有路径分数-前向后向算法</h3><p>计算所有路径的总分面对的难题是要不要穷举所有路径。对于一个长度为N的序列，可能状态数为M，所有可能路径数为$M^N$，这是一个指数级的计算量。计算每条路径分数的计算量是$O(N)$,直接用穷举法计算所有路径总分的计算量是$O(N\cdot M^N)$。这个计算量是无法接受的。<br>《统计学习方法》p176写，前向算法是基于“路径结构”递推计算所有路径分数。前向算法高效的关键是局部计算前向概率，再递推到全局。前向算法的计算量是$O(N\cdot M^2)$，前向算法减少计算量的原因是：每一次递推计算直接利用了前一个时刻的计算结果，避免了重复计算。</p><p>对于$w_0 \to w_1$的局部路径。<br>先计算$w_0$所有状态到$w_1$单个状态0的分数之和，并更新$w_1$的状态0的状态分数。有M条局部路径，计算量是$O(M)$</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/24.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>用同样的方法更新$w_1$所有状态的状态分数，这就是所有局部路径的分数。要计算M个状态，计算量是$O(M^2)$</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/25.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>依次递推到全局。序列长度为N,总的计算量是$O(N \cdot M^2)$</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/26.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>从图的角度解释了前向算法，我们再从数学计算的角度来看前向算法。简化一下问题，假设句子长度为3，$X = [w_0,w_1,w_2]$,只有2个类别[1,2]<br>我们引入两个变量<code>previous</code>和<code>obs</code>。<code>previous</code>存储前一时刻的计算结果，<code>obs</code>存储当前状态分数。<br>对于$w_0$:$$obs = [x_{01},x_{02}];previous = none$$<br>对于$w_0 \to w_1:$,$$previous = [x_{01},x_{02}],obs = [x_{11},x_{12}]$$<br>先扩展<code>previous</code>和<code>obs</code>：$$previous = \begin{pmatrix} x_{01}&amp;x_{01}\x_{02}&amp;x_{02} \end{pmatrix} \quad$$$$obs = \begin{pmatrix} x_{11}&amp;x_{12}\x_{11}&amp;x_{12} \end{pmatrix} \quad$$将<code>previous</code>和<code>obs</code>和转移矩阵相加：$$score =\begin{pmatrix} x_{01}&amp;x_{01}\x_{02}&amp;x_{02} \end{pmatrix} +\begin{pmatrix} x_{11}&amp;x_{12}\x_{11}&amp;x_{12} \end{pmatrix}+\begin{pmatrix} t_{11}&amp;t_{12}\t_{21}&amp;t_{22} \end{pmatrix}$$$$  = \begin{pmatrix} x_{01}+x_{11}+t_{11}&amp;x_{01}+x_{12}+t_{12}\x_{02}+x_{11}+t_{21}&amp;x_{02}+x_{12}+t_{22} \end{pmatrix}$$<br>score同列相加，更新<code>previous</code>:$$previous = [x_{01}+x_{11}+t_{11}+x_{02}+x_{11}+t_{21},x_{01}+x_{12}+t_{12}+x_{02}+x_{12}+t_{22}]$$<br>这样第二次迭代就完成了。用图来表示到目前为止的计算：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/27.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>用同样的方法迭代递推，就可以得到所有路径的分数。</p><p>这样我们就计算出了负对数似然函数，也就是CRF模型的损失函数。<br>条件随机场的第二个基本问题是学习问题，给定训练集估计条件随机场的模型参数（转移矩阵）。我们可以通过最小化对数似然函数来求参数模型。可以用梯度下降法来实现。</p><h2 id="维特比算法解码"><a href="#维特比算法解码" class="headerlink" title="维特比算法解码"></a>维特比算法解码</h2><p>条件随机场的第三个基本问题是预测问题，给出条件随机场的模型 和 输入序列x，求条件概率最大输出序列$y^*$。 也就是找出所有路径中得分最高的那条路径作为标注路径。与计算所有路径总分一样，我们面对的难题是要不要求出所有路径的分数。当然是不用的，我们用维特比算法来解码。<br>通信专业的同学一定知道大名鼎鼎的维特比算法，卷积码的译码就是用的维特比算法。<br>对于 $w_0 \to w_1$:<br>先计算$w_0$到$w_1$的状态1五条路径的分数，找出分数最大的一条保留下来，其他全都丢弃掉。计算量为$O(M)$</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/28.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>同样的找出$w_1$每个状态分数最大的一条路径，要计算$w_1$的5个状态，计算量为$O(M^2)$</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/29.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>依次递推到全局。序列长度为N,计算量为$O(N \cdot M^2)$</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/30.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>比较下前向算法与维特比算法的异同：<br>相同的地方在他们都面临要不要计算所有路径分数的问题，都是基于路径结构，用局部递推到全局。<br>不同的地方在于前向算法在更新previous的单个状态时是做求和sum运算，而维特比算法是做max运算，只保留分数最大的，丢弃掉其他路径。此外，维特比算法找到分数最大的路径后，还要反向递推</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/" target="_blank" rel="noopener">CRF Layer on the Top of BiLSTM</a></li><li><a href="https://zhuanlan.zhihu.com/p/44042528" target="_blank" rel="noopener">最通俗易懂的BiLSTM-CRF模型中的CRF层介绍</a></li><li><a href="https://mp.weixin.qq.com/s/1KAbFAWC3jgJTE-zp5Qu6g" target="_blank" rel="noopener">如何直观地理解条件随机场，并通过PyTorch简单地实现</a></li><li><a href="http://www.cnblogs.com/pinard/p/7048333.html" target="_blank" rel="noopener">条件随机场CRF—刘建平</a></li><li>《统计学习方法》—李航</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近学习了条件随机场CRF，做下总结。主要参考&lt;a href=&quot;https://createmomo.github.io/2017/09/12/CRF_Layer_on_the_Top_of_BiLSTM_1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BiLSTM+CRF模型中的CRF层&lt;/a&gt;为主线，结合李航老师的《统计机器学习》，记录自己对CRF的理解。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="条件随机场" scheme="http://yoursite.com/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    
      <category term="前向后向算法" scheme="http://yoursite.com/tags/%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E7%AE%97%E6%B3%95/"/>
    
      <category term="维特比算法" scheme="http://yoursite.com/tags/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实现基于LSTM的循环神经网络</title>
    <link href="http://yoursite.com/2019/03/20/pytorch%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/03/20/pytorch实现基于LSTM的循环神经网络/</id>
    <published>2019-03-20T14:41:10.000Z</published>
    <updated>2019-07-22T01:01:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>用pytorch实现基于LSTM的循环神经网络。</p><a id="more"></a> <h2 id="涉及函数详解"><a href="#涉及函数详解" class="headerlink" title="涉及函数详解"></a>涉及函数详解</h2><h3 id="class-torch-nn-LSTM-args-kwargs"><a href="#class-torch-nn-LSTM-args-kwargs" class="headerlink" title="class torch.nn.LSTM(args,*kwargs)"></a>class torch.nn.LSTM(args,*kwargs)</h3><ul><li><p>参数说明：</p><ul><li>input_size: 输入的特征维度</li><li>output_size: 输出的特征维度</li><li>num_layers: 层数（注意与时序展开区分）</li><li>bidirectional: 如果为<code>True</code>，为双向LSTM。默认为<code>False</code></li></ul></li><li><p>LSTM的输入：input,$(h_0,c_0)$</p><ul><li>input(seq_len,batch,input_size): 包含输入特征的<code>tensor</code>,注意输入是<code>tensor</code>。</li><li>$h_0$(num_layers $\cdot$ num_directions,batch,hidden_size): 保存初始化隐藏层状态的<code>tensor</code></li><li>$c_0$(num_layers $\cdot$ num_directions,batch,hidden_size): 保存初始化细胞状态的<code>tensor</code></li></ul></li><li><p>LSTM的输出： output,$(h_n,c_n)$</p><ul><li>output(seq_len, batch, hidden_size * num_directions): 保存<code>RNN</code>最后一层输出的<code>tensor</code></li><li>$h_n$(num_layers * num_directions,batch,hidden_size): 保存<code>RNN</code>最后一个时间步隐藏状态的<code>tensor</code></li><li>$c_n$(num_layers * num_directions,batch,hidden_size): 保存<code>RNN</code>最后一个时间步细胞状态的<code>tensor</code></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn </span><br><span class="line">import torch</span><br><span class="line">lstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，该单元输入维度embedding_dim,输出维度为hidden_dim</span><br><span class="line">input = Variable(torch.randn(seq_len,1,embedding_dim)) # 输入input应该是三维的，第一维度是seq-length,也就是多个词构成的一句话；第二维度为1，不用管；第三个维度是一个词的词嵌入维度，即embedding_dim</span><br><span class="line">h0 = Variable(torch.randn(1,1,hidden_dim)) </span><br><span class="line">c0 = Variable(torch.randn(1,1,hidden_dim))</span><br><span class="line">lstm_out,hidden = lstm(input,(h0,c0))</span><br></pre></td></tr></table></figure><h3 id="class-torch-nn-Linear"><a href="#class-torch-nn-Linear" class="headerlink" title="class torch.nn.Linear()"></a>class torch.nn.Linear()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class torch.nn.Linear(in_features,out_features,bias = True)</span><br></pre></td></tr></table></figure><ul><li>作用：对输入数据做线性变换。$y = Ax+b$</li><li>参数：<ul><li>in_features：每个输入样本的大小</li><li>out_features: 每个输出样本的大小</li><li>bias: 默认值为True。是否学习偏置。</li></ul></li><li>形状：<ul><li>输入： (N,in_features)</li><li>输出： (N,out_features)</li></ul></li><li>变量：<ul><li>weights: 可学习的权重，形状为(in_features,out_features)</li><li>bias: 可学习的偏置，形状为(out_features)</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Linear(20,30)</span><br><span class="line">input = torch.randn(128,20)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><h2 id="先看个小例子"><a href="#先看个小例子" class="headerlink" title="先看个小例子"></a>先看个小例子</h2><p>用pytorch实现LSTM，先实例化一个LSTM单元，再给出tensor类型的输入数据inputs及初始隐藏状态hidden = $(h_0,c_0)$。值得注意的是，LSTM单元的输入inputs必须是三维的，第一维是seq-length，即一句话，元素是词。第二维是mini-batch,从来不用，设为1即可。第三维是embedding-size,即一个词向量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torch.nn as nn </span><br><span class="line"></span><br><span class="line">lstm = nn.LSTM(4,3) #实例化一个LSTM单元，单元输入维度是4，输出维度是3</span><br><span class="line">inputs = [torch.randn(1,5) for _ in range(5)] #产生输入inputs。为tensor序列。</span><br><span class="line">hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #初始化隐藏状态</span><br></pre></td></tr></table></figure><p>做好三步准备：实例化一个LSTM单元，准备好inputs，初始化隐藏状态hidden。我们就可以计算LSTM单元的输出了。<br>我们有两种选择，将序列一个元素一个元素地送入LSTM单元，或是将整个序列一下子全送入LSTM单元。先看看第一种：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for x in inputs:</span><br><span class="line">    lstm_out,hidden = lstm(x.view(1,1,-1),hidden) #x.view(1,1,-1)将tensor整形为三维。前面说过LSTM单元的输入必须是三维的。</span><br><span class="line">print(lstm_out,hidden)</span><br></pre></td></tr></table></figure><p>接下来，将整个序列送入LSTM单元：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.cat(inputs).view(len(inputs),1,-1) #将整个序列连接为tensor，并整形为三维。</span><br><span class="line">hidden = (torch.randn(1,1,3),torch.randn(1,1,3)) #清楚隐藏状态</span><br><span class="line">lstm_out,hidden = lstm(inputs,hidden)</span><br><span class="line">print(lstm_out,hidden)</span><br></pre></td></tr></table></figure><p>我们可以看到：</p><ul><li>lstm_out 中包含了序列所有的隐藏状态。</li><li>hidden 中包含了最后一个时间步的隐藏状态和细胞状态。可以作为下个时间步LSTM单元的输入参数，继续输入序列或反向传播。</li></ul><h2 id="用lstm做词性标注"><a href="#用lstm做词性标注" class="headerlink" title="用lstm做词性标注"></a>用lstm做词性标注</h2><p>先准备训练数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_data = [</span><br><span class="line">    (&quot;The dog ate the apple&quot;.split(), [&quot;DET&quot;, &quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;]),</span><br><span class="line">    (&quot;Everybody read that book&quot;.split(), [&quot;NN&quot;, &quot;V&quot;, &quot;DET&quot;, &quot;NN&quot;])</span><br><span class="line">]</span><br><span class="line"># 词汇表字典</span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line">for sent,tags in train_data:</span><br><span class="line">    for word in sent:</span><br><span class="line">        if word not in word_to_ix:</span><br><span class="line">            word_to_ix[word] = len(word_to_ix)</span><br><span class="line"># 标签集字典</span><br><span class="line">tag_to_ix = &#123;&quot;DET&quot;: 0, &quot;NN&quot;: 1, &quot;V&quot;: 2&#125;</span><br><span class="line"></span><br><span class="line">EMBEDDING_DIM = 6</span><br><span class="line">HIDDEN_DIM = 6</span><br></pre></td></tr></table></figure><p>构建LSTM模型:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class LSTMtagger(nn.Module):</span><br><span class="line">    def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size):</span><br><span class="line">        super(LSTMtagger,self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size,embedding_dim) #随机初始化词向量表，是神经网络的参数</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim,hidden_dim) #实例化一个LSTM单元，单元输入维度是embedding_dim，输出维度是hidden_dim</span><br><span class="line">        self.hidden2tag = torch.Linear(hidden_dim,tagset_size) #线性层从隐藏状态空间映射到标签空间</span><br><span class="line">    def forward(self,sentence):</span><br><span class="line">        embeds = self.word_embeddings(sentence) #查询句子的词向量表示。输入应该是二维tensor。</span><br><span class="line">        lstm_out,hidden = self.lstm(embeds.view(len(sentence),1,-1))</span><br><span class="line">        tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1))</span><br><span class="line">        tag_scores = F.log_softmax(tag_space)</span><br></pre></td></tr></table></figure><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">model = LSTMtagger(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr = 0.1)</span><br><span class="line"></span><br><span class="line">def prepare_sequence(seq,to_ix):</span><br><span class="line">    idxs = [to_ix[w] for w in seq]</span><br><span class="line">    return torch.tensor(idxs,dtype = torch.long)</span><br><span class="line"># 在训练模型之前，看看模型预测结果</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(train_data[0][0],word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    print(tag_scores)</span><br><span class="line">    predict = np.argmax(tag_scores,axis = 1)</span><br><span class="line">    print(predict)</span><br><span class="line"></span><br><span class="line">for epoch in range(300):</span><br><span class="line">    for sentence,tags in train_data:</span><br><span class="line">        # step 1:pytorch会累积梯度，要清楚所有variable的梯度。</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        # step 2:准备好数据，变成tensor</span><br><span class="line">        sentence_in = prepare_sequence(sentence,word_to_ix)</span><br><span class="line">        targets = prepare_sequence(tags,tag_to_ix)</span><br><span class="line">        # step 3:得到输出</span><br><span class="line">        tag_scores = model(sentence_in)</span><br><span class="line">        # step4: 计算loss</span><br><span class="line">        loss = loss_function(tag_scores,targets)</span><br><span class="line">        # step5: 计算loss对所有variable的梯度</span><br><span class="line">        loss.backward()</span><br><span class="line">        # step6： 单步优化，根据梯度更新参数</span><br><span class="line">        optimizer.step()</span><br><span class="line"># 模型训练后，看看预测结果</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(train_data[0][0],word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    print(tag_scores)</span><br><span class="line">    predict = np.argmax(tag_scores,axis = 1)</span><br><span class="line">    print(predict)</span><br></pre></td></tr></table></figure><p>输出结果为：<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/19.png" alt title>                </div>                <div class="image-caption"></div>            </figure>我们可以看到，训练之后的预测序列为 [0,1,2,0,1]也就是[“DET”, “NN”, “V”, “DET”, “NN”]</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/28448135" target="_blank" rel="noopener">序列模型和基于LSTM的循环神经网络</a></li><li><a href="https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html" target="_blank" rel="noopener">Sequence Models and Long-Short Term Memory Networks-官方</a></li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li><li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用pytorch实现基于LSTM的循环神经网络。&lt;/p&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
      <category term="循环神经网络" scheme="http://yoursite.com/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>pytorch实现Word embedding</title>
    <link href="http://yoursite.com/2019/03/20/pytorch%E5%AE%9E%E7%8E%B0Word-embedding/"/>
    <id>http://yoursite.com/2019/03/20/pytorch实现Word-embedding/</id>
    <published>2019-03-20T12:16:21.000Z</published>
    <updated>2019-07-07T07:11:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p><a id="more"></a> <h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><p>在自然语言处理任务中，我们常常要与词打交道。那么在计算机上，我们怎么表示一个单词呢？一种思路是one-hot编码。假设词汇表为$V$,词汇表大小(vocab_size)为$N_V$。我们可以用向量$N_V$维向量$[1,0,0…,0,0]$来表示第一个词。以此类推，来表示所有的词。<br>这种方法有致命的弱点。首先是向量维度太大，太稀疏，效率太低。更要命的是，one-hot编码把词与词间看做完全独立的，没有表达出词与词之间的联系和相似性。而这正是我们想要的。<br>举个例子，我们想要构建一个语言模型。有以下三个句子</p><ul><li>数学家待在实验室里。</li><li>物理学家待在实验室里。</li><li>数学家解决了一个难题。</li></ul><p>我们又看到一个新的句子：</p><ul><li>物理学家解决了一个难题。</li></ul><p>我们希望语言模型可以学习到以下特点：</p><ul><li><code>数学家</code>和<code>物理学家</code>在一个句子中同样的位置出现。这两个词之间有某种语义上的联系</li><li><code>数学家</code>曾经出现在我们看到的这个新句子中<code>物理学家</code>出现的位置。</li></ul><p>这就是<strong>语义相似性</strong>想表达的。语义相似性可以将没见过的数据与已经见过的数据联系起来，来解决语言数据的稀疏性问题。这个例子基于一个基本的语义学假设：出现在相似文本中的词汇在语义上是相互联系的。这称为<strong>distributional hypothesis</strong><br>值得一提的是，在分类问题中，one-hot编码很适合用在类别的编码上。</p><h2 id="word-embedding"><a href="#word-embedding" class="headerlink" title="word embedding"></a>word embedding</h2><p>我们怎样编码来表达词汇的语义相似性呢？我们考虑词汇的semantic attributes。例如，物理学家和数学家学可能[头发不多，爱喝咖啡，会看论文，会说英语]。我们可以用这四个属性来编码<code>物理学家</code>和<code>数学家</code>。$$q_物 = [0.9,0.8,0.98,0.8]$$$$q_数 = [0.91,0.89,0.9,0.85]$$<br>我们可以衡量这两个词之间的语义相似度：$$similarity(q_物,q_数) = \frac{q_物\cdot q_数}{|q_物| \cdot |q_数|}=cos(\phi)    其中\phi是两个向量之间的夹角。$$<br>但我们如何选择属性特征，并决定每个属性的值呢？深度学习的核心思想是神经网络学习特征表示，而不用人为指定特征。我们干脆将Word embedding作为神经网络的参数，让神经网络在训练的过程中学习Word embedding。<br>神经网络学到的Word embedding是潜在语义属性。也就是说，如果两个词在某个维度上都有大的值，我们并不知道这个维度代表了什么属性，这不能人为解释。这就是潜在语义属性的含义。<br>总的来说，Word embedding是一个词的语义表示，有效地编码了词的语义信息。</p><h2 id="PyTorch实现word-embedding"><a href="#PyTorch实现word-embedding" class="headerlink" title="PyTorch实现word embedding"></a>PyTorch实现word embedding</h2><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># 词汇表字典</span><br><span class="line">word_to_ix = &#123;&apos;The&apos;: 0, &apos;dog&apos;: 1, &apos;ate&apos;: 2, &apos;the&apos;: 3, &apos;apple&apos;: 4, &apos;Everybody&apos;: 5, &apos;read&apos;: 6, &apos;that&apos;: 7, &apos;book&apos;: 8&#125;</span><br><span class="line">vocab_size = len(word_to_ix) </span><br><span class="line">embedding_dim = 15</span><br><span class="line">word_embeddings = nn.Embedding(vocab_size,embedding_dim)</span><br></pre></td></tr></table></figure><p><code>nn.Embedding()</code>随机初始化了一个形状为[vocab_size,embedding_dim]的词向量矩阵，是神经网络的参数。<br>接下来我们查询”dog”这个词的向量表示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dog_idx = torch.LongTensor([word_to_ix[&apos;dog&apos;]]) #注意输入应该是一维数组。</span><br><span class="line">dog_idx = Variable(dog_idx)</span><br><span class="line">dog_embed = word_embeddings(dog_idx) #注意不是索引</span><br><span class="line">print(dog_embed)</span><br></pre></td></tr></table></figure><p>上述代码中，要访问<code>dog</code>的词向量，要得到一个Variable。word_embeddings的输入应该是一个一维tensor。<br>接下来，我们查询一句话的向量表示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sent = &apos;The dog ate the apple&apos;.split()</span><br><span class="line">sent_idxs = [word_to_ix[w] for w in sent]</span><br><span class="line">sent_idxs = torch.LongTensor(sent_idxs)</span><br><span class="line">sent_idxs = Variable(sent_idxs)</span><br><span class="line">sent_embeds = embeds(sent_idxs) </span><br><span class="line">print(sent_embeds)</span><br></pre></td></tr></table></figure><h2 id="pytorch加载预训练词向量"><a href="#pytorch加载预训练词向量" class="headerlink" title="pytorch加载预训练词向量"></a>pytorch加载预训练词向量</h2><p>之前的方法中，词向量是随机初始化的，作为模型参数在训练过程中不断优化。通常我们要用到预训练的词向量，这样可以节省训练时间，并可能取得更好的训练结果。下面介绍两种加载预训练词向量的方式。<br>方式一：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">pretrain_embedding  = np.array(np.load(np_path),dtype = &apos;float32&apos;) #np_path是一个存储预训练词向量的文件路径</span><br><span class="line">word_embeddings.weight.data.copy_(troch.from_numpy(pretrain_embedding)) #思路是将np.ndarray形式的词向量转换为pytorch的tensor，再复制到原来创建的词向量矩阵中</span><br></pre></td></tr></table></figure><p>方式二：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_embeddings = torch.nn.Embedding(vocab_size,embedding_dim) #创建一个词向量矩阵</span><br><span class="line">word_embeddings.weight = nn.Parameter(torch.FloatTensor(pretrain_embedding))</span><br></pre></td></tr></table></figure><h3 id="涉及函数详解"><a href="#涉及函数详解" class="headerlink" title="涉及函数详解"></a>涉及函数详解</h3><h4 id="numpy-与from-numpy"><a href="#numpy-与from-numpy" class="headerlink" title="numpy()与from_numpy()"></a>numpy()与from_numpy()</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(ndarray) $\to$ tensor</span><br></pre></td></tr></table></figure><ul><li>作用：numpy桥，将<code>numpy.ndarray</code>转换为pytorch的<code>tensor</code>.返回的张量与numpy.ndarray共享同一内存空间，修改一个另一个也会被修改。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor.numpy()</span><br></pre></td></tr></table></figure><ul><li>作用：numpy桥，将pytorch的<code>tensor</code>转换为<code>numpy.ndarray</code>.二者共享同一内存空间，修改一个另一个也会被修改。</li></ul><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">c = b.numpy()</span><br></pre></td></tr></table></figure><h4 id="tensor-copy-src"><a href="#tensor-copy-src" class="headerlink" title="tensor.copy_(src)"></a>tensor.copy_(src)</h4><ul><li>作用：将<code>src</code>中的元素复制到tensor并返回。两个tensor应该有相同数目的元素和形状，可以是不同数据类型或存储在不同设备上。</li></ul><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(1,5)</span><br><span class="line">b = torch.randn(1,5)</span><br><span class="line">b.copy_(a)</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html?highlight=embed" target="_blank" rel="noopener">Word Embeddings: Encoding Lexical Semantics</a></li><li><a href="https://ptorch.com/news/11.html" target="_blank" rel="noopener">PyTorch快速入门教程七（RNN做自然语言处理）</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;word embedding是稠密的实数向量。Word embedding是一个词的语义表示，有效地编码了词的语义信息。&lt;/p&gt;
    
    </summary>
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="word embedding" scheme="http://yoursite.com/tags/word-embedding/"/>
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>去新藏线上骑车</title>
    <link href="http://yoursite.com/2019/03/14/%E5%8E%BB%E6%96%B0%E8%97%8F%E7%BA%BF%E4%B8%8A%E9%AA%91%E8%BD%A6/"/>
    <id>http://yoursite.com/2019/03/14/去新藏线上骑车/</id>
    <published>2019-03-14T09:49:34.000Z</published>
    <updated>2019-07-22T00:58:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>18年的夏天，热烈的阳光透过浓密的叶子照在窗台的盆栽上，有着丝丝的风。就在这美丽的季节里，我们毕业了。毕业后的暑假，我终于做到了我想做的一件事，骑车去那小小的尼泊尔。</p><a id="more"></a><p>该说说出发的动机和情景。我曾经产生这样的念头：“骑车出国，横穿整个欧亚大陆，是件了不起的事啊！”直到旺哥在论坛上发帖征新藏线的队友，这个念头促使我下了决心去新藏线上骑车。出发前还需要做些准备，克服一些阻力。我回家办了护照和边防证，在北京办好了尼泊尔的签证，在学校做好路书和高程图，准备好了衣服装备。出发前，超哥、大胖儿和狗子帮我修理了我那辆难骑的车。我曾怀疑我这个念头是不是心血来潮，我问弟弟：“我骑车太多了，是不是有点不务正业啊？”弟弟跟我说：“这是你喜欢做的事情嘛！”家人和朋友给了我很多的帮助和支持，我才得以做好出发的准备。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/14.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>7月21日-24日，西行的火车经过57个小时的颠簸，载着我到了新疆喀什，我和旺哥约好了从这出发。喀什市是一个不大的城市，分为旧城区和新城区，除了维吾尔族，还有不少定居在这的汉族人。正式出发前的一天，我和旺哥去逛了旧城区，常可以见到在小巷子里、在空地上踢足球的小男孩。第一天的目的地是莎车县，行程约190公里，加上中午的酷热，下午的逆风，到莎车县安顿下来已经是晚上九点，一路上几近崩溃，身体已经被掏空。用鲁豫对于谦的访谈内容来形容第一天的骑行恰如其分。</p><ul><li>鲁豫：成名之前，你和郭德纲经常整天整天地表演相声，很少有休息时间。那段日子快乐吗？</li><li>于谦：快死了！</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/15.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>新疆地广人稀，大片的地方是没有人烟的荒漠，有人烟的村落常隔着几十甚至上百公里远，至于城镇则更少。新藏线差不多已经是我国边疆区域，有着很多“地方特色”。城里的银行、邮局、药店，进出都要刷身份证登记。曾在县城里看到一场浩大的阵势，一辆装甲车开路，后面跟着两排公安巡逻车，得有二十多辆，整整齐齐。出英吉沙县城区的路上，有段路有许多防暴的警察，个个全副武装，头戴黑色头盔，身着黑色制服，外面再套上黑马甲，上面写着“警察 police”。这些警察大多是维族青壮年，手执的武器变化多端，有一人高的黑色长棍，半人高前端配有刺刀的棍子，防暴叉，冲锋枪…….真是十八般兵器，令人眼花缭乱。稳定是发展的前提，这也是情有可原的事。新疆的治安十分严格，这里写一则趣事。趁吃午饭的空档，我骑车去邮局盖邮戳，回饭店的路上偷懒逆行了一段路。一个警察挥挥手示意我停下来，接着便一顿训斥“不走人行道，逆行，啊！能不能有点素质！”我赶忙认错，等候发落，等了一会，他没再理我，我才灰溜溜地走开了。</p><p>第三天的行程是叶城到阿克美其特村。叶城是219国道的0公里所在，新藏线真正的起点。219国道的零公里处有个大拱门，左右各写：“天路零公里”，“昆仑第一城”，横批就是一个大大的“0”。叶城之后就很少有城镇了，大多是村落，物资补给就比较困难了。我和旺哥的后货架载得满满的，像是个小山。骑车出了叶城，自然环境是这样变化的，大树变小树，小树变低矮的灌木，灌木变成野草，戈壁滩渐渐露出了它的面目。在新藏线上，如果你能看到高高的树，那这个地方就有比较丰富的水，有水源的地方一般就有人烟。晚上，我们到了阿克美其特村，一个经常停电的小村子，我和旺哥住在村民家里。</p><p>阿克美其特村，故事在这里发生，这是个我铭记着的村子。早上出发，骑车出去没多远，我发现我的车子后轮花鼓断了。我们决定：旺哥继续前行，到物资补给更方便的地方等我；我返回去修理后轮。断花鼓的第一天，我卸下自行车的后轮，背上小包，村干部找了辆车把我送到了柯克亚乡，我在乡里坐出租车到了叶城县。我偷懒没去更远的喀什市，只是在叶城县找了一个不靠谱的修车师傅给我换了个花鼓，修理完已经快天黑了，于是夜宿叶城。断花鼓的第二天，一大早我就到汽车站坐车，上午十点返回阿克美其特村，把后轮换好再次出发，没上次骑得远，车子故障，发现后轮没修好。我要再返回叶城，这次没有车送我了，我边走边搭车，一百公里的路，走了十几公里，搭了四五辆车到了叶城。在叶城买火车票，坐火车到喀什市。断花鼓的第三天，一大早我到了喀什，前往捷安特店，买了个后轮。买火车票返回叶城，边走边搭车，晚上十点返回到阿克美其特村。</p><p>断花鼓的第三天，我几乎想放弃了。高婷婷同学恰好发微信慰问我，是我艰难日子里的一缕阳光！返回阿克美其特村时，我搭了一辆大卡车，司机师傅是山西的，一路上陪师傅聊天。师傅说，海拔高，氧气少，路难走，在新藏线上开大卡车就是在玩命。新藏线上常可以看到翻倒在路边的大卡车。有的司机带着老婆出来开大卡车拉货，出了事，就剩个孩子跟着爷爷奶奶。新藏线上有各样的人，踩单车的，骑摩托的，自驾游的，来谋生的，开卡车，摆水果摊，开旅店。</p><p>我搭车走了一百多公里，在三十里营房追上了旺哥。在红柳滩我们遇到了两个新伙伴，吴锡贤和邓翱，分别大一和大二。他们俩都走过川藏线，体力好，干练，真不愧英雄出少年！加入新的队友后，我还是队里体力最差的那个人。红柳滩后三百公里被称为无人区，人烟极少，必须严格按照行程赶到住宿地点，不然只能露宿荒野。出发前的那个晚上，有个骑友滔滔不绝地谈论这段路：“去年有两个骑友，没到达住宿点，露营在一个没有门的厕所里，被狼啃得只剩一条腿了。”听了这不知真假的话，我如临大敌，连夜把驼包收拾好，给车打满气。第二天早早地就起床了，早饭丰盛而美味，香甜的玉米糊，榨菜、拍黄瓜、花生米三样小菜，热乎松软的馒头，每人一个蒸鸡蛋，像是上刑场前的盛宴。<br> <figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/16.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><br>三天后，我们终于穿过了无人区，后面的路就好走了。无限风光在险峰，我们几个人在无人区远远地看到了藏羚羊或是黄羊，看不清她的样子丝毫不影响我见到她时的激动和惊喜。新藏线辽阔俊美，大大小小的湖泊像是美玉点缀其间，远远近近的雪山在阳光照耀下度过岁月，起起伏伏的路延伸向远方没有尽头。新藏线上有时可以看到长长的军车车队驶过，像是士兵突击里的样子，不禁感叹一句：“好男儿就是要当兵。”</p><p>在西藏萨嘎县，我和他们仨分道扬镳，他们仨前往珠峰，我转向吉隆口岸去尼泊尔。出萨嘎县后，有六七十公里的路还在修，大部分是烂泥路。遇到大水坑，我就脱下鞋挂在车把上，推着车趟过去。实在骑不动了，我放弃了一定要全程骑完的念头，心安理得地推着车前进。就这样骑了一整天十个小时，只前进了五十公里。下午起了很大的逆风，遇到一个对面来的骑友，颇有惺惺相惜之感。他说前面有条河拦住了路，打算原路返回。天色渐晚，我再往前只能露宿荒野，于是借宿在修路的项目部。六七个西安的爷们收留我住了一宿，还用高压锅煮了面。有位做饭的阿姨聊天总是笑不停，让这座在大风中摇摆不停的帐篷里充满了温暖。</p><p>从吉隆镇到吉隆口岸的一百公里路，海拔陡降，全是下坡，而且景色渐美，仿佛置身于画中，空气中有淡淡的木香味。独自欣赏这美景，不禁想念起我的队友来。推着自行车，过了海关，就踏出了国门，骑行在尼泊尔！再翻过两座山，我就要到加德满都了。尼泊尔境内的路有一大半是烂路，但景色很不错，常有泉水从山上流到路上来，后来我干脆踩水玩，让泉水冲去我鞋上的泥沙。这段路太烂了，非常不推荐骑车去尼泊尔。在尼泊尔境内骑行了两天，8月22日我终于抵达了加德满都！洗个澡，换身干净衣服，卖掉自行车，去杜巴广场喝酸奶，去看黄昏里的梦花园，看猴子在城市里游荡。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/17.jpg" alt="尼泊尔的小孩子" title>                </div>                <div class="image-caption">尼泊尔的小孩子</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/18.jpg" alt="俯瞰加德满都" title>                </div>                <div class="image-caption">俯瞰加德满都</div>            </figure><p>理性地来说，旺哥的队医知识、修车技术和体力都胜过我（除了帅，我真是一无是处啊！），如果没有旺哥的帮助，我很难走完这条线。新藏线这一路上，没怎么写日记，倒是跟旺哥斗了很多盘地主，给一些朋友写了好多明信片，盖到了几个邮戳。</p><p>骑车有什么意义呢？当谈到尼泊尔，我对自己说：“嘿，这个地方我骑车去过哦！”曾经到达就是我的意义。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;18年的夏天，热烈的阳光透过浓密的叶子照在窗台的盆栽上，有着丝丝的风。就在这美丽的季节里，我们毕业了。毕业后的暑假，我终于做到了我想做的一件事，骑车去那小小的尼泊尔。&lt;/p&gt;
    
    </summary>
    
      <category term="生活记录" scheme="http://yoursite.com/categories/%E7%94%9F%E6%B4%BB%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="新藏线" scheme="http://yoursite.com/tags/%E6%96%B0%E8%97%8F%E7%BA%BF/"/>
    
      <category term="骑车" scheme="http://yoursite.com/tags/%E9%AA%91%E8%BD%A6/"/>
    
  </entry>
  
  <entry>
    <title>sublime插件</title>
    <link href="http://yoursite.com/2019/03/11/sublime%E6%8F%92%E4%BB%B6/"/>
    <id>http://yoursite.com/2019/03/11/sublime插件/</id>
    <published>2019-03-11T11:41:12.000Z</published>
    <updated>2019-03-11T13:04:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录sublime的一些插件。</p><a id="more"></a> <h2 id="OmniMarkupPreviewer"><a href="#OmniMarkupPreviewer" class="headerlink" title="OmniMarkupPreviewer"></a>OmniMarkupPreviewer</h2><p><strong>作用：</strong>插件OmniMarkupPreviewer支持将markdown语言渲染为html并且在浏览器上实时预览，也就是将markdown内容实时显示为网页，效果之好令人惊叹。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>可以使用<code>Package Control</code>的<code>Insatll Package</code>来安装，也可以直接从<a href="https://github.com/timonwong/OmniMarkupPreviewer" target="_blank" rel="noopener">OmniMarkupPreviewer的github主页</a>下载压缩包，解压到目录<code>\Sublime Text 3\Packages\</code>下。</p><ol><li>快捷键<code>Ctrl + shift + p</code>打开<code>Package Control</code></li><li>输入<code>install</code>选择<code>Package Control: Install Package</code></li><li>从列表中选择<code>OmniMarkupPreviewer</code>安装。</li></ol><h3 id="使用方法："><a href="#使用方法：" class="headerlink" title="使用方法："></a>使用方法：</h3><p>对于window和Linux：</p><ul><li><code>Ctrl+Alt+O</code> 在浏览器中预览</li><li><code>Ctrl+Alt+X</code> 输出为html文件</li><li><code>Ctrl+Alt+C</code> 复制为HTML文件</li></ul><h3 id="插件配置"><a href="#插件配置" class="headerlink" title="插件配置"></a>插件配置</h3><p>修改插件的配置，点击菜单栏的<code>Preferences - Packages Settings - OmniMarkdownPreviwer - Setting-User</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;server_host&quot;: &quot;127.0.0.1&quot;,  //默认为localhost,修改为你电脑的ip，可以实现远程访问。也就是从其他电脑预览网页效果</span><br><span class="line">    &quot;server_port&quot;: 51004,</span><br><span class="line">    &quot;refresh_on_modified&quot;: true,</span><br><span class="line">    &quot;refresh_on_modified_delay&quot;: 500,</span><br><span class="line">    &quot;refresh_on_saved&quot;: true,</span><br><span class="line">    &quot;browser_command&quot;: [],</span><br><span class="line">    &quot;html_template_name&quot;: &quot;github&quot;,</span><br><span class="line">    &quot;ajax_polling_interval&quot;: 500,</span><br><span class="line">    &quot;ignored_renderers&quot;: [&quot;LiterateHaskellRenderer&quot;],</span><br><span class="line">    &quot;mathjax_enabled&quot;: true,  //渲染数学公式要用到MathJax库，将值设为true,mathjax会在后端自动下载。</span><br><span class="line">    &quot;export_options&quot; : &#123;</span><br><span class="line">        &quot;template_name&quot;: &quot;github-export&quot;,</span><br><span class="line">        &quot;target_folder&quot;: &quot;.&quot;,</span><br><span class="line">        &quot;timestamp_format&quot; : &quot;_%y%m%d%H%M%S&quot;,</span><br><span class="line">        &quot;copy_to_clipboard&quot;: false,</span><br><span class="line">        &quot;open_after_exporting&quot;: false</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;renderer_options-MarkdownRenderer&quot;: &#123;</span><br><span class="line">        &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="遇到的错误"><a href="#遇到的错误" class="headerlink" title="遇到的错误"></a>遇到的错误</h3><p>预览文本时报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Error: 404 Not Found</span><br><span class="line">Sorry, the requested URL &apos;http://127.0.0.1:51004/view/593&apos; caused an error:</span><br><span class="line"></span><br><span class="line">&apos;buffer_id(593) is not valid (closed or unsupported file format)&apos;</span><br><span class="line"></span><br><span class="line">**NOTE:** If you run multiple instances of Sublime Text, you may want to adjust</span><br><span class="line">the `server_port` option in order to get this plugin work again.</span><br></pre></td></tr></table></figure><p>解决办法是修改配置文件<code>Sublime Text &gt; Preferences &gt; Package Settings &gt; OmniMarkupPreviewer &gt; Settings - User</code>粘贴下面的代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;renderer_options-MarkdownRenderer&quot;: &#123;</span><br><span class="line">        &quot;extensions&quot;: [&quot;tables&quot;, &quot;fenced_code&quot;, &quot;codehilite&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://github.com/timonwong/OmniMarkupPreviewer" target="_blank" rel="noopener">OmniMarkupPreviewer的github主页</a></li><li><a href="https://blog.csdn.net/qq_30490125/article/details/53230408" target="_blank" rel="noopener">近乎完美的Markdown写作体验 - SublimeText3 + OmniMarkupPreviewer</a></li></ul><h2 id="OmniMarkupPreviewer-MathJax"><a href="#OmniMarkupPreviewer-MathJax" class="headerlink" title="OmniMarkupPreviewer + MathJax"></a>OmniMarkupPreviewer + MathJax</h2><p>OmniMarkupPreviewerx渲染markdown内容为网页，MathJax对LATEX编辑的数学公式进行渲染。</p><h3 id="下载mathjax"><a href="#下载mathjax" class="headerlink" title="下载mathjax"></a>下载mathjax</h3><ol><li>下载<a href="https://link.jianshu.com/?t=https://github.com/downloads/timonwong/OmniMarkupPreviewer/mathjax.zip" target="_blank" rel="noopener">mathjax</a>，解压到目录<code>Sublime Text 3\Packages\OmniMarkupPreviewer\public</code>下。</li><li>在目录<code>Sublime Text3\Packages\OmniMarkupPreviewer\</code>创建空文件<code>MATHJAX.DOWNLOADED</code>。这样就安装好了。</li></ol><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>新建markdown文件输入内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">This expression </span><br><span class="line">$\sqrt&#123;3x-1&#125;+(1+x)^2$ is an example of a $\LaTeX$ inline equation.he Lorenz Equations:</span><br><span class="line">$$\begin&#123;aligned&#125;\dot&#123;x&#125; &amp; = \sigma(y-x) \\\dot&#123;y&#125; &amp; = \rho x - y - xz \\\dot&#123;z&#125; &amp; = -\beta z + xy\end&#123;aligned&#125;$$</span><br></pre></td></tr></table></figure><p>在sublime中用<code>Ctrl+Alt+O</code>预览，显示效果如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/13.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h3><ul><li><a href="https://www.jianshu.com/p/23b02c1708ae" target="_blank" rel="noopener">使用Markdown的时候需要插入LaTeX公式方法</a></li></ul><p>关于LATEX:</p><ul><li><a href="https://liam.page/2014/09/08/latex-introduction/" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li><li><a href="https://www.kancloud.cn/thinkphp/latex/41806" target="_blank" rel="noopener">一份其实很短的 LaTeX 入门文档</a></li><li><a href="http://mohu.org/info/symbols/symbols.htm" target="_blank" rel="noopener">常用数学符号的 LaTeX 表示方法</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录sublime的一些插件。&lt;/p&gt;
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="sublime" scheme="http://yoursite.com/tags/sublime/"/>
    
  </entry>
  
  <entry>
    <title>熵、交叉熵与KL散度</title>
    <link href="http://yoursite.com/2019/03/11/%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E4%B8%8EKL%E6%95%A3%E5%BA%A6/"/>
    <id>http://yoursite.com/2019/03/11/熵、交叉熵与KL散度/</id>
    <published>2019-03-11T06:31:33.000Z</published>
    <updated>2019-07-22T00:57:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>介绍交叉熵和KL散度。</p><a id="more"></a> <h2 id="从信息量到信源熵"><a href="#从信息量到信源熵" class="headerlink" title="从信息量到信源熵"></a>从信息量到信源熵</h2><ol><li>信息量是通信专业的名词。一个变量的主要特征就是不确定性，也就是发生的概率。信息量用来衡量不确定性的大小。一个事情发生的概率越小，使人越感到意外，则这件事的信息量越大；反之，概率越大，越不意外，信息量越小。举个例子，有一架波音747飞机失事，发生的概率很小，让人很意外，带给人的信息量很大。<br> 信息量函数应满足两个特性：1）随着概率的增大而减小，即是概率的减函数；2）信息量函数满足可加性，即两个统计独立的消息提供的信息量等于他们分别提供的信息量之和。同时满足递减性和可加性的函数是对数函数，即<br> $$ I[p(x_i)] = log \frac{1}{p(x_i)} = -log p(x_i)$$</li><li>信源熵定义为信源输出的平均信息量，即信息量的数学期望。$$ H(X) = E(I[p(x_i)]) = E(-log p(x_i)) = - \sum_{i=1}^{n}p(x_i)log p(x_i)$$信源实际上是一个概率分布，信源熵可以解释为表示这个概率分布至少需要的信息量。</li></ol><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>对于一个随机事件，真实概率分布是$p(x_i)$ 是未知的，从数据中得到概率分布为$q(x_i)$。我们用概率分布$q(x_i)$来近似和逼近真实的概率分布$p(x_i)$ 。交叉熵定义为：$$H(p,q) = \sum_{i=1}^{n}p(x_i) I[q(x_i)] =- \sum_{i=1}^{n}p(x_i)log(x_i) $$交叉熵$H(p,q)$是用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量。上面我们说过，信源熵$H(X)$是表示真实概率分布$p(x_i)$需要的最小信息量。可以得到结论：$$H(p,q) \ge H(p)$$由吉布斯不等式可以证明，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。这个不等式的意义是：用概率分布$q(x_i)$来近似真实概率分布$p(x_i)$需要的信息量一定大于等于概率分布$p(x_i)$本身的信源熵。交叉熵比信源熵多出来的这部分，就是冗余信息量，我们称为KL散度（相对熵）。<br>$$KL(p||q)= H(p,q) - H(p) \ge 0$$容易看出交叉熵并不是一个对称量，即$ H(p,q) \not=H(q,p)$。同样的,KL散度也不是一个对称量，即$KL(p||q) \not =KL(q||p) $<br>给定概率分布$p(x_i)$,信源熵$H(p)$就是固定不变的。在机器学习中，交叉熵常用作分类问题的损失函数。交叉熵刻画了预测概率分布$q(x_i)$与真实概率分布$p(x_i)$之间的距离。通过减小交叉熵$H(p,q)$,我们可以使得预测概率分布$q(x_i)$不断逼近真实概率分布$p(x_i)$</p><h2 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h2><p>真实的概率分布为$p(x_i)$，我们用预测概率分布$q(x_i)$对它进行建模和近似。我们需要的平均附加量，也就是冗余量是：<br>$$KL(p,q) = H(p,q) - H(q) = -\sum_{i=1}^{n}p(x_i)logq(x_i) - \biggl(-\sum_{i=1}^{n}p(x_i)logp(x_i)\biggr) = -\sum_{i=1}^{n}p(x_i)log{\frac{q(x_i)}{p(x_i)}}$$KL散度有以下几个特性：</p><ul><li>KL散度不是一个对称量，即$KL(p||q) \not =KL(q||p) $</li><li>$KL(p||q)\ge 0$，当且仅当分布$p(x_i)$与$q(x_i)$完全一致时，等号才成立。</li><li>KL散度可以看做两个分布之间不相似程度的度量。KL散度越小，两个分布的不相似程度越小，分布$q(x_i)$越适合来近似$p(x_i)$。</li></ul><h2 id="tensorflow用交叉熵做损失函数"><a href="#tensorflow用交叉熵做损失函数" class="headerlink" title="tensorflow用交叉熵做损失函数"></a>tensorflow用交叉熵做损失函数</h2><p>在机器学习中交叉熵常常用作分类问题的损失函数。这里有个问题，交叉熵用于概率分布，但神经网络的输出并不一定是一个概率分布。<br>概率分布应满足2个条件:<br>1) $0 \le p(X =x) \le 1$<br>2) $\sum_{x}{} p(X=x) = 1$<br>如何把神经网络的输出变成概率分布呢？这里就要用到softmax回归。假设输出层的输出为$y_0,y_1,y_2 \dots y_n$,则softmax函数的形式为：$$softmax(y_i) = \frac{exp(y_i)}{\sum_{j}exp(y_j)}$$由于交叉熵一般会与softmax回归一起使用，TensorFlow对这两个功能进行了统一，可以直接用函数<a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">tf.nn.softmax_cross_entropy_with_logits</a>来计算softmax后的交叉熵函数。对于只有一个正确答案的分类问题，可以用函数<a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">tf.nn.sparse_nn.softmax_cross_entropy_with_logits</a>来加速计算过程。</p><h2 id="pytorch中交叉熵损失函数的实现"><a href="#pytorch中交叉熵损失函数的实现" class="headerlink" title="pytorch中交叉熵损失函数的实现"></a>pytorch中交叉熵损失函数的实现</h2><p>在多分类问题中，实际概率分布是 $y = [y_0,y_1,…,y_{C-1}]$,其中C为类别数;y是样本标签的one-hot表示，当样本属于第$i$类时$y_i=1$,否则$y_i=0$。预测概率分布为$p = [p_0,p_1,p_2,…,p_{C-1}]$。$c$是样本标签。此时，交叉熵损失函数为$$loss = -\sum_{i=0}^{C-1}y_i log(p_i) = - y_c \cdot log(p_c) = - log(p_c)$$<br>接下来介绍pytorch中具体实现这个数学式子的函数。</p><h3 id="torch-nn-functional-log-softmax-与class-torch-nn-NLLLoss"><a href="#torch-nn-functional-log-softmax-与class-torch-nn-NLLLoss" class="headerlink" title="torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()"></a>torch.nn.functional.log_softmax()与class torch.nn.NLLLoss()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.log_softmax()</span><br></pre></td></tr></table></figure><ul><li>作用：先做softmax运算，再做log运算。在数学上等价于$log(softmax(x))$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class torch.nn.NLLLoss(weight = None)</span><br></pre></td></tr></table></figure><ul><li>作用：这是neg log likelihood loss（NLLLoss），即负对数似然函数。</li><li>参数：   <ul><li>weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。</li></ul></li><li>输入：<ul><li>input(N,C): C是类别个数。为<code>log_probabilities</code>形式，即概率分布再取log。可以在最后一层加<code>log_softmax</code>,这就要用到函数<code>torch.nn.functional.log_softmax()</code></li><li>targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为<code>2</code>,而不是<code>[0,0,1,0,0]</code></li></ul></li></ul><p>loss可以表示为：$$loss(x,class) = -x[class]$$如果指定了weight，可以表示为：$$loss(x,class) = - weight[class]*x[class]$$<br>举个例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">log_m = torch.nn.functional.log_softmax()</span><br><span class="line">loss_function = torch.nn.NLLLoss()</span><br><span class="line">inputs = torch.randn(3,5) #batch_size * num_classes = 3 * 5</span><br><span class="line">target = torch.LongTensor([1,0,4])</span><br><span class="line">loss = loss_function(log_m(inputs),target)  # inputs要先做log_softmax，再送入loss_function</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><h3 id="class-torch-nn-CrossEntropyLoss-weight-None"><a href="#class-torch-nn-CrossEntropyLoss-weight-None" class="headerlink" title="class torch.nn.CrossEntropyLoss(weight = None)"></a>class torch.nn.CrossEntropyLoss(weight = None)</h3><ul><li>作用：将函数<code>log_softmax</code>和<code>NLLLoss</code>集成到一起。在多分类问题中非常有用。</li><li>参数：   <ul><li>weight(tensor,optional): 一维tensor，里面的值对应类别的权重。当训练集样本分布不均匀时，使用这个参数非常重要。手动指定类别的权重，长度应为类别个数C。</li></ul></li><li>输入：<ul><li>input(N,C): C是类别个数。每个类别的分数，不用过softmax层。</li><li>targets(N): 是类别的索引，而不是类别的one-hot表示。比如，5个类别中的第3类，target应为<code>2</code>,而不是<code>[0,0,1,0,0]</code>。</li></ul></li></ul><p>loss可以表示为：$$loss(x,class) = - \text{log}\frac{e^{x[class]}}{ \sum_{j=0}^{C-1}e^{x[j]}} = -x[class] + \text{log}(\sum_{j=0}^{C-1}e^{x[j]}) $$当指定了weight时，loss计算公式为： $$ loss(x, class) = weights[class] \cdot (-x[class] + \text{log}(\sum_{j=0}^{C-1}e^{x[j]})) $$<br>参见：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/56638625" target="_blank" rel="noopener">PyTorch学习笔记——多分类交叉熵损失函数</a></li><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#loss-functions" target="_blank" rel="noopener">pytorch官方手册</a><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2></li><li><a href="https://wizyoung.github.io/%E4%BF%A1%E6%81%AF%E7%86%B5%EF%BC%8C%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%8C%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E7%90%86%E8%A7%A3/#more" target="_blank" rel="noopener">信息熵，相对熵，交叉熵的理解</a></li><li><a href="https://sthsf.github.io/wiki/Algorithm/DeepLearning/Tensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Tensorflow%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86---%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">Tensorflow基础知识—损失函数详解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍交叉熵和KL散度。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="交叉熵" scheme="http://yoursite.com/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    
      <category term="相对熵" scheme="http://yoursite.com/tags/%E7%9B%B8%E5%AF%B9%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>python的一些函数</title>
    <link href="http://yoursite.com/2019/03/10/python%E7%9A%84%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/03/10/python的一些函数/</id>
    <published>2019-03-10T08:12:51.000Z</published>
    <updated>2019-07-07T07:07:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录python的一些函数，实现某些功能。</p><a id="more"></a> <h2 id="求最大-小值的索引"><a href="#求最大-小值的索引" class="headerlink" title="求最大/小值的索引"></a>求最大/小值的索引</h2><h3 id="对于list"><a href="#对于list" class="headerlink" title="对于list"></a>对于list</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = range(100)</span><br><span class="line">np.random.shuffle(a)</span><br><span class="line">index_max = a.index(max(a)) #求最大值的索引</span><br><span class="line">index_min = a.index(min(a)) #求最小值的索引</span><br></pre></td></tr></table></figure><h3 id="对于numpy的数组ndarray"><a href="#对于numpy的数组ndarray" class="headerlink" title="对于numpy的数组ndarray"></a>对于numpy的数组ndarray</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = np.array(a)</span><br><span class="line">index_max = np.argmax(a) #求最大值的索引</span><br><span class="line">index_min = np.argmin(a) #求最小值的索引</span><br><span class="line"># 对于二维的数组</span><br><span class="line">b = np.arange(100).reshape(10,-1)</span><br><span class="line">row_max_list = np.argmax(b,axis = 1) #按行计算最大值在行中的索引</span><br><span class="line">line_max_list = np.argmin(b,axis = 0) #按列计算最小值在列中的索引</span><br></pre></td></tr></table></figure><h2 id="sort与sorted"><a href="#sort与sorted" class="headerlink" title="sort与sorted"></a>sort与sorted</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sorted(iterable,key,reverse)</span><br></pre></td></tr></table></figure><ul><li>iterable: 可迭代对象</li><li>key：用来进行比较的元素。常用函数： lambda x:x[i]</li><li>reverse：排序规则。reverse=True按降序排列，reverse=False按升序排列（默认）</li></ul><p>比较sort与sorted:</p><ul><li>作用对象:sort()只能作用于list,sorted()可以作用于所有可迭代对象。</li><li>返回值：sort()没有返回值；sorted()返回一个新的list</li></ul><h2 id="字典的items-方法"><a href="#字典的items-方法" class="headerlink" title="字典的items()方法"></a>字典的items()方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict.items()</span><br></pre></td></tr></table></figure><p>返回可遍历的元素为（键，值）元组的数组。</p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><h3 id="collections–容器数据类型"><a href="#collections–容器数据类型" class="headerlink" title="collections–容器数据类型"></a>collections–容器数据类型</h3><p><a href="https://docs.python.org/zh-cn/3/library/collections.html#collections.Counter" target="_blank" rel="noopener">collections</a>模块是python内建的一个集合模块，提供了许多有用的集成类。提供了<code>list</code>,<code>dict</code>,<code>set</code>,<code>tuple</code>的替代选择，相当于这几个数据类型的加强版。</p><h4 id="collections-Counter-iterable"><a href="#collections-Counter-iterable" class="headerlink" title="collections.Counter(iterable)"></a>collections.Counter(iterable)</h4><p>Counter是一个计数器，用于计数可哈希对象，统计元素出现的个数。它是一个集合，<code>元素-计数</code>像<code>键-值</code>对一样存储。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; import collections</span><br><span class="line"></span><br><span class="line">&gt;&gt; a = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;a&quot;,&quot;b&quot;,&quot;a&quot;]</span><br><span class="line">&gt;&gt; counter = collections.Counter(a)</span><br><span class="line">&gt;&gt; print(counter)</span><br><span class="line">Counter(&#123;&apos;a&apos;: 3, &apos;b&apos;: 2, &apos;c&apos;: 1&#125;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录python的一些函数，实现某些功能。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>全文搜索引擎-Elasticsearch入门</title>
    <link href="http://yoursite.com/2019/03/10/%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-Elasticsearch%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2019/03/10/全文搜索引擎-Elasticsearch入门/</id>
    <published>2019-03-10T07:11:17.000Z</published>
    <updated>2019-07-22T00:38:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Elasticsearch是一个开源的搜索引擎框架。</p><a id="more"></a><h2 id="Elasticsearch安装和启动"><a href="#Elasticsearch安装和启动" class="headerlink" title="Elasticsearch安装和启动"></a>Elasticsearch安装和启动</h2><p><strong>安装前提：</strong>Elasticsearch需要Java7或以上的版本。</p><ol><li><p>下载压缩包并解压：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.6.1.zip</span><br><span class="line">unzip elasticsearch-6.6.1.zip</span><br><span class="line">cd elasticsearch-6.6.1</span><br></pre></td></tr></table></figure></li><li><p>进入解压后的文件目录，启动elasticsearch：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure></li><li><p>如果一切正常，elasticsearch默认在本机9200端口运行。打开另一个命令行窗口，执行以下命令，检查elasticsearch是否运行成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:9200</span><br></pre></td></tr></table></figure><p> 如果输出以下信息，则运行正常。<figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/images/12.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p></li><li><p>默认情况下，elasticsearch只允许本机访问。要想其他电脑可以访问，也就是实现远程访问，需要修改文件<code>/config/elasticsearch.yml</code>,取消字段<code>network.host</code>的注释，把该字段的值改为<code>0.0.0.0</code>。这样的话所有的电脑都能访问，实际情况中，最好不要这样。</p></li><li><p>如果启动遇到错误“Native controller process has stopped - no new native processes can be started”或“max virtual memory areas vm.maxmapcount [65530] is too low”。解决方法是执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl -w vm.max_map_count=262144</span><br></pre></td></tr></table></figure></li></ol><h2 id="在python中使用elasticsearch"><a href="#在python中使用elasticsearch" class="headerlink" title="在python中使用elasticsearch"></a>在python中使用elasticsearch</h2><p>要先安装elasticsearch包。在python中使用elasticsearch要先启动elasticsearch。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install elasticsearch</span><br></pre></td></tr></table></figure><h3 id="创建index"><a href="#创建index" class="headerlink" title="创建index"></a>创建index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from elasticsearch import Elasticsearch </span><br><span class="line">es = Elasticsearch() #创建实例，默认localhost:9200</span><br><span class="line"># es = Elasticsearch([&#123;&apos;host&apos;:&apos;10.112.1.109&apos;,&apos;port&apos;:&apos;9200&apos;&#125;]) #远程访问</span><br><span class="line">result = es.indices.create(index = &apos;news&apos;,ignore = 400)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>如果创建成功，会返回以下信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;acknowledged&apos;: True, &apos;shards_acknowledged&apos;: True, &apos;index&apos;: &apos;test_es&apos;&#125;</span><br></pre></td></tr></table></figure><p>如果再次创建，就会返回以下信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;error&apos;: &#123;&apos;root_cause&apos;: [&#123;&apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;], &apos;type&apos;: &apos;resource_already_exists_exception&apos;, &apos;reason&apos;: &apos;index [news/TrkzNdXZRi6ReiZqOM2Dvg] already exists&apos;, &apos;index_uuid&apos;: &apos;TrkzNdXZRi6ReiZqOM2Dvg&apos;, &apos;index&apos;: &apos;news&apos;&#125;, &apos;status&apos;: 400&#125;</span><br></pre></td></tr></table></figure><p>表示创建失败，失败的原因是要创建的index已经存在了。status状态码是400。</p><h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">datas = [</span><br><span class="line">    &#123;&apos;title&apos;:&quot;算法导论（原书第2版）&quot;,</span><br><span class="line">    &apos;url&apos;:&quot;https://book.douban.com/subject/1885170/&quot;,</span><br><span class="line">    &apos;introduction&apos;:&quot;这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。这本书深入浅出，全面地介绍了计算机算法。对每一个算法的分析既易于理解又十分有趣，并保持了数学严谨性。本书的设计目标全面，适用于多种用途。涵盖的内容有：算法在计算中的作用，概率分析和随机算法的介绍。书中专门讨论了线性规划，介绍了动态规划的两个应用，随机化和线性规划技术的近似算法等，还有有关递归求解、快速排序中用到的划分方法与期望线性时间顺序统计算法，以及对贪心算法元素的讨论。此书还介绍了对强连通子图算法正确性的证明，对哈密顿回路和子集求和问题的NP完全性的证明等内容。全书提供了900多个练习题和思考题以及叙述较为详细的实例研究。&quot;&#125;,</span><br><span class="line">    &#123;&apos;title&apos;:&quot;计算机程序的构造和解释&quot;,</span><br><span class="line">    &apos;url&apos;:&quot;https://book.douban.com/subject/1148282/&quot;,</span><br><span class="line">    &apos;introduction&apos;:&quot;《计算机程序的构造和解释(原书第2版)》1984年出版，成型于美国麻省理工学院(MIT)多年使用的一本教材，1996年修订为第2版。在过去的二十多年里，《计算机程序的构造和解释(原书第2版)》对于计算机科学的教育计划产生了深刻的影响。第2版中大部分重要程序设计系统都重新修改并做过测试，包括各种解释器和编译器。作者根据其后十余年的教学实践，还对其他许多细节做了相应的修改。&quot;&#125;,</span><br><span class="line">    </span><br><span class="line">]</span><br><span class="line">for i in range(len(datas)):</span><br><span class="line">    es.index(index = &apos;book&apos;,doc_type = &apos;computer&apos;,id = i+1,body = datas[i])</span><br></pre></td></tr></table></figure><p>index()方法可以完成两个操作，如果数据不存在，那就插入数据；如果数据已经存在，那就更新数据。</p><h3 id="get-按ID查询"><a href="#get-按ID查询" class="headerlink" title="get()按ID查询"></a>get()按ID查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result= es.get(index=&apos;book&apos;,doc_type=&apos;computer&apos;,id =1)</span><br><span class="line">print(result[&apos;_source&apos;])</span><br></pre></td></tr></table></figure><h3 id="search-实现全文检索"><a href="#search-实现全文检索" class="headerlink" title="search()实现全文检索"></a>search()实现全文检索</h3><p>对于中文，我们要安装一个中文分词插件<a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">elasticsearch-analysis-ik</a>。可以使用elastic的另一个命令行工具elastisearch-plugin来安装，要确保版本号一致。进入elastic的目录下，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip</span><br></pre></td></tr></table></figure><p>注意将<code>6.3.0</code>替换为自己的版本号。安装成功后，重启elasticsearch，就会自动加载这个中文分词插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">es = Elasticsearch()</span><br><span class="line">mapping = &#123;</span><br><span class="line">    &apos;properties&apos;: &#123;</span><br><span class="line">        &apos;title&apos;: &#123;</span><br><span class="line">            &apos;type&apos;: &apos;text&apos;,</span><br><span class="line">            &apos;analyzer&apos;: &apos;ik_max_word&apos;,</span><br><span class="line">            &apos;search_analyzer&apos;: &apos;ik_max_word&apos;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">result = es.indices.put_mapping(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=mapping)</span><br></pre></td></tr></table></figure><p>指定使用中文分词器，如果不指定默认使用英文分词器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dsl = &#123;</span><br><span class="line">    &apos;query&apos;: &#123;</span><br><span class="line">        &apos;match&apos;: &#123;</span><br><span class="line">            &apos;introduction&apos;: &apos;计算机&apos;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">result = es.search(index=&apos;news&apos;, doc_type=&apos;politics&apos;, body=dsl)</span><br></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul><li><a href="http://www.ruanyifeng.com/blog/2017/08/elasticsearch.html" target="_blank" rel="noopener">全文搜索引擎 Elasticsearch 入门教程—–阮一峰</a></li><li><a href="https://elasticsearch-py.readthedocs.io/en/master/" target="_blank" rel="noopener">Python Elasticsearch文档</a></li><li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/running-elasticsearch.html" target="_blank" rel="noopener">Elasticsearch官方文档</a></li><li><a href="https://cuiqingcai.com/6214.html" target="_blank" rel="noopener">Elasticsearch基本介绍及其与Python的对接实现–崔庆才</a></li><li><a href="https://www.jianshu.com/p/914f102bc174" target="_blank" rel="noopener">Elasticsearch搜索中文分词优化</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Elasticsearch是一个开源的搜索引擎框架。&lt;/p&gt;
    
    </summary>
    
      <category term="web搜索" scheme="http://yoursite.com/categories/web%E6%90%9C%E7%B4%A2/"/>
    
    
      <category term="Elasticsearch" scheme="http://yoursite.com/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu服务器遇到的一些问题</title>
    <link href="http://yoursite.com/2019/03/07/Ubuntu%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2019/03/07/Ubuntu服务器遇到的一些问题/</id>
    <published>2019-03-07T11:53:00.000Z</published>
    <updated>2019-03-12T06:13:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录Ubuntu服务器遇到的一些问题。</p><a id="more"></a> <h2 id="linux服务器连不上网"><a href="#linux服务器连不上网" class="headerlink" title="linux服务器连不上网"></a>linux服务器连不上网</h2><ol><li><p>先检查网线是否插好了，若网线口发亮才是插好。检查电脑是否能<code>ping</code>通局域网的其他电脑。</p><ul><li>查看其他电脑的ip地址<br><code>ifconfig | grep inet</code></li><li>ping其他电脑的IP地址<br><code>ping 10.112.0.1</code><br>如果可以ping通其他电脑，再检查下一步。</li></ul></li><li><p>可以ping通其他电脑，但<code>ping 10.3.8.211</code>校园网网关失败。这时可能是路由出错，查看服务器的路由是否正确。</p><ul><li>查看比较服务器与可以正常联网的电脑的路由。<br><code>route -n</code></li><li>添加正确的默认路由。<br><code>route add default gw 10.112.0.1</code></li><li>检查能否连接到校园网。<br><code>ping 10.3.8.211</code><br>Ubuntu配置路由参见: <a href="https://blog.51cto.com/solin/1900865" target="_blank" rel="noopener">ubuntu配置静态路由及重启生效</a></li></ul></li><li><p>连接到校园网，但是<code>ping www.baidu.com</code>失败。服务器ping不通域名，但可以ping通百度的ip地址<code>112.34.112.40</code>。这是服务器的DNS配置出错了，无法解析域名。</p><ul><li><p>修改文件<code>/etc/resolv.conf</code>，必须有sudo权限。<br><code>sudo vim /etc/resolv.conf</code></p></li><li><p>添加以下内容<br><code>nameserver 8.8.8.8</code></p></li><li><p>重启网络使修改立即生效。<br><code>sudo /etc/init.d/networking restart</code></p></li><li><p>这时应该能ping通百度了。</p><p>重启电源后，以上方法会被清除而失效，导致开机后需要重新配置。有效的方法是卸载开机重写<code>/etc/resolv.conf</code>的<code>resolvconf</code>，执行命令<br><code>sudo apt-get autoremove resolvconf</code><br>配置域名解析参见：<a href="https://blog.csdn.net/danielpei1222/article/details/65452451" target="_blank" rel="noopener">Ubuntu 无法解析域名</a></p></li></ul></li></ol><h2 id="不能通过Xshell或ssh命令连接到服务器"><a href="#不能通过Xshell或ssh命令连接到服务器" class="headerlink" title="不能通过Xshell或ssh命令连接到服务器"></a>不能通过Xshell或ssh命令连接到服务器</h2><ol><li>检查是否安装了<code>ssh-server</code>服务。<br><code>ps -e | grep ssh</code><ul><li>若没有安装，使用以下命令安装：<br><code>sudo apt-get install openssh-server</code></li></ul></li><li>若安装了<code>ssh-server</code>服务，检查ssh服务是否打开。需要sudo权限<ul><li>检查ssh服务状态<br><code>service sshd status</code><br>或<br><code>/etc/init.d/ssh status</code></li><li>开启ssh服务<br><code>service sshd start</code><br>或<br><code>/etc/init.d/ssh start</code></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录Ubuntu服务器遇到的一些问题。&lt;/p&gt;
    
    </summary>
    
      <category term="技术资料" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
      <category term="服务器" scheme="http://yoursite.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>pytorch学习笔记</title>
    <link href="http://yoursite.com/2019/03/05/pytorch%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/03/05/pytorch学习笔记/</id>
    <published>2019-03-05T01:27:14.000Z</published>
    <updated>2019-07-07T07:09:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里是pytorch学习笔记。</p><a id="more"></a> <h2 id="创建操作"><a href="#创建操作" class="headerlink" title="创建操作"></a>创建操作</h2><h3 id="torch-randn"><a href="#torch-randn" class="headerlink" title="torch.randn()"></a>torch.randn()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(*size,out = None)</span><br></pre></td></tr></table></figure><ul><li>输入：<ul><li>size(int)：指定了输出张量的形状</li></ul></li><li>输出：输出结果为张量</li><li>作用：返回一个张量，从标准正态分布中抽取一组随机数。形状由*size决定</li></ul><h2 id="张量维度操作"><a href="#张量维度操作" class="headerlink" title="张量维度操作"></a>张量维度操作</h2><h3 id="转置：transpose"><a href="#转置：transpose" class="headerlink" title="转置：transpose"></a>转置：transpose</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.transpose(input,dim0,dim1)</span><br></pre></td></tr></table></figure><ul><li>参数：<ul><li>input: 输入张量，可以是二维及二维以上的张量</li><li>dim0,dim1: 要转置的两个维度。</li></ul></li><li>作用： 返回输入矩阵的转置。一次只能转置张量的两个维度。输出张量与输入张量共享内存，同步改变。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.t(tensor)</span><br></pre></td></tr></table></figure><p>输入一个二维张量（矩阵），并转置0,1维。可以看做函数<code>torch.transpose(input,0,1)</code>的简写函数。<br>比较下<code>transpose</code>与<code>view</code>这两个函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a = torch.randn(2,3,5)</span><br><span class="line">&gt;&gt; b = torch.transpose(a,1,2)</span><br><span class="line">&gt;&gt; c = a.view(2,5,3)</span><br><span class="line">&gt;&gt; print(a)</span><br><span class="line">tensor([[[ 0.9926, -0.1669, -1.6571, -0.2730, -0.1313],</span><br><span class="line">         [ 0.9811, -1.9854,  1.5519,  0.1383,  1.4571],</span><br><span class="line">         [ 0.8221, -1.1283, -0.7675, -2.0497,  0.4748]],</span><br><span class="line"></span><br><span class="line">        [[ 0.1594,  0.7166, -0.2603,  1.1027,  1.5283],</span><br><span class="line">         [-0.7652, -1.4711,  0.5077,  0.6639,  0.0374],</span><br><span class="line">         [ 1.8121, -1.4864, -2.9863, -0.5769, -0.2915]]]) </span><br><span class="line">&gt;&gt; print(b)</span><br><span class="line">tensor([[[ 0.9926,  0.9811,  0.8221],</span><br><span class="line">         [-0.1669, -1.9854, -1.1283],</span><br><span class="line">         [-1.6571,  1.5519, -0.7675],</span><br><span class="line">         [-0.2730,  0.1383, -2.0497],</span><br><span class="line">         [-0.1313,  1.4571,  0.4748]],</span><br><span class="line"></span><br><span class="line">        [[ 0.1594, -0.7652,  1.8121],</span><br><span class="line">         [ 0.7166, -1.4711, -1.4864],</span><br><span class="line">         [-0.2603,  0.5077, -2.9863],</span><br><span class="line">         [ 1.1027,  0.6639, -0.5769],</span><br><span class="line">         [ 1.5283,  0.0374, -0.2915]]]) </span><br><span class="line">&gt;&gt; print(c)</span><br><span class="line">tensor([[[ 0.9926, -0.1669, -1.6571],</span><br><span class="line">         [-0.2730, -0.1313,  0.9811],</span><br><span class="line">         [-1.9854,  1.5519,  0.1383],</span><br><span class="line">         [ 1.4571,  0.8221, -1.1283],</span><br><span class="line">         [-0.7675, -2.0497,  0.4748]],</span><br><span class="line"></span><br><span class="line">        [[ 0.1594,  0.7166, -0.2603],</span><br><span class="line">         [ 1.1027,  1.5283, -0.7652],</span><br><span class="line">         [-1.4711,  0.5077,  0.6639],</span><br><span class="line">         [ 0.0374,  1.8121, -1.4864],</span><br><span class="line">         [-2.9863, -0.5769, -0.2915]]])</span><br></pre></td></tr></table></figure><p>可以看到:二者得到的结果并不相同。<code>transpose</code>是进行转置操作。<code>view</code>对张量整形时，张量中元素的顺序保持不变。相当于将这个三维张量按顺序</p><h2 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h2><h3 id="torch-manual-seed"><a href="#torch-manual-seed" class="headerlink" title="torch.manual_seed()"></a>torch.manual_seed()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(seed)</span><br></pre></td></tr></table></figure><ul><li>输入：<ul><li>seed(int or long)：设定种子，为int类型或long类型</li></ul></li><li>作用：设定生成随机数的种子。种子相同，生成的随机数就是相同的，实验结果就可以复现。</li></ul><p>参见：<a href="https://cloud.tencent.com/developer/article/1149041" target="_blank" rel="noopener">利用随机数种子来使pytorch中的结果可以复现</a></p><h3 id="view-整形"><a href="#view-整形" class="headerlink" title=".view() 整形"></a>.view() 整形</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor.view(*size)</span><br></pre></td></tr></table></figure><ul><li>输入：<ul><li>*size(int)：指定了输出张量的形状</li></ul></li><li>输出：输出结果为张量</li><li>作用：整形，只改变原张量的形状，形状由*size指定。</li></ul><p>例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; x = torch.randn(5,4)</span><br><span class="line">&gt;&gt; x.size()</span><br><span class="line">torch.Size([5,4]</span><br><span class="line">&gt;&gt; x.view(30)</span><br><span class="line">&gt;&gt; x.size()</span><br><span class="line">torch.Size([20])</span><br><span class="line">&gt;&gt; x.view(1,1,-1) # -1表示该维度由其他的维度推断。</span><br><span class="line">&gt;&gt; x.size()</span><br><span class="line">torch.Size([1, 1, 20])</span><br></pre></td></tr></table></figure><h3 id="torch-cat-连接"><a href="#torch-cat-连接" class="headerlink" title="torch.cat() 连接"></a>torch.cat() 连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(inputs,dimension = 0)</span><br></pre></td></tr></table></figure><ul><li>输入：<ul><li>inputs(sequence of Tensors)： 多个Tensor的python序列。 如[tensor1,tensor2…]或(Tensor1，tensor2)</li><li>dimension(int,optional): 沿着该维连接张量序列。默认为0。</li></ul></li><li>作用：在指定维度上，对输入张量序列进行连接操作。</li></ul><p>举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; impotr torch </span><br><span class="line">&gt;&gt; x = torch.randn(4,3)</span><br><span class="line">&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 3])</span><br><span class="line">&gt;&gt; y = torch.cat((x,x,x),0)</span><br><span class="line">&gt;&gt; y.size()</span><br><span class="line">torch.Size([12, 3])</span><br><span class="line">&gt;&gt; z = torch.cat((x,x,x),1)</span><br><span class="line">&gt;&gt; z.size() </span><br><span class="line">torch.Size([4, 9])</span><br></pre></td></tr></table></figure><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="class-torch-optim-SGD-params-lr-momentum-0-weight-decay-0"><a href="#class-torch-optim-SGD-params-lr-momentum-0-weight-decay-0" class="headerlink" title="class torch.optim.SGD(params,lr=,momentum=0,weight_decay=0)"></a>class torch.optim.SGD(params,lr=,momentum=0,weight_decay=0)</h3><ul><li>参数：<ul><li>params： 待优化参数的iterable</li><li>lr(float): 学习率</li><li>momentum(float,可选)： 动量因子，默认为0</li><li>weight_decay(float,可选): 权重衰减，默认为0</li></ul></li><li>作用：实现随机梯度下降算法。</li></ul><p>如何使用optimizer?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim </span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr = 0.01) #构建一个optimizer,model.parameters()给出了所有要优化的参数</span><br><span class="line">for input,target in dataset:</span><br><span class="line">    optimizer.zero_grad() #清空所有被优化过的Variable的梯度</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = loss_fn(output,target)</span><br><span class="line">    loss.backward()  #反向传播算法，计算好所有要优化Variable的梯度。</span><br><span class="line">    optimizer.step() #单步优化，基于计算得到的梯度进行参数更新。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里是pytorch学习笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
</feed>
